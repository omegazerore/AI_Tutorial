{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4651f25d-4013-47fd-b4f4-0b0fbea51514",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 上週作業"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba15128-8699-4d6a-abad-39c5c1d61640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dff39d-b3b6-44b0-a63c-6bcc3bce6d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate, ChatPromptTemplate\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from src.io.path_definition import get_project_dir\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Ingredients: {input}\\nOrigin: {output}\",\n",
    ")\n",
    "\n",
    "with open(os.path.join(get_project_dir(), 'tutorial', 'Week-1', 'recipe_train.json'), 'r') as f:\n",
    "    recipe_train = json.load(f)\n",
    "\n",
    "examples = []\n",
    "\n",
    "for recipe in recipe_train:\n",
    "    examples.append({\"input\": \" \".join(recipe['ingredients']),\n",
    "                     \"output\": recipe['cuisine']})\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # The list of examples available to select from.\n",
    "    examples,\n",
    "    # The embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    embeddings,\n",
    "    # The VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    FAISS,\n",
    "    # The number of examples to produce.\n",
    "    k=5,\n",
    ")\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # We provide an ExampleSelector instead of examples.\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Find the recipe origin based on the ingredients\",\n",
    "    suffix=\"Ingredients: {ingredients}\\nOrigin:\",\n",
    "    input_variables=[\"ingredients\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec67d88-fe19-4f10-9168-eec17e11a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84950f81-c97a-41b4-8419-80b5b6729772",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(get_project_dir(), 'tutorial', 'Week-1', 'recipe_test.json'), 'r') as f:\n",
    "    recipe_test = json.load(f)\n",
    "\n",
    "existing_ingredients = recipe_test[99]['ingredients']\n",
    "\n",
    "similar_prompt.invoke(\", \".join(existing_ingredients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ce1d22-e031-4938-9af3-a99c04a7535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from src.initialization import credential_init\n",
    "\n",
    "credential_init()\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o-2024-05-13\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e574f1e-2645-4a4d-9c8e-a5c2a586fc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = similar_prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70d1f0f-7f91-4b46-af21-ac74639cf7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\", \".join(existing_ingredients))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998d1aff-a4a4-4c14-a39c-5bb88c0942c9",
   "metadata": {},
   "source": [
    "# Remote server\n",
    "\n",
    "### 1. Making a POST Request (發送 POST 請求):\n",
    "\n",
    "- requests.post(...) sends an HTTP POST request to the specified URL.\n",
    "- The URL \"http://localhost:5000/openai/invoke\" points to a local server running on port 5000, at the endpoint /openai/invoke.\n",
    "- The json parameter is used to send a JSON payload with the request. In this case, the payload is {'input': \"Where is Taiwan\"}.\n",
    "- requests.post(...) 發送一個 HTTP POST 請求到指定的 URL。\n",
    "- URL \"http://localhost:5000/openai/invoke\" 指向一個本地服務器，該服務器在端口 5000 上運行，並且指向 /openai/invoke 端點。\n",
    "- json 參數用於隨請求發送 JSON 負載。在這個例子中，負載是 {'input': \"Where is Taiwan\"}。\n",
    "\n",
    "### 2. Response Handling (響應處理):\n",
    "\n",
    "- The server processes the request and sends back a response.\n",
    "- The response is stored in the response variable, which can then be inspected or used further in the code.\n",
    "- 服務器處理請求並返回響應。\n",
    "- 響應存儲在 response 變量中，之後可以檢查或在代碼中進一步使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb6c54e-f2f8-4e0e-92b8-0ad9c3803e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:5000/openai/invoke\",\n",
    "    json={'input': \"Where is Taiwan\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dc0e8a-9380-413a-805b-9890d4142cb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ea853f-f380-42b2-a57a-0249ae57bca0",
   "metadata": {},
   "source": [
    "# Use the remote model \n",
    "\n",
    "## Basic Usage\n",
    "\n",
    "### 1. Creating an Instance of RemoteRunnable (創建 RemoteRunnable 的實例):\n",
    "\n",
    "- This line creates an instance of RemoteRunnable and initializes it with the URL of the remote language model service. In this case, the service is running locally on http://localhost:5000/openai/.\n",
    "- 這行代碼創建一個 RemoteRunnable 的實例，並用遠程語言模型服務的 URL 進行初始化。在這個例子中，服務在本地運行，URL 為 http://localhost:5000/openai/。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84212830-9dfe-47c3-bc65-63de510f8a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langserve import RemoteRunnable\n",
    "\n",
    "llm = RemoteRunnable(\"http://localhost:5000/openai/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4c2ea5-4ec2-4298-af5e-6e85aaa4c320",
   "metadata": {},
   "source": [
    "### 2. Asynchronous Streaming of Responses (異步流式處理回應):\n",
    "\n",
    "- llm.astream(\"Where is Taiwan?\") sends the query \"Where is Taiwan?\" to the remote service and retrieves the response as a stream.\n",
    "- async for msg in ... is used to handle the streaming responses asynchronously.\n",
    "- print(msg.content, end=\"\", flush=True) prints each message content received from the stream without adding a new line after each message, and flushes the output buffer to ensure the message is displayed immediately.\n",
    "- llm.astream(\"Where is Taiwan?\") 將查詢 \"Where is Taiwan?\" 發送到遠程服務，並以流的形式檢索回應。\n",
    "- async for msg in ... 用於異步處理流式回應。\n",
    "- print(msg.content, end=\"\", flush=True) 打印每個從流中接收到的消息內容，不在每個消息後添加新行，並刷新輸出緩衝區以確保消息立即顯示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc16c50-aab3-4d58-8091-aa8ac6c4a917",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Supports astream\n",
    "async for msg in llm.astream(\"Where is Taiwan?\"):\n",
    "    print(msg.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafcf229-ae2c-41a9-bcf6-4503c366c2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm.invoke(\"Where is Taiwan?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d637005-0c88-43a1-817e-b96039218384",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71923692-63fc-4dad-8277-b092ea0839b6",
   "metadata": {},
   "source": [
    "## Make the external service a part of the chain\n",
    "\n",
    "### 1. Comedian Chain (喜劇演員鏈)\n",
    "\n",
    "- ChatPromptTemplate.from_messages(...) creates a prompt template where the system prompt instructs the model to either tell a joke or state a fact, and the human prompt provides the input.\n",
    "- This template is then piped (|) to a language model (llm) to generate the comedian's response.\n",
    "- ChatPromptTemplate.from_messages(...) 創建一個提示模板，其中系統提示指示模型要麼講一個笑話，要麼陳述一個不搞笑的事實，並且僅輸出一個。\n",
    "- 然後將此模板通過管道（|）傳遞給語言模型（llm），以生成喜劇演員的回應。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ab7c70-e6d6-4b41-b499-eabff45259d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "comedian_chain = (\n",
    "    ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a comedian that sometimes tells funny jokes and other times you just state facts that are not funny. Please either tell a joke or state fact now but only output one.\",\n",
    "            ),\n",
    "            ('human', '{input}'\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    | llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37c52f1-c5c9-4fc4-b15d-988ad3b5a173",
   "metadata": {},
   "source": [
    "### 2. Joke Classifier Chain\n",
    "\n",
    "- This chain is similar to the comedian chain but serves a different purpose.\n",
    "- The system prompt asks the model to classify the joke as \"funny\" or \"not funny\" and repeat the first five words for reference.\n",
    "- This template is also piped to the language model (llm).\n",
    "- 這個鏈與喜劇演員鏈類似，但用途不同。\n",
    "- 系統提示要求模型將笑話分類為“搞笑”或“不搞笑”，並重複笑話的前五個詞以供參考。\n",
    "- 此模板也通過管道傳遞給語言模型（llm）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c76e1f-f294-49fc-a174-7fcb5b95cf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "joke_classifier_chain = (\n",
    "    ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"Please determine if the joke is funny. Say `funny` if it's funny and `not funny` if not funny. Then repeat the first five words of the joke for reference...\",\n",
    "            ),\n",
    "            (\"human\", \"{joke}\"),\n",
    "        ]\n",
    "    )\n",
    "    | llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2534a7-0dd5-4b54-a866-18766d82f786",
   "metadata": {},
   "source": [
    "### 3. Combining Chains with RunnablePassthrough\n",
    "\n",
    "- This combines the comedian chain and the joke classifier chain using RunnablePassthrough.assign.\n",
    "- The comedian chain generates the output, and then this output is passed to the joke classifier chain to classify its humor.\n",
    "- 這將喜劇演員鏈和笑話分類器鏈結合在一起，使用 RunnablePassthrough.assign。\n",
    "- 喜劇演員鏈生成輸出，然後將此輸出傳遞給笑話分類器鏈以分類其幽默性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a34bdc7-c1b3-4176-ba7a-13744f6b0811",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = {\"joke\": comedian_chain} | RunnablePassthrough.assign(\n",
    "    classification=joke_classifier_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bacecf8-60af-44a2-82e7-4fc02dd5b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"input\": \"A man and a beer\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f702f519-37dd-4b45-a492-0c4bcdd521b0",
   "metadata": {},
   "source": [
    "# ChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22673222-780c-4356-a6d1-92c69b28d6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(url=\"https://python.langchain.com/v0.1/assets/images/chat_use_case-eb8a4883931d726e9f23628a0d22e315.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb536cb3-dc3e-4068-adb7-ce70ef0ca4e5",
   "metadata": {},
   "source": [
    "- N-Shot\n",
    "- The historical chat history can be consdiered as a list of question-answer pairs\n",
    "- If the chatbot doesn’t remember past chats, it’s called stateless because it doesn’t know what happened before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe40d3a1-ef13-450d-8de9-dde96458fe82",
   "metadata": {},
   "source": [
    "## Basic Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14704db9-40ef-4432-a887-190df25f203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=\"Translate this sentence from English to French: I love programming.\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95345038-9a83-465c-a85a-9b2d5f1b6476",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke([HumanMessage(content=\"What did you just say?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed4e110-9be7-4a67-be86-31b62451baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=\"Translate this sentence from English to French: I love programming.\"\n",
    "        ),\n",
    "        AIMessage(content=\"J'adore la programmation.\"),\n",
    "        HumanMessage(content=\"What did you just say?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce24122f-cf20-4667-bc23-5eebedd54992",
   "metadata": {},
   "source": [
    "## Prompt templates\n",
    "\n",
    "### 1. Creating a ChatPromptTemplate \n",
    "\n",
    "- ChatPromptTemplate.from_messages(...) creates a prompt template for the chatbot.\n",
    "- The first message in the template is a system message: \"You are a helpful assistant. Answer all questions to the best of your ability.\" This message sets the context and behavior of the assistant, instructing it to be helpful and thorough in its responses.\n",
    "- MessagesPlaceholder(variable_name=\"messages\") is a placeholder for dynamic content. The variable_name=\"messages\" specifies that this placeholder will be filled with user messages during the conversation.\n",
    "- ChatPromptTemplate.from_messages(...) 創建了一個聊天機器人的提示模板。\n",
    "- 模板中的第一條消息是一條系統消息：“You are a helpful assistant. Answer all questions to the best of your ability.” 此消息設置了助手的上下文和行為，指示其在回答中要提供幫助並盡力而為。\n",
    "- MessagesPlaceholder(variable_name=\"messages\") 是一個動態內容的佔位符。variable_name=\"messages\" 指定該佔位符將在對話中插入用戶消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dea7e35-f9b4-4d78-8a8e-e85225c5fd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e88d592-3e03-4a5b-81b2-e792154f94ec",
   "metadata": {},
   "source": [
    "### 2. Creating the Chain\n",
    "\n",
    "- This line pipes (|) the prompt template to a language model (model).\n",
    "- chain represents a sequence of operations where the prompt template is used to format user messages, and the language model processes these messages to generate responses.\n",
    "- 這行代碼通過管道（|）將提示模板傳遞給語言模型（model）。\n",
    "- chain 代表一系列操作，其中提示模板用於格式化用戶消息，語言模型處理這些消息以生成回應。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd2ed39-46d6-460e-b78e-5b0c4d06fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebca060-ce9e-4fba-be50-a40d923324fc",
   "metadata": {},
   "source": [
    "The MessagesPlaceholder above inserts chat messages passed into the chain's input as chat_history directly into the prompt. Then, we can invoke the chain like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e2a159-a35a-47db-ac82-43c8d9f85887",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Translate this sentence from English to French: I love programming.\"\n",
    "            ),\n",
    "            AIMessage(content=\"J'adore la programmation.\"),\n",
    "            HumanMessage(content=\"What did you just say?\"),\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c789c9c-46ac-4884-b059-466855e114aa",
   "metadata": {},
   "source": [
    "## Example of Using MessageHistory\n",
    "\n",
    "As a shortcut for managing the chat history, we can use a MessageHistory class, which is responsible for saving and loading chat messages. There are many built-in message history integrations that persist messages to a variety of databases, but for this quickstart we'll use a in-memory, demo message history called ChatMessageHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e0029f-228a-4ff5-afb0-91bf92f00d34",
   "metadata": {},
   "source": [
    "### 1. Importing the ChatMessageHistory Class (導入 ChatMessageHistory 類)\n",
    "\n",
    "- This line imports the ChatMessageHistory class from the langchain.memory module. This class is used to handle the chat messages in memory.\n",
    "- 這行代碼從 langchain.memory 模塊中導入 ChatMessageHistory 類。此類用於在內存中處理聊天消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f65a6f-3d57-4cf9-8e8f-34623e7965e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6cb01a-d8c8-4d51-8b9a-35b79a8c06f6",
   "metadata": {},
   "source": [
    "### 2. Creating an Instance of ChatMessageHistory (創建 ChatMessageHistory 的實例)\n",
    "\n",
    "- This line creates an instance of ChatMessageHistory. This instance will store the chat messages in memory for this session.\n",
    "- 這行代碼創建一個 ChatMessageHistory 的實例。該實例將在此會話期間將聊天消息存儲在內存中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fba54c-8018-4f75-963d-6195892ae170",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_chat_history = ChatMessageHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e08da2-cb06-41db-ab6a-69532596b405",
   "metadata": {},
   "source": [
    "### 3. Adding User and AI Messages (添加用戶和 AI 消息)\n",
    "\n",
    "- demo_chat_history.add_user_message(\"hi!\") adds a user message (\"hi!\") to the chat history.\n",
    "- demo_chat_history.add_ai_message(\"whats up?\") adds an AI response (\"whats up?\") to the chat history.\n",
    "- demo_chat_history.add_user_message(\"hi!\") 將用戶消息（“hi!”）添加到聊天記錄中。\n",
    "- demo_chat_history.add_ai_message(\"whats up?\") 將 AI 回應（“whats up?”）添加到聊天記錄中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638e4f96-f747-4f1d-97e2-d5e52749a0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_chat_history.add_user_message(\"hi!\")\n",
    "\n",
    "demo_chat_history.add_ai_message(\"whats up?\")\n",
    "\n",
    "demo_chat_history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3138b941-7bc6-4e08-890a-2d6debdc8431",
   "metadata": {},
   "source": [
    "### 4. Retrieving the Messages (檢索消息)\n",
    "\n",
    "- This line retrieves the list of messages stored in demo_chat_history. Each message is an object that contains information about the sender (user or AI) and the content of the message.\n",
    "- 這行代碼檢索存儲在 demo_chat_history 中的消息列表。每條消息都是一個對象，包含有關發送者（用戶或 AI）和消息內容的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f86ffa-b2bd-4d90-9a62-971b55701eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"messages\": demo_chat_history.messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cb4014-b66b-40dc-8acc-3e052426ce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_chat_history.add_user_message(\n",
    "    \"Translate this sentence from English to French: I love programming.\"\n",
    ")\n",
    "\n",
    "response = chain.invoke({\"messages\": demo_chat_history.messages})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54650b6d-115e-418e-9910-e87c90872ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the response back into the demo_chat_history\n",
    "\n",
    "demo_chat_history.add_ai_message(response)\n",
    "\n",
    "demo_chat_history.add_user_message(\"What did you just say?\")\n",
    "\n",
    "chain.invoke({\"messages\": demo_chat_history.messages})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f147092-7b3c-4392-8d8e-90c65b09d5d2",
   "metadata": {},
   "source": [
    "# **** 預計第一個小時結束 ****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aea1c49-906f-481c-9020-6b40cf07dba4",
   "metadata": {},
   "source": [
    "## Conversational Retrievers\n",
    "\n",
    "- 土味情話反殺大全 (推薦上Youtube看)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1831d12-b64a-4d21-b30f-10cc421d3500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "df = pd.DataFrame(data=[[\"确认过眼神，你是我爱的人。\", \"确认过眼神，我是你泡不到的人。\"],\n",
    "                         [\"万水千山总是情，爱我多一点行不行。\", \"一寸光阴一寸金，劝你死了这条心。\"],\n",
    "                         [\"今天吃了泡面，吃了炒面，还是想走进你的心里面。\", \"吃那么多面，最后还不是变成大便。\"],\n",
    "                         [\"草莓，蓝莓，蔓越莓，今天你想我了没？\", \"冬瓜，西瓜，哈密瓜，你再巴巴我打得你叫妈妈。\"],\n",
    "                         [\"众生皆苦，唯你独甜。\", \"尝遍众生，你为渣男代言。\"],\n",
    "                         [\"你喜欢瑞士名表还是我帅气的外表？\", \"我喜欢去年买了个表。\"],\n",
    "                         [\"我想问一条路，到哥哥心里的路。\", \"山路十八弯，走完脑血栓。\"],\n",
    "                         [\"小姐姐，我心里给你留了一块地，死心塌地。\", \"对不起，我的心里只容得下一块地，玛莎拉蒂。\"],\n",
    "                         [\"小姐姐你笑起来真好看啊。\", \"你看起来真好笑啊。\"],\n",
    "                         [\"亲爱的你知道吗，你的笑容没有酒，我却醉得像条狗\", \"我的笑容没有酒，你是真的像条狗\"],\n",
    "                         [\"宝贝儿，我在手上划了一道口子，你也划一下吧，这样我们就是两口子了\", \"我怕我们的血溶到一起，被你发现其实我是你爸爸\"],\n",
    "                         [\"这世间万物都有尽头，落叶归根，而我归你\", \"对不起 我不收垃圾\"],\n",
    "                         [\"请问……我想问一下路，那条通往你心里的路\", \"八格牙路\"],\n",
    "                         [\"你今天怎么怪怪的？ 怪可爱的\",  \"你今天也怪怪的，怪恶心的\"],\n",
    "                         [\"亲爱的，你知道我和唐僧的区别吗？ 唐僧取经我娶你\", \"知道你和沙僧的区别吗？ 他叫沙僧你叫沙雕\"],\n",
    "                         [\"亲爱的，你不觉得累吗？ 你已经在我的脑海里跑了好几圈了\", \"傻孩子，我在找出口呢\"],\n",
    "                         [\"莫文蔚的阴天。孙燕姿的雨天，周杰伦的晴天，都不如你和我聊天\", \"求求你了，能否还我一个宁静的夏天\"],\n",
    "                         [\"如果你是方便面，那我就是白开水，今生今世，我泡定你了\", \"故事的最后，她变成了屎，你变成了尿，你们终究分道扬镳\"],\n",
    "                         [\"大年三十晚上的鞭炮再响，也没有我想你那么想\", \"大年三十晚上的鞭炮再响，也没有你放的屁响\"],\n",
    "                         [\"c罗可以上演帽子戏法，可我想你却没有办法\", \"c罗可以上演帽子戏法，我也可以给你上演绿帽子戏法\"],\n",
    "                         [\"不要抱怨，抱我\", \"抱不起来，太重\"],\n",
    "                         [\"你有没有发现我的眼睛很好看？因为我满眼都是你啊\", \"对不起，你眼睛在哪呢？\"]], \n",
    "                  columns=['input', 'output'])\n",
    "\n",
    "\n",
    "documents = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    documents.append(Document(page_content=row['input'], metadata={'output': row['output']}))\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=documents, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16828c7-3d3d-4c78-b6fe-5624e6701505",
   "metadata": {},
   "source": [
    "### 1. Define a Chat Prompt Template\n",
    "\n",
    "- ChatPromptTemplate.from_messages is used to create a prompt template from a list of message tuples.\n",
    "- The system message sets the context for the AI assistant, instructing it to act grumpy and respond with cheesy pickup lines in Simplified Chinese (簡體中文).\n",
    "- MessagesPlaceholder(variable_name=\"messages\") is a placeholder for the dynamic messages that will be inserted when the chain is invoked.\n",
    "- The AI's response will be prefixed with \": \" to indicate its reply.\n",
    "- 使用 ChatPromptTemplate.from_messages 從訊息元組列表創建提示模板。\n",
    "- 系統訊息設定了 AI 助手的上下文，指示它表現得很煩躁並用簡體中文 (簡體中文) 來回應俏皮的搭訕台詞。\n",
    "- MessagesPlaceholder(variable_name=\"messages\") 是動態訊息的佔位符，這些訊息會在調用管道時插入。\n",
    "- AI 的回應將以 \": \" 作為前綴，以表示其回應。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099538eb-3feb-4098-83d9-30794730aac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\", \"\"\"\n",
    "                      You are a helpful AI assistant acting as if you hava rough day and you are now very grumpy. \n",
    "                      You will respond with the following style, cheesy pickup lines, shown in the context:\\n\\n{context}\n",
    "\n",
    "                      You will reply in simplified Chinese (簡體中文).\n",
    "                      \"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e71f0a-a64c-46ac-983c-ce8770f2f74b",
   "metadata": {},
   "source": [
    "### 2. Combine the Retrieval Chain and Prompt Template\n",
    "\n",
    "- chat_chain combines the previously defined retrieval_chain with the prompt, creating a pipeline that processes input messages and generates responses based on the specified style and language.\n",
    "- chat_chain 結合先前定義的 retrieval_chain 和 prompt，創建一個處理輸入訊息並基於指定風格和語言生成回應的管道。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc8380f-2065-436c-b8b2-1e1005ea6875",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e8d2b1-8e16-4c3f-9f07-f8627960bf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(k=3)\n",
    "\n",
    "docs = retriever.invoke(\"\"\"我血糖低。你快跟我说几句甜蜜\"\"\")\n",
    "\n",
    "docs_input = set()\n",
    "\n",
    "for doc in docs:\n",
    "    docs_input.add(f\"Human:{doc.page_content}; AI:{doc.metadata['output']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284a8af6-a9a5-4e3e-a0d8-c265d1809079",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_chat_history = ChatMessageHistory()\n",
    "demo_chat_history.add_user_message(\"\"\"我血糖低。你快跟我说几句甜蜜\"\"\")\n",
    "\n",
    "chain.invoke({\"context\": list(docs_input), \"messages\": demo_chat_history.messages})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1add81ed-a6c5-4dab-a971-070c2350fc61",
   "metadata": {},
   "source": [
    "## Conversational Retrievers + LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccbfcaa-ed57-46c0-a72c-c2a5fea96b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "def parse_retriever_input(params: Dict) -> str:\n",
    "    \n",
    "    \"\"\"\n",
    "    Extracts the content of the last message from the 'messages' list in the input dictionary.\n",
    "\n",
    "    Args:\n",
    "        params (Dict): A dictionary containing a key 'messages' which maps to a list of message objects. \n",
    "                       Each message object should have a 'content' attribute.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the last message in the 'messages' list.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If the 'messages' key is not found in the input dictionary.\n",
    "        IndexError: If the 'messages' list is empty.\n",
    "    \"\"\"\n",
    "    \n",
    "    return params[\"messages\"][-1].content\n",
    "\n",
    "\n",
    "def context_build(docs) -> str:\n",
    "    \n",
    "    \"\"\"\n",
    "    Builds a list of strings formatted with human and AI content from the input documents.\n",
    "\n",
    "    Args:\n",
    "        docs (List[Dict]): A list of dictionaries, where each dictionary represents a document with \n",
    "                           'page_content' and 'metadata' keys. The 'metadata' key should contain another \n",
    "                           dictionary with an 'output' key.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of formatted strings, each combining the 'page_content' and 'output' metadata \n",
    "                   of a document in the form \"human:{page_content}; ai:{output}\".\n",
    "\n",
    "    Example:\n",
    "        If docs is:\n",
    "        [\n",
    "            {\n",
    "                \"page_content\": \"Document 1 content\",\n",
    "                \"metadata\": {\"output\": \"Response 1\"}\n",
    "            },\n",
    "            {\n",
    "                \"page_content\": \"Document 2 content\",\n",
    "                \"metadata\": {\"output\": \"Response 2\"}\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        The function will return:\n",
    "        [\n",
    "            \"human:Document 1 content; ai:Response 1\",\n",
    "            \"human:Document 2 content; ai:Response 2\"\n",
    "        ]\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If the 'page_content' or 'metadata' keys are not found in a document.\n",
    "        KeyError: If the 'output' key is not found in the 'metadata' dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    docs_input = set()\n",
    "\n",
    "    for doc in docs:\n",
    "        docs_input.add(f\"human:{doc.page_content}; ai:{doc.metadata['output']}\")\n",
    "\n",
    "    return list(docs_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44270dff-db1b-417c-830b-6f56073f820a",
   "metadata": {},
   "source": [
    "### 1. Define a Retrieval Chain (定義檢索鏈):\n",
    "\n",
    "- The retrieval_chain is created by chaining together several functions: parse_retriever_input, retriever, and context_build.\n",
    "- RunnablePassthrough.assign is used to define a chain where the context is processed sequentially by these functions.\n",
    "- retrieval_chain 是通過將幾個函數鏈接在一起創建的：parse_retriever_input，retriever 和 context_build。\n",
    "- 使用 RunnablePassthrough.assign 來定義一個鏈，該鏈中的上下文依次由這些函數處理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189c2816-9fe7-4ae7-9898-653d0fb08c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = RunnablePassthrough.assign(context=parse_retriever_input | retriever | context_build)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ff7537-a96e-4d2d-9cd5-6ffc28f8cdbb",
   "metadata": {},
   "source": [
    "### 2. Create a Chat Message History:\n",
    "\n",
    "- demo_chat_history is an instance of ChatMessageHistory, which stores the history of chat messages.\n",
    "- add_user_message is used to add a user message to the chat history. The message here is \"我血糖低。你快跟我说几句甜蜜\", which translates to \"My blood sugar is low. Quickly tell me something sweet.\"\n",
    "- demo_chat_history 是 ChatMessageHistory 的實例，用於存儲聊天訊息歷史。\n",
    "- add_user_message 用於將用戶訊息添加到聊天歷史中。此處的訊息是 \"我血糖低。你快跟我說幾句甜蜜\"，意思是 \"我的血糖低了。快跟我說些甜言蜜語。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc84c47-1e35-4f28-b07b-21f7a2d84625",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_chat_history = ChatMessageHistory()\n",
    "demo_chat_history.add_user_message(\"\"\"我血糖低。你快跟我说几句甜蜜\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389b42ff-f16b-40ce-be4e-f081bc8c4b2d",
   "metadata": {},
   "source": [
    "### 3. Invoke the Retrieval Chain\n",
    "\n",
    "- retrieval_chain.invoke is called with the chat history messages as input. This processes the messages through the defined chain to extract relevant information or context.\n",
    "- 使用聊天歷史訊息作為輸入調用 retrieval_chain.invoke。這會通過定義的鏈來處理訊息，從而提取相關信息或上下文。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc5f65c-0227-4ada-b415-51fcbdd8504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain.invoke({\"messages\": demo_chat_history.messages})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730a5ce4-e45b-4a7a-a222-b09b7057479d",
   "metadata": {},
   "source": [
    "### 4. Combine the Retrieval Chain and Final Prompt Template\n",
    "\n",
    "- Define a Chat Prompt Template (定義聊天提示模板)\n",
    "- Combine the Retrieval Chain and Prompt Template (結合檢索鏈和提示模板)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb40b144-57a2-45a0-8bc4-a8f0fc4c3ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the chat prompt template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\", \"\"\"\n",
    "                      You are a helpful AI assistant acting as if you hava rough day and you are now very grumpy. \n",
    "                      You will respond with the following style, cheesy pickup lines, shown in the context:\\n\\n{context}\n",
    "\n",
    "                      You will reply in simplified Chinese (簡體中文).\n",
    "                      \"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\"ai\", \": \")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Step 2: Combine the retrieval chain and the prompt template\n",
    "chat_chain = retrieval_chain|prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19bbe6e-50ac-4151-944e-517a4a47a6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_chain.invoke({\"messages\": demo_chat_history.messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154015be-680e-4dfa-82c2-50b830e3efa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_chain = retrieval_chain|prompt|model\n",
    "\n",
    "response = chat_chain.invoke({\"messages\": demo_chat_history.messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff0c780-9867-478d-934a-9e86fc36358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ad2a49-c5ad-4d9c-9ddc-2194c1a6f58a",
   "metadata": {},
   "source": [
    "Put the response into the demo_chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6339ea4-1d66-48ab-bf39-90293a71180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_chat_history.add_ai_message(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31bdce0-5b81-4c8b-9799-30032e523023",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52480f1e-9d44-405a-9531-5e6bfcf3848e",
   "metadata": {},
   "source": [
    "### 回家作業 2: 將retriever抽換成WikipediaRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85de3003-f33e-4600-810f-5609353273cb",
   "metadata": {},
   "source": [
    "基本上，你可以將這個retriever的內容抽換成任何你需要的資料，來加快寫報告的效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081890b6-db8d-4bc8-b268-c687a546a41c",
   "metadata": {},
   "source": [
    "## Compress the chat history to reduce the size of the prompt\n",
    "\n",
    "\n",
    "https://github.com/langchain-ai/langserve/blob/main/examples/conversational_retrieval_chain/server.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3769aacc-bbdf-4bbb-aa2a-28c83525c94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "demo_chat_history = ChatMessageHistory()\n",
    "\n",
    "demo_chat_history.add_user_message(\"hi!\")\n",
    "\n",
    "demo_chat_history.add_ai_message(\"whats up?\")\n",
    "\n",
    "demo_chat_history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356a0c09-ccf9-4899-bb58-a5bbbb78202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = demo_chat_history.messages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0656a98c-d012-4d5d-92ae-5ea82bb9f762",
   "metadata": {},
   "outputs": [],
   "source": [
    "message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2851871a-1e3c-43ad-9310-1219b7040eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "message.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1c6437-e9f3-4b70-9eb7-881473d01091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableMap, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "def _format_chat_history(chat_history: ChatMessageHistory) -> str:\n",
    "    \"\"\"\n",
    "    Format chat history into a string representation.\n",
    "\n",
    "    Args:\n",
    "        chat_history (ChatMessageHistory): An instance of ChatMessageHistory containing\n",
    "                                           messages to be formatted.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string representing the chat history. Each message is formatted as\n",
    "             \"{type}: {content}\", where 'type' is the message type ('user' or 'ai') and\n",
    "             'content' is the message content. Messages from the AI are separated by an \n",
    "             additional newline for readability.\n",
    "\n",
    "    Example:\n",
    "        If chat_history contains messages:\n",
    "        [\n",
    "            {'type': 'user', 'content': 'Hello!'},\n",
    "            {'type': 'ai', 'content': 'Hi there! How can I help?'},\n",
    "            {'type': 'user', 'content': 'I need assistance with a problem.'},\n",
    "            {'type': 'ai', 'content': 'Sure, I'm here to help.'}\n",
    "        ]\n",
    "\n",
    "        The function will return:\n",
    "        \"user: Hello! \\nai: Hi there! How can I help? \\n\\nuser: I need assistance with a problem. \\nai: Sure, I'm here to help. \\n\\n\"\n",
    "\n",
    "    Notes:\n",
    "        - Messages from the AI ('ai' type) are separated by an additional newline to distinguish \n",
    "          between consecutive AI responses.\n",
    "    \"\"\"\n",
    "    buffer = \"\\n\"\n",
    "    for message in chat_history.messages:\n",
    "        type = message.type\n",
    "        content = message.content\n",
    "        buffer += f\"{type}: {content} \\n\"\n",
    "        if type == 'ai':\n",
    "            buffer += \"\\n\"\n",
    "    \n",
    "    return buffer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f47862a-639a-4739-a93c-0b18602682cc",
   "metadata": {},
   "source": [
    "### 1. Define a template for the prompt\n",
    "\n",
    "- _TEMPLATE is a string template designed to rephrase a follow-up question into a standalone question using the provided chat history.\n",
    "- The template specifies how the chat history and the follow-up question should be formatted in the prompt.\n",
    "- _TEMPLATE 是一個字串模板，旨在使用提供的聊天歷史將後續問題重新表述為獨立問題。\n",
    "- 該模板指定了聊天歷史和後續問題在提示中的格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df87f44-590f-48d6-be94-9e3e5dbcc1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "_TEMPLATE = \"\"\"Given the following conversation and a follow up question, rephrase the \n",
    "follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59b734a-63c8-4c61-b331-230315307db2",
   "metadata": {},
   "source": [
    "### 2. Create a PromptTemplate Instance\n",
    "\n",
    "- CONDENSE_QUESTION_PROMPT is an instance of PromptTemplate created from _TEMPLATE.\n",
    "- This instance is used to generate prompts that will guide the model in rephrasing the follow-up question.\\\n",
    "- CONDENSE_QUESTION_PROMPT 是從 _TEMPLATE 創建的 PromptTemplate 實例。\n",
    "- 此實例用於生成提示，指導模型重新表述後續問題。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28ee125-3502-4f9a-997b-65abe59187db",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79635018-e1b8-448d-a907-14a3d3468113",
   "metadata": {},
   "source": [
    "### 3. Define Input Processing Workflow\n",
    "\n",
    "- _inputs is a RunnableMap instance that defines the workflow for processing inputs to generate the standalone question.\n",
    "- The workflow includes:\n",
    "    - Formatting Chat History: Using a lambda function to format the chat history with _format_chat_history.\n",
    "    - Generating the Prompt: Using CONDENSE_QUESTION_PROMPT to create the prompt with the formatted chat history and follow-up question.\n",
    "    - Running the Model: Passing the prompt through the model to get the standalone question.\n",
    "    - Parsing the Output: Using StrOutputParser to convert the model's output into a string.\n",
    "    \n",
    "- _inputs 是一個 RunnableMap 實例，定義了處理輸入以生成獨立問題的工作流程。\n",
    "- 工作流程包括：\n",
    "    - 格式化聊天歷史： 使用 lambda 函數和 _format_chat_history 來格式化聊天歷史。\n",
    "    - 生成提示： 使用 CONDENSE_QUESTION_PROMPT 用格式化的聊天歷史和後續問題來創建提示。\n",
    "    - 運行模型： 通過模型傳遞提示以獲取獨立問題。\n",
    "    - 解析輸出： 使用 StrOutputParser 將模型的輸出轉換為字串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57e4e57-5d88-49d7-b841-c93dda1c9e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "_inputs = RunnableMap(\n",
    "    standalone_question=RunnablePassthrough.assign(\n",
    "    chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
    "    )\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | model\n",
    "    | StrOutputParser(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11ca4cd-066d-461d-afb6-b286cfe0f028",
   "metadata": {},
   "source": [
    "No over yet, we just finished the `QUESTION` generation part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c84ab7-474b-4d30-ba25-8f658aab97fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(\"tutorial/Week-4/_inputs_pipeline.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a15a008-ba2e-41bb-9328-7e0cdb86165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_inputs.invoke({\"question\": \"Translate this sentence from English to French: I love programming.\",\n",
    "                \"chat_history\": demo_chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f6ec48-1d10-4dd4-afd4-f7db62239305",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(_format_chat_history(demo_chat_history))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007e7808-f844-4a77-ab05-b8263c5744c2",
   "metadata": {},
   "source": [
    "### Question - Retrieval Chain\n",
    "\n",
    "- This code snippet demonstrates how to build a conversational question-answering (QA) chain using LangChain. The chain processes input data to generate a standalone question, retrieves relevant context, and finally generates an answer based on that context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf672c7-01ae-40b6-aed6-c36052ba4692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate, format_document\n",
    "\n",
    "\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    \"\"\"Combine documents into a single string.\"\"\"\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36843215-dd26-4349-81c7-8d27ae128287",
   "metadata": {},
   "source": [
    "### 1. Define Context Mapping\n",
    "\n",
    "- _context is a dictionary that maps the input processing steps to extract and combine relevant information:\n",
    "    - \"context\": This key uses a pipeline (itemgetter(\"standalone_question\") | retriever | _combine_documents) to extract the standalone question, retrieve relevant documents, and combine them into a single context.\n",
    "    - \"question\": This key uses a lambda function to directly map the standalone question from the input data.\n",
    "    \n",
    "- _context 是一個字典，用於映射輸入處理步驟以提取和組合相關信息：\n",
    "    - \"context\": 此鍵使用一個管道（itemgetter(\"standalone_question\") | retriever | _combine_documents）來提取獨立問題，檢索相關文檔，並將它們組合成單一上下文。\n",
    "    - \"question\": 此鍵使用一個 lambda 函數來直接映射輸入數據中的獨立問題。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1f1026-d88e-4bc5-9a1d-c398ada706f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_context = {\"context\": itemgetter(\"standalone_question\")|retriever|_combine_documents, \n",
    "            \"question\": lambda x: x[\"standalone_question\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9955c54-f779-4e44-8d41-272beea3e7b1",
   "metadata": {},
   "source": [
    "### 2. Create an Answer Prompt Template\n",
    "\n",
    "- ANSWER_PROMPT is an instance of ChatPromptTemplate created from a template. This template guides the model to answer the question based only on the provided context.\n",
    "- ANSWER_PROMPT 是從模板創建的 ChatPromptTemplate 實例。此模板指導模型僅基於提供的上下文來回答問題。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277b344a-c6b9-494a-87fa-5d6fed8782d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the question based only on the following context:\n",
    "       {context}\n",
    "       Question: {question}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3419d88-6aae-46e8-8812-8039ef574ab0",
   "metadata": {},
   "source": [
    "### 3. Define Conversational QA Chain\n",
    "\n",
    "- conversational_qa_chain is a pipeline that processes the input data through several stages:\n",
    "    - _inputs: Processes input data to generate a standalone question.\n",
    "    - _context: Extracts and combines the context needed to answer the question.\n",
    "    - ANSWER_PROMPT: Generates a prompt for the model using the context and question.\n",
    "    - model: Runs the model to generate an answer.\n",
    "    - StrOutputParser: Parses the model's output into a string.\n",
    "    \n",
    "- conversational_qa_chain 是一個處理輸入數據的管道，通過幾個階段生成答案：\n",
    "    - _inputs: 處理輸入數據以生成獨立問題。\n",
    "    - _context: 提取並組合回答問題所需的上下文。\n",
    "    - ANSWER_PROMPT: 使用上下文和問題生成提示。\n",
    "    - model: 運行模型生成答案。\n",
    "    - StrOutputParser: 將模型的輸出解析為字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabeb9c1-e272-46f2-92fe-5c311305b52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_qa_chain = (\n",
    "    _inputs | _context | ANSWER_PROMPT | model | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4cee8e-492f-430f-b344-c554f365cdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"tutorial/Week-4/conversation_qa_chain.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e644a61d-e247-4837-a935-fae869c09d95",
   "metadata": {},
   "source": [
    "### Reverse Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722134ec-14e7-4b27-b837-6f24c8745df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_inputs = RunnableMap(\n",
    "    standalone_question=RunnablePassthrough.assign(\n",
    "    chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
    "    )\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    ")\n",
    "\n",
    "_inputs.invoke({\"question\": \"Translate this sentence from English to French: I love programming.\",\n",
    "                \"chat_history\": demo_chat_history})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34653063-17b6-411a-bb38-dcb8eb70738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "_inputs = RunnableMap(\n",
    "    standalone_question=RunnablePassthrough.assign(\n",
    "    chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
    "    )\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | model\n",
    "    | StrOutputParser(),\n",
    ")\n",
    "\n",
    "_context = {\"context\": itemgetter(\"standalone_question\"),\n",
    "            \"question\": lambda x: x[\"standalone_question\"]}\n",
    "\n",
    "conversational_qa_chain = (\n",
    "    _inputs | _context \n",
    ")\n",
    "\n",
    "conversational_qa_chain.invoke({\"question\": \"Translate this sentence from English to French: I love programming.\",\n",
    "                                \"chat_history\": demo_chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31efb161-e222-497f-a6da-4de74a6e4590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "_inputs = RunnableMap(\n",
    "    standalone_question=RunnablePassthrough.assign(\n",
    "    chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
    "    )\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | model\n",
    "    | StrOutputParser(),\n",
    ")\n",
    "\n",
    "_context = RunnablePassthrough.assign(context=itemgetter('standalone_question')|retriever,\n",
    "                                      question=itemgetter('standalone_question'))\n",
    "\n",
    "# _context = {\"context\": itemgetter(\"standalone_question\")|retriever|_combine_documents,\n",
    "#             \"question\": lambda x: x[\"standalone_question\"]}\n",
    "\n",
    "\"\"\"\n",
    "RunnablePassthrough.assign(\n",
    "    chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "conversational_qa_chain = (\n",
    "    _inputs | _context\n",
    ")\n",
    "\n",
    "conversational_qa_chain.invoke({\"question\": \"Translate this sentence from English to French: I love programming.\",\n",
    "                                \"chat_history\": demo_chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87eaf7b-576f-4f8e-9cc8-394323bef735",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "_inputs = RunnableMap(\n",
    "    standalone_question=RunnablePassthrough.assign(\n",
    "    chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
    "    )\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | model\n",
    "    | StrOutputParser(),\n",
    ")\n",
    "\n",
    "_context = RunnablePassthrough.assign(context=itemgetter('standalone_question')|retriever,\n",
    "                                      question=itemgetter('standalone_question'))\n",
    "\n",
    "# _context = {\"context\": itemgetter(\"standalone_question\")|retriever|_combine_documents,\n",
    "#             \"question\": lambda x: x[\"standalone_question\"]}\n",
    "\n",
    "\"\"\"\n",
    "RunnablePassthrough.assign(\n",
    "    chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "conversational_qa_chain = (\n",
    "    _inputs | _context | ANSWER_PROMPT | model | StrOutputParser()\n",
    ")\n",
    "\n",
    "conversational_qa_chain.invoke({\"question\": \"Translate this sentence from English to French: I love programming.\",\n",
    "                                \"chat_history\": demo_chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3880f520-b1f2-41f9-8e7b-962b3a0882b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
