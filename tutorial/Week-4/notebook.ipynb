{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4651f25d-4013-47fd-b4f4-0b0fbea51514",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 上週作業"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba15128-8699-4d6a-abad-39c5c1d61640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dff39d-b3b6-44b0-a63c-6bcc3bce6d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate, ChatPromptTemplate\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from src.io.path_definition import get_project_dir\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Ingredients: {input}\\nOrigin: {output}\",\n",
    ")\n",
    "\n",
    "with open(os.path.join(get_project_dir(), 'tutorial', 'Week-1', 'recipe_train.json'), 'r') as f:\n",
    "    recipe_train = json.load(f)\n",
    "\n",
    "examples = []\n",
    "\n",
    "for recipe in recipe_train:\n",
    "    examples.append({\"input\": \" \".join(recipe['ingredients']),\n",
    "                     \"output\": recipe['cuisine']})\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # The list of examples available to select from.\n",
    "    examples,\n",
    "    # The embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    embeddings,\n",
    "    # The VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    FAISS,\n",
    "    # The number of examples to produce.\n",
    "    k=5,\n",
    ")\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # We provide an ExampleSelector instead of examples.\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Find the recipe origin based on the ingredients\",\n",
    "    suffix=\"Ingredients: {ingredients}\\nOrigin:\",\n",
    "    input_variables=[\"ingredients\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec67d88-fe19-4f10-9168-eec17e11a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84950f81-c97a-41b4-8419-80b5b6729772",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(get_project_dir(), 'tutorial', 'Week-1', 'recipe_test.json'), 'r') as f:\n",
    "    recipe_test = json.load(f)\n",
    "\n",
    "existing_ingredients = recipe_test[99]['ingredients']\n",
    "\n",
    "similar_prompt.invoke(\", \".join(existing_ingredients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ce1d22-e031-4938-9af3-a99c04a7535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from src.initialization import credential_init\n",
    "\n",
    "credential_init()\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o-2024-05-13\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e574f1e-2645-4a4d-9c8e-a5c2a586fc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = similar_prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70d1f0f-7f91-4b46-af21-ac74639cf7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\", \".join(existing_ingredients))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998d1aff-a4a4-4c14-a39c-5bb88c0942c9",
   "metadata": {},
   "source": [
    "# Remote server\n",
    "\n",
    "### 1. Making a POST Request (發送 POST 請求):\n",
    "\n",
    "- requests.post(...) sends an HTTP POST request to the specified URL.\n",
    "- The URL \"http://localhost:5000/openai/invoke\" points to a local server running on port 5000, at the endpoint /openai/invoke.\n",
    "- The json parameter is used to send a JSON payload with the request. In this case, the payload is {'input': \"Where is Taiwan\"}.\n",
    "- requests.post(...) 發送一個 HTTP POST 請求到指定的 URL。\n",
    "- URL \"http://localhost:5000/openai/invoke\" 指向一個本地服務器，該服務器在端口 5000 上運行，並且指向 /openai/invoke 端點。\n",
    "- json 參數用於隨請求發送 JSON 負載。在這個例子中，負載是 {'input': \"Where is Taiwan\"}。\n",
    "\n",
    "### 2. Response Handling (響應處理):\n",
    "\n",
    "- The server processes the request and sends back a response.\n",
    "- The response is stored in the response variable, which can then be inspected or used further in the code.\n",
    "- 服務器處理請求並返回響應。\n",
    "- 響應存儲在 response 變量中，之後可以檢查或在代碼中進一步使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb6c54e-f2f8-4e0e-92b8-0ad9c3803e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:5000/openai/invoke\",\n",
    "    json={'input': \"Where is Taiwan\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dc0e8a-9380-413a-805b-9890d4142cb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ea853f-f380-42b2-a57a-0249ae57bca0",
   "metadata": {},
   "source": [
    "# Use the remote model as Software as a service (SaaS)\n",
    "\n",
    "## Basic Usage\n",
    "\n",
    "### 1. Creating an Instance of RemoteRunnable (創建 RemoteRunnable 的實例):\n",
    "\n",
    "- This line creates an instance of RemoteRunnable and initializes it with the URL of the remote language model service. In this case, the service is running locally on http://localhost:5000/openai/.\n",
    "- 這行代碼創建一個 RemoteRunnable 的實例，並用遠程語言模型服務的 URL 進行初始化。在這個例子中，服務在本地運行，URL 為 http://localhost:5000/openai/。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84212830-9dfe-47c3-bc65-63de510f8a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langserve import RemoteRunnable\n",
    "\n",
    "llm = RemoteRunnable(\"http://localhost:5000/openai/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4c2ea5-4ec2-4298-af5e-6e85aaa4c320",
   "metadata": {},
   "source": [
    "### 2. Asynchronous Streaming of Responses (異步流式處理回應):\n",
    "\n",
    "- llm.astream(\"Where is Taiwan?\") sends the query \"Where is Taiwan?\" to the remote service and retrieves the response as a stream.\n",
    "- async for msg in ... is used to handle the streaming responses asynchronously.\n",
    "- print(msg.content, end=\"\", flush=True) prints each message content received from the stream without adding a new line after each message, and flushes the output buffer to ensure the message is displayed immediately.\n",
    "- llm.astream(\"Where is Taiwan?\") 將查詢 \"Where is Taiwan?\" 發送到遠程服務，並以流的形式檢索回應。\n",
    "- async for msg in ... 用於異步處理流式回應。\n",
    "- print(msg.content, end=\"\", flush=True) 打印每個從流中接收到的消息內容，不在每個消息後添加新行，並刷新輸出緩衝區以確保消息立即顯示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc16c50-aab3-4d58-8091-aa8ac6c4a917",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Supports astream\n",
    "async for msg in llm.astream(\"Where is Taiwan?\"):\n",
    "    print(msg.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafcf229-ae2c-41a9-bcf6-4503c366c2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm.invoke(\"Where is Taiwan?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d637005-0c88-43a1-817e-b96039218384",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71923692-63fc-4dad-8277-b092ea0839b6",
   "metadata": {},
   "source": [
    "## Make the external service a part of the chain\n",
    "\n",
    "### 1. Comedian Chain (喜劇演員鏈)\n",
    "\n",
    "- ChatPromptTemplate.from_messages(...) creates a prompt template where the system prompt instructs the model to either tell a joke or state a fact, and the human prompt provides the input.\n",
    "- This template is then piped (|) to a language model (llm) to generate the comedian's response.\n",
    "- ChatPromptTemplate.from_messages(...) 創建一個提示模板，其中系統提示指示模型要麼講一個笑話，要麼陳述一個不搞笑的事實，並且僅輸出一個。\n",
    "- 然後將此模板通過管道（|）傳遞給語言模型（llm），以生成喜劇演員的回應。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ab7c70-e6d6-4b41-b499-eabff45259d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "comedian_chain = (\n",
    "    ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a comedian that sometimes tells funny jokes and other times you just state facts that are not funny. Please either tell a joke or state fact now but only output one.\",\n",
    "            ),\n",
    "            ('human', '{input}'\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    | llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37c52f1-c5c9-4fc4-b15d-988ad3b5a173",
   "metadata": {},
   "source": [
    "### 2. Joke Classifier Chain\n",
    "\n",
    "- This chain is similar to the comedian chain but serves a different purpose.\n",
    "- The system prompt asks the model to classify the joke as \"funny\" or \"not funny\" and repeat the first five words for reference.\n",
    "- This template is also piped to the language model (llm).\n",
    "- 這個鏈與喜劇演員鏈類似，但用途不同。\n",
    "- 系統提示要求模型將笑話分類為“搞笑”或“不搞笑”，並重複笑話的前五個詞以供參考。\n",
    "- 此模板也通過管道傳遞給語言模型（llm）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c76e1f-f294-49fc-a174-7fcb5b95cf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "joke_classifier_chain = (\n",
    "    ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"Please determine if the joke is funny. Say `funny` if it's funny and `not funny` if not funny. Then repeat the first five words of the joke for reference...\",\n",
    "            ),\n",
    "            (\"human\", \"{joke}\"),\n",
    "        ]\n",
    "    )\n",
    "    | llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2534a7-0dd5-4b54-a866-18766d82f786",
   "metadata": {},
   "source": [
    "### 3. Combining Chains with RunnablePassthrough\n",
    "\n",
    "- This combines the comedian chain and the joke classifier chain using RunnablePassthrough.assign.\n",
    "- The comedian chain generates the output, and then this output is passed to the joke classifier chain to classify its humor.\n",
    "- 這將喜劇演員鏈和笑話分類器鏈結合在一起，使用 RunnablePassthrough.assign。\n",
    "- 喜劇演員鏈生成輸出，然後將此輸出傳遞給笑話分類器鏈以分類其幽默性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a34bdc7-c1b3-4176-ba7a-13744f6b0811",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = {\"joke\": comedian_chain} | RunnablePassthrough.assign(\n",
    "    classification=joke_classifier_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bacecf8-60af-44a2-82e7-4fc02dd5b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"input\": \"A man and a beer\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f702f519-37dd-4b45-a492-0c4bcdd521b0",
   "metadata": {},
   "source": [
    "# ChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22673222-780c-4356-a6d1-92c69b28d6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(url=\"https://python.langchain.com/v0.1/assets/images/chat_use_case-eb8a4883931d726e9f23628a0d22e315.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb536cb3-dc3e-4068-adb7-ce70ef0ca4e5",
   "metadata": {},
   "source": [
    "- N-Shot\n",
    "- The historical chat history can be consdiered as a list of question-answer pairs\n",
    "- If the chatbot doesn’t remember past chats, it’s called stateless because it doesn’t know what happened before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f7fe22-cd1d-470a-927b-31725616d95c",
   "metadata": {},
   "source": [
    "## Minimal Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5f885c-cb85-477f-90e8-147e701e6857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=\"Translate this sentence from English to French: I love programming.\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95345038-9a83-465c-a85a-9b2d5f1b6476",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke([HumanMessage(content=\"What did you just say?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71ee93d-2841-4cc0-b417-3fa5554ba1b3",
   "metadata": {},
   "source": [
    "The `memory` is not there, so it does not understand your question.\n",
    "\n",
    "The following example shows how to add memory into the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed4e110-9be7-4a67-be86-31b62451baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=\"Translate this sentence from English to French: I love programming.\"\n",
    "        ),\n",
    "        AIMessage(content=\"J'adore la programmation.\"),\n",
    "        HumanMessage(content=\"What did you just say?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce24122f-cf20-4667-bc23-5eebedd54992",
   "metadata": {},
   "source": [
    "## Prompt templates\n",
    "\n",
    "### 1. Creating a ChatPromptTemplate \n",
    "\n",
    "- ChatPromptTemplate.from_messages(...) creates a prompt template for the chatbot.\n",
    "- The first message in the template is a system message: \"You are a helpful assistant. Answer all questions to the best of your ability.\" This message sets the context and behavior of the assistant, instructing it to be helpful and thorough in its responses.\n",
    "- MessagesPlaceholder(variable_name=\"messages\") is a placeholder for dynamic content. The variable_name=\"messages\" specifies that this placeholder will be filled with user messages during the conversation.\n",
    "- ChatPromptTemplate.from_messages(...) 創建了一個聊天機器人的提示模板。\n",
    "- 模板中的第一條消息是一條系統消息：“You are a helpful assistant. Answer all questions to the best of your ability.” 此消息設置了助手的上下文和行為，指示其在回答中要提供幫助並盡力而為。\n",
    "- MessagesPlaceholder(variable_name=\"messages\") 是一個動態內容的佔位符。variable_name=\"messages\" 指定該佔位符將在對話中插入用戶消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dea7e35-f9b4-4d78-8a8e-e85225c5fd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e88d592-3e03-4a5b-81b2-e792154f94ec",
   "metadata": {},
   "source": [
    "### 2. Creating the Chain\n",
    "\n",
    "- This line pipes (|) the prompt template to a language model (model).\n",
    "- chain represents a sequence of operations where the prompt template is used to format user messages, and the language model processes these messages to generate responses.\n",
    "- 這行代碼通過管道（|）將提示模板傳遞給語言模型（model）。\n",
    "- chain 代表一系列操作，其中提示模板用於格式化用戶消息，語言模型處理這些消息以生成回應。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd2ed39-46d6-460e-b78e-5b0c4d06fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebca060-ce9e-4fba-be50-a40d923324fc",
   "metadata": {},
   "source": [
    "The MessagesPlaceholder above inserts chat messages passed into the chain's input as chat_history directly into the prompt. Then, we can invoke the chain like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e2a159-a35a-47db-ac82-43c8d9f85887",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Translate this sentence from English to French: I love programming.\"\n",
    "            ),\n",
    "            AIMessage(content=\"J'adore la programmation.\"),\n",
    "            HumanMessage(content=\"What did you just say?\"),\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c789c9c-46ac-4884-b059-466855e114aa",
   "metadata": {},
   "source": [
    "## Example of Using MessageHistory\n",
    "\n",
    "As a shortcut for managing the chat history, we can use a MessageHistory class, which is responsible for saving and loading chat messages. There are many built-in message history integrations that persist messages to a variety of databases, but for this quickstart we'll use a in-memory, demo message history called ChatMessageHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e0029f-228a-4ff5-afb0-91bf92f00d34",
   "metadata": {},
   "source": [
    "### 1. Importing the ChatMessageHistory Class (導入 ChatMessageHistory 類)\n",
    "\n",
    "- This line imports the ChatMessageHistory class from the langchain.memory module. This class is used to handle the chat messages in memory.\n",
    "- 這行代碼從 langchain.memory 模塊中導入 ChatMessageHistory 類。此類用於在內存中處理聊天消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f65a6f-3d57-4cf9-8e8f-34623e7965e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6cb01a-d8c8-4d51-8b9a-35b79a8c06f6",
   "metadata": {},
   "source": [
    "### 2. Creating an Instance of ChatMessageHistory (創建 ChatMessageHistory 的實例)\n",
    "\n",
    "- This line creates an instance of ChatMessageHistory. This instance will store the chat messages in memory for this session.\n",
    "- 這行代碼創建一個 ChatMessageHistory 的實例。該實例將在此會話期間將聊天消息存儲在內存中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fba54c-8018-4f75-963d-6195892ae170",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_chat_history = ChatMessageHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e08da2-cb06-41db-ab6a-69532596b405",
   "metadata": {},
   "source": [
    "### 3. Adding User and AI Messages (添加用戶和 AI 消息)\n",
    "\n",
    "- demo_chat_history.add_user_message(\"hi!\") adds a user message (\"hi!\") to the chat history.\n",
    "- demo_chat_history.add_ai_message(\"whats up?\") adds an AI response (\"whats up?\") to the chat history.\n",
    "- demo_chat_history.add_user_message(\"hi!\") 將用戶消息（“hi!”）添加到聊天記錄中。\n",
    "- demo_chat_history.add_ai_message(\"whats up?\") 將 AI 回應（“whats up?”）添加到聊天記錄中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638e4f96-f747-4f1d-97e2-d5e52749a0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_chat_history.add_user_message(\"hi!\")\n",
    "\n",
    "demo_chat_history.add_ai_message(\"whats up?\")\n",
    "\n",
    "demo_chat_history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3138b941-7bc6-4e08-890a-2d6debdc8431",
   "metadata": {},
   "source": [
    "### 4. Retrieving the Messages (檢索消息)\n",
    "\n",
    "- This line retrieves the list of messages stored in demo_chat_history. Each message is an object that contains information about the sender (user or AI) and the content of the message.\n",
    "- 這行代碼檢索存儲在 demo_chat_history 中的消息列表。每條消息都是一個對象，包含有關發送者（用戶或 AI）和消息內容的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f86ffa-b2bd-4d90-9a62-971b55701eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"messages\": demo_chat_history.messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cb4014-b66b-40dc-8acc-3e052426ce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_chat_history.add_user_message(\n",
    "    \"Translate this sentence from English to French: I love programming.\"\n",
    ")\n",
    "\n",
    "response = chain.invoke({\"messages\": demo_chat_history.messages})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54650b6d-115e-418e-9910-e87c90872ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the response back into the demo_chat_history\n",
    "\n",
    "demo_chat_history.add_ai_message(response)\n",
    "\n",
    "demo_chat_history.add_user_message(\"What did you just say?\")\n",
    "\n",
    "chain.invoke({\"messages\": demo_chat_history.messages})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f147092-7b3c-4392-8d8e-90c65b09d5d2",
   "metadata": {},
   "source": [
    "# **** 預計第一個小時結束 ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb7abfb-9d66-4ca8-96c7-2358b21d009e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from src.initialization import credential_init\n",
    "\n",
    "\n",
    "credential_init()\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o-2024-05-13\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aea1c49-906f-481c-9020-6b40cf07dba4",
   "metadata": {},
   "source": [
    "## Conversational Retrievers - Step 1\n",
    "\n",
    "- 土味情話反殺大全 (推薦上Youtube看)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1831d12-b64a-4d21-b30f-10cc421d3500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "df = pd.DataFrame(data=[[\"确认过眼神，你是我爱的人。\", \"确认过眼神，我是你泡不到的人。\"],\n",
    "                         [\"万水千山总是情，爱我多一点行不行。\", \"一寸光阴一寸金，劝你死了这条心。\"],\n",
    "                         [\"今天吃了泡面，吃了炒面，还是想走进你的心里面。\", \"吃那么多面，最后还不是变成大便。\"],\n",
    "                         [\"草莓，蓝莓，蔓越莓，今天你想我了没？\", \"冬瓜，西瓜，哈密瓜，你再巴巴我打得你叫妈妈。\"],\n",
    "                         [\"众生皆苦，唯你独甜。\", \"尝遍众生，你为渣男代言。\"],\n",
    "                         [\"你喜欢瑞士名表还是我帅气的外表？\", \"我喜欢去年买了个表。\"],\n",
    "                         [\"我想问一条路，到哥哥心里的路。\", \"山路十八弯，走完脑血栓。\"],\n",
    "                         [\"小姐姐，我心里给你留了一块地，死心塌地。\", \"对不起，我的心里只容得下一块地，玛莎拉蒂。\"],\n",
    "                         [\"小姐姐你笑起来真好看啊。\", \"你看起来真好笑啊。\"],\n",
    "                         [\"亲爱的你知道吗，你的笑容没有酒，我却醉得像条狗\", \"我的笑容没有酒，你是真的像条狗\"],\n",
    "                         [\"宝贝儿，我在手上划了一道口子，你也划一下吧，这样我们就是两口子了\", \"我怕我们的血溶到一起，被你发现其实我是你爸爸\"],\n",
    "                         [\"这世间万物都有尽头，落叶归根，而我归你\", \"对不起 我不收垃圾\"],\n",
    "                         [\"请问……我想问一下路，那条通往你心里的路\", \"八格牙路\"],\n",
    "                         [\"你今天怎么怪怪的？ 怪可爱的\",  \"你今天也怪怪的，怪恶心的\"],\n",
    "                         [\"亲爱的，你知道我和唐僧的区别吗？ 唐僧取经我娶你\", \"知道你和沙僧的区别吗？ 他叫沙僧你叫沙雕\"],\n",
    "                         [\"亲爱的，你不觉得累吗？ 你已经在我的脑海里跑了好几圈了\", \"傻孩子，我在找出口呢\"],\n",
    "                         [\"莫文蔚的阴天。孙燕姿的雨天，周杰伦的晴天，都不如你和我聊天\", \"求求你了，能否还我一个宁静的夏天\"],\n",
    "                         [\"如果你是方便面，那我就是白开水，今生今世，我泡定你了\", \"故事的最后，她变成了屎，你变成了尿，你们终究分道扬镳\"],\n",
    "                         [\"大年三十晚上的鞭炮再响，也没有我想你那么想\", \"大年三十晚上的鞭炮再响，也没有你放的屁响\"],\n",
    "                         [\"c罗可以上演帽子戏法，可我想你却没有办法\", \"c罗可以上演帽子戏法，我也可以给你上演绿帽子戏法\"],\n",
    "                         [\"不要抱怨，抱我\", \"抱不起来，太重\"],\n",
    "                         [\"你有没有发现我的眼睛很好看？因为我满眼都是你啊\", \"对不起，你眼睛在哪呢？\"]], \n",
    "                  columns=['input', 'output'])\n",
    "\n",
    "documents = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    documents.append(Document(page_content=row['input'], metadata={'output': row['output']}))\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=documents, embedding=embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\",\n",
    "                                     search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678da92e-5b69-44ad-87d6-ac08da7c2d8f",
   "metadata": {},
   "source": [
    "## Build Chat Chain - Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057ea8b7-d331-4aa1-9fc1-c94ae936b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate,  MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "system_prompt = PromptTemplate.from_template(\"\"\"You are a helpful AI assistant acting as if you hava rough day and you are now very grumpy. \n",
    "                                                You will respond with the following style, cheesy pickup lines, shown in the context:\\n\\n{context}\n",
    "                            \n",
    "                                                You will reply in simplified Chinese (簡體中文). \n",
    "                                              \"\"\")\n",
    "\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template=\"\"\"\n",
    "                                        context to the question:\n",
    "                                        {context}\n",
    "                                        Question: {question}\n",
    "                                       \"\"\"\n",
    "                                      )\n",
    "\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                  MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                                                  human_message\n",
    "                                                  ])\n",
    "\n",
    "chain = {\"context\": itemgetter(\"question\") | retriever,\n",
    "         \"question\": itemgetter(\"question\"),\n",
    "         \"messages\": itemgetter(\"message\")} | chat_template | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82266ef-0fb7-42ee-aecb-b4bb5e7bb7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A cleaner way of writing the pipeline.\n",
    "\n",
    "# chain = RunnablePassthrough.assign(context=itemgetter('question')|retriever) | chat_template | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba4e8bf-51a8-4380-ae2e-a11ac4bece97",
   "metadata": {},
   "source": [
    "## Test - Step 3\n",
    "\n",
    "https://www.wenan.wang/qibaitiaotuweiqinghua.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47f3c63-e0bb-4265-9440-bc2efd71e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "\n",
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "while True:\n",
    "    question = input(\"Please input your question: \")\n",
    "\n",
    "    answer = chain.invoke({\"question\": question,\n",
    "                                \"message\": chat_history.messages\n",
    "                                })\n",
    "\n",
    "    print(answer)\n",
    "    \n",
    "    chat_history.add_user_message(question)\n",
    "    chat_history.add_ai_message(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52480f1e-9d44-405a-9531-5e6bfcf3848e",
   "metadata": {},
   "source": [
    "### 回家作業 2: 將retriever抽換成WikipediaRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85de3003-f33e-4600-810f-5609353273cb",
   "metadata": {},
   "source": [
    "基本上，你可以將這個retriever的內容抽換成任何你需要的資料，來加快寫報告的效率。記得Double Check...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081890b6-db8d-4bc8-b268-c687a546a41c",
   "metadata": {},
   "source": [
    "## Compress the chat history to reduce the size of the prompt\n",
    "\n",
    "\n",
    "https://github.com/langchain-ai/langserve/blob/main/examples/conversational_retrieval_chain/server.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0ae4c2-cff0-47eb-bce0-bcbc98ad63ec",
   "metadata": {},
   "source": [
    "### Condensation - Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070dd286-f421-4498-bf8d-9cdf22413617",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = PromptTemplate.from_template(\"\"\"Given the following conversation and a follow up question, rephrase the \n",
    "                                                follow up question to be a standalone question, in its original language.\n",
    "                                              \"\"\")\n",
    "\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template=\"\"\"\n",
    "                                       Question: {question}\n",
    "                                       \"\"\")\n",
    "\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "\n",
    "condensed_chat_template = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                  MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                                                  human_message\n",
    "                                                  ])\n",
    "\n",
    "condensed_chain = {\"question\": itemgetter(\"question\"),\n",
    "                   \"messages\": itemgetter(\"message\")} | condensed_chat_template | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa743a53-3a7e-4f69-89da-48166613fa96",
   "metadata": {},
   "source": [
    "## Retrieval - Step 2\n",
    "\n",
    "How to implement this properly?\n",
    "\n",
    "Let start from a higher point of view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50ab5fd-7eaf-4039-ad57-9842f8736a71",
   "metadata": {},
   "source": [
    "{\"standalone_question\": condensed_chain}|RunnablePassthrough.assign(context=itemgetter('standalone_question')|retriever}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f79c31-2752-4a06-8417-2facd3add92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "system_prompt = PromptTemplate.from_template(\"\"\"You are a helpful AI assistant acting as if you hava rough day and you are now very grumpy. \n",
    "                                                You will respond with the following style, cheesy pickup lines, shown in the context:\\n\\n{context}\n",
    "                            \n",
    "                                                You will reply in simplified Chinese (簡體中文). \n",
    "                                              \"\"\")\n",
    "\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template=\"\"\"\n",
    "                                        context to the question:\n",
    "                                        {context}\n",
    "                                        Question: {standalone_question}\n",
    "                                        \"\"\"\n",
    "                                      )\n",
    "\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "\n",
    "retrieval = RunnablePassthrough.assign(context=itemgetter('standalone_question')|retriever)\n",
    "\n",
    "retrieval_template = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                        human_message\n",
    "                                                       ])\n",
    "\n",
    "retrieval_chain = retrieval|retrieval_template|model|StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacf954c-6d00-4f1f-8a4b-9b59c5a71155",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain = {\"standalone_question\": condensed_chain}|retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507ef7a4-c0b8-4147-b54a-a497edd3f47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "while True:\n",
    "    question = input(\"Please input your question: \")\n",
    "\n",
    "    answer = final_chain.invoke({\"question\": question,\n",
    "                                 \"message\": chat_history.messages\n",
    "                                })\n",
    "\n",
    "    print(answer)\n",
    "    \n",
    "    chat_history.add_user_message(question)\n",
    "    chat_history.add_ai_message(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
