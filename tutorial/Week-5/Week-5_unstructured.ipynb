{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9AgiUyZ9Ab6N",
    "outputId": "74a9cbfd-be5d-4c8f-8254-7cb6942d9c72"
   },
   "outputs": [],
   "source": [
    "!apt-get update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJxP9HvvDrcQ",
    "outputId": "e04c3e15-a6e7-48f3-cd78-06c818d537ec"
   },
   "outputs": [],
   "source": [
    "!apt-get install poppler-utils libleptonica-dev tesseract-ocr libtesseract-dev python3-pil tesseract-ocr-eng tesseract-ocr-script-latn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PnnjjTV9_hhz",
    "outputId": "1f15aea0-54d9-481f-bc54-82ba716b6362"
   },
   "outputs": [],
   "source": [
    "!pip install unstructured[all-docs] cmake python-dotenv pdf2image python-dateutil faiss-cpu sentence-transformers langchain==0.2.5 langchain-community==0.2.5 langchain-core==0.2.9 langchain-openai==0.1.9 bitsandbytes accelerate xformers triton transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YnbeP7pP5_nc"
   },
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.staging.base import elements_to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMIG3PRN7t76"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "filename = \"2401.00222v1.pdf\"\n",
    "\n",
    "elements = partition_pdf(filename,\n",
    "              chunking_strategy='by_title',\n",
    "              infer_table_structure=True,\n",
    "              extract_image_block_types=['Image'],\n",
    "              max_characters=4000,\n",
    "              new_after_n_chars=3800,\n",
    "              combine_text_under_n_chars=2000,\n",
    "              strategy='hi_res')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LyE7c_2KjxLB"
   },
   "source": [
    "- extract_images_in_pdf:\n",
    "Only applicable if `strategy=hi_res`.\n",
    "If True, any detected images will be saved in the path specified by\n",
    "'extract_image_block_output_dir' or stored as base64 encoded data within metadata fields.\n",
    "Deprecation Note: This parameter is marked for deprecation. Future versions will use\n",
    "'extract_image_block_types' for broader extraction capabilities.\n",
    "\n",
    "- extract_image_block_types:\n",
    "Only applicable if `strategy=hi_res`.\n",
    "Images of the element type(s) specified in this list (e.g., [\"Image\", \"Table\"]) will be\n",
    "saved in the path specified by 'extract_image_block_output_dir' or stored as base64\n",
    "encoded data within metadata fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aew1BKkahQkI"
   },
   "outputs": [],
   "source": [
    "# 影像的位置\n",
    "\n",
    "os.listdir('figures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Xrqybq7lE4X"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- CompositeElement: 文字\n",
    "- Table: 表格\n",
    "\"\"\"\n",
    "\n",
    "elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYkW9dtpnfMY"
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "def credential_init():\n",
    "\n",
    "    credential_file = \"credentials.ini\"\n",
    "\n",
    "    if os.path.exists(credential_file):\n",
    "        credentials = configparser.ConfigParser()\n",
    "        credentials.read(credential_file)\n",
    "        os.environ['OPENAI_API_KEY'] = credentials['openai'].get('api_key')\n",
    "    else:\n",
    "        os.environ['OPENAI_API_KEY'] = os.environ['OPENAI']\n",
    "\n",
    "credential_init()\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "           model_name=\"gpt-4o-2024-05-13\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bK4BsLu-oG1-"
   },
   "outputs": [],
   "source": [
    "prompt = f\"Summarize the following text:\\n\\n{elements[0]}\\n\\nSummary:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bup32QWpoxK_"
   },
   "outputs": [],
   "source": [
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F54V_EaBo0rZ"
   },
   "outputs": [],
   "source": [
    "type(elements[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nITDej3Vo6nv"
   },
   "outputs": [],
   "source": [
    "elements[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aRTgtTkco-fd"
   },
   "outputs": [],
   "source": [
    "prompt = f\"Summarize the following text:\\n\\n{elements[7]}\\n\\nSummary:\"\n",
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aR_DNArbqC9a"
   },
   "source": [
    "## 要如何判別文字或是表格?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EAK2YyWNpWgw"
   },
   "outputs": [],
   "source": [
    "str(type(elements[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZFXWps7qIIB"
   },
   "outputs": [],
   "source": [
    "str(type(elements[7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cYlLFiAq0sp"
   },
   "source": [
    "## 影像，表格，文字 三位一體 檢索系統\n",
    "\n",
    "一個簡單的範例，你當然可以做得很複雜。像是整合text的部分然後用Semantic Splitting拆分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yCO-DUfOrEyL"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "\n",
    "from PIL import Image\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.messages.human import HumanMessage\n",
    "\n",
    "\n",
    "def image_to_base64(image_path):\n",
    "\n",
    "    with Image.open(image_path) as image:\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=\"JPEG\")\n",
    "        image_str = base64.b64encode(buffered.getvalue())\n",
    "    return image_str.decode('utf-8')\n",
    "\n",
    "\n",
    "def get_summary(element, model):\n",
    "\n",
    "    str_type = str(type(element))\n",
    "\n",
    "    if 'CompositeElement' in str_type:\n",
    "        prompt = f\"Summarize the following text:\\n\\n{element}\\n\\nSummary:\"\n",
    "    if 'Table' in str_type:\n",
    "        prompt = f\"Summarize the following table:\\n\\n{element}\\n\\nSummary:\"\n",
    "\n",
    "    response = model.invoke(prompt)\n",
    "\n",
    "    return response.content\n",
    "\n",
    "\n",
    "def get_image_summary(filename, model):\n",
    "\n",
    "    image_str = image_to_base64(filename)\n",
    "\n",
    "    human_message = HumanMessage(content=[{'type': 'text',\n",
    "                    'text': 'What is in this image?'},\n",
    "                    {'type': 'image_url',\n",
    "                     'image_url': {\n",
    "                      'url': f\"data:image/png;base64,{image_str}\"}\n",
    "                    }])\n",
    "\n",
    "    response = model.invoke([human_message])\n",
    "\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2EXHLumu0Gg"
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "\n",
    "for element in elements:\n",
    "\n",
    "    str_type = str(type(element))\n",
    "\n",
    "    summary = get_summary(element, model)\n",
    "\n",
    "    if 'CompositeElement' in str_type:\n",
    "        type_ = 'text'\n",
    "    if 'Table' in str_type:\n",
    "        type_ = 'table'\n",
    "\n",
    "    documents.append(Document(page_content=summary, metadata={'type': type_}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLj3xPNxuxvb"
   },
   "outputs": [],
   "source": [
    "for image_file in os.listdir('figures'):\n",
    "\n",
    "    image_path = f'figures/{image_file}'\n",
    "\n",
    "    summary = get_image_summary(image_path, model)\n",
    "\n",
    "    documents.append(Document(page_content=summary, metadata={'type': 'image', 'filename': image_path}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UDp6GGtIxgZB"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=documents, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L3HP8OWPz3w6"
   },
   "outputs": [],
   "source": [
    "image_retriever = vectorstore.as_retriever(search_kwargs={\"filter\": {'type': 'image'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pYOUPJDA2WeW"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based only on the following context, which can include text, images, and tables:\n",
    "{context}\n",
    "Question: {question}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = {\"context\": retriever, 'question': RunnablePassthrough()}| prompt| model| StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQtXAXz12aA-"
   },
   "outputs": [],
   "source": [
    "chain.invoke(\"What is suggested by the images?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aRID4dk44mRY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
