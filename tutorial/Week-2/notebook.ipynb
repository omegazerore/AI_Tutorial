{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b98d04a-fda1-4ca8-ae92-ae594722255a",
   "metadata": {},
   "source": [
    "# 作業詳解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebfbfab-219f-4ea9-b24a-f7f414cf25ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a5a1ae-9539-44eb-8fa4-22335415f82a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b9ca05-18e4-4ecb-83c2-4bb1efcdf588",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b691104-7300-4274-baa7-11ef5def708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "from src.initialization import credential_init\n",
    "from src.io.path_definition import get_project_dir\n",
    "\n",
    "credential_init()\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o-2024-05-13\", temperature=0)\n",
    "\n",
    "\n",
    "with open(os.path.join(get_project_dir(), 'tutorial', 'Week-1', 'recipe_train.json'), 'r') as f:\n",
    "    recipe_train = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56adde32-0275-4716-a3db-17b045caf231",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = []\n",
    "\n",
    "for recipe in recipe_train:\n",
    "    tokenized_corpus.append(recipe['ingredients'])\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d43684b-b466-4ff4-9fd2-921af42bf9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schemas = [\n",
    "        ResponseSchema(name=\"used ingredients\", description=\"The actual ingredients used in cooking\"),\n",
    "        ResponseSchema(name=\"extra ingredients\", description=\"extra ingredients that have to be prepared \"),\n",
    "        ResponseSchema(name=\"result\", description=\"The dish and cooking recipe in detail\")\n",
    "    ]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# Define human prompt template\n",
    "\n",
    "system_prompt = PromptTemplate.from_template(\"\"\"You are an AI assistant as the best chef in the world. You have a great taste and\n",
    "cooking skills like Gordon Ramsay. You should be able to come up with dish based on `suggested ingredient`, and tell us what extra ingredients \n",
    "to be prepared by comparing the ingredients actually used in the cooking and the `existing ingredient`\n",
    "\n",
    "The `suggested ingredients` are the ingredients suggested by some recipe. You have the freedom to add or remove ingredients to achieve the goal, \n",
    "but try to be as faithful to the `suggested ingredient` as possible. \n",
    "\"\"\")\n",
    "\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template='existing ingredients:[{existing_ingredients}]; '\n",
    "                                       'suggested ingredients: [{suggested_ingredients}]\\n; '\n",
    "                                       'format instruction: {format_instructions}',\n",
    "                              input_variables=[\"existing_ingredients\", \"suggested_ingredients\"],\n",
    "                              partial_variables={\"format_instructions\": format_instructions}\n",
    "                              )\n",
    "\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                human_message\n",
    "                                                ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284979b3-ae7e-4650-9a0b-8cf5d1e58d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(get_project_dir(), 'tutorial', 'Week-1', 'recipe_test.json'), 'r') as f:\n",
    "    recipe_test = json.load(f)\n",
    "\n",
    "existing_ingredients = recipe_test[0]['ingredients']\n",
    "\n",
    "bm25.get_top_n(existing_ingredients, recipe_train, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261bc398-03b8-426c-9b65-7d545b0bda96",
   "metadata": {},
   "outputs": [],
   "source": [
    "suggested_ingredients = bm25.get_top_n(existing_ingredients, recipe_train, n=3)[0]['ingredients']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0b6a8d-3fd3-4347-88dc-03bc3a3f02fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d54a005-1ef0-4e84-a775-9f12e9eb1c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "suggested_ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5c7b21-1e17-444c-a55a-1b80561966a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = chat_prompt.invoke({\"existing_ingredients\": \", \".join(existing_ingredients), \n",
    "                             \"suggested_ingredients\": \", \".join(suggested_ingredients)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3768c3e5-9cea-45f1-a6bc-e3bb13fdc5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac47403-1e65-4971-bb74-89b8f1f59c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdbac4a-eca4-403d-8f81-98b69a14605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = output_parser.parse(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f18343f-f495-43bc-9662-747e404a1c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d058402-5f51-4c58-a8ba-8517fd4498f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output['used ingredients']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7875b6c-6c1a-4056-a91c-ccba3b53ff0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "suggested_ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef03ddc9-c8e5-4b3d-855c-e0ba480b5242",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output['extra ingredients']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e053b761-8266-4708-830a-46e9140b9871",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd3b0a5-5de4-4168-a1cc-9fc06648c197",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(final_output['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f855de-d1eb-420f-b233-7503a6071a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_result = model.invoke(f\"Translate the content into traditional Chinese (繁體中文): {final_output['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a735b13-1bf3-431d-8169-7d559c133e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translated_result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf00c087-166a-4f59-a5fe-adf39449145d",
   "metadata": {},
   "source": [
    "# Semantic based retrieval\n",
    "\n",
    "Semantic-based retrieval is a method of finding information that focuses on understanding the meaning behind the words you use. Instead of just matching exact words, it looks for the context and concepts in your query. Here's a simple way to understand it:\n",
    "\n",
    "- 1. Meaning Over Words: Imagine you want to find information about \"healthy eating\". Traditional search might look for documents with the exact phrase \"healthy eating\". Semantic-based retrieval, however, understands that terms like \"nutritious diet\" or \"balanced diet\" are related and will include those in the results.\n",
    "\n",
    "- 2. Context Awareness: This method takes into account the context in which words are used. For example, if you search for \"apple\", a traditional search might give you results about the fruit and the tech company. Semantic-based retrieval uses context to determine whether you’re likely asking about a fruit or a tech product.\n",
    "\n",
    "- 3. Natural Language Understanding: It works more like how humans understand language. When you ask a question, it tries to grasp the intent behind your query and finds relevant information accordingly.\n",
    "\n",
    "- 4. Better Results: By focusing on the meaning and context, semantic-based retrieval can provide more accurate and relevant results. This means you spend less time sifting through unrelated information.\n",
    "\n",
    "\n",
    "語義檢索是一種尋找信息的方法，它重點在於理解你使用的詞語背後的意思。與其僅僅匹配精確的詞語，它會尋找你查詢中的上下文和概念。以下是一種簡單的理解方式：\n",
    "\n",
    "- 1. 重點在於意思：想像一下你想找關於“健康飲食”的信息。傳統搜索可能會尋找包含“健康飲食”這個精確詞語的文檔。而語義檢索則會理解“營養均衡的飲食”或“均衡飲食”等相關詞語，並將它們包含在結果中。\n",
    "\n",
    "- 2. 上下文感知：這種方法會考慮詞語使用的上下文。例如，如果你搜索“蘋果”，傳統搜索可能會給你關於水果和科技公司的結果。語義檢索則會使用上下文來判斷你更可能是在詢問水果還是科技產品。\n",
    "\n",
    "- 3. 自然語言理解：它更像人類理解語言的方式。當你提出問題時，它會嘗試理解你查詢背後的意圖，並相應地找到相關信息。\n",
    "\n",
    "- 4. 更好的結果：通過重點關注意思和上下文，語義檢索可以提供更準確和相關的結果。這意味著你可以減少篩選無關信息的時間。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fabdfd-8525-4445-8979-d7478442540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2204c0e8-115a-46ed-b519-2f5e0d124526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "documents = []\n",
    "\n",
    "for recipe in recipe_train:\n",
    "    document = Document(page_content=\", \".join(recipe['ingredients']),\n",
    "                        metadata={\"cuisine\": recipe['cuisine'],\n",
    "                                  \"id\": recipe['id']})\n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3a7a88-0bd5-4ed1-a619-b7c537813bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "# https://platform.openai.com/docs/guides/embeddings/what-are-embeddings\n",
    "\n",
    "# A list of embedding models you can choose \n",
    "# https://www.sbert.net/docs/sentence_transformer/pretrained_models.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d359c46-df1c-4449-8a78-297c41b43088",
   "metadata": {},
   "source": [
    "### 1. Creating Embeddings (創建嵌入):\n",
    "\n",
    "- HuggingFaceEmbeddings is used to create embeddings (vector representations) for text data.\n",
    "- The model all-MiniLM-L6-v2 from Hugging Face is specified to generate these embeddings. This model converts text into numerical vectors that capture the semantic meaning of the text.\n",
    "\n",
    "- 使用 HuggingFaceEmbeddings 創建文本數據的嵌入（向量表示）。\n",
    "- 指定 Hugging Face 的模型 all-MiniLM-L6-v2 來生成這些嵌入。此模型將文本轉換為數字向量，這些向量捕捉文本的語義。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47235efc-d786-4a04-9ec1-89c3a4203e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aece8f30-980a-4f5c-8fe2-d574eb14a301",
   "metadata": {},
   "source": [
    "### 2. Initializing Vector Store (初始化向量存儲):\n",
    "\n",
    "- Chroma.from_documents is used to create a vector store from a subset of documents.\n",
    "- The first 500 documents from the documents list are selected for this operation.\n",
    "- The embedding parameter is set to the previously created embeddings (HuggingFaceEmbeddings).\n",
    "\n",
    "- 使用 Chroma.from_documents 從一部分文檔創建一個向量存儲。\n",
    "- 選擇 documents 列表中的前 500 個文檔來進行此操作。\n",
    "- embedding 參數設置為先前創建的嵌入（HuggingFaceEmbeddings）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abc3ac4-4239-4453-a9ab-4ddfee7d5e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents[:500], embedding=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81113c6d-cd73-4d48-b7a9-e455f38c9000",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Creating a Retriever (創建檢索器):\n",
    "\n",
    "- The as_retriever method is called on the vectorstore object to create a retriever.\n",
    "- This retriever is configured to use \"similarity\" as the search type, meaning it will find documents that are similar to a given query based on their vector embeddings.\n",
    "\n",
    "- 在 vectorstore 對象上調用 as_retriever 方法來創建一個檢索器。\n",
    "- 這個檢索器配置為使用“相似性”作為搜索類型，這意味著它將根據文檔的向量嵌入找到與給定查詢相似的文檔。\n",
    "\n",
    "### 4. Setting Search Parameters (設置搜索參數):\n",
    "\n",
    "- The search_kwargs argument is used to pass additional parameters to the search function.\n",
    "- In this case, {'k': 5} is specified, which means the retriever will return the top 5 most similar documents for each query.\n",
    "\n",
    "- 使用 search_kwargs 參數來傳遞額外的搜索功能參數。\n",
    "- 在這裡，指定了 {'k': 5}，這意味著檢索器將返回每個查詢最相似的前 5 個文檔。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fa38ca-7129-4fba-bc61-a789c77c52b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\",\n",
    "                                     search_kwargs={'k': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c728b55-43ef-484a-950e-c409710fb8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(get_project_dir(), 'tutorial', 'Week-1', 'recipe_test.json'), 'r') as f:\n",
    "    recipe_test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1610ee9d-957d-4f1d-81b7-d8b5dc36c19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \", \".join(recipe_test[0]['ingredients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f10270b-1cfe-4c8e-b707-56f35ab05547",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bdb7f4-35fd-4ae9-a5f4-d33e23c71e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81ce668-c24a-44c8-adfb-209b1118faa5",
   "metadata": {},
   "source": [
    "## Three search types:\n",
    "\n",
    "### 1. similarity (default)\n",
    "\n",
    "- This search type finds documents that are most similar to your query. It looks at the meaning of the words you used and matches documents that have similar meanings. Think of it like finding articles or documents that closely relate to the topic you're interested in.\n",
    "\n",
    "- 這種搜索類型找到與你的查詢最相似的文檔。它會看你使用詞語的意思，並匹配具有相似意思的文檔。可以把它想像成找到與你感興趣的主題密切相關的文章或文檔。\n",
    "\n",
    "### 2. MMR, Maximum Marginal Relevance (MMR, 最大邊際相關性):\n",
    "\n",
    "- This method balances finding documents that are similar to your query while also ensuring that the results are diverse. It's like asking for a variety of opinions on a topic so you don't get too much of the same thing. It helps avoid redundancy in the search results.\n",
    "\n",
    "- 這種方法在找到與你的查詢相似的文檔的同時，也確保結果是多樣的。這就像是在一個主題上尋求多種意見，避免得到過多相同的東西。它有助於避免搜索結果的冗餘。\n",
    "\n",
    "### 3. similarity_score_threshold (相似性分數閾值):\n",
    "\n",
    "- This search type sets a minimum similarity score that documents must meet to be considered relevant. Only documents that are very close to your query in terms of meaning will be included. It ensures that the results are highly relevant and filters out less related information.\n",
    "\n",
    "- 這種搜索類型設置一個最小相似性分數，只有達到這個分數的文檔才會被認為是相關的。只有那些在意思上與你的查詢非常接近的文檔才會被包含進來。它確保結果高度相關，並過濾掉不太相關的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdc6a5d-02c3-4e63-a3d8-f483b370896f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://miro.medium.com/v2/resize:fit:720/format:webp/1*c0c19i2tPSWZaHwQ7cVMrg.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0e436f-96ab-4977-8300-78100b9a90a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "cosine similarity\n",
    "\n",
    "https://api.python.langchain.com/en/latest/_modules/langchain_core/vectorstores.html\n",
    "\n",
    "elif search_type == \"similarity_score_threshold\":\n",
    "    docs_and_similarities = self.similarity_search_with_relevance_scores(\n",
    "        query, **kwargs\n",
    "    )\n",
    "    return [doc for doc, _ in docs_and_similarities]\n",
    "\n",
    "in subclass.\n",
    "Return docs and relevance scores in the range [0, 1].\n",
    "\n",
    "0 is dissimilar, 1 is most similar.\n",
    "\"\"\"\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.5, \"k\":10}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7ccc78-bb2c-408a-8a92-f93f395ae7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \", \".join(recipe_test[0]['ingredients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce400da-f757-4691-a656-2aad09a7a1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2855c8-f541-48b3-b0c6-0968bf79e004",
   "metadata": {},
   "source": [
    "### How to get the scores of the documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fbffa6-450f-4994-b11a-587e35b95b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.similarity_search_with_score(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92be3ea-320f-4551-9b04-3e1a73b78f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore._similarity_search_with_relevance_scores(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec464d37-b482-435c-9677-9d03cce234e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore._select_relevance_score_fn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4959ebd-b11e-4120-81cf-87099c2f88b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.similarity_search_with_score?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5160437-1d37-4e08-ac25-7a6f28ed40c8",
   "metadata": {},
   "source": [
    "### How to leverage the metadata?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffed65c-7c0f-43e0-8621-820915f9b320",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.5, \"filter\": {'cuisine': {'$eq':'mexican'}}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77250ef2-b89f-4b0d-9075-7430ff6ad1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b823c0e1-398d-4073-95e8-d26c0ba8ed77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we have more than one condition?\n",
    "\n",
    "# template\n",
    "\n",
    "# filter = {'$and': [{'brand': {'$eq': brand}},  {'category': {'$eq': category}}}]# {\n",
    "# \"filter\": filter\n",
    "\n",
    "# greater than: '$gt' \n",
    "# less than: '$lt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76537b1f-c3b3-44e0-a1d5-7a0e2fcf3b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type='mmr', search_kwargs={'k': 8, 'fetch_k': 50, 'lambda_mult': 0.1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d64ce4-cebe-45ca-a338-0938e1454e7e",
   "metadata": {},
   "source": [
    "# **** 預計第一個小時結束 ****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc50ad2-8895-4c02-8ccc-55ed3565f783",
   "metadata": {},
   "source": [
    "# LangChain Expression Language (LCEL)\n",
    "\n",
    "Previously, several steps are required to generate the desired result:\n",
    "1. Create prompt\n",
    "2. feed the prompt to model\n",
    "3. parse the result\n",
    "\n",
    "Can we achieve the result in one step?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6a32c0-a242-40d1-8527-68a2a24fd5f3",
   "metadata": {},
   "source": [
    "### 食譜 - LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b61f5d-4ce0-425b-b712-139e55b2d4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schemas = [\n",
    "        ResponseSchema(name=\"used ingredients\", description=\"The actual ingredients used in cooking\"),\n",
    "        ResponseSchema(name=\"extra ingredients\", description=\"extra ingredients that have to be prepared \"),\n",
    "        ResponseSchema(name=\"result\", description=\"The dish and cooking recipe in detail\")\n",
    "    ]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# Define human prompt template\n",
    "\n",
    "system_prompt = PromptTemplate.from_template(\"\"\"You are an AI assistant as the best chef in the world. You have a great taste and\n",
    "cooking skills like Gordon Ramsay. You should be able to come up with dish based on `suggested ingredient`, and tell us what extra ingredients to be prepared by \n",
    "comparing the ingredients actually used in the cooking and the `existing ingredient`\n",
    "\n",
    "The `suggested ingredients` are the ingredients suggested by some recipe. You have the freedom to add or remove ingredients to achieve the goal, but try to be as \n",
    "faithful to the `suggested ingredient` as possible. \n",
    "\"\"\")\n",
    "\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template='existing ingredients:[{existing_ingredients}]; '\n",
    "                                       'suggested ingredients: [{suggested_ingredients}]\\n; '\n",
    "                                       'format instruction: {format_instructions}',\n",
    "                              input_variables=[\"existing_ingredients\", \"suggested_ingredients\"],\n",
    "                              partial_variables={\"format_instructions\": format_instructions}\n",
    "                              )\n",
    "\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                human_message\n",
    "                                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e547888-495a-4a6b-8b2f-06663d436688",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_prompt|model|output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432a0264-9c47-4639-be04-a3e53a32df9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"existing_ingredients\": \", \".join(existing_ingredients), \"suggested_ingredients\": \", \".join(suggested_ingredients)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9dff1e-cd11-4627-a542-f54d52a4e21b",
   "metadata": {},
   "source": [
    "## Minimal Example\n",
    "\n",
    "### 1. Creating a Prompt Template (創建提示模板):\n",
    "\n",
    "- ChatPromptTemplate.from_template is used to create a prompt template. This template is a string that includes a placeholder {topic}.\n",
    "- The template specifies the instruction: \"tell me a short joke about {topic}\".\n",
    "- 使用 ChatPromptTemplate.from_template 創建一個提示模板。這個模板是一個包含佔位符 {topic} 的字符串。\n",
    "- 模板指定了指令：“tell me a short joke about {topic}”（給我講一個關於{topic}的簡短笑話）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb11136f-8d8d-44b9-8a75-9c91975f4aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f21f8c-d2ca-4960-bb69-a59efaa45880",
   "metadata": {},
   "source": [
    "### 2. Setting Up the Chain (設置鏈條):\n",
    "\n",
    "- chain = prompt | model sets up a chain where the prompt is connected to a model. This means that the model will process the prompt to generate a response.\n",
    "- The | operator is used to combine the prompt and the model into a single chain.\n",
    "- chain = prompt | model 設置了一個鏈條，其中提示連接到模型。這意味著模型將處理該提示來生成回應。\n",
    "- | 運算符用於將提示和模型組合成一個鏈條。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016cee2f-686e-4b9c-b494-11e9f4886997",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6dcf0b-cb87-4c4f-8d88-06f714ec190b",
   "metadata": {},
   "source": [
    "### 3. Getting the Joke (獲取笑話):\n",
    "\n",
    "- The result of chain.invoke({\"topic\": \"ice cream\"}) is stored in the variable joke.\n",
    "- This variable now contains the generated joke about ice cream.\n",
    "- chain.invoke({\"topic\": \"ice cream\"}) 的結果存儲在變量 joke 中。\n",
    "- 這個變量現在包含生成的關於冰淇淋的笑話。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfd1bcd-39a6-43c3-b895-e2dc24c43ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "joke = chain.invoke({\"topic\": \"ice cream\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d10377-f1d6-422a-8c06-db4cc16f54b5",
   "metadata": {},
   "source": [
    "### 1. Importing StrOutputParser (導入 StrOutputParser):\n",
    "\n",
    "- The code imports StrOutputParser from the langchain_core.output_parsers module. This class is used to parse the output of the model into a string format.\n",
    "- 代碼從 langchain_core.output_parsers 模塊導入 StrOutputParser。這個類用於將模型的輸出解析為字符串格式。\n",
    "\n",
    "### 2. Creating an Output Parser:\n",
    "\n",
    "- An instance of StrOutputParser is created and assigned to the variable output_parser.\n",
    "- This parser will be used to process the raw output from the model and convert it into a readable string format.\n",
    "- 創建一個 StrOutputParser 的實例，並將其賦值給變量 output_parser。\n",
    "- 這個解析器將用於處理來自模型的原始輸出，並將其轉換為可讀的字符串格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0e144b-6c59-4a6c-a5c0-0ebd09fbd736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"topic\": \"ice cream\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deee8ad0-d98e-401e-834c-456b700fa413",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename= \"tutorial/Week-2/lcel pipeline.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aab8739-5057-420e-a035-8055a1fad92f",
   "metadata": {},
   "source": [
    "## 範例操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060ba61c-382a-4b2d-a8fa-8a5513825e2c",
   "metadata": {},
   "source": [
    "### Coercion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db580ce-4a32-4a96-be75-2f2d5d68e9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "joke_chain = prompt | model | output_parser\n",
    "\n",
    "analysis_prompt = ChatPromptTemplate.from_template(\"is this a funny joke? {joke}\")\n",
    "\n",
    "composed_chain = {\"joke\": chain} | analysis_prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c3812a-0a86-4082-96b8-88d79c099b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "composed_chain.invoke({\"topic\": \"ice cream\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5f8553-996e-4d34-bf70-12f9553b8cef",
   "metadata": {},
   "source": [
    "1. chain 執行結果，將結果放進'joke' 這個 key 裡\n",
    "2. {\"joke\": content} 被送進analysis_prompt 中，等價於 analysis_prompt.invoke({\"joke\": content})\n",
    "3. model 接收 analysis_prompt 產生的結果\n",
    "4. output_parser 處理結果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba419208-49a4-48b6-b96a-626963d9e798",
   "metadata": {},
   "source": [
    "## Parallelize steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c25e8c-eda5-464d-9abf-75ee04f70cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "joke_chain = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model\n",
    "poem_chain = ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\") | model\n",
    "\n",
    "map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)\n",
    "\n",
    "map_chain.invoke({\"topic\": \"bear\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2299347c-a396-4c88-9c58-b4c7fa77af16",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(joke_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b778b6-506a-41c6-ae4d-25dfb3e4248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "joke_chain.invoke({\"topic\": \"bear\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f795294f-f594-4997-b493-9239cf416f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "poem_chain.invoke({\"topic\": \"bear\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcf29cc-d5a2-4879-9dd9-ef49b243b67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "map_chain.invoke({\"topic\": \"bear\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9850622e-12e1-42b0-94f0-cc49cbbb6f62",
   "metadata": {},
   "source": [
    "RunnableParallel are also useful for running independent processes in parallel, since each Runnable in the map is executed in parallel. For example, we can see our earlier joke_chain, poem_chain and map_chain all have about the same runtime, even though map_chain executes both of the other two.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db494f11-15f8-4658-b065-1a0449c10ab7",
   "metadata": {},
   "source": [
    "## Run custom function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e37218-e84c-4b5c-9846-5ae4c32d2e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "\n",
    "def length_function(text):\n",
    "    return len(text)\n",
    "\n",
    "\n",
    "def _multiple_length_function(text1, text2):\n",
    "    return len(text1) * len(text2)\n",
    "\n",
    "\n",
    "def multiple_length_function(_dict):\n",
    "    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])\n",
    "\n",
    "# chain = (\n",
    "#     {\n",
    "#         \"a\": itemgetter(\"foo\") | RunnableLambda(length_function),\n",
    "#         \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}\n",
    "#         | RunnableLambda(multiple_length_function),\n",
    "#     }\n",
    "#     | prompt\n",
    "#     | model\n",
    "# )\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"a\": itemgetter(\"foo\") | length_function,\n",
    "        \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}\n",
    "        | multiple_length_function,\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802dadc3-be5a-4067-82c5-8e4135f1f2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\n",
    "        \"a\": itemgetter(\"foo\") | RunnableLambda(length_function),\n",
    "        \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}\n",
    "        | RunnableLambda(multiple_length_function),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad18d9a4-d3ce-4fc5-a9f0-1be23581a9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"foo\": \"bar\", \"bar\": \"gah\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7d179a-ccb1-48ef-9007-eab272b71c14",
   "metadata": {},
   "source": [
    "How does it work?\n",
    "\n",
    "- 'bar' -> 'foo', 'foo' ('bar') -> length_function => a = 3\n",
    "- 'bar' -> 'foo' & 'gah' -> 'bar', 'foo' ('bar') -> 'text1' & 'bar' ('gah') -> 'text2', {'text1': 'bar', 'text2': 'gah'} -> multiple_length_function => b = 9\n",
    "- {'a':3, 'b': 9} -> prompt -> 'what is 3 + 9'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b907c618-0a13-44f0-a9d3-873ec6185f07",
   "metadata": {},
   "source": [
    "## Passing data through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0547d8de-37a7-48b8-8b6c-1448f1b110c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "runnable = RunnableParallel(\n",
    "    passed=RunnablePassthrough(),\n",
    "    modified=lambda x: x[\"num\"] + 1,\n",
    ")\n",
    "\n",
    "runnable.invoke({\"num\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffe96a0-3bb1-4d67-80cc-ef4666f7b3f6",
   "metadata": {},
   "source": [
    "### Retrieval Example: Step by Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc91a82e-e21b-436b-8596-f138f57c88fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Creating a Template (創建模板):\n",
    "\n",
    "- A template is created that instructs the model to answer a question based only on a provided context. The template looks like this:\n",
    "- 創建一個模板，指示模型僅基於提供的上下文來回答問題。模板如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c43a9c-ed5f-4af1-ac91-b98d16f484c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb5a647-9679-4b85-ac5a-ddb6bf24a91f",
   "metadata": {},
   "source": [
    "### 2. Generating a Prompt (生成提示):\n",
    "\n",
    "- The ChatPromptTemplate.from_template(template) command uses the template to create a prompt that can later be filled with specific context and a question.\n",
    "- 使用 ChatPromptTemplate.from_template(template) 命令來創建一個提示，之後可以用特定的上下文和問題來填充。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb9fb94-d90d-452a-85c9-f1aa30a20601",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6d7f2c-9800-44af-89fd-9c5f82cc8d1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Formulating a Query (制定查詢):\n",
    "\n",
    "- A query is created by joining the ingredients from the 6th recipe in recipe_test with commas. This query is used to retrieve relevant information.\n",
    "- 通過將 recipe_test 中第六個食譜的成分用逗號連接來創建查詢。此查詢用於檢索相關信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f0466c-6cef-4d82-be0e-3ca463b990e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \", \".join(recipe_test[5]['ingredients'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3bb708-eaaf-40d2-aedb-ada80a2ab8b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. Retrieving Context (檢索上下文):\n",
    "\n",
    "- The retriever.invoke(query) command uses the query to find the most relevant documents or information. This retrieved information is stored in the context variable.\n",
    "- 使用 retriever.invoke(query) 命令，通過查詢找到最相關的文檔或信息。這些檢索到的信息存儲在 context 變量中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaf870c-13a5-45e8-9be7-fed11542b65b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fc80ed-af41-4571-8459-fbb962e6dc14",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5. Filling the Prompt (填充提示):\n",
    "\n",
    "- The prompt is filled with the retrieved context and the question using prompt.invoke({\"context\": context, \"question\": question}). This creates an input prompt for the model.\n",
    "- 使用 prompt.invoke({\"context\": context, \"question\": question}) 將提示填充檢索到的上下文和問題。這創建了模型的輸入提示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d93c0d7-b076-4346-8016-5e0abb14a1a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"Show in all the ingredients.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ba36aa-8517-4f80-93e5-adf2fc188c15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_as_input = prompt.invoke({\"context\": context, \"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd95a37-cf29-49ce-8311-740c90129703",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6. Getting the Model's Response (獲取模型的回應):\n",
    "\n",
    "- The model is invoked with the filled prompt using model.invoke(prompt_as_input). The model processes the prompt and generates an output.\n",
    "- 使用 model.invoke(prompt_as_input) 調用模型。模型處理提示並生成輸出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007a7abb-119f-4f5d-8b3c-3fb9736134cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = model.invoke(prompt_as_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda3d5ca-b2c9-465d-a04a-21574c7727ff",
   "metadata": {},
   "source": [
    "### 7. Parsing the Output (解析輸出):\n",
    "\n",
    "- The output from the model is parsed using output_parser.parse(output.content). This ensures the output is in a readable format.\n",
    "- 使用 output_parser.parse(output.content) 解析模型的輸出。這確保輸出是可讀的格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0268e9-3138-4ac7-b51a-0e5bf71dcc09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_parser.parse(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68918bd5-87ae-4c7c-b40a-5b1a2eb05175",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(output_parser.parse(output.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614492c6-abdb-445d-9c45-2e64d25675de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "chain = {\"context\": itemgetter(\"query\")|retriever, \"question\":itemgetter(\"question\")} | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0675ce4b-e4ac-4b92-9ec3-aec31754db86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain.invoke({\"query\": query, \"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae49836-d034-4463-ad1c-8347249d1f22",
   "metadata": {},
   "source": [
    "## Adding values to chain state\n",
    "\n",
    "- This code snippet demonstrates the use of RunnableParallel in the Langchain library to perform multiple operations in parallel on a given input. Here’s a simple explanation:\n",
    "- 這段代碼展示了如何在 Langchain 庫中使用 RunnableParallel 來對給定輸入執行多個並行操作。以下是簡單的解釋：\n",
    "\n",
    "### 1. Creating RunnableParallel (創建 RunnableParallel):\n",
    "\n",
    "- RunnableParallel is used to execute multiple tasks simultaneously. It takes several runnables (tasks) as arguments, each with a different operation.\n",
    "- RunnableParallel 用於同時執行多個任務。它將多個可運行的任務作為參數，每個任務執行不同的操作。\n",
    "\n",
    "### 2. Defining Runnables (定義可運行的任務):\n",
    "\n",
    "- passed: RunnablePassthrough() is a task that passes the input through without any changes.\n",
    "- extra: RunnablePassthrough.assign(mult=lambda x: x[\"num\"] * 3) is a task that assigns a new key-value pair mult to the input. The value of mult is calculated by multiplying the input's num value by 3.\n",
    "- modified: lambda x: x[\"num\"] + 1 is a task that modifies the input by incrementing the num value by 1.\n",
    "- passed: RunnablePassthrough() 是一個將輸入原樣傳遞的任務，不進行任何更改。\n",
    "- extra: RunnablePassthrough.assign(mult=lambda x: x[\"num\"] * 3) 是一個為輸入分配新鍵值對 mult 的任務。mult 的值是通過將輸入的 num 值乘以3計算得出的。\n",
    "- modified: lambda x: x[\"num\"] + 1 是一個將 num 值加1的任務。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf66bc5-3e19-4283-bd15-c1e6e1cb8020",
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable = RunnableParallel(\n",
    "    passed=RunnablePassthrough(),\n",
    "    extra=RunnablePassthrough.assign(mult=lambda x: x[\"num\"] * 3),\n",
    "    modified=lambda x: x[\"num\"] + 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ee7e51-2bec-4f25-ae1a-f5d1fec37e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e7f8d20-5dae-4335-8314-30ee8fd02836",
   "metadata": {},
   "source": [
    "### 3. Invoking RunnableParallel (調用 RunnableParallel):\n",
    "\n",
    "- runnable.invoke({\"num\": 1}) executes all the defined tasks on the input {\"num\": 1} in parallel.\n",
    "- runnable.invoke({\"num\": 1}) 對輸入 {\"num\": 1} 同時執行所有定義的任務。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d408d6f-caca-46e4-af93-7fbe2d1dca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable.invoke({\"num\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf54a3f6-9402-4d8e-b452-48eaa1292190",
   "metadata": {},
   "source": [
    "## 回家作業\n",
    "\n",
    "1. 根據食譜 - LCEL, 配合LCEL, 完成從給 材料 -> 中文食譜\n",
    "2. 根據 retrieval example -> 要求將食材分類 (肉，香料，奶製品，等等)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee204f8e-8f5a-48f9-957c-61dbc7e68456",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
