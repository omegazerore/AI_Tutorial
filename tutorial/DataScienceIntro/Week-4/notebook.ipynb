{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce3236e5-2c12-4059-a843-d7582becf226",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "\n",
    "TL;DR\n",
    "Given data $X$, find a function $f$ which maps $X$ on to $y$:\n",
    "\n",
    "$$f: X -> y$$\n",
    "\n",
    "\n",
    "Supervised learning is a type of machine learning where the goal is to learn a mapping from input data $X$ to an output $y$. Given a dataset of input-output pairs, the objective is to find a function $f$ that best maps each input $X$ to its corresponding output $y$. This function $f$ can then be used to predict the output for new, unseen inputs.\n",
    "\n",
    "- Key Concepts:\n",
    "    - Training Data: A set of input-output pair ($X$, $y$) used to train the model. The input $X$ can be a single feature or a set of features, and $y$ is the corresponding target value.\n",
    "    - Function $f$: The model or algorithm that maps inputs to outputs $y$. This function is learned from the training data.\n",
    "    - Prediction: Once the function $f$ is learned, it can be used to predict the output $\\hat{y}$ for new input data.\n",
    "\n",
    "- Types of Supervised Learning:\n",
    "    - Regression: When the target variable $y$ is continuous. For example, predicting house prices based on features like size and location.\n",
    "    - Classification: When the target variable $y$ is categorical. For example, classifying emails as spam or not spam.\n",
    "\n",
    "- Example:\n",
    "    - Regression: Given a dataset of house prices with features such as size, number of bedrooms, and location, supervised learning can be used to predict the price of a house based on these features. The function $f$ would map the features (input $X$) to the price (output $y$).\n",
    "\n",
    "    - Classification: Given a dataset of emails labeled as \"spam\" or \"not spam,\" supervised learning can be used to classify new emails. The function $f$ would map the email features (input $X$) to the categories \"spam\" or \"not spam\" (output $y$).\n",
    "\n",
    "- Process of Supervised Learning:\n",
    "    - Data Collection: Gather a dataset with input-output pairs.\n",
    "    - Data Preprocessing: Clean and prepare the data for training.\n",
    "    - Model Selection: Choose an appropriate algorithm to learn the mapping function $f$.\n",
    "    - Training: Use the training data to learn the function $f$.\n",
    "    - Evaluation: Assess the model's performance using a separate set of data (validation or test data).\n",
    "    - Prediction: Use the trained model to make predictions on new data.\n",
    "\n",
    "Supervised learning is widely used in various applications, such as image recognition, natural language processing, and predictive analytics, due to its effectiveness in learning from labeled data and making accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49f630f-bb6e-4a00-84e2-4b04660e3caf",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90fc190-9bdc-4fd1-9aa5-122edf6c91b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bba9292-0897-455e-af72-bd54a48e26e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "# url=\"https://online.stat.psu.edu/stat462/sites/onlinecourses.science.psu.edu.stat462/files/data/skincancer/index.txt\"\n",
    "# r = requests.get(url, allow_redirects=True)\n",
    "# with open('index.txt', 'wb') as f:\n",
    "#     f.write(r.content)\n",
    "    \n",
    "df = pd.read_csv('index.txt', delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68495525-7626-4469-a17b-8d3963bbd071",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488645a4-c031-4af4-9428-7ae5704a3f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f176bc-3cc2-4f51-a818-2f452e51a1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearRegression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4625b75-7617-45be-b73d-3efc2b18245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Lat']]\n",
    "y = df['Mort']\n",
    "\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d4d718-b7d0-404d-9237-5910b198ab42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mort_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4c4894-4ce5-4849-9aff-3295514b743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(df['Lat'], df['Mort'], label='observation')\n",
    "ax.scatter(df['Lat'], mort_pred, label='fit')\n",
    "\n",
    "ax.set_xlabel('Lat')\n",
    "ax.set_ylabel('Mort')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb12a5a5-5b42-4265-8598-9cadacef14df",
   "metadata": {},
   "source": [
    "### Metrics:\n",
    "\n",
    "#### Mean Squared Error (MSE)\n",
    "\n",
    "MSE measures the average squared difference between the observed actual outcomes (true values) and the predictions made by the model. It gives an idea of how well the model's predictions approximate the actual data points. A lower MSE indicates a better fit.\n",
    "\n",
    "- Formula:\n",
    "$$\\text{MSE} = \\frac{1}{n}\\sum^{n}_{i=1}(y_i-\\hat{y}_i)^2$$\n",
    "\n",
    "where:\n",
    "<ul style=\"padding-left: 40px;\">\n",
    "    <li>$n$ is the number of observations</li>\n",
    "    <li>$y_i$ is the actual value</li>\n",
    "    <li>$\\hat{y}_i$ the predicted value</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "- Interpretation:\n",
    "    - Low MSE: Indicates that the model's predictions are close to the actual values.\n",
    "    - High MSE: Indicates that there is a significant difference between the predicted and actual values, suggesting a poor fit.\n",
    "    \n",
    "MSE provides an absolute measure of the average squared difference between actual and predicted values. It is useful for understanding the model's prediction accuracy in terms of error magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2968f41d-9a49-465f-81ff-3608bb5aca9b",
   "metadata": {},
   "source": [
    "#### $R^2$ Score\n",
    "\n",
    "The $R^2$ score is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model. It indicates how well the independent variables explain the variability of the dependent variable.\n",
    "\n",
    "- Formula:\n",
    "$$R^2 = 1 - \\frac{\\sum^{n}_{i=1}(y_i-\\hat{y}_i)^2}{\\sum^{n}_{i=1}(y_i-\\bar{y}_i)^2}$$\n",
    "\n",
    "where:\n",
    "<ul style=\"padding-left: 40px;\">\n",
    "    <li>$y_i$ is the actual value</li>\n",
    "    <li>$\\hat{y}_i$ is the predicted value</li>\n",
    "    <li>$\\bar{y}_i$ is the mean of the actual values</li>\n",
    "    <li>$\\sum^{n}_{i=1}(y_i-\\hat{y}_i)^2$ is the sum of squares of residuals</li>\n",
    "    <li>$\\sum^{n}_{i=1}(y_i-\\bar{y}_i)^2$ is the total sum of squares</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "- Interpretation:\n",
    "    - $R^2=1$: The model perfectly predicts the dependent variable.\n",
    "    - $R^2=0$: The model does not explain any variability in the dependent variable.\n",
    "    - $R^2<0$: The model performs worse than a horizontal line (mean of actual values), indicating poor predictive ability.\n",
    "    \n",
    "$R^2$ Score provides a relative measure of how well the model explains the variance in the dependent variable compared to the mean model (predicting the mean of actual values). It helps understand the goodness-of-fit of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0c31bc-35c2-4872-9925-e07f1b8a8e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "mse = mean_squared_error(mort_pred, df['Mort'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020fb37c-e90b-4339-9adc-16a5f6f03cc2",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e206cc-7b64-4d51-ad93-6a9bbf0394f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://files.realpython.com/media/log-reg-1.e32deaa7cbac.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd76220-5666-49e2-8048-c101ebab36aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://raw.githubusercontent.com/johnmyleswhite/ML_for_Hackers/master/02-Exploration/data/01_heights_weights_genders.csv\"\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "with open('heights_weights_genders.csv', 'wb') as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "df = pd.read_csv('heights_weights_genders.csv', sep=',')\n",
    "\n",
    "X = df[['Height', 'Weight']]\n",
    "y = df['Gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc779a-1e63-442a-9c78-a26d0c38561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2616bc-f2ae-4333-ba29-31065c860bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3177d6c6-6cc8-470b-8596-6157c192e018",
   "metadata": {},
   "source": [
    "### Metrics:\n",
    "\n",
    "#### Confusion Matrix\n",
    "\n",
    "A confusion matrix is a table used to evaluate the performance of a classification algorithm. It summarizes the counts of true positive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions.\n",
    "\n",
    "||   0   |   1   |\n",
    "|---|---|---|\n",
    "|  0   |   TN  |   FP  |\n",
    "|  1   |   FN  |   TP  |\n",
    "\n",
    "- True Positives (TP): The number of instances correctly predicted as positive.\n",
    "- True Negatives (TN): The number of instances correctly predicted as negative.\n",
    "- False Positives (FP): The number of instances incorrectly predicted as positive (Type I error).\n",
    "- False Negatives (FN): The number of instances incorrectly predicted as negative (Type II error).\n",
    "\n",
    "Provides a detailed breakdown of correct and incorrect classifications.\n",
    "\n",
    "#### Precision\n",
    "Precision (also called Positive Predictive Value) is the ratio of correctly predicted positive observations to the total predicted positives. It tells us how many of the predicted positive cases were actually positive.\n",
    "\n",
    "$$\\text{Precision}=\\frac{TP}{TP+FP}$$\n",
    "\n",
    "Measures the accuracy of the positive predictions.\n",
    "\n",
    "#### Recall\n",
    "Recall (also called Sensitivity or True Positive Rate) is the ratio of correctly predicted positive observations to the all observations in the actual class. It tells us how many of the actual positive cases were correctly predicted.\n",
    "\n",
    "$$\\text{Recall}=\\frac{TP}{TP+FN}$$\n",
    "\n",
    "Measures the ability to capture all positive instances.\n",
    "\n",
    "#### F1-Score\n",
    "The F1-score is the harmonic mean of Precision and Recall. It provides a balance between Precision and Recall, especially useful when the class distribution is imbalanced.\n",
    "\n",
    "$$\\text{F1-Score}=2 * \\frac{\\text{Precision} * \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
    "\n",
    "Balances precision and recall, useful for imbalanced datasets.\n",
    "\n",
    "#### Accuracy\n",
    "Accuracy is the ratio of correctly predicted observations to the total observations. It gives an overall effectiveness of the classifier.\n",
    "\n",
    "$$\\text{Accuracy}=\\frac{TP+TN}{TP+TN+FP+FN}$$\n",
    "\n",
    "Overall correctness of the model, but can be misleading for imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d643a64f-95df-4d58-bbe4-9d8a27759a1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, accuracy_score, f1_score, precision_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855f5726-15ec-4d13-afe3-9a244448bddf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "b = model.intercept_[0]\n",
    "w1, w2 = model.coef_.T\n",
    "c = -b/w2\n",
    "m = -w1/w2\n",
    "\n",
    "xmin, xmax = 50, 80\n",
    "ymin, ymax = 50, 300\n",
    "xd = np.array([xmin, xmax])\n",
    "yd = m*xd + c\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "male = df[df['Gender']=='Male']\n",
    "female = df[df['Gender']=='Female']\n",
    "\n",
    "ax.plot(xd, yd, 'k', lw=1, ls='--', label='boundary')\n",
    "ax.scatter(male['Height'], male['Weight'], c='b', alpha=0.5, label='Male')\n",
    "ax.scatter(female['Height'], female['Weight'], c='r', alpha=0.5, label='Female')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f40ac0-4038-40de-b477-a57c0cfbad98",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "A Decision Tree is a supervised learning algorithm used for both classification and regression tasks. It works by splitting the data into subsets based on the most significant attributes, creating a tree-like model of decisions. Each internal node of the tree represents a decision based on a feature, each branch represents an outcome of the decision, and each leaf node represents a class label (for classification) or a continuous value (for regression).\n",
    "\n",
    "- Key Concepts:\n",
    "    - Root Node: Represents the entire dataset, which is then split into two or more homogeneous sets.\n",
    "    - Splitting: The process of dividing a node into two or more sub-nodes.\n",
    "    - Decision Node: When a sub-node splits into further sub-nodes.\n",
    "    - Leaf/Terminal Node: Nodes that do not split further.\n",
    "    - Pruning: The process of removing sub-nodes to prevent overfitting.\n",
    "    - Entropy and Gini Index: Measures to find the best split.\n",
    "\n",
    "- Steps in Building a Decision Tree:\n",
    "    - Select the Best Attribute: Use a measure like entropy or Gini index to find the attribute that best separates the dataset.\n",
    "    - Split the Dataset: Divide the dataset into subsets based on the best attribute.\n",
    "    - Repeat the Process: Apply the process recursively for each subset until stopping criteria (e.g., maximum depth, minimum samples per leaf) are met.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd22812b-ef7d-449c-a2fe-f5d5131a20e7",
   "metadata": {},
   "source": [
    "Example with the Iris Dataset\n",
    "\n",
    "- Let's use the DecisionTreeClassifier from Scikit-Learn to create a decision tree based on the famous Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44022dbc-56f6-45a9-9f9f-f25e1595e6a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Initialize and train the Decision Tree classifier\n",
    "clf = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb57d0b-d9ad-45b9-9c64-b4b4db6e9437",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the decision tree\n",
    "plt.figure(figsize=(15,10))\n",
    "tree.plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf9b421-daf9-4712-a7ae-9b5ab816b5d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
