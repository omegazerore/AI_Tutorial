{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OCMrly7znijN",
    "outputId": "f961c609-c316-4327-eaaa-7fee1eaf659d"
   },
   "outputs": [],
   "source": [
    "!pip install -qU langchain==0.2.5 langchain-community==0.2.5 langchain-core==0.2.9 langchain-openai==0.1.9 bitsandbytes accelerate xformers triton transformers torch==2.3.0 sentence-transformers chromadb datasets peft trl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Db48jkPRsOPn"
   },
   "source": [
    "- llama3-8B is the base model which basically just do the completions to the input prompt, But llama3-8B Instruct is finetuned for instruction following and multi-turn conversation templates for assistant completions as chat response.\n",
    "\n",
    "- If your specific purpose is for chat completions then instruct is the best choice otherwise if it is for simple completions of input then base model is fine. But there might be a chance for the model to continue generation till max_seq_len is achieved while generating while using base model.\n",
    "\n",
    "- Llama 3 8B model has a knowledge cut-off of March, 2023.\n",
    "- Llama 3 70B model has a knowledge cut-off of December, 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hGWkwUcsOPo"
   },
   "source": [
    "# Use the one directly from meta\n",
    "\n",
    "## Create the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing Libraries:\n",
    "\n",
    "- Various necessary modules from the Transformers library and PyTorch are imported. langchain.llms is also imported for integration with LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from torch import cuda\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, pipeline, StoppingCriteria, StoppingCriteriaList, AutoConfig, AutoModelForCausalLM\n",
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_token_ids = None\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "HF_TOKEN = \"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\"\n",
    "\n",
    "device = f\"cuda:{cuda.current_device()}\" if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bits and Bytes Configuration:\n",
    "\n",
    "- Configures the model to use 4-bit quantization to reduce memory usage and computation cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating the Tokenizer:\n",
    "\n",
    "- Defines a function to create a tokenizer from a pre-trained model. It also sets up stop token IDs which are sequences of tokens that, when encountered, will stop the text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer():\n",
    "\n",
    "    global stop_token_ids\n",
    "\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_id, use_auth_token=HF_TOKEN)\n",
    "    stop_list = ['\\nHuman:', '\\n```\\n']\n",
    "\n",
    "    stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
    "    stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
    "    stop_token_ids = stop_token_ids\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stopping Criteria Class:\n",
    "\n",
    "- Custom stopping criteria class that stops text generation when certain sequences of tokens (stop tokens) are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gE-3a3tEqCah",
    "outputId": "3ffb43ec-a327-4489-d42e-fb3405caa955"
   },
   "outputs": [],
   "source": [
    "class StopOnTokens(StoppingCriteria):\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "\n",
    "    global stop_token_ids\n",
    "\n",
    "    # print(f\"input_ids: {input_ids}\")\n",
    "    # print(f\"content: { tokenizer.decode(input_ids[0])}\")\n",
    "    for stop_ids in stop_token_ids:\n",
    "        if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "              # print(\"Stopping\")\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Loading the Model Configuration and Model:\n",
    "\n",
    "- Loads the configuration and the model from Hugging Face with specified parameters, including the bits and bytes configuration for quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=HF_TOKEN\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    token=HF_TOKEN,\n",
    "    config=model_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "4d5055a325e14b7daf8d7f7e8d3d0808",
      "1f3e1298ba37428cbe2790a94661168c",
      "b6bbfe73158543c28b24c16af9873364",
      "f0670206f2644defabb694550e432649",
      "b2cdb066fea0418ebffa81c2351dc8c7",
      "f6131cd342d54330819bc84f973458c5",
      "311c869724dc4520912fe2846cf16b23",
      "e19cd3bc75ae4fe59fcc9f9301c108ce",
      "cb4beea6f2f94cb9a7ee688e049f0feb",
      "e3f4e1e7aacc465cac82a6a2fc55222b",
      "56fc5f0cded0480f8f5441e197490f1e"
     ]
    },
    "id": "JYNlEqm0CbQn",
    "outputId": "0e5a4004-acb0-41b8-9806-933d23fa66ed"
   },
   "source": [
    "### 6. Setting Up Stopping Criteria:\n",
    "\n",
    "- Creates a list of stopping criteria with the custom StopOnTokens class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Tokenizing and Generating Text:\n",
    "\n",
    "- Initializes the tokenizer, sets padding token, and defines terminators. Then it sets up a text generation pipeline with specific parameters like temperature, max new tokens, and stopping criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Up9JgHA7Ctop",
    "outputId": "22e0cd1a-9b09-401e-f83c-0dce1794b72a"
   },
   "outputs": [],
   "source": [
    "tokenizer = create_tokenizer()\n",
    "\n",
    "\"\"\"\n",
    "Source Code:\n",
    "\n",
    "if generation_config.pad_token_id is None and generation_config.eos_token_id is not None:\n",
    "    if model_kwargs.get(\"attention_mask\") is None:\n",
    "        logger.warning(\n",
    "            \"The attention mask and the pad token id were not set. As a consequence, you may observe \"\n",
    "            \"unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\"\n",
    "        )\n",
    "    eos_token_id = generation_config.eos_token_id\n",
    "    if isinstance(eos_token_id, list):\n",
    "        eos_token_id = eos_token_id[0]\n",
    "    logger.warning(f\"Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.\")\n",
    "    generation_config.pad_token_id = eos_token_id\n",
    "\n",
    "Discussion on the eos_token_id:\n",
    "\n",
    "https://github.com/vllm-project/vllm/issues/4180\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\")\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "]\n",
    "\n",
    "generate_text = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    task='text-generation',\n",
    "    temperature=0.0,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    do_sample=False,\n",
    "    eos_token_id=terminators,\n",
    "    max_new_tokens=512,  # mex number of tokens to generate in the output\n",
    "    repetition_penalty=1.2,  # without this output begins repeating\n",
    "    pad_token_id = tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- stopping_criteria (StoppingCriteriaList, optional)\n",
    "\n",
    "    - Custom stopping criteria that complements the default stopping criteria built from arguments and a generation config. If a stopping criteria is passed that is already created with the arguments or a generation config an error is thrown. If your stopping criteria depends on the scores input, make sure you pass return_dict_in_generate=True, output_scores=True to generate. This feature is intended for advanced users.\n",
    "\n",
    "- Special tokens that can be used at generation time\n",
    "\n",
    "    - pad_token_id (int, optional) — The id of the padding token.\n",
    "    - bos_token_id (int, optional) — The id of the beginning-of-sequence token.\n",
    "    - eos_token_id (Union[int, List[int]], optional) — The id of the end-of-sequence token. Optionally, use a list to set multiple end-of-sequence tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWozDrGuPSKQ"
   },
   "source": [
    "### LLama3 Standard Template\n",
    "\n",
    "- <|begin_of_text|>: This is equiavalent ot the BOS token\n",
    "- <|eot_id|>: This signifies the end of the message in a turn\n",
    "- <|start_header_id|>{role}<|end_header_id|>: These tokens enclose the role for a particular message. The possible roles can be: `system`, `user`, `assistant`\n",
    "- <|end_of_text|>: This is equivalent to the EOS token. On generating this token, Llama 3 will cease to generate more tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6SLO1ylG_u-"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "You are a honest and unbiased AI assistant who answer User queries with accurate responses.\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "What's the capital of Australia?\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "12mVceaOVJOF",
    "outputId": "216aab23-b5c6-4bbe-a144-095345cd323c"
   },
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m7gtWrN3HBlo",
    "outputId": "44bdc5c8-e567-45d6-c5e3-86b5892a77c1"
   },
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MSVI5W3wHMp-",
    "outputId": "d58ba0f1-2408-466a-bb81-7d6084082db3"
   },
   "outputs": [],
   "source": [
    "output = llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "JvGBBMOEHnUu",
    "outputId": "35535661-f95e-41ee-d889-0b9c10bcf19e"
   },
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bU53sd_1OCLY"
   },
   "source": [
    "### Can we invoke with ChatPromptTemplate directly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5zP8coHN91v"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "\n",
    "system_prompt = PromptTemplate.from_template(\"\"\"You are a honest and unbiased AI assistant who answer User queries with accurate responses.\"\"\")\n",
    "\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template='{query}', input_variables=[\"query\"])\n",
    "\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message, human_message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULRHZxKNPyOZ"
   },
   "outputs": [],
   "source": [
    "chain = chat_prompt|llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5B872jPQQAPw",
    "outputId": "1a411808-126e-4b59-c38f-3153cb0b7fde"
   },
   "outputs": [],
   "source": [
    "output = chain.invoke(\"What's the capital of Australia?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "PMVaYWboTbiC",
    "outputId": "8ef7db20-0e2e-4365-98f2-1be19268af52"
   },
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P0sxNxDET2yp"
   },
   "outputs": [],
   "source": [
    "prompt = chat_prompt.invoke(\"What's the capital of Australia?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rxsOdkqMT_6k",
    "outputId": "fe12e0a6-bcba-41e9-e262-87ef7ad6e950"
   },
   "outputs": [],
   "source": [
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CI_sYrQyUC7L",
    "outputId": "fd6f4b42-f11c-4cb9-b87a-4d74b6353505"
   },
   "outputs": [],
   "source": [
    "for message in prompt.messages:\n",
    "    print(message.type, message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to construct the prompt properly from messages?\n",
    "\n",
    "#### Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QsIduAZBUQzB"
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"<|begin_of_text|>\"\"\"\n",
    "\n",
    "for message in prompt.messages:\n",
    "    if message.type == \"system\":\n",
    "        prompt_template += f\"<|start_header_id|>system<|end_header_id|>{message.content}<|eot_id|>\"\n",
    "    elif message.type == \"human\":\n",
    "        prompt_template += f\"<|start_header_id|>user<|end_header_id|>{message.content}<|eot_id|>\"\n",
    "\n",
    "prompt_template += f\"<|start_header_id|>assistant<|end_header_id|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "lRPE0Jc8UwHz",
    "outputId": "20aa9648-0064-4648-e306-22a3a7f6e4c1"
   },
   "outputs": [],
   "source": [
    "llm.invoke(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attempt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bqa1vjbDVCLi"
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\\n<|begin_of_text|>\"\"\"\n",
    "\n",
    "for message in prompt.messages:\n",
    "    if message.type == \"system\":\n",
    "        prompt_template += f\"\\n<|start_header_id|>system<|end_header_id|>\\n{message.content}\\n<|eot_id|>\"\n",
    "    elif message.type == \"human\":\n",
    "        prompt_template += f\"\\n<|start_header_id|>user<|end_header_id|>{message.content}\\n<|eot_id|>\"\n",
    "\n",
    "prompt_template += f\"\\n<|start_header_id|>assistant<|end_header_id|>\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "6uh85RMAVFFG",
    "outputId": "16ee6fa1-cfd8-4161-bcc5-fb3b99113622"
   },
   "outputs": [],
   "source": [
    "llm.invoke(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t3d8-6qSV6HS"
   },
   "outputs": [],
   "source": [
    "def llama3_prompt_parser(prompt):\n",
    "\n",
    "    prompt_template = \"\"\"\\n<|begin_of_text|>\"\"\"\n",
    "\n",
    "    for message in prompt.messages:\n",
    "        if message.type == \"system\":\n",
    "            prompt_template += f\"\\n<|start_header_id|>system<|end_header_id|>\\n{message.content}\\n<|eot_id|>\"\n",
    "        elif message.type == \"human\":\n",
    "            prompt_template += f\"\\n<|start_header_id|>user<|end_header_id|>{message.content}\\n<|eot_id|>\"\n",
    "\n",
    "    prompt_template += f\"\\n<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "\n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "-vatFCvnWTWd",
    "outputId": "d40b5c51-52b4-41bf-e8d1-dedbf5ad2ec0"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "chain = chat_prompt|RunnableLambda(llama3_prompt_parser)|llm\n",
    "\n",
    "chain.invoke(\"What's the capital of Australia?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnu4vGJJXd0z"
   },
   "source": [
    "## Retrieval\n",
    "\n",
    "Because this might be the most frequent functionality you will use in your work.\n",
    "\n",
    "source: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_rag_agent_llama3_local.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qnMwDT1wXcDh"
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "import pandas as pd\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnablePassthrough\n",
    "\n",
    "df_query = pd.read_excel(\"100cases_for testing.xlsx\")\n",
    "df_key = pd.read_excel(\"HFACS_Benchmark.xlsx\")\n",
    "\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "documents = []\n",
    "\n",
    "for idx, row in df_key.iterrows():\n",
    "    document = Document(page_content=row[\"Combined Narratives\"],\n",
    "                        metadata={\"level_1\": row.get('Level 1: Unsafe Acts', \"\"),\n",
    "                                  \"level_2\": row.get('Level 2: Preconditions for Unsafe Acts', \"\")})\n",
    "    documents.append(document)\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents, embedding=embedding)\n",
    "\n",
    "embedding_retriever = vectorstore.as_retriever(search_type=\"similarity\",\n",
    "                                              search_kwargs={'k': 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Prompt Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LFPpEaLaZp2C"
   },
   "outputs": [],
   "source": [
    "system_template = \"\"\"\n",
    "You are an expert in aviation safety accident analysis. You are highly analytical and pay close attention to details. I want you to analyse the accident narratives based on The Human Factors Analysis and Classification System (HFACS).\n",
    "Tell me which categories of errors caused the accident, only identify the factor that belongs to HFACS level 1: Unsafe acts and Level 2: Precondition for unsafe acts.\n",
    "\n",
    "There could be multiple errors in this level or none at all. I only want errors that can be directly deducted from the narratives, no speculations or guesses.\n",
    "\n",
    "The candidates of the output are from:\n",
    "level_1:\n",
    "- `Decision Errors`\n",
    "- `Skill-Based Errors`\n",
    "- `Perceptual Errors`\n",
    "- `Routine violation`\n",
    "- `Violation: Exceptional`\n",
    "level_2:\n",
    "- `Physical Environment`\n",
    "- `Technological Environment`\n",
    "- `Adverse Mental State`\n",
    "- `Adverse Physiological State`\n",
    "- `Physical/Mental Limitations`\n",
    "- `Crew Resource Management`\n",
    "- `Personal Readiness`\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "response_schemas = [\n",
    "        ResponseSchema(name=\"level_1\", description=\"HFACS Level_1 evaluation as a python list\"),\n",
    "        ResponseSchema(name=\"level_2\", description=\"HFACS Level_2 evaluation as a python list\")\n",
    "    ]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "\n",
    "def few_shot_prompt_fn(data):\n",
    "\n",
    "    example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [('human', '{input}'), ('ai', 'level_1 error: [{level_1}], level_2 error: [{level_2}]')]\n",
    ")\n",
    "\n",
    "    examples = []\n",
    "    \n",
    "    for example in data['examples']:\n",
    "        examples.append({\"input\": example.page_content, \"level_1\": example.metadata['level_1'],\n",
    "                         \"level_2\": example.metadata['level_2']})\n",
    "    \n",
    "    few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "        example_prompt=example_prompt,\n",
    "        examples=examples,\n",
    "    )\n",
    "\n",
    "    system_prompt = PromptTemplate.from_template(system_template)\n",
    "    \n",
    "    system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "    \n",
    "    human_prompt = PromptTemplate(template='{incident_narrative} \\n format instruction: {format_instructions}', \n",
    "                                  partial_variables={\"format_instructions\": format_instructions, \"incident_narrative\": data['input']})\n",
    "        \n",
    "    human_message = HumanMessagePromptTemplate(prompt=human_prompt) \n",
    "    \n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                    few_shot_prompt,\n",
    "                                                    human_message\n",
    "                                                   ])\n",
    "    \n",
    "    return chat_prompt\n",
    "\n",
    "chat_prompt = {\"examples\": embedding_retriever, \"input\": RunnablePassthrough()}|RunnableLambda(few_shot_prompt_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74yL4HFdaKWS"
   },
   "outputs": [],
   "source": [
    "chain = chat_prompt|llama3_prompt_parser|llm|output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1f3e1298ba37428cbe2790a94661168c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f6131cd342d54330819bc84f973458c5",
      "placeholder": "​",
      "style": "IPY_MODEL_311c869724dc4520912fe2846cf16b23",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "311c869724dc4520912fe2846cf16b23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4d5055a325e14b7daf8d7f7e8d3d0808": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1f3e1298ba37428cbe2790a94661168c",
       "IPY_MODEL_b6bbfe73158543c28b24c16af9873364",
       "IPY_MODEL_f0670206f2644defabb694550e432649"
      ],
      "layout": "IPY_MODEL_b2cdb066fea0418ebffa81c2351dc8c7"
     }
    },
    "56fc5f0cded0480f8f5441e197490f1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b2cdb066fea0418ebffa81c2351dc8c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b6bbfe73158543c28b24c16af9873364": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e19cd3bc75ae4fe59fcc9f9301c108ce",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cb4beea6f2f94cb9a7ee688e049f0feb",
      "value": 4
     }
    },
    "cb4beea6f2f94cb9a7ee688e049f0feb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e19cd3bc75ae4fe59fcc9f9301c108ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3f4e1e7aacc465cac82a6a2fc55222b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0670206f2644defabb694550e432649": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e3f4e1e7aacc465cac82a6a2fc55222b",
      "placeholder": "​",
      "style": "IPY_MODEL_56fc5f0cded0480f8f5441e197490f1e",
      "value": " 4/4 [01:16&lt;00:00, 16.41s/it]"
     }
    },
    "f6131cd342d54330819bc84f973458c5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
