{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLnmbK7VV7ey"
   },
   "source": [
    "https://abvijaykumar.medium.com/fine-tuning-llm-parameter-efficient-fine-tuning-peft-lora-qlora-part-2-d8e23877ac6f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpHKzJ3tV-NE"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git\n",
    "!pip install -q datasets bitsandbytes einops\n",
    "!pip install -q wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZxWnHPP1WCZE"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import login\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDkr08p2WUus"
   },
   "source": [
    "Let us understand why we need these various dependencies\n",
    "\n",
    "- trl: This Python package “Transformer Reinforcement Learning” is used for fine-tuning the transformer model, using reinforcement learning. We will use our instruction dataset to perform this reinforcement learning and fine-tune the model. We will be using SFTrainer object to perform the fine-tuning.\n",
    "\n",
    "- transformers: This package provides all the APIs for downloading and working with various pre-trained models that are in the huggingface model hub. In our example, we will be downloading Salesforce/codegen-350M-mono. We will also be using the bits and bytes library from transformers, for quantization and AutoTokenizers for creating a tokenizer for the pre-trained model.\n",
    "\n",
    "- accelerate: This is another very powerful huggingface package, that hides the complexity of the developer trying to write/manage code needed to use multi-GPUs/TPU/fp16.\n",
    "\n",
    "- peft: This package provides all the APIs we will need to perform the LoRA technique.\n",
    "\n",
    "- datasets: This huggingface package provides access to the various datasets in the huggingface hub.\n",
    "\n",
    "- wandb: This library provides access to the Weights and Biases library to capture various metrics, during the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3OCdj4cWaLe"
   },
   "outputs": [],
   "source": [
    "model_name = \"Salesforce/codegen-350M-mono\"\n",
    "dataset_name = \"iamtarun/python_code_instructions_18k_alpaca\"\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ly1MsgccWxXx"
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "      lora_alpha=16,\n",
    "      lora_dropout=0.1,\n",
    "      r=64,\n",
    "      bias=\"none\",\n",
    "      task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8ee--gfXA2q"
   },
   "source": [
    "The LoraConfig has the following attributes.\n",
    "\n",
    "- lora_alpha: scaling factor for the weight matrices. alpha is a scaling factor that adjusts the magnitude of the combined result (base model output + low-rank adaptation). We have set it to 16. You can find more details of this in the LoRA paper here.\n",
    "\n",
    "- lora_dropout: dropout probability of the LoRA layers. This parameter is used to avoid overfitting. This technique basically drop-outs some of the neurons during both forward and backward propagation, this will help in removing dependency on a single unit of neurons. We are setting this to 0.1 (which is 10%), which means each neuron has a dropout chance of 10%.\n",
    "\n",
    "- r: This is the dimension of the low-rank matrix, Refer to Part 1 of this blog for more details. In this case, we are setting this to 64 (which effectively means we will have 512x64 and 64x512 parameters in our LoRA adapter.\n",
    "\n",
    "- bias: We will not be training the bias in this example, so we are setting that to “none”. If we have to train the biases, we can set this to “all”, or if we want to train only the LORA biases then we can use “lora_only”\n",
    "\n",
    "- task_type: Since we are using the Causal language model, the task type we set to CAUSAL_LM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "py6vQQf5YnlJ"
   },
   "source": [
    "Source: https://medium.com/@fartypantsham/what-rank-r-and-alpha-to-use-in-lora-in-llm-1b4f025fd133\n",
    "\n",
    "\n",
    "alpha = rank is scaling weights at 1.0\n",
    "\n",
    "What you train in LORA weights will be then merged with the main weights of model at x 1.0\n",
    "\n",
    "Previously people were suggesting alpha = (2 x rank), which is like yelling at your model really loud — all in order to make the newly learned weights “louder” than the model’s own. That requires a really good and large dataset, otherwise you are just amplifying nonsense.\n",
    "\n",
    "The model knows how to speak well already, while your dataset is too small to teach (or scream at) the model any language fundamentals. Increasing alpha amplifies everything, not just the stuff you wish the model learns from it.\n",
    "\n",
    "I would suggest rank = alpha, most of the time as your base — because it is very easily to attenuate the LORA data after the training is done if it appears to be too “loud”, overtaking the entire model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NVG01g35W8KY"
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Oxuhqgva5cG"
   },
   "source": [
    "- load_in_4bit: we are loading the base model with a 4-bit quantization, so we are setting this value to True.\n",
    "\n",
    "- bnb_4bit_use_double_quant: We also want double quantization so that even the quantization constant is quantized. So we are setting this to True.\n",
    "\n",
    "- bnb_4bit_quant_type: We are setting this to nf4.\n",
    "\n",
    "- bnb_4bit_compute_dtype: and the compute datatype we are setting to float16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LIJvu6kXbItY"
   },
   "outputs": [],
   "source": [
    "def prompt_instruction_format(sample):\n",
    "    return f\"\"\"### Instruction:\n",
    "    Use the Task below and the Input given to write the Response, which is a programming code that can solve the following Task:\n",
    "\n",
    "    ### Task:\n",
    "    {sample['instruction']}\n",
    "\n",
    "    ### Input:\n",
    "    {sample['input']}\n",
    "\n",
    "    ### Response:\n",
    "    {sample['output']}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PsiuffaTbQ8K"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_name, split=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eh6_DTEebYNF"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "          quantization_config=bnb_config,\n",
    "          use_cache = False,\n",
    "          device_map=device_map)\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MxcH69ebirL"
   },
   "outputs": [],
   "source": [
    "trainingArgs = TrainingArguments(\n",
    "    output_dir=finetunes_model_name,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=False,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    disable_tqdm=True,\n",
    "    report_to=\"wandb\",\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06rwnBpWeehh"
   },
   "source": [
    "- output_dir: Output directory where the model predictions and checkpoints will be stored\n",
    "- num_train_epochs=3: Number of training epochs\n",
    "- per_device_train_batch_size=4: Batch size per GPU for training\n",
    "- gradient_accumulation_steps=2: Number of update steps to accumulate the gradients for\n",
    "- gradient_checkpointing=True: Enable gradient checkpointing. Gradient checkpointing is a technique used to reduce memory consumption during the training of deep neural networks, especially in situations where memory usage is a limiting factor. Gradient checkpointing selectively re-computes intermediate activations during the backward pass instead of storing them all, thus performing some extra computation to reduce memory usage.\n",
    "- optim=”paged_adamw_32bit”: Optimizer to use, We will be using paged_adamw_32bit\n",
    "- logging_steps=5: Log on to the console on the progress every 5 steps.\n",
    "- save_strategy=”epoch”: save after every epoch\n",
    "- learning_rate=2e-4: Learning rate\n",
    "- weight_decay=0.001: Weight decay is a regularization technique used while training the models, to prevent overfitting by adding a penalty term to the loss function. Weight decay works by adding a term to the loss function that penalizes large values of the model’s weights.\n",
    "- max_grad_norm=0.3: This parameter sets the maximum gradient norm for gradient clipping.\n",
    "- warmup_ratio=0.03: The warm-up ratio is a value that determines what fraction of the total training steps or epochs will be used for the warm-up phase. In this case, we are setting it to 3%. Warm-up refers to a specific learning rate scheduling strategy that gradually increases the learning rate from its initial value to its full value over a certain number of training steps or epochs.\n",
    "- lr_scheduler_type=”cosine”: Learning rate schedulers are used to adjust the - learning rate dynamically during training to help improve convergence and model performance. We will be using the cosine type for the learning rate scheduler.\n",
    "- report_to=”wandb”: We want to report our metrics to Weights and Bias\n",
    "- seed=42: This is the random seed that is set during the beginning of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wg9lQ9Goep1e"
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=2048,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    formatting_func=prompt_instruction_format,\n",
    "    args=trainingArgs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRs5Zv1Ze_be"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7LU7jfN1e_6o"
   },
   "outputs": [],
   "source": [
    "# Merge LoRA with the base model and save the merged model\n",
    "merged = trained_model.merge_and_unload()\n",
    "merged.save_pretrained(\"merged\",safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"merged\")\n",
    "\n",
    "#push merged model to the hub\n",
    "merged.push_to_hub(\"codegen-350M-mono-python-18k-alpaca\")\n",
    "tokenizer.push_to_hub(\"codegen-350M-mono-python-18k-alpaca\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
