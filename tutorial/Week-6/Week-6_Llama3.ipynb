{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OCMrly7znijN",
    "outputId": "f961c609-c316-4327-eaaa-7fee1eaf659d"
   },
   "outputs": [],
   "source": [
    "!pip install -qU langchain==0.2.5 langchain-community==0.2.5 langchain-core==0.2.9 langchain-openai==0.1.9 bitsandbytes accelerate xformers triton transformers torch==2.3.0 sentence-transformers chromadb datasets peft trl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Db48jkPRsOPn"
   },
   "source": [
    "- llama3-8B is the base model which basically just do the completions to the input prompt, But llama3-8B Instruct is finetuned for instruction following and multi-turn conversation templates for assistant completions as chat response.\n",
    "\n",
    "- If your specific purpose is for chat completions then instruct is the best choice otherwise if it is for simple completions of input then base model is fine. But there might be a chance for the model to continue generation till max_seq_len is achieved while generating while using base model.\n",
    "\n",
    "- Llama 3 8B model has a knowledge cut-off of March, 2023.\n",
    "- Llama 3 70B model has a knowledge cut-off of December, 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hGWkwUcsOPo"
   },
   "source": [
    "# Use the one directly from meta\n",
    "\n",
    "## Create the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing Libraries:\n",
    "\n",
    "- Various necessary modules from the Transformers library and PyTorch are imported. langchain.llms is also imported for integration with LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from torch import cuda\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, pipeline, StoppingCriteria, StoppingCriteriaList, AutoConfig, AutoModelForCausalLM\n",
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_token_ids = None\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "HF_TOKEN = \"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\"\n",
    "\n",
    "device = f\"cuda:{cuda.current_device()}\" if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bits and Bytes Configuration:\n",
    "\n",
    "- Configures the model to use 4-bit quantization to reduce memory usage and computation cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating the Tokenizer:\n",
    "\n",
    "- Defines a function to create a tokenizer from a pre-trained model. It also sets up stop token IDs which are sequences of tokens that, when encountered, will stop the text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer():\n",
    "\n",
    "    global stop_token_ids\n",
    "\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_id, use_auth_token=HF_TOKEN)\n",
    "    stop_list = ['\\nHuman:', '\\n```\\n']\n",
    "\n",
    "    stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
    "    stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
    "    stop_token_ids = stop_token_ids\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stopping Criteria Class:\n",
    "\n",
    "- Custom stopping criteria class that stops text generation when certain sequences of tokens (stop tokens) are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gE-3a3tEqCah",
    "outputId": "3ffb43ec-a327-4489-d42e-fb3405caa955"
   },
   "outputs": [],
   "source": [
    "class StopOnTokens(StoppingCriteria):\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "\n",
    "        global stop_token_ids\n",
    "    \n",
    "        # print(f\"input_ids: {input_ids}\")\n",
    "        # print(f\"content: { tokenizer.decode(input_ids[0])}\")\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                  # print(\"Stopping\")\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Loading the Model Configuration and Model:\n",
    "\n",
    "- Loads the configuration and the model from Hugging Face with specified parameters, including the bits and bytes configuration for quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=HF_TOKEN\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    token=HF_TOKEN,\n",
    "    config=model_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "4d5055a325e14b7daf8d7f7e8d3d0808",
      "1f3e1298ba37428cbe2790a94661168c",
      "b6bbfe73158543c28b24c16af9873364",
      "f0670206f2644defabb694550e432649",
      "b2cdb066fea0418ebffa81c2351dc8c7",
      "f6131cd342d54330819bc84f973458c5",
      "311c869724dc4520912fe2846cf16b23",
      "e19cd3bc75ae4fe59fcc9f9301c108ce",
      "cb4beea6f2f94cb9a7ee688e049f0feb",
      "e3f4e1e7aacc465cac82a6a2fc55222b",
      "56fc5f0cded0480f8f5441e197490f1e"
     ]
    },
    "id": "JYNlEqm0CbQn",
    "outputId": "0e5a4004-acb0-41b8-9806-933d23fa66ed"
   },
   "source": [
    "### 6. Setting Up Stopping Criteria:\n",
    "\n",
    "- Creates a list of stopping criteria with the custom StopOnTokens class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Tokenizing and Generating Text:\n",
    "\n",
    "- Initializes the tokenizer, sets padding token, and defines terminators. Then it sets up a text generation pipeline with specific parameters like temperature, max new tokens, and stopping criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Up9JgHA7Ctop",
    "outputId": "22e0cd1a-9b09-401e-f83c-0dce1794b72a"
   },
   "outputs": [],
   "source": [
    "tokenizer = create_tokenizer()\n",
    "\n",
    "\"\"\"\n",
    "Source Code:\n",
    "\n",
    "if generation_config.pad_token_id is None and generation_config.eos_token_id is not None:\n",
    "    if model_kwargs.get(\"attention_mask\") is None:\n",
    "        logger.warning(\n",
    "            \"The attention mask and the pad token id were not set. As a consequence, you may observe \"\n",
    "            \"unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\"\n",
    "        )\n",
    "    eos_token_id = generation_config.eos_token_id\n",
    "    if isinstance(eos_token_id, list):\n",
    "        eos_token_id = eos_token_id[0]\n",
    "    logger.warning(f\"Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.\")\n",
    "    generation_config.pad_token_id = eos_token_id\n",
    "\n",
    "Discussion on the eos_token_id:\n",
    "\n",
    "https://github.com/vllm-project/vllm/issues/4180\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"),\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "]\n",
    "\n",
    "generate_text = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    task='text-generation',\n",
    "    temperature=0.5,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    do_sample=True,\n",
    "    eos_token_id=terminators,\n",
    "    max_new_tokens=512,  # mex number of tokens to generate in the output\n",
    "    repetition_penalty=1.2,  # without this output begins repeating\n",
    "    top_p=0.5,\n",
    "    pad_token_id = tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- stopping_criteria (StoppingCriteriaList, optional)\n",
    "\n",
    "    - Custom stopping criteria that complements the default stopping criteria built from arguments and a generation config. If a stopping criteria is passed that is already created with the arguments or a generation config an error is thrown. If your stopping criteria depends on the scores input, make sure you pass return_dict_in_generate=True, output_scores=True to generate. This feature is intended for advanced users.\n",
    "\n",
    "- Special tokens that can be used at generation time\n",
    "\n",
    "    - pad_token_id (int, optional) — The id of the padding token.\n",
    "    - bos_token_id (int, optional) — The id of the beginning-of-sequence token.\n",
    "    - eos_token_id (Union[int, List[int]], optional) — The id of the end-of-sequence token. Optionally, use a list to set multiple end-of-sequence tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWozDrGuPSKQ"
   },
   "source": [
    "### LLama3 Standard Template\n",
    "\n",
    "- <|begin_of_text|>: This is equiavalent ot the BOS token\n",
    "- <|eot_id|>: This signifies the end of the message in a turn\n",
    "- <|start_header_id|>{role}<|end_header_id|>: These tokens enclose the role for a particular message. The possible roles can be: `system`, `user`, `assistant`\n",
    "- <|end_of_text|>: This is equivalent to the EOS token. On generating this token, Llama 3 will cease to generate more tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6SLO1ylG_u-"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "You are a honest and unbiased AI assistant who answer User queries with accurate responses.\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "What's the capital of Australia?\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "12mVceaOVJOF",
    "outputId": "216aab23-b5c6-4bbe-a144-095345cd323c"
   },
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m7gtWrN3HBlo",
    "outputId": "44bdc5c8-e567-45d6-c5e3-86b5892a77c1"
   },
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MSVI5W3wHMp-",
    "outputId": "d58ba0f1-2408-466a-bb81-7d6084082db3"
   },
   "outputs": [],
   "source": [
    "output = llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "JvGBBMOEHnUu",
    "outputId": "35535661-f95e-41ee-d889-0b9c10bcf19e"
   },
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bU53sd_1OCLY"
   },
   "source": [
    "### Can we invoke with ChatPromptTemplate directly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5zP8coHN91v"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "\n",
    "system_prompt = PromptTemplate.from_template(\"\"\"You are a honest and unbiased AI assistant who answer User queries with accurate responses.\"\"\")\n",
    "\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template='{query}', input_variables=[\"query\"])\n",
    "\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message, human_message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULRHZxKNPyOZ"
   },
   "outputs": [],
   "source": [
    "chain = chat_prompt|llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5B872jPQQAPw",
    "outputId": "1a411808-126e-4b59-c38f-3153cb0b7fde"
   },
   "outputs": [],
   "source": [
    "output = chain.invoke(\"What's the capital of Australia?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "PMVaYWboTbiC",
    "outputId": "8ef7db20-0e2e-4365-98f2-1be19268af52"
   },
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P0sxNxDET2yp"
   },
   "outputs": [],
   "source": [
    "prompt = chat_prompt.invoke(\"What's the capital of Australia?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rxsOdkqMT_6k",
    "outputId": "fe12e0a6-bcba-41e9-e262-87ef7ad6e950"
   },
   "outputs": [],
   "source": [
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CI_sYrQyUC7L",
    "outputId": "fd6f4b42-f11c-4cb9-b87a-4d74b6353505"
   },
   "outputs": [],
   "source": [
    "for message in prompt.messages:\n",
    "    print(message.type, message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to construct the prompt properly from messages?\n",
    "\n",
    "#### Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QsIduAZBUQzB"
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"<|begin_of_text|>\"\"\"\n",
    "\n",
    "for message in prompt.messages:\n",
    "    if message.type == \"system\":\n",
    "        prompt_template += f\"<|start_header_id|>system<|end_header_id|>{message.content}<|eot_id|>\"\n",
    "    elif message.type == \"human\":\n",
    "        prompt_template += f\"<|start_header_id|>user<|end_header_id|>{message.content}<|eot_id|>\"\n",
    "\n",
    "prompt_template += f\"<|start_header_id|>assistant<|end_header_id|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "lRPE0Jc8UwHz",
    "outputId": "20aa9648-0064-4648-e306-22a3a7f6e4c1"
   },
   "outputs": [],
   "source": [
    "llm.invoke(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attempt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bqa1vjbDVCLi"
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\\n<|begin_of_text|>\"\"\"\n",
    "\n",
    "for message in prompt.messages:\n",
    "    if message.type == \"system\":\n",
    "        prompt_template += f\"\\n<|start_header_id|>system<|end_header_id|>\\n{message.content}\\n<|eot_id|>\"\n",
    "    elif message.type == \"human\":\n",
    "        prompt_template += f\"\\n<|start_header_id|>user<|end_header_id|>{message.content}\\n<|eot_id|>\"\n",
    "\n",
    "prompt_template += f\"\\n<|start_header_id|>assistant<|end_header_id|>\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "6uh85RMAVFFG",
    "outputId": "16ee6fa1-cfd8-4161-bcc5-fb3b99113622"
   },
   "outputs": [],
   "source": [
    "llm.invoke(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t3d8-6qSV6HS"
   },
   "outputs": [],
   "source": [
    "def llama3_prompt_parser(prompt):\n",
    "\n",
    "    prompt_template = \"\"\"\\n<|begin_of_text|>\"\"\"\n",
    "\n",
    "    for message in prompt.messages:\n",
    "        if message.type == \"system\":\n",
    "            prompt_template += f\"\\n<|start_header_id|>system<|end_header_id|>\\n{message.content}\\n<|eot_id|>\"\n",
    "        elif message.type == \"human\":\n",
    "            prompt_template += f\"\\n<|start_header_id|>user<|end_header_id|>{message.content}\\n<|eot_id|>\"\n",
    "\n",
    "    prompt_template += f\"\\n<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "\n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "-vatFCvnWTWd",
    "outputId": "d40b5c51-52b4-41bf-e8d1-dedbf5ad2ec0"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "chain = chat_prompt|RunnableLambda(llama3_prompt_parser)|llm\n",
    "\n",
    "chain.invoke(\"What's the capital of Australia?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnu4vGJJXd0z"
   },
   "source": [
    "## Retrieval\n",
    "\n",
    "Because this might be the most frequent functionality you will use in your work.\n",
    "\n",
    "source: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_rag_agent_llama3_local.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qnMwDT1wXcDh"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=doc_splits,\n",
    "    # collection_name=\"rag-chroma\",\n",
    "    embedding=embedding,\n",
    ")\n",
    "retriever = vectorstore.as_retriever(k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LFPpEaLaZp2C"
   },
   "outputs": [],
   "source": [
    "system_prompt = PromptTemplate.from_template(\"\"\"You are a grader assessing relevance\n",
    "    of a retrieved document to a user question. If the document contains keywords related to the user question,\n",
    "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\"\"\")\n",
    "\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template=\"\"\"Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "Here is the user question: {question} \\n \"\"\", input_variables=[\"document\", \"question\"])\n",
    "\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message, human_message])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ubYOYmFsOPp"
   },
   "source": [
    "## Create the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74yL4HFdaKWS"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "chain = {\"question\": RunnablePassthrough(), \"document\": retriever|RunnableLambda(format_docs)}|chat_prompt|llama3_prompt_parser|llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "id": "E41KodDub2Dq",
    "outputId": "d34d0b44-0ba7-4d71-c2d5-bb750e01755c"
   },
   "outputs": [],
   "source": [
    "chain.invoke(\"agent memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHo8yPC8bEcl"
   },
   "outputs": [],
   "source": [
    "# Let us apply the structure\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "response_schemas = [\n",
    "        ResponseSchema(name=\"score\", description=\"a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question\")\n",
    "    ]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "\n",
    "system_prompt = PromptTemplate.from_template(\"\"\"You are a grader assessing relevance\n",
    "    of a retrieved document to a user question. If the document contains keywords related to the user question,\n",
    "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\")\n",
    "\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template=\"\"\"Here is the retrieved document: \n",
    "                                       {document}\n",
    "                                       Here is the user question: {question}.\n",
    "                                       format instruction: {format_instructions}\n",
    "                                       \"\"\",\n",
    "                                       input_variables=[\"document\", \"question\"],\n",
    "                                       partial_variables={\"format_instructions\": format_instructions})\n",
    "\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message, human_message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WhU56tq7dWVY",
    "outputId": "a025fd68-865c-434b-e439-c8ae9d452bd8"
   },
   "outputs": [],
   "source": [
    "chain = {\"question\": RunnablePassthrough(), \"document\": retriever|RunnableLambda(format_docs)}|chat_prompt|llama3_prompt_parser|llm|output_parser\n",
    "chain.invoke(\"agent memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7whx4z3fsOPq"
   },
   "source": [
    "## Can we do this faster?\n",
    "\n",
    "https://ollama.com/library/llama3/tags\n",
    "\n",
    "*** Under Construction ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(url='https://ollama.com/public/ollama.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning\n",
    "\n",
    "...Under Test...\n",
    "\n",
    "It is quite complicated ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1f3e1298ba37428cbe2790a94661168c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f6131cd342d54330819bc84f973458c5",
      "placeholder": "​",
      "style": "IPY_MODEL_311c869724dc4520912fe2846cf16b23",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "311c869724dc4520912fe2846cf16b23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4d5055a325e14b7daf8d7f7e8d3d0808": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1f3e1298ba37428cbe2790a94661168c",
       "IPY_MODEL_b6bbfe73158543c28b24c16af9873364",
       "IPY_MODEL_f0670206f2644defabb694550e432649"
      ],
      "layout": "IPY_MODEL_b2cdb066fea0418ebffa81c2351dc8c7"
     }
    },
    "56fc5f0cded0480f8f5441e197490f1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b2cdb066fea0418ebffa81c2351dc8c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b6bbfe73158543c28b24c16af9873364": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e19cd3bc75ae4fe59fcc9f9301c108ce",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cb4beea6f2f94cb9a7ee688e049f0feb",
      "value": 4
     }
    },
    "cb4beea6f2f94cb9a7ee688e049f0feb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e19cd3bc75ae4fe59fcc9f9301c108ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3f4e1e7aacc465cac82a6a2fc55222b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0670206f2644defabb694550e432649": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e3f4e1e7aacc465cac82a6a2fc55222b",
      "placeholder": "​",
      "style": "IPY_MODEL_56fc5f0cded0480f8f5441e197490f1e",
      "value": " 4/4 [01:16&lt;00:00, 16.41s/it]"
     }
    },
    "f6131cd342d54330819bc84f973458c5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
