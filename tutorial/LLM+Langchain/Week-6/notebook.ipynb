{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e9bd1a7-ba1e-491e-b83e-a31ac4dd06d9",
   "metadata": {},
   "source": [
    "# 語音轉文字 (Speech to Text)\n",
    "\n",
    "在許多應用中，語音資料需要被轉換為文字，例如字幕產生、語音助理、會議紀錄或多語言翻譯。OpenAI 提供了 **Whisper** 模型，可以高效地進行以下兩種主要任務：\n",
    "\n",
    "- **轉錄 (Transcribe)：** 將音訊檔案轉換成相同語言的文字。  \n",
    "- **翻譯轉錄 (Translate & Transcribe)：** 將音訊檔案的內容轉錄並翻譯成英文。  \n",
    "\n",
    "目前檔案上傳限制為 **25 MB**，支援的音訊檔案格式包括：  \n",
    "`mp3`, `mp4`, `mpeg`, `mpga`, `m4a`, `wav`, `webm`\n",
    "\n",
    "---\n",
    "\n",
    "# 文字轉語音 (Text to Speech, TTS)\n",
    "\n",
    "除了語音轉文字，OpenAI 也提供了 **文字轉語音 (TTS)** 功能。透過 TTS，我們可以將輸入的文字轉換為自然流暢的語音，適合應用於：  \n",
    "\n",
    "- **語音助理**：讓機器能以自然語音回覆使用者。  \n",
    "- **內容創作**：自動生成旁白或有聲讀物。  \n",
    "- **輔助功能**：幫助視覺障礙者或閱讀困難者更容易獲取資訊。  \n",
    "\n",
    "TTS 支援多種語音風格與音質選擇，能夠在不同應用場景中提供更自然的使用體驗。\n",
    "\n",
    "---\n",
    "\n",
    "接下來，我們將透過程式範例來展示如何使用 Python 串接 **Whisper** (語音轉文字) 與 **TTS** (文字轉語音)，並逐步完成音訊與文字的雙向轉換。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacd9ac3-0fd0-4ce8-9c63-87a94202ed22",
   "metadata": {},
   "source": [
    "## Audio models\n",
    "\n",
    "| Model   | Usage                                            |\n",
    "|---------|--------------------------------------------------|\n",
    "| Whisper |  \\$ 0.006 / minute rounded to the nearest second     |\n",
    "| TTS     |  \\$ 15.00 / 1M characters                          |\n",
    "| TTS HD  |  \\$ 30.00 / 1M characters                          |\n",
    "\n",
    "\n",
    "Whisper 按**分鐘**計費；TTS 按**字元**計費。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bb5d86-d176-43a1-9f92-d0e3babe9529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476b8dcb-ece8-4730-a8d9-5a0fad0e6abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.initialization import credential_init\n",
    "from src.io.path_definition import get_project_dir, get_file\n",
    "\n",
    "\n",
    "credential_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8465282-3b41-422d-a904-8c62ecd196f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baa97bc-b426-418e-820f-618b18a2df30",
   "metadata": {},
   "source": [
    "# Transcription\n",
    "\n",
    "## Batch Transcription\n",
    "\n",
    "在 非串流模式（使用 transcriptions.create 上傳檔案）時，Whisper 會等待整個音訊檔案完全上傳後，才會對完整音檔進行轉錄，並在完成後一次性回傳結果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cfb513-eefd-4db7-8dce-4998eb968e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file= open(\"tutorial/LLM+Langchain/Week-6/孩子上網，小心上當.mp3\", \"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3973567-5159-4e06-9a12-6ed69cb77cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription = client.audio.transcriptions.create(\n",
    "  model=\"whisper-1\", \n",
    "  file=audio_file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71566743-14a9-4c61-9cfb-156c973f8639",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc16266-46c3-4f3a-baf7-2feea9744225",
   "metadata": {},
   "source": [
    "## Prompt 參數\n",
    "\n",
    ">- 給予Context\n",
    ">- 給予關鍵字，像是私line，應該只有台灣人用這個詞，別太指望OpenAI會知道這種用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da612f87-bb9f-4f3e-b41c-ca9b28cb305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription = client.audio.transcriptions.create(\n",
    "  model=\"whisper-1\", \n",
    "  file=audio_file,\n",
    "  prompt=\"私line\"\n",
    ")\n",
    "\n",
    "transcription.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946ec45b-839c-451e-9f24-4bb21e51c198",
   "metadata": {},
   "source": [
    "修正是有用，但沒有scalability。因為這表示你可能需要\n",
    "\n",
    "- 1.先聽過所有內容，然後用人工校正\n",
    "- 2.把所有可能有問題的字建立好一個數據庫，然後送入prompt。\n",
    "\n",
    "Prompt 的最佳使用時機是提供 上下文 (Context) 或 特定領域詞彙 (Domain-specific terminology)\n",
    "\n",
    "- 例如：「此錄音是關於一個 AI 會議的，請注意其中的專有名詞：GPT-4o, RAG, LangChain。」這樣能更有效利用 Prompt 的價值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52696de6-210b-425f-8c69-f8174f00ff3f",
   "metadata": {},
   "source": [
    "### gpt-4o-mini-transcribe\n",
    "\n",
    "- 在gpt-4o-transcript 和 gpt-4o-mini-transcript 模型記得加上語言編碼，不然中文輸出極大概率是簡體中文。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce32e32-7c7a-4ff1-8b54-e56e1e621d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription = client.audio.transcriptions.create(\n",
    "  model=\"gpt-4o-mini-transcribe\", \n",
    "  file=audio_file,\n",
    "  prompt=\"私line\",\n",
    "  language='zh-tw',\n",
    ")\n",
    "\n",
    "transcription.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4c677a-6f5c-4689-ae25-bc19de1f1547",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription = client.audio.transcriptions.create(\n",
    "  model=\"gpt-4o-transcribe\", \n",
    "  file=audio_file,\n",
    "  prompt=\"私line\",\n",
    "  language='zh-tw',\n",
    ")\n",
    "\n",
    "transcription.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ec60c1-a6b9-41f2-a239-cb134e6cb39b",
   "metadata": {},
   "source": [
    "假設你需要時間軸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78953051-5e05-40c0-8da2-6e7eeeafb2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription = client.audio.transcriptions.create(\n",
    "  model=\"whisper-1\", \n",
    "  file=audio_file,\n",
    "  response_format='srt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe93c97-ebe1-4c9c-84bc-29f5a5bc6649",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef53b59d-769b-42b2-bacb-e271a07dd641",
   "metadata": {},
   "source": [
    "### Stream\n",
    "\n",
    "如果你在呼叫時使用 stream=True，SDK 仍然會先上傳整個音訊檔（它並不是一段一段分塊上傳），但不同的是，它不會等到完整的轉錄結束才回傳，而是會在 Whisper 生成的同時，逐步傳回結果。\n",
    "\n",
    "因此，你會在轉錄過程中陸續看到部分文字片段。\n",
    "\n",
    "等到 Whisper 完成後，你就會得到最終的完整轉錄結果。\n",
    "\n",
    "- 只支援 gpt-4o-mini 和 gpt-4o transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f974ba4b-a498-4df7-99cb-c7ab243e9186",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file= open(\"tutorial/LLM+Langchain/Week-6/孩子上網，小心上當.mp3\", \"rb\")\n",
    "\n",
    "stream = client.audio.transcriptions.create(\n",
    "  file=audio_file,\n",
    "  model=\"gpt-4o-mini-transcribe\",\n",
    "  stream=True,\n",
    "  language='zh-tw',\n",
    "  prompt='私line',\n",
    "  temperature=0\n",
    ")\n",
    "\n",
    "buffer = \"\"\n",
    "for event in stream:\n",
    "    # Depending on the event format; e.g. maybe:\n",
    "    # - event.type might be “transcript.text.delta”\n",
    "    # - or event.delta or event.partial\n",
    "    try:\n",
    "        # Example if event has attribute 'delta'\n",
    "        partial = event.delta\n",
    "        buffer += partial\n",
    "        print(partial, end=\"\", flush=True)\n",
    "    except AttributeError:\n",
    "        # fallback / final result\n",
    "        print(\"\\r\" + \" \" * len(buffer), end=\"\")  # 清掉這行\n",
    "        print(\"\\r\" + event.text)  # 印最終結果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48f4af9-9135-465e-82b4-e1f0c98da149",
   "metadata": {},
   "source": [
    "簡單的比較gpt-4o-mini-transcribe和gpt-4o-transcribe的差異"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd11eaf1-a31c-4dc6-b2f2-eed43d8cf552",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = client.audio.transcriptions.create(\n",
    "  file=audio_file,\n",
    "  model=\"gpt-4o-transcribe\",\n",
    "  stream=True,\n",
    "  language='zh-tw',\n",
    "  prompt='私line',\n",
    "  temperature=0\n",
    ")\n",
    "\n",
    "buffer = \"\"\n",
    "for event in stream:    \n",
    "    try:\n",
    "        partial = event.delta\n",
    "        buffer += partial\n",
    "        print(partial, end=\"\", flush=True)\n",
    "    except AttributeError:\n",
    "        print(\"\\r\" + \" \" * len(buffer), end=\"\")  # 清掉這行\n",
    "        print(\"\\r\" + event.text)  # 印最終結果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e79740-ff7e-49d4-b597-31d7156f14d4",
   "metadata": {},
   "source": [
    "gpt-4o-transcribe在表現上比gpt-4o-mini好一些。但要實捶仍然需要大量的測試。\n",
    "但是大致上應該符合下面的表現:\n",
    "\n",
    ">- gpt-4o-transcribe (更高品質，更高成本)\n",
    ">- gpt-4o-mini-transcribe (速度更快，成本更低，但品質略遜)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19f86f8-f939-4081-8b66-a59a684e2b67",
   "metadata": {},
   "source": [
    "# 閩南語語音辨識（ASR）可行性評估\n",
    "\n",
    "## \n",
    "1. **書寫標準不統一是瓶頸**  \n",
    "   - 語音模型訓練需要「語音 + 對應文字轉錄」資料。  \n",
    "   - 閩南語存在多種書寫系統（漢字、台羅文、白話字/羅馬拼音），缺乏一致標準會造成資料集混亂，難以收斂。  \n",
    "\n",
    "2. **資源不足**  \n",
    "   - 相比英語、中文，閩南語的「語音–文字平行語料」極少。  \n",
    "   - 缺乏大量公開 dataset，從零開始訓練成本極高。  \n",
    "\n",
    "3. **實用性受限**  \n",
    "   - 沒有公認的書寫方式，ASR 輸出的結果難以被廣泛採用。  \n",
    "   - 例如：同一句話輸出為「漢字版」或「台羅文版」，使用者群體可能互不接受。  \n",
    "\n",
    "---\n",
    "\n",
    "## 💡 技術上值得補充的觀點\n",
    "1. **語音模型本身不依賴「官方標準」**  \n",
    "   - 模型只需要「統一的訓練標籤」。  \n",
    "   - 無論是台羅文、漢字或羅馬拼音，只要 dataset 標註一致，模型即可學習。  \n",
    "   - 困難點在於「社群能否就某種書寫系統達成共識」。  \n",
    "\n",
    "2. **可採「多輸出 / 後處理」策略**  \n",
    "   - 訓練時使用 **台羅文**（與音素對應性較佳）。  \n",
    "   - 應用層可透過轉換器將台羅文輸出轉換成漢字或其他拼音系統。  \n",
    "   - 這樣可繞過「標準未定」的問題。  \n",
    "\n",
    "3. **現代語音技術降低門檻**  \n",
    "   - Whisper 等大模型已證明：低資源語言只要有數百小時資料即可微調出可用系統。  \n",
    "   - 技術上「可行」，只是 **成本高 + 資料缺乏**。  \n",
    "\n",
    "4. **平行案例**  \n",
    "   - 客家話、藏語、威爾斯語、愛爾蘭語等語言也有「語料少、書寫多樣」的挑戰。  \n",
    "   - 仍有人成功建立 ASR → 證明並非「技術上不可能」。  \n",
    "\n",
    "---\n",
    "\n",
    "## 📌 綜合評估\n",
    "- **正確：** 在台灣現況下，缺乏書寫標準確實限制模型實用性與推廣。  \n",
    "- **補充：** 從純技術角度，書寫標準不是必要條件，只要 dataset 標註一致即可建模。  \n",
    "- **真正挑戰：**  \n",
    "  1. 語料不足（收集成本高）  \n",
    "  2. 缺乏社群共識（採哪一種文字標準）  \n",
    "  3. 下游應用有限（市場需求小）  \n",
    "\n",
    "➡️ **結論：** 技術上可行，但現實條件下效益有限。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dead4005-affb-441d-abaf-bb32adf17eaf",
   "metadata": {},
   "source": [
    "### 試試看從電腦透過麥克風進行錄音然後使用 Transcribe\n",
    "\n",
    "- https://www.gyan.dev/ffmpeg/builds/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3146efd-0666-4f64-97db-35e7874e92f6",
   "metadata": {},
   "source": [
    "錄音"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a45099b-b8cd-4223-8d24-d5cce1d6023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# AudioSegment.converter = r\"tutorial\\LLM+Langchain\\Week-6\\ffmpeg-essentials_build\\bin\\ffmpeg.exe\"\n",
    "\n",
    "\n",
    "DURATION = 5  # seconds\n",
    "FS = 44100    # sample rate\n",
    "\n",
    "print(\"Recording...\")\n",
    "\n",
    "# Record audio\n",
    "audio = sd.rec(int(DURATION * FS), samplerate=FS, channels=1, dtype='int16')\n",
    "sd.wait()  # Wait until recording is finished\n",
    "print(\"Recording finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14b65ef-1b66-4197-bca5-88b160263a4c",
   "metadata": {},
   "source": [
    "將聲音數據從 np.array 轉換成 mp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d628ef-6e95-44c0-ad5d-deb96465a257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy array to AudioSegment\n",
    "audio_bytes = audio.tobytes()\n",
    "\n",
    "audio_segment = AudioSegment(\n",
    "    data=audio_bytes,\n",
    "    sample_width=audio.dtype.itemsize,\n",
    "    frame_rate=FS,\n",
    "    channels=1\n",
    ")\n",
    "\n",
    "# Save as MP3 in-memory (as an object)\n",
    "mp3_io = io.BytesIO()\n",
    "audio_segment.export(mp3_io, format=\"mp3\")\n",
    "mp3_io.seek(0)  # Rewind to start\n",
    "\n",
    "# https://community.openai.com/t/openai-whisper-send-bytes-python-instead-of-filename/84786/3\n",
    "\n",
    "mp3_io.name = \"word.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cc61c8-ee07-456e-b6a0-af63a98b1319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將上述步驟寫成一函數\n",
    "\n",
    "def audio_to_mp3_io(audio, FS:int, channels: int):\n",
    "\n",
    "    # Convert numpy array to AudioSegment\n",
    "    audio_bytes = audio.tobytes()\n",
    "    \n",
    "    audio_segment = AudioSegment(\n",
    "        data=audio_bytes,\n",
    "        sample_width=audio.dtype.itemsize,\n",
    "        frame_rate=FS,\n",
    "        channels=channels\n",
    "    )\n",
    "    \n",
    "    # Save as MP3 in-memory (as an object)\n",
    "    mp3_io = io.BytesIO()\n",
    "    audio_segment.export(mp3_io, format=\"mp3\")\n",
    "    mp3_io.seek(0)  # Rewind to start\n",
    "\n",
    "    mp3_io.name = \"word.mp3\"\n",
    "\n",
    "    return mp3_io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1852540-bff7-4296-82e8-56d3f440b80f",
   "metadata": {},
   "source": [
    "在給transcription的endpoint中，file的部分不是給檔案名稱，而是直接將檔案(在記憶體中)直接傳給API。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60919275-dec8-4f6c-bad1-7b2bab39ae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = client.audio.transcriptions.create(\n",
    "    model=\"gpt-4o-transcribe\", \n",
    "    file=mp3_io,\n",
    "    response_format=\"text\",\n",
    "    language='zh-tw',\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "buffer = \"\"\n",
    "for event in stream:\n",
    "    # Depending on the event format; e.g. maybe:\n",
    "    # - event.type might be “transcript.text.delta”\n",
    "    # - or event.delta or event.partial\n",
    "    try:\n",
    "        # Example if event has attribute 'delta'\n",
    "        partial = event.delta\n",
    "        buffer += partial\n",
    "        print(\"\\r\" + buffer, end=\"\", flush=True)\n",
    "        # print(partial, end=\"\", flush=True)\n",
    "    except AttributeError:\n",
    "        # fallback / final result\n",
    "        print(\"\\r\" + \" \" * len(buffer), end=\"\")  # 清掉這行\n",
    "        print(\"\\r\" + event.text)  # 印最終結果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a08467-5af8-4802-bb27-0b9e04f85a64",
   "metadata": {},
   "source": [
    "在先前的範例中，我們沒辦法決定從哪個時候開始錄音\n",
    "\n",
    "現在我們加入起始和結束的控制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982003f2-112d-4f98-99ab-201057574c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "FS = 44100\n",
    "CHANNELS = 1\n",
    "dtype = 'int16'\n",
    "\n",
    "recorded_frames = []\n",
    "\n",
    "def callback(indata, frames, time, status):\n",
    "    recorded_frames.append(indata.copy())\n",
    "\n",
    "def record_audio():\n",
    "    # 加入起始和結束的控制\n",
    "    with sd.InputStream(samplerate=FS, channels=CHANNELS, \n",
    "                        dtype=dtype, callback=callback):\n",
    "        input(\"Press Enter to start recording...\")\n",
    "        print(\"Recording... Press Enter again to stop.\")\n",
    "        input()\n",
    "        print(\"Recording stopped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d09aae-78a2-4346-99db-615fcd489d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear frames before each recording\n",
    "recorded_frames.clear()\n",
    "recording_thread = threading.Thread(target=record_audio)\n",
    "recording_thread.start()\n",
    "recording_thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e74e6c-aab6-489b-9751-06a99f3b185a",
   "metadata": {},
   "source": [
    "為何要使用threading而不是直接執行record_audio?\n",
    "\n",
    "- 直接執行record_audio會堵塞整個程式\n",
    "- 其他的代碼，像是UI update, logging, 背景處理等等都無法做任何事\n",
    "\n",
    "一個小型示範:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025cdeec-7885-4ac3-9b0a-3f245de7ead7",
   "metadata": {},
   "outputs": [],
   "source": [
    "recorded_frames.clear()\n",
    "recording_thread = threading.Thread(target=record_audio)\n",
    "recording_thread.start()\n",
    "\n",
    "print(\"同時在做其他的事情\")\n",
    "\n",
    "recording_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e535c9e-67ab-4b3b-8d55-9764ba6ad81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine recorded frames\n",
    "if recorded_frames:\n",
    "    audio_np = np.concatenate(recorded_frames, axis=0)\n",
    "else:\n",
    "    audio_np = np.array([], dtype=dtype)\n",
    "\n",
    "mp3_io =  audio_to_mp3_io(audio=audio_np, FS=FS, channels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cca2fd-f0bd-4671-8900-d3e6ebf213ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = client.audio.transcriptions.create(\n",
    "    model=\"gpt-4o-transcribe\", \n",
    "    file=mp3_io,\n",
    "    response_format=\"text\",\n",
    "    language='zh-tw',\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "buffer = \"\"\n",
    "for event in stream:\n",
    "    # Depending on the event format; e.g. maybe:\n",
    "    # - event.type might be “transcript.text.delta”\n",
    "    # - or event.delta or event.partial\n",
    "    try:\n",
    "        # Example if event has attribute 'delta'\n",
    "        partial = event.delta\n",
    "        buffer += partial\n",
    "        print(\"\\r\" + buffer, end=\"\", flush=True)\n",
    "        # print(partial, end=\"\", flush=True)\n",
    "    except AttributeError:\n",
    "        # fallback / final result\n",
    "        print(\"\\r\" + \" \" * len(buffer), end=\"\")  # 清掉這行\n",
    "        print(\"\\r\" + event.text)  # 印最終結果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9127f2e6-d2c2-4a5b-a994-7e7619c02492",
   "metadata": {},
   "source": [
    "## Translations\n",
    "\n",
    "翻譯 API 接收支持的任何語言的音頻文件作為輸入，並轉錄為英文。這與我們的 /Transcriptions 端點不同，因為輸出不是原始輸入語言的文本，而是轉換為英文文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3587450c-e007-447b-8b10-e8d3fcf90cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "recorded_frames.clear()\n",
    "recording_thread = threading.Thread(target=record_audio)\n",
    "recording_thread.start()\n",
    "recording_thread.join()\n",
    "\n",
    "# Combine recorded frames\n",
    "if recorded_frames:\n",
    "    audio_np = np.concatenate(recorded_frames, axis=0)\n",
    "else:\n",
    "    audio_np = np.array([], dtype=dtype)\n",
    "\n",
    "mp3_io =  audio_to_mp3_io(audio=audio_np, FS=FS, channels=1)\n",
    "\n",
    "# 這個接口(end point)不支援 stream\n",
    "output = client.audio.translations.create(\n",
    "    model=\"whisper-1\", \n",
    "    file=mp3_io,\n",
    "    response_format=\"text\",\n",
    ")\n",
    "\n",
    "print(output.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddef2b1d-4a22-498a-b5ad-b031d184c084",
   "metadata": {},
   "source": [
    "# Text to Speech\n",
    "\n",
    "對於智慧型即時應用程式，請使用 gpt-4o-mini-tts 模型——最新且最可靠的文字轉語音（Text-to-Speech, TTS）模型。\n",
    "你可以透過提示（prompt）來控制語音的多種特性，包括：\n",
    "\n",
    ">- 口音 (Accent)\n",
    ">- 情感表現範圍 (Emotional range)\n",
    ">- 語調 (Intonation)\n",
    ">- 語氣風格（模仿或印象）(Impressions)\n",
    ">- 語速 (Speed of speech)\n",
    ">- 聲音音色 (Tone)\n",
    ">- 低語（耳語）效果 (Whispering)\n",
    "\n",
    "Audio API 提供一個基於我們 TTS 模型的語音端點（speech endpoint）。它內建 11 種不同的語音，可用於語音生成與相關應用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29ac7ce-e1ca-4ec4-aeaf-649bbfae2751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "# By default, output format is in mp3\n",
    "\n",
    "speech_file_path = os.path.join(\"tutorial\", \"LLM+Langchain\", \"Week-6\", \"Sample.mp3\")\n",
    "\n",
    "response = client.audio.speech.create(\n",
    "  model=\"tts-1\",\n",
    "  voice=\"alloy\",\n",
    "  input=\"Today is Saturday, how are you?\")\n",
    "\n",
    "# 儲存為檔案\n",
    "response.stream_to_file(speech_file_path)\n",
    "\n",
    "# 播放 (simple, blocking)\n",
    "sound = AudioSegment.from_mp3(speech_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ca4eaa-01cf-43f4-8de8-8e2a4d8be536",
   "metadata": {},
   "source": [
    "gpt-4o-mini-tts 和 gp4-4o-tts 可以透過 prompt來控制語調"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f7d509-5873-4a50-8947-72b33321576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from playsound import playsound\n",
    "\n",
    "speech_file_path = os.path.join(\"tutorial/LLM+Langchain/Week-6/Sample.mp3\")\n",
    "\n",
    "response = client.audio.speech.create(\n",
    "    model=\"gpt-4o-mini-tts\",\n",
    "    voice=\"nova\",\n",
    "    input=\"おかえりなさい\",\n",
    "    instructions=\"Speak in a cute and affection tone, similar to the style of an anime girl.\")\n",
    "\n",
    "response.stream_to_file(speech_file_path)\n",
    "\n",
    "sound = AudioSegment.from_mp3(speech_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15255748-103c-4b05-8c96-ce1f3b7d63fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c49c90-b92c-4998-a4a5-834bac529007",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.audio.speech.create(\n",
    "    model=\"gpt-4o-mini-tts\",\n",
    "    voice=\"nova\",\n",
    "    input=\"おかえりなさい\",\n",
    "    instructions=\"Speak in a tiresome tone.\")\n",
    "\n",
    "response.stream_to_file(speech_file_path)\n",
    "\n",
    "sound = AudioSegment.from_mp3(speech_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4053601-b8ff-4644-ad0c-060dcb961d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "sound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758e0c1e-30cd-42d1-8a56-8a66b5b42c70",
   "metadata": {},
   "source": [
    "### pygame (good for GUI apps / async playback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed806af-6e5d-4f3f-acfb-6896b7ea393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79ad88c-46e0-4f7f-b543-f148726dbd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "\n",
    "pygame.mixer.init()\n",
    "pygame.mixer.music.load(speech_file_path)\n",
    "pygame.mixer.music.play()\n",
    "\n",
    "while pygame.mixer.music.get_busy():\n",
    "    pygame.time.Clock().tick(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a2fed0-5cc2-4f80-9c74-19bce586df66",
   "metadata": {},
   "source": [
    "### 串流\n",
    "\n",
    "在整個音訊檔案還沒完全下載完畢之前，播放就已經開始了\n",
    "將格式從壓縮的mp3換成無壓縮的wav\n",
    "\n",
    "** 開源社群臥虎藏龍\n",
    "\n",
    "https://community.openai.com/t/streaming-from-text-to-speech-api/493784/29\n",
    "\n",
    "在整個音訊檔案還沒完全下載完畢之前，播放就已經開始了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad57820-9be4-404a-a776-37171ea797f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c20973-e0fd-4998-ae32-4b9085368c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "input_ = dedent(\"\"\"\n",
    "鳴大鐘一次！\n",
    "推動杠杆，啟動活塞和泵……\n",
    "鳴大鐘兩次！\n",
    "按下按鈕，發動引擎，點燃渦輪，注入生命……\n",
    "鳴大鐘三次！\n",
    "齊聲歌唱，讚美萬機之神！\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75f4647-d525-4aac-b10c-ddb37cc40159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import sleep\n",
    "import wave\n",
    "import requests\n",
    "import pyaudio\n",
    "\n",
    "# WAV: Uncompressed WAV audio, suitable for low-latency applications to avoid decoding overhead.\n",
    "\n",
    "url = \"https://api.openai.com/v1/audio/speech\"\n",
    "headers = {\n",
    "    \"Authorization\": f'Bearer {os.getenv(\"OPENAI_API_KEY\")}',\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"model\": \"tts-1\",\n",
    "    \"input\": input_,\n",
    "    \"voice\": \"shimmer\",\n",
    "    \"response_format\": \"wav\",\n",
    "}\n",
    "\n",
    "# You send the request to the API (requests.post).\n",
    "# 這裡的 stream=True 是整個行為的關鍵。\n",
    "#🔹 stream=True 的作用\n",
    "\n",
    "# 設定 stream=True 之後，requests 函式庫不會一次把整個伺服器回應內容全部下載下來。\n",
    "# 它會回傳一個「串流的回應物件」（response.raw），\n",
    "# 讓你可以邊接收資料、邊處理內容。\n",
    "\n",
    "# 因此，當 OpenAI API 開始傳送音訊資料時，\n",
    "# 你就能立即從網路串流中讀取部分音訊，\n",
    "# 不需要等待整個 WAV 檔案都傳完。\n",
    "response = requests.post(url, headers=headers, json=data, stream=True)\n",
    "\n",
    "CHUNK_SIZE = 1024\n",
    "\n",
    "if response.ok:\n",
    "    # 這段程式中：\n",
    "    # wave.open(response.raw, 'rb') 開啟的是一個可「邊讀邊播」的串流。\n",
    "    with wave.open(response.raw, 'rb') as wf:\n",
    "        p = pyaudio.PyAudio()\n",
    "        # You configure the PyAudio stream (p.open(...)).\n",
    "        stream = p.open(format=p.get_format_from_width(wf.getsampwidth()),\n",
    "                        channels=wf.getnchannels(),\n",
    "                        rate=wf.getframerate(),\n",
    "                        output=True)\n",
    "\n",
    "        # wf.readframes(CHUNK_SIZE) 每次只讀取一小塊音訊（例如 1024 幀）。\n",
    "        # 如果下一塊音訊還沒從伺服器傳完，它會短暫等待，然後繼續播放。\n",
    "        # 因此，音訊一邊從伺服器傳過來，一邊就被播放出去。\n",
    "        while len(data := wf.readframes(CHUNK_SIZE)):\n",
    "            #這行負責撥放聲音\n",
    "            stream.write(data)\n",
    "\n",
    "        # Sleep to make sure playback has finished before closing\n",
    "        sleep(1)\n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "else:\n",
    "    response.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0c30fd-b30e-473e-aeab-38dfaa92e1cb",
   "metadata": {},
   "source": [
    "### 實時串流 (Real time streaming)\n",
    "\n",
    "語音 API 支援使用「分塊傳輸編碼（chunk transfer encoding）」進行即時音訊串流。\n",
    "這代表音訊可以在整個檔案尚未完全生成並可存取之前，就開始播放。\n",
    "\n",
    "為了獲得最快的回應速度，我們建議使用 wav 或 pcm 作為回傳的音訊格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8714661a-20ed-467a-ae94-a45a4799f105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "from openai.helpers import LocalAudioPlayer\n",
    "\n",
    "async_openai = AsyncOpenAI()\n",
    "\n",
    "async def main() -> None:\n",
    "    async with async_openai.audio.speech.with_streaming_response.create(\n",
    "        model=\"gpt-4o-mini-tts\",\n",
    "        voice=\"coral\",\n",
    "        input=\"Today is a wonderful day to build something people love!\",\n",
    "        instructions=\"Speak in a cheerful and positive tone.\",\n",
    "        response_format=\"pcm\",\n",
    "    ) as response:\n",
    "        await LocalAudioPlayer().play(response)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b73d33d-93b5-4a8a-baf4-46c6e3e489b0",
   "metadata": {},
   "source": [
    "# 語音自助點飲料系統\n",
    "\n",
    "這只是一個DEMO，所以會非常簡陋。我也不清楚這到底有沒有做的價值\n",
    "\n",
    "假設一個很簡單的使用情境: 一個人講了他對於飲料的需求，然後whisper從語音中抽取他對於飲料的需求:\n",
    "\n",
    "- 哪種飲料\n",
    "- 冰\n",
    "- 糖\n",
    "\n",
    "先不管利用TTS最回應機制，看看whisper能不能正確抽取內容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9042df29-7f51-4dc6-b357-9662d07b5954",
   "metadata": {},
   "source": [
    "### 清心\n",
    "\n",
    "正式做的話應該是直接開 WebScraping\n",
    "\n",
    "- 珍珠蜂蜜鮮奶普洱\n",
    "- 茶凍奶綠\n",
    "- 嚴選高山茶\n",
    "- 咖啡奶茶\n",
    "- 冬瓜檸檬\n",
    "\n",
    "冰熱: 正常冰 - 少冰 - 微冰 - 去冰\n",
    "甜度: 無糖 - 微糖 - 半糖 - 少糖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e08088a-d765-48bf-8354-00e631b359d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8171c1dd-8084-4f18-afae-a378de073e2b",
   "metadata": {},
   "source": [
    "把之前學的模板copy/paste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36f0ebdb-bc99-4a66-9ac1-34a3e9308b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ling\\miniconda3\\envs\\aicg\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from textwrap import dedent\n",
    "from typing import Literal, List\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts.image import ImagePromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_core.runnables import Runnable, chain\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from src.initialization import credential_init\n",
    "\n",
    "\n",
    "def build_standard_chat_prompt_template(kwargs):\n",
    "    messages = []\n",
    "\n",
    "    if 'system' in kwargs:\n",
    "        content = kwargs.get('system')\n",
    "\n",
    "        # allow list of prompts for multimodal\n",
    "        if isinstance(content, list):\n",
    "            prompts = [PromptTemplate(**c) for c in content]\n",
    "        else:\n",
    "            prompts = [PromptTemplate(**content)]\n",
    "\n",
    "        message = SystemMessagePromptTemplate(prompt=prompts)\n",
    "        messages.append(message)\n",
    "\n",
    "    if 'human' in kwargs:\n",
    "        content = kwargs.get('human')\n",
    "\n",
    "        # allow list of prompts for multimodal\n",
    "        if isinstance(content, list):\n",
    "            prompts = []\n",
    "            for c in content:\n",
    "                if c.get(\"type\") == \"image\":\n",
    "                    prompts.append(ImagePromptTemplate(**c))\n",
    "                else:\n",
    "                    prompts.append(PromptTemplate(**c))\n",
    "        else:\n",
    "            if content.get(\"type\") == \"image\":\n",
    "                prompts = [ImagePromptTemplate(**content)]\n",
    "            else:\n",
    "                prompts = [PromptTemplate(**content)]\n",
    "\n",
    "        message = HumanMessagePromptTemplate(prompt=prompts)\n",
    "        messages.append(message)\n",
    "\n",
    "    chat_prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "    \n",
    "    return chat_prompt_template\n",
    "\n",
    "credential_init()\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o-mini\", \n",
    "                   temperature=0\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd818cd5-8ab0-4c90-a1bf-2365cb86ace3",
   "metadata": {},
   "source": [
    "### 定義輸出格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68478b3c-25a2-4ddf-b566-a59be9476f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Drink(BaseModel):\n",
    "\n",
    "    name: Literal['珍珠蜂蜜鮮奶普洱', '茶凍奶綠', '嚴選高山茶', \n",
    "                  '咖啡奶茶', '冬瓜檸檬'] = Field(None, description=\"飲料名稱\")\n",
    "    ice_level: Literal['正常冰', '少冰', '微冰', '去冰'] = Field(\"正常冰\", description='冰熱程度')\n",
    "    sugar_level: Literal['無糖', '微糖', '半糖' , '少糖'] = Field(\"少糖\", description='糖度')\n",
    "\n",
    "\n",
    "class Order(BaseModel):\n",
    "\n",
    "    names: List[Drink] = Field(description=(\"用戶點的飲料\"))\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=Order)\n",
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ba413b-344e-47d6-8cb7-9f24dd6326fc",
   "metadata": {},
   "source": [
    "### 模擬從whisper得到用戶需求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d81447a-54e2-48c0-8377-ec4207ea973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = dedent(\"\"\"\\\n",
    "You are a friendly and helpful AI assistant acting as a beverage shop clerk (飲料店店員).\n",
    "\n",
    "Your role is to interact naturally with customers, understand their preferences, and help them place drink orders.\n",
    "\n",
    "The ordered beverage must be from the 飲料名稱. If not, you will reply `錯誤`\n",
    "\"\"\")\n",
    "\n",
    "human_template = dedent(\"\"\"\\\n",
    "                 {query}\n",
    "                 format instruction: {format_instructions}\n",
    "                 \"\"\")\n",
    "\n",
    "input_ = {\"sytem\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"query\"],\n",
    "                    \"partial_variables\": {\"format_instructions\": \n",
    "                                          format_instructions}}}\n",
    "\n",
    "chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "extraction_pipeline = chat_prompt_template | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d80b9c-81e2-4d3e-af01-30880709edd3",
   "metadata": {},
   "source": [
    "## 透過語言模型提取關鍵字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c278420-1689-4725-baf4-578fc12bc57f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Order(names=[Drink(name='冬瓜檸檬', ice_level='正常冰', sugar_level='少糖'), Drink(name='珍珠蜂蜜鮮奶普洱', ice_level='正常冰', sugar_level='少糖')])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_pipeline.invoke({\"query\": \"我要一杯冬瓜檸檬和一杯珍珠蜂蜜鮮奶普洱\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d212b3-f2bf-4fe8-b566-dd07fb1adb0c",
   "metadata": {},
   "source": [
    "不指定糖度和冰量的話，就會回傳預設值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb90e57f-b7c1-46bf-9531-dc42d870f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_pipeline.invoke({\"query\": \"我要一杯冬瓜檸檬，微糖，去冰\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17d4efd-a084-4437-979c-236f19459f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_pipeline.invoke({\"query\": \"我要一杯冬瓜檸檬和一杯嚴選高山茶。冬瓜檸檬微糖去冰，高山茶無糖微冰\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d09b68-89cc-49c9-b5cc-3b1cb2fcc50e",
   "metadata": {},
   "source": [
    "目前看起來都很正常，萬一用戶要求一個風馬牛不相及的問題呢?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6721362-e5ba-4469-a38c-eb69b768a0bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Order(names=[Drink(name='珍珠蜂蜜鮮奶普洱', ice_level='正常冰', sugar_level='少糖'), Drink(name='茶凍奶綠', ice_level='少冰', sugar_level='微糖')])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_pipeline.invoke({\"query\": \"我要一本漫畫\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659da892-06af-4204-9998-d7bc38ae67a5",
   "metadata": {},
   "source": [
    "Oops...\n",
    "\n",
    "- PydanticOutputParser 會強制讓輸出結果符合你定義的資料結構（schema）。\n",
    "\n",
    "- 大型語言模型（LLM）本身並沒有「拒絕回答」或「不適用」的概念 —— 它會盡力讓輸出看起來符合這個結構。\n",
    "\n",
    "- 因此，即使輸入的內容與飲料訂單完全無關，模型仍可能「幻覺式地」生成一個看似有效的 Order 結果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "13a63724-dcc3-48bb-9fd3-7cbe0a2f6815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "class Drink(BaseModel):\n",
    "    \n",
    "    name: Literal['珍珠蜂蜜鮮奶普洱', '茶凍奶綠', '嚴選高山茶', \n",
    "                  '咖啡奶茶', '冬瓜檸檬'] = Field(..., description=\"飲料名稱\")\n",
    "    ice_level: Literal['正常冰', '少冰', '微冰', '去冰'] = Field(\"正常冰\", description='冰熱程度')\n",
    "    sugar_level: Literal['無糖', '微糖', '半糖' , '少糖'] = Field(\"少糖\", description='糖度')\n",
    "\n",
    "\n",
    "class Order(BaseModel):\n",
    "\n",
    "    relevant: bool = Field(..., description=\"是否為飲料訂單。若使用者並非點飲料，請輸出 False。\")\n",
    "    names: Optional[List[Drink]] = Field(default=None, description=\"若 relevant 為 True，則列出飲料。\")\n",
    "\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=Order)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "input_ = {\"sytem\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"query\"],\n",
    "                    \"partial_variables\": {\"format_instructions\": \n",
    "                                          format_instructions}}}\n",
    "\n",
    "chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "extraction_pipeline = chat_prompt_template | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ea2b3740-6f45-48e7-9669-f09d0bef8f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Order(relevant=False, names=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_pipeline.invoke({\"query\": \"我要一本漫畫\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "13264cf8-92e6-4c10-85c3-bdf728b3d682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Order(relevant=False, names=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_pipeline.invoke({\"query\": \"我要一杯啤酒\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "770601d1-b8bd-43f7-97ce-7665d74f0b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Order(relevant=True, names=[Drink(name='冬瓜檸檬', ice_level='去冰', sugar_level='微糖'), Drink(name='嚴選高山茶', ice_level='微冰', sugar_level='無糖')])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_pipeline.invoke({\"query\": \"我要一杯冬瓜檸檬和一杯嚴選高山茶。冬瓜檸檬微糖去冰，高山茶無糖微冰\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32162fa-70fa-479d-b440-ecb2c3f6eac8",
   "metadata": {},
   "source": [
    "透過這個方法，可以在一避免用戶搞些有的沒的。\n",
    "\n",
    "另一個選擇是使用Guardrails: 可以使用OpenAI Agent SDK，然後再輸入的地方加入一個檢查系統。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8846fb3d-e6c3-4541-9502-901c4405f3cf",
   "metadata": {},
   "source": [
    "## 輸入從文字換成語音"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3253472-0f80-46bf-aa53-8758bb13c60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import threading\n",
    "\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "from openai import OpenAI\n",
    "from pydub import AudioSegment\n",
    "\n",
    "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "\n",
    "FS = 44100\n",
    "CHANNELS = 1\n",
    "dtype = 'int16'\n",
    "\n",
    "recorded_frames = []\n",
    "\n",
    "def callback(indata, frames, time, status):\n",
    "    recorded_frames.append(indata.copy())\n",
    "\n",
    "\n",
    "def record_audio():\n",
    "    # 加入起始和結束的控制\n",
    "    with sd.InputStream(samplerate=FS, channels=CHANNELS, \n",
    "                        dtype=dtype, callback=callback):\n",
    "        input(\"Press Enter to start recording...\")\n",
    "        print(\"Recording... Press Enter again to stop.\")\n",
    "        input()\n",
    "        print(\"Recording stopped.\")\n",
    "\n",
    "\n",
    "def audio_to_mp3_io(audio, FS:int, channels: int):\n",
    "\n",
    "    # Convert numpy array to AudioSegment\n",
    "    audio_bytes = audio.tobytes()\n",
    "    \n",
    "    audio_segment = AudioSegment(\n",
    "        data=audio_bytes,\n",
    "        sample_width=audio.dtype.itemsize,\n",
    "        frame_rate=FS,\n",
    "        channels=channels\n",
    "    )\n",
    "    \n",
    "    # Save as MP3 in-memory (as an object)\n",
    "    mp3_io = io.BytesIO()\n",
    "    audio_segment.export(mp3_io, format=\"mp3\")\n",
    "    mp3_io.seek(0)  # Rewind to start\n",
    "\n",
    "    mp3_io.name = \"word.mp3\"\n",
    "\n",
    "    return mp3_io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03faf9d3-ac0a-40d8-86de-ae457ce3bf9f",
   "metadata": {},
   "source": [
    "我要一杯冬瓜檸檬和一杯嚴選高山茶。冬瓜檸檬微糖去冰，高山茶無糖微冰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34a9c39-21db-424b-a570-ed25673753b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "recorded_frames.clear()\n",
    "recording_thread = threading.Thread(target=record_audio)\n",
    "recording_thread.start()\n",
    "recording_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27420898-7b98-4e6e-a8a0-f7fd15af957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Combine recorded frames\n",
    "if recorded_frames:\n",
    "    audio_np = np.concatenate(recorded_frames, axis=0)\n",
    "else:\n",
    "    audio_np = np.array([], dtype=dtype)\n",
    "\n",
    "mp3_io =  audio_to_mp3_io(audio=audio_np, FS=FS, channels=1)\n",
    "\n",
    "# 這個接口(end point)不支援 stream\n",
    "\n",
    "time_begin = datetime.now()\n",
    "whisper_output = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\", \n",
    "    file=mp3_io,\n",
    "    response_format=\"text\",\n",
    "    prompt=\"珍珠蜂蜜鮮奶普洱, 茶凍奶綠, 嚴選高山茶, 咖啡奶茶, 冬瓜檸檬, 正常冰, 少冰, 微冰, 去冰, 無糖, 微糖, 半糖, 少糖\",\n",
    ")\n",
    "time_end = datetime.now()\n",
    "\n",
    "print(time_end - time_begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204dba78-4324-42c4-890d-67a1eaaed187",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fb412f-64a1-43f8-a4ad-a3c919f40be2",
   "metadata": {},
   "source": [
    "## 計算帳單和回應"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df62862-844f-45b5-94f7-8de7ecc1c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = extraction_pipeline.invoke({\"query\": whisper_output})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb54636f-fd31-43ff-996e-1f05663e6552",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_map = {\"珍珠蜂蜜鮮奶普洱\": 70,\n",
    "         \"茶凍奶綠\": 50,\n",
    "         \"嚴選高山茶\": 35,\n",
    "         \"咖啡奶茶\": 75,\n",
    "         \"冬瓜檸檬\": 60}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07a22d7-1731-4a7b-83b2-e35281c31a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6138e1-a737-4bb2-a165-bfb6ea0507ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_price = 0\n",
    "\n",
    "for order in orders.names:\n",
    "    total_price += price_map[order.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9209b457-86d6-44eb-903c-ac7d5981ee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9c6897-0e84-44e8-a76b-7e7f2c66b706",
   "metadata": {},
   "source": [
    "使用語音進行最後的回應"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd991a8-6a16-4e3c-9d01-56c3a6e6f0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "from openai.helpers import LocalAudioPlayer\n",
    "\n",
    "async_openai = AsyncOpenAI()\n",
    "\n",
    "async def main(price) -> None:\n",
    "    async with async_openai.audio.speech.with_streaming_response.create(\n",
    "        model=\"gpt-4o-mini-tts\",\n",
    "        voice=\"nova\",\n",
    "        input=f\"一共{price}元 💕\",\n",
    "        instructions=\"Speak in a sweet, energetic, anime-girl style with a cute and playful tone.\",\n",
    "        response_format=\"pcm\",\n",
    "    ) as response:\n",
    "        await LocalAudioPlayer().play(response)\n",
    "\n",
    "await main(price=total_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca789bf9-07c9-49e2-bd4c-e3e3fe845b48",
   "metadata": {},
   "source": [
    "可以的話看能不能找個第三方語音服務供應商，提供那種甜死人不償命的語音服務。生意保證好。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05121cc-d7c6-4eaa-8962-982600e9abce",
   "metadata": {},
   "source": [
    "開啟Langserve，進行後端測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5c9e9b-ef26-44e2-b8da-c3a08d7a8429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "whisper_output = '我要一杯冬瓜檸檬和一杯嚴選高山茶, 冬瓜檸檬微糖去冰, 嚴選高山茶無糖微冰。'\n",
    "\n",
    "payload = {'input': {\"query\": whisper_output}}\n",
    "\n",
    "\"\"\"\n",
    "- ensure_ascii=False → keeps Chinese characters as-is.\n",
    "\n",
    "- .encode('utf-8') → ensures bytes sent are UTF-8.\n",
    "\n",
    "- Header \"charset=utf-8\" → tells the server the encoding.\n",
    "\"\"\"\n",
    "\n",
    "response = requests.post(\n",
    "        \"http://localhost:8080/drinking_app/invoke\",\n",
    "        json={'input': {\"query\": whisper_output}}\n",
    "    )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c78c3efa-ad1a-416e-bd7a-74e39040d5b3",
   "metadata": {},
   "source": [
    "response.encoding = 'utf-8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5998ed-e65c-498a-a72e-1f6b5f3e1566",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = eval(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77f5fbf-1107-43fb-aee3-3aec63700a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "order['output']['names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed5299d-a748-479f-aa99-a45438a500c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "price_map = {\"珍珠蜂蜜鮮奶普洱\": 70,\n",
    "         \"茶凍奶綠\": 50,\n",
    "         \"嚴選高山茶\": 35,\n",
    "         \"咖啡奶茶\": 75,\n",
    "         \"冬瓜檸檬\": 60}\n",
    "\n",
    "df = pd.DataFrame(order['output']['names'])\n",
    "\n",
    "df['price'] = df['name'].map(price_map)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c2e433-c5b8-4ef3-914b-445e523a6a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe128a5a-c196-4ad1-88dd-a181cf1ae883",
   "metadata": {},
   "source": [
    "建立起 streamlit + flask + langserve的服務。詳細的內容看 python script檔"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ec0b23-08c6-4fcc-946d-36ee03b5075e",
   "metadata": {},
   "source": [
    "# Ollama\n",
    "\n",
    "使用開源的優勢:\n",
    "\n",
    "- 機敏資料\n",
    "- 由於API都有Censorship，所以開源另一個主戰場是NSFW內容生成\n",
    "\n",
    "OpenAI 預告在今年12月，ChatGPT文字上可以產生NFSW內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0020b5ba-c7cc-48f3-adb1-a074c04abe16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: space-around;\">\n",
       "    <div>\n",
       "        <img src=\"Sam Altman.png\" height=\"900\" width=\"600\" />\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Define the HTML to display images side by side\n",
    "html = \"\"\"\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "    <div>\n",
    "        <img src=\"Sam Altman.png\" height=\"900\" width=\"600\" />\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "# Display the HTML\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea87747-33ae-4c4f-bd86-1cd40fb12ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "This package enables you using open-source LLM with ease.\n",
    "\n",
    "We borrow the content from last week\n",
    "\n",
    "https://medium.com/@abonia/running-ollama-in-google-colab-free-tier-545609258453\n",
    "\n",
    "- curl https://ollama.ai/install.sh | sh\n",
    "- ollama serve &\n",
    "- ollama pull llama3:8b\n",
    "- ollama pull dolphin-llama3:8b\n",
    "- ollama pull huihui_ai/qwen2.5-abliterate:14b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9fb9e0-16eb-4c03-a1b2-2af693b479d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52511b57-7779-4db4-8e8e-dd9d0071f50e",
   "metadata": {},
   "source": [
    "In Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89b5037-ba65-4dac-bf0f-c4d43daeb732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install colab-xterm\n",
    "# %load_ext colabxterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04cc1ad-5440-4e65-bc38-07d2139afa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %xterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd597a5-1f65-4b5d-935d-5bc97fb2ad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from torch import cuda\n",
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_core.prompts.image import ImagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import Runnable, chain\n",
    "\n",
    "from src.io.path_definition import get_project_dir\n",
    "\n",
    "\n",
    "def build_standard_chat_prompt_template(kwargs):\n",
    "    messages = []\n",
    "\n",
    "    if 'system' in kwargs:\n",
    "        content = kwargs.get('system')\n",
    "\n",
    "        # allow list of prompts for multimodal\n",
    "        if isinstance(content, list):\n",
    "            prompts = [PromptTemplate(**c) for c in content]\n",
    "        else:\n",
    "            prompts = [PromptTemplate(**content)]\n",
    "\n",
    "        message = SystemMessagePromptTemplate(prompt=prompts)\n",
    "        messages.append(message)\n",
    "\n",
    "    if 'human' in kwargs:\n",
    "        content = kwargs.get('human')\n",
    "\n",
    "        # allow list of prompts for multimodal\n",
    "        if isinstance(content, list):\n",
    "            prompts = []\n",
    "            for c in content:\n",
    "                if c.get(\"type\") == \"image\":\n",
    "                    prompts.append(ImagePromptTemplate(**c))\n",
    "                else:\n",
    "                    prompts.append(PromptTemplate(**c))\n",
    "        else:\n",
    "            if content.get(\"type\") == \"image\":\n",
    "                prompts = [ImagePromptTemplate(**content)]\n",
    "            else:\n",
    "                prompts = [PromptTemplate(**content)]\n",
    "\n",
    "        message = HumanMessagePromptTemplate(prompt=prompts)\n",
    "        messages.append(message)\n",
    "\n",
    "    chat_prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "    \n",
    "    return chat_prompt_template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888775f5-e632-41c5-99cf-406d70ee63b7",
   "metadata": {},
   "source": [
    "Ollama 使用模型跟 OpenAI API 沒有區別\n",
    "\n",
    "但建議架設在有強大算力的機器上，並且使用Langserve呼叫服務，來減輕本地的算力需求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3d7f97-f573-4350-a946-4a6a8af963df",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"You are a helpful AI assistant with excellent writing skill\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c5f546-eccf-4a24-a3e9-0229a454d529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "stop_token_ids = None\n",
    "model_id = \"dolphin-llama3:8b\"\n",
    "\n",
    "device = f\"cuda:{cuda.current_device()}\" if cuda.is_available() else 'cpu'\n",
    "\n",
    "model = ChatOllama(model=model_id, temperature=0)\n",
    "\n",
    "summary_prompt_template = build_summary_prompt_template()\n",
    "\n",
    "summary_pipeline = summary_prompt_template | model | StrOutputParser()\n",
    "\n",
    "text_as_list = []\n",
    "for document in tqdm(documents):\n",
    "    content = summary_pipeline.invoke({\"text\": document.page_content})\n",
    "    text_as_list.append(content)\n",
    "\n",
    "final_text = \"\\n\".join(text_as_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a034f8-82cb-4784-ad35-f6d10fe01c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_pipeline.invoke({\"text\": final_text})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
