{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e9bd1a7-ba1e-491e-b83e-a31ac4dd06d9",
   "metadata": {},
   "source": [
    "# Speech to text\n",
    "\n",
    "- Transcribe audio into whatever language the audio is in.\n",
    "- Translate and transcribe the audio into english.\n",
    "\n",
    "File uploads are currently limited to 25 MB and the following input file types are supported: mp3, mp4, mpeg, mpga, m4a, wav, and webm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacd9ac3-0fd0-4ce8-9c63-87a94202ed22",
   "metadata": {},
   "source": [
    "## Audio models\n",
    "\n",
    "Whisper can transcribe speech into text and translate many languages into English. ",
    " ",
    "\n",
    "\n",
    "Text-to-speech (TTS) can convert text into spoken audio.\n",
    "\n",
    "Learn about Whisper (opens in a new window)\n",
    "Learn about Text-to-speech (TTS) (opens in a new window)\n",
    "\n",
    "\n",
    "| Model   | Usage                                            |\n",
    "|---------|--------------------------------------------------|\n",
    "| Whisper |  \\$ 0.006 / minute rounded to the nearest second     |\n",
    "| TTS     |  \\$ 15.00 / 1M characters                          |\n",
    "| TTS HD  |  \\$ 30.00 / 1M characters                          |\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bb5d86-d176-43a1-9f92-d0e3babe9529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476b8dcb-ece8-4730-a8d9-5a0fad0e6abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.initialization import credential_init\n",
    "from src.io.path_definition import get_project_dir, get_file\n",
    "\n",
    "\n",
    "credential_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8465282-3b41-422d-a904-8c62ecd196f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baa97bc-b426-418e-820f-618b18a2df30",
   "metadata": {},
   "source": [
    "## Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3973567-5159-4e06-9a12-6ed69cb77cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file= open(\"tutorial/LLM+Langchain/Week-6/05_12_2013_Torti_CLAS_1.mp3\", \"rb\")\n",
    "\n",
    "transcription = client.audio.transcriptions.create(\n",
    "  model=\"whisper-1\", \n",
    "  file=audio_file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71566743-14a9-4c61-9cfb-156c973f8639",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription.text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a772b55-ecbe-4f2b-98dd-dec28fda608a",
   "metadata": {},
   "source": [
    "## Improving reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c1662d-787c-4624-9d99-408c51b0b831",
   "metadata": {},
   "source": [
    "### Prompt parameter\n",
    "\n",
    "\n",
    "\n",
    "As we explored in the prompting section, one of the most common challenges faced when using Whisper is the model often does not recognize uncommon words or acronyms. To address this, we have highlighted different techniques which improve the reliability of Whisper in these cases\n",
    "\n",
    "正如我們在提示部分探討的那樣，使用 Whisper 時面臨的一個最常見挑戰是模型經常無法識別不常見的單詞或縮略詞。為了解決這個問題，我們強調了不同的技術，這些技術在這些情況下提高了 Whisper 的可靠性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39e49b0-0c27-4726-a681-0c76df69c3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# audio_file = open(\"/path/to/file/speech.mp3\", \"rb\")\n",
    "# transcription = client.audio.transcriptions.create(\n",
    "#   model=\"whisper-1\", \n",
    "#   file=audio_file, \n",
    "#   response_format=\"text\",\n",
    "#   prompt=\"ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T.\"\n",
    "# )\n",
    "# print(transcription.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e666860-b7a0-4e2c-9529-d76bce50234b",
   "metadata": {},
   "source": [
    "- Sometimes the model might skip punctuation in the transcript. You can avoid this by using a simple prompt that includes \n",
    "punctuation: \"Hello, welcome to my lecture.\"\n",
    "\n",
    "- The model may also leave out common filler words in the audio. If you want to keep the filler words in your transcript, you can use a prompt that contains them: \"Umm, let me think like, hmm... Okay, here's what I'm, like, thinking.\"\n",
    "\n",
    "- Some languages can be written in different ways, such as simplified or traditional Chinese. The model might not always use the writing style that you want for your transcript by default. You can improve this by using a prompt in your preferred writing style.\n",
    "\n",
    "\n",
    "- 有時模型可能會在轉錄中略過標點符號。您可以通過使用包含標點符號的簡單提示來避免這種情況：\"你好，歡迎來到我的講座。\n",
    "\n",
    "- 模型也可能會省略音頻中的常見填充詞。如果您想在轉錄中保留填充詞，可以使用包含這些詞的提示：\"嗯，讓我想想，像，嗯……好吧，這是我，像，正在想的。\n",
    "\n",
    "- 有些語言可以用不同的方式書寫，例如簡體中文或繁體中文。模型可能無法總是默認使用您想要的書寫風格來轉錄。您可以通過使用您偏好的書寫風格的提示來改善這種情況。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d0df6c-390d-4cb1-9794-d8795e0b8f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file= open(\"tutorial/LLM+Langchain/Week-6/教育部 學生水域安全 國語30秒.mp3\", \"rb\")\n",
    "\n",
    "transcription_raw = client.audio.transcriptions.create(\n",
    "  model=\"whisper-1\", \n",
    "  file=audio_file\n",
    ")\n",
    "print(transcription_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992a53ef-0cc9-4c9c-bb56-9df5a482fbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file= open(\"tutorial/LLM+Langchain/Week-6/教育部 學生水域安全 國語30秒.mp3\", \"rb\")\n",
    "\n",
    "transcription_raw = client.audio.transcriptions.create(\n",
    "      model=\"whisper-1\", \n",
    "      file=audio_file,\n",
    "      language='zh'\n",
    ")\n",
    "print(transcription_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b252c0-0467-4a97-9b88-29897b3d6055",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file= open(\"tutorial/LLM+Langchain/Week-6/教育部 學生水域安全 國語30秒.mp3\", \"rb\")\n",
    "\n",
    "transcription_raw = client.audio.transcriptions.create(\n",
    "      model=\"whisper-1\", \n",
    "      file=audio_file,\n",
    "      language='zh',\n",
    "      response_format=\"srt\"\n",
    ")\n",
    "print(transcription_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62330a42-2eb2-43d0-b7f1-d1f3379c7570",
   "metadata": {},
   "source": [
    "We can instruct the translation style.\n",
    "\n",
    "- \"gpt-4o(-mini)-tts\"\n",
    "- \"gpt-4o(-mini)-transcribe\"\n",
    "\n",
    "openai 1.68.2 is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c3030d-22cf-42ca-a5c4-d0cb16cd177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file= open(\"tutorial/LLM+Langchain/Week-6/教育部 學生水域安全 國語30秒.mp3\", \"rb\")\n",
    "\n",
    "transcription_raw = client.audio.transcriptions.create(\n",
    "    model=\"gpt-4o-mini-transcribe\", \n",
    "    file=audio_file,\n",
    "    response_format=\"text\",\n",
    "\n",
    ")\n",
    "print(transcription_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbb2ada-adc9-4d05-819e-8cb743b4208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file= open(\"tutorial/LLM+Langchain/Week-6/教育部 學生水域安全 國語30秒.mp3\", \"rb\")\n",
    "\n",
    "transcription_raw = client.audio.transcriptions.create(\n",
    "    model=\"gpt-4o-mini-transcribe\", \n",
    "    file=audio_file,\n",
    "    response_format=\"text\",\n",
    "    language='zh' \n",
    ")\n",
    "print(transcription_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eeecb8-b793-4611-8d92-d10e47ecaff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file= open(\"tutorial/LLM+Langchain/Week-6/教育部 學生水域安全 國語30秒.mp3\", \"rb\")\n",
    "\n",
    "transcription_raw = client.audio.transcriptions.create(\n",
    "    model=\"gpt-4o-mini-transcribe\", \n",
    "    file=audio_file,\n",
    "    response_format=\"text\",\n",
    "    language='zh-tw'\n",
    " \n",
    ")\n",
    "print(transcription_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c441b7ee-8b90-44e2-a735-09415feeb254",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file= open(\"tutorial/LLM+Langchain/Week-6/教育部 學生水域安全 國語30秒.mp3\", \"rb\")\n",
    "\n",
    "transcription_raw = client.audio.transcriptions.create(\n",
    "    model=\"gpt-4o-mini-transcribe\", \n",
    "    file=audio_file,\n",
    "    response_format=\"json\",\n",
    "    language='zh-tw'\n",
    " \n",
    ")\n",
    "print(transcription_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818cf67e-4cd8-426c-8aad-13953cb08dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file= open(\"tutorial/LLM+Langchain/Week-6/教育部 學生水域安全 國語30秒.mp3\", \"rb\")\n",
    "\n",
    "transcription_raw = client.audio.transcriptions.create(\n",
    "    model=\"gpt-4o-transcribe\", \n",
    "    file=audio_file,\n",
    "    response_format=\"json\",\n",
    "    language='zh-tw'\n",
    " \n",
    ")\n",
    "print(transcription_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78276efd-7783-49f9-bb32-b65a6983b93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription_raw.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3682fb84-3457-4fb9-8520-8a591cbe6c7e",
   "metadata": {},
   "source": [
    "## Let's try to record our words\n",
    "\n",
    "- https://www.gyan.dev/ffmpeg/builds/\n",
    "- pip install sounddevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ba539e-8801-4061-9f4d-d257df2d41ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "from pydub import AudioSegment\n",
    "\n",
    "\n",
    "DURATION = 5  # seconds\n",
    "FS = 44100    # sample rate\n",
    "\n",
    "print(\"Recording...\")\n",
    "\n",
    "# Record audio\n",
    "audio = sd.rec(int(DURATION * FS), samplerate=FS, channels=1, dtype='int16')\n",
    "sd.wait()  # Wait until recording is finished\n",
    "print(\"Recording finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1b0049-cbaf-49db-acd5-76b332838673",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "AudioSegment.converter = r\"C:\\Users\\MengChieh\\ffmpeg-7.1.1-essentials_build\\bin\\ffmpeg.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4273256-2a37-4f3e-ae45-8e94d653e405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy array to AudioSegment\n",
    "audio_bytes = audio.tobytes()\n",
    "audio_segment = AudioSegment(\n",
    "    data=audio_bytes,\n",
    "    sample_width=audio.dtype.itemsize,\n",
    "    frame_rate=FS,\n",
    "    channels=1\n",
    ")\n",
    "\n",
    "# Save as MP3 in-memory (as an object)\n",
    "mp3_io = io.BytesIO()\n",
    "audio_segment.export(mp3_io, format=\"mp3\")\n",
    "mp3_io.seek(0)  # Rewind to start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b77e6b-38f0-4029-8322-c5eb6bdfa28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tutorial/LLM+Langchain/Week-6/output.mp3', 'wb') as f:\n",
    "    f.write(mp3_io.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc7f154-0e3e-4965-84dc-974e430ff0e6",
   "metadata": {},
   "source": [
    "Feed voice to transcriptions:\n",
    "1. feed the file\n",
    "2. feed the mp3_io object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9823310b-cbd6-4f13-9934-ac5925735bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://community.openai.com/t/openai-whisper-send-bytes-python-instead-of-filename/84786/3\n",
    "\n",
    "mp3_io.name = \"word.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9229f8c1-3833-4d0d-aa5e-f7132a73a3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription_raw = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\", \n",
    "    file=mp3_io,\n",
    "    response_format=\"text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fd4bcb-3935-4046-8f60-c67f4c4c9d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a08467-5af8-4802-bb27-0b9e04f85a64",
   "metadata": {},
   "source": [
    "In the previous example, we could not decide when to start and when to end the input, can we have a better control?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982003f2-112d-4f98-99ab-201057574c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "FS = 44100\n",
    "CHANNELS = 1\n",
    "dtype = 'int16'\n",
    "\n",
    "recorded_frames = []\n",
    "\n",
    "def callback(indata, frames, time, status):\n",
    "    recorded_frames.append(indata.copy())\n",
    "\n",
    "def record_audio():\n",
    "    with sd.InputStream(samplerate=FS, channels=CHANNELS, \n",
    "                        dtype=dtype, callback=callback):\n",
    "        input(\"Press Enter to start recording...\")\n",
    "        print(\"Recording... Press Enter again to stop.\")\n",
    "        input()\n",
    "        print(\"Recording stopped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d09aae-78a2-4346-99db-615fcd489d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear frames before each recording\n",
    "recorded_frames.clear()\n",
    "recording_thread = threading.Thread(target=record_audio)\n",
    "recording_thread.start()\n",
    "recording_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e535c9e-67ab-4b3b-8d55-9764ba6ad81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine recorded frames\n",
    "if recorded_frames:\n",
    "    audio_np = np.concatenate(recorded_frames, axis=0)\n",
    "else:\n",
    "    audio_np = np.array([], dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6116a571-c334-408c-bdc4-603a1c051900",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_bytes = audio_np.tobytes()\n",
    "audio_segment = AudioSegment(\n",
    "    data=audio_bytes,\n",
    "    sample_width=audio.dtype.itemsize,\n",
    "    frame_rate=FS,\n",
    "    channels=1\n",
    ")\n",
    "\n",
    "# Save as MP3 in-memory (as an object)\n",
    "mp3_io = io.BytesIO()\n",
    "audio_segment.export(mp3_io, format=\"mp3\")\n",
    "mp3_io.seek(0)  # Rewind to start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cca2fd-f0bd-4671-8900-d3e6ebf213ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp3_io.name = \"next_example.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93434bec-ab03-4c9d-87f9-5489027af7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription_raw = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\", \n",
    "    file=mp3_io,\n",
    "    response_format=\"text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf1d6f8-e16c-4100-a3a7-80beedaa2035",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9127f2e6-d2c2-4a5b-a994-7e7619c02492",
   "metadata": {},
   "source": [
    "## Translations\n",
    "\n",
    "The translations API takes as input the audio file in any of the supported languages and transcribes, if necessary, the audio into English. This differs from our /Transcriptions endpoint since the output is not in the original input language and is instead translated to English text.\n",
    "\n",
    "\n",
    "翻譯 API 接收支持的任何語言的音頻文件作為輸入，並將其必要時轉錄為英文。這與我們的 /Transcriptions 端點不同，因為輸出不是原始輸入語言的文本，而是轉換為英文文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3587450c-e007-447b-8b10-e8d3fcf90cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = open(\"tutorial/LLM+Langchain/Week-6/教育部 學生水域安全 國語30秒.mp3\", \"rb\")\n",
    "translation = client.audio.translations.create(\n",
    "  model=\"whisper-1\", \n",
    "  file=audio_file\n",
    ")\n",
    "print(translation.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a795574-9203-409d-8df0-05191bcfb85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = open(\"tutorial/LLM+Langchain/Week-6/教育部 學生水域安全 國語30秒.mp3\", \"rb\")\n",
    "translation = client.audio.translations.create(\n",
    "    model=\"whisper-1\", \n",
    "    file=audio_file,\n",
    "    prompt=\"funny style\"\n",
    ")\n",
    "print(translation.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10dd861-69c4-4253-aa97-60f2416f5ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = open(\"tutorial/LLM+Langchain/Week-6/教育部 學生水域安全 國語30秒.mp3\", \"rb\")\n",
    "translation = client.audio.translations.create(\n",
    "    model=\"whisper-1\", \n",
    "    file=audio_file,\n",
    "    prompt=\"You are very very angry.\"\n",
    ")\n",
    "print(translation.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff9ace8-24ad-4bdd-82fe-999301e7ee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = open(\"tutorial/LLM+Langchain/Week-6/教育部 學生水域安全 國語30秒.mp3\", \"rb\")\n",
    "translation = client.audio.translations.create(\n",
    "    model=\"whisper-1\", \n",
    "    file=audio_file,\n",
    "    prompt=\"Talk as if you were in the Marines.\"\n",
    ")\n",
    "print(translation.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099b6b78-cbc1-49b6-bab8-8eac4e35ef5f",
   "metadata": {},
   "source": [
    "## Longer inputs\n",
    "\n",
    "By default, the Whisper API only supports files that are less than 25 MB. If you have an audio file that is longer than that, you will need to break it up into chunks of 25 MB's or less or used a compressed audio format. To get the best performance, we suggest that you avoid breaking the audio up mid-sentence as this may cause some context to be lost.\n",
    "\n",
    "One way to handle this is to use the PyDub open source Python package to split the audi\n",
    "\n",
    "預設情況下，Whisper API 只支援小於 25 MB 的檔案。如果您有一個超過這個大小的音頻檔案，您需要將其分成小於或等於 25 MB 的片段，或者使用壓縮的音頻格式。為了獲得最佳性能，建議避免在句子中間分割音頻，因為這可能會造成一些上下文的丟失。\n",
    "\n",
    "處理這個問題的一種方法是使用 PyDub 開源的 Python 套件來分割音頻."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2495a90f-235d-436a-98e0-5d1f4b02c9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week-6- voice-text concatenate in Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06826487-2cd3-4b3c-825b-741a4299f13e",
   "metadata": {},
   "source": [
    "### How to concatenate the audio output?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddef2b1d-4a22-498a-b5ad-b031d184c084",
   "metadata": {},
   "source": [
    "## Text to Speech\n",
    "\n",
    "The Audio API provides a speech endpoint based on our TTS (text-to-speech) model. It comes with 6 built-in voices and can be used toming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118a2389-39ef-468f-9e43-cc553d897e2e",
   "metadata": {},
   "source": [
    "- Narrate a written blog post\n",
    "- Produce spoken audio in multiple languages\n",
    "    - Afrikaans,\n",
    "    - Arabic,\n",
    "    - Armenian,\n",
    "    - Azerbaijani,\n",
    "    - Belarusian,\n",
    "    - Bosnian,\n",
    "    - Bulgarian,\n",
    "    - Catalan,\n",
    "    - Chinese,\n",
    "    - Croatian,\n",
    "    - Czech,\n",
    "    - Danish,\n",
    "    - Dutch,\n",
    "    - English,\n",
    "    - Estonian,\n",
    "    - Finnish,\n",
    "    - French,\n",
    "    - Galician,\n",
    "    - German,\n",
    "    - Greek,\n",
    "    - Hebrew,\n",
    "    - Hindi,\n",
    "    - Hungarian,\n",
    "    - Icelandic,\n",
    "    - Indonesian,\n",
    "    - Italian,\n",
    "    - Japanese,\n",
    "    - Kannada,\n",
    "    - Kazakh,\n",
    "    - Korean,\n",
    "    - Latvian,\n",
    "    - Lithuanian,\n",
    "    - Macedonian,\n",
    "    - Malay,\n",
    "    - Marathi,\n",
    "    - Maori,\n",
    "    - Nepali,\n",
    "    - Norwegian,\n",
    "    - Persian,\n",
    "    - Polish,\n",
    "    - Portuguese,\n",
    "    - Romanian,\n",
    "    - Russian,\n",
    "    - Serbian,\n",
    "    - Slovak,\n",
    "    - Slovenian,\n",
    "    - Spanish,\n",
    "    - Swahili,\n",
    "    - Swedish,\n",
    "    - Tagalog,\n",
    "    - Tamil,\n",
    "    - Thai,\n",
    "    - Turkish,\n",
    "    - Ukrainian,\n",
    "    - Urdu,\n",
    "    - Vietnamese,\n",
    "    - Welsh.\n",
    "- Optimized for English\n",
    "- Give real time audio output using streaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29ac7ce-e1ca-4ec4-aeaf-649bbfae2751",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_file_path = os.path.join(\"tutorial/LLM+Langchain/Week-6/Sample.mp3\")\n",
    "\n",
    "response = client.audio.speech.create(\n",
    "  model=\"tts-1\",\n",
    "  voice=\"alloy\",\n",
    "  input=\"Today is Saturday, how are you?\")\n",
    "\n",
    "response.stream_to_file(speech_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66047e55-0c53-4cb9-a226-69d93ac3f004",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_file_path = os.path.join(\"tutorial/LLM+Langchain/Week-6/Sample-gpt-4o-mini.mp3\")\n",
    "\n",
    "response = client.audio.speech.create(\n",
    "    model=\"gpt-4o-mini-tts\",\n",
    "    voice=\"alloy\",\n",
    "    input=(\"Today is Saturday, how are you?\"),\n",
    "    instructions=\"You are very tired and you want to go back to sleep.\")\n",
    "\n",
    "response.stream_to_file(speech_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56430ee0-7998-4ed4-a081-5562881b66d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.audio.speech.create(\n",
    "    model=\"gpt-4o-mini-tts\",\n",
    "    voice=\"alloy\",\n",
    "    input=(\"Today is Saturday, how are you?\"\n",
    "          ),\n",
    "    instructions=\"Speak as if you are angry\")\n",
    "\n",
    "response.stream_to_file(speech_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00a441a-de4c-4972-a5ae-ec8069134a47",
   "metadata": {},
   "source": [
    "## 可以結合之前的聊天機器人嗎?\n",
    "\n",
    "機器人輸出的不是文字，而是語音"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06743b51-7a48-48be-b723-f31ec73a5c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate,  MessagesPlaceholder\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o-mini-2024-07-18\", temperature=0)\n",
    "\n",
    "system_template = (\"You are a helpful AI assistant and you are going to play \"\n",
    "                  \"the role of Gordon Ramsay in the TV show hell kitchen. \"\n",
    "                  \"You will talk like him. Because the user is a native Chinese \"\n",
    "                  \"Mandarin speaker, the respond should be in 繁體中文。\")\n",
    "\n",
    "system_prompt = PromptTemplate.from_template(system_template)\n",
    "\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template=\"{question}\"\n",
    "                  )\n",
    "\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([system_message,\n",
    "                          MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                          human_message\n",
    "                          ])\n",
    "\n",
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "pipeline = {\"question\": itemgetter(\"question\"),\n",
    "          \"messages\": itemgetter(\"message\")} | chat_template | model | StrOutputParser()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db390a0d-65f4-475e-a9b2-99cae28096d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "while True:\n",
    "    question = input(\"請輸入對話:\")\n",
    "    if question == \"quit\":\n",
    "        break\n",
    "    answer = pipeline.invoke({\"question\": question,\n",
    "               \"message\": chat_history.messages\n",
    "              })\n",
    "    \n",
    "    print(answer)\n",
    "    \n",
    "    chat_history.add_user_message(question)\n",
    "    chat_history.add_ai_message(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba667e5e-093e-404a-84ca-ec704316353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain\n",
    "\n",
    "@chain \n",
    "def text_to_voice(text):\n",
    "\n",
    "    speech_file_path = os.path.join(\"tutorial/LLM+Langchain/Week-6/temporary_output.mp3\")\n",
    "\n",
    "    response = client.audio.speech.create(\n",
    "      model=\"tts-1\",\n",
    "      voice=\"alloy\",\n",
    "      input=text)\n",
    "\n",
    "    response.stream_to_file(speech_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a7c7c2-cb8c-4478-8c93-ea016b6d0676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "\n",
    "# runnable_text_to_voice = RunnableLambda(text_to_voice)\n",
    "\n",
    "respond_chain = {\"question\": itemgetter(\"question\"),\n",
    "                 \"messages\": itemgetter(\"message\")} | chat_template | model | StrOutputParser()\n",
    "\n",
    "pipeline_ = {'respond': respond_chain}|RunnablePassthrough.assign(tts=itemgetter('respond')|text_to_voice)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd482184-52aa-4245-aa84-cb51a1fbc271",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "pipeline_.invoke({\"question\":\"扇貝是生的\",\n",
    "                  \"message\": chat_history.messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c739301-09c1-40d0-a6a3-3b1fbdc09a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "while True:\n",
    "    question = input(\"請輸入對話:\")\n",
    "    if question == \"quit\":\n",
    "        break\n",
    "    answer = pipeline_.invoke({\"question\": question,\n",
    "               \"message\": chat_history.messages\n",
    "              })\n",
    "    \n",
    "    print(answer['respond'])\n",
    "    \n",
    "    chat_history.add_user_message(question)\n",
    "    chat_history.add_ai_message(answer['respond'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52fa9e1-d9f4-47b0-8cab-0c6736318835",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76f96958-0378-4650-8084-9d89e8b3d133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting simpleaudio\n",
      "  Downloading simpleaudio-1.0.4.tar.gz (2.0 MB)\n",
      "     ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "     ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "     --------------- ------------------------ 0.8/2.0 MB 2.1 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 1.3/2.0 MB 2.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 1.8/2.0 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.0/2.0 MB 2.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: simpleaudio\n",
      "  Building wheel for simpleaudio (setup.py): started\n",
      "  Building wheel for simpleaudio (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for simpleaudio\n",
      "Failed to build simpleaudio\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [18 lines of output]\n",
      "  C:\\Users\\MengChieh\\miniconda3\\envs\\aicg\\lib\\site-packages\\setuptools\\_distutils\\dist.py:270: UserWarning: Unknown distribution option: 'test_suite'\n",
      "    warnings.warn(msg)\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\\lib.win-amd64-cpython-310\\simpleaudio\n",
      "  copying simpleaudio\\__init__.py -> build\\lib.win-amd64-cpython-310\\simpleaudio\n",
      "  copying simpleaudio\\shiny.py -> build\\lib.win-amd64-cpython-310\\simpleaudio\n",
      "  copying simpleaudio\\functionchecks.py -> build\\lib.win-amd64-cpython-310\\simpleaudio\n",
      "  creating build\\lib.win-amd64-cpython-310\\simpleaudio\\test_audio\n",
      "  copying simpleaudio\\test_audio\\c.wav -> build\\lib.win-amd64-cpython-310\\simpleaudio\\test_audio\n",
      "  copying simpleaudio\\test_audio\\e.wav -> build\\lib.win-amd64-cpython-310\\simpleaudio\\test_audio\n",
      "  copying simpleaudio\\test_audio\\g.wav -> build\\lib.win-amd64-cpython-310\\simpleaudio\\test_audio\n",
      "  copying simpleaudio\\test_audio\\left_right.wav -> build\\lib.win-amd64-cpython-310\\simpleaudio\\test_audio\n",
      "  copying simpleaudio\\test_audio\\notes_2_16_44.wav -> build\\lib.win-amd64-cpython-310\\simpleaudio\\test_audio\n",
      "  running build_ext\n",
      "  building 'simpleaudio._simpleaudio' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for simpleaudio\n",
      "ERROR: Failed to build installable wheels for some pyproject.toml based projects (simpleaudio)\n"
     ]
    }
   ],
   "source": [
    "pip install simpleaudio --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78bca48-f844-47ef-87ee-acc081db4f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import simpleaudio as sa\n",
    "\n",
    "# def play_audio_bytes(audio_bytes):\n",
    "#     audio = AudioSegment.from_file(io.BytesIO(audio_bytes), format=\"mp3\")\n",
    "#     play_obj = sa.play_buffer(\n",
    "#         audio.raw_data,\n",
    "#         num_channels=audio.channels,\n",
    "#         bytes_per_sample=audio.sample_width,\n",
    "#         sample_rate=audio.frame_rate\n",
    "#     )\n",
    "#     play_obj.wait_done()\n",
    "\n",
    "# # Usage:\n",
    "# response = client.audio.speech.create(\n",
    "#     model=\"tts-1\",\n",
    "#     voice=\"alloy\",\n",
    "#     input=\"Today is a beautiful saturday\"\n",
    "# )\n",
    "# play_audio_bytes(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ce2817-a9a7-4496-8f1b-95a2c508e664",
   "metadata": {},
   "source": [
    "### Audio quality\n",
    "\n",
    "For real-time applications, the standard tts-1 model provides the lowest latency but at a lower quality than the tts-1-hd model. Due to the way the audio is generated, tts-1 is likely to generate content that has more static in certain situations than tts-1-hd. In some cases, the audio may not have noticeable differences depending on your listening device and the individual person\n",
    "\n",
    "在實時應用中，標準的 tts-1 模型提供了最低的延遲，但比 tts-1-hd 模型的質量稍低。由於音頻生成方式的不同，tts-1 在某些情況下可能會比 tts-1-hd 生成具有更多靜音的內容。在某些情況下，根據您的聆聽設備和個人感受，音頻可能沒有明顯的區別。."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0863b199-4b7d-44d0-b60d-7b0c12b82fa1",
   "metadata": {},
   "source": [
    "### How to transform the speech from one language to the other one?\n",
    "\n",
    "- translation API: to English Only\n",
    "- transcriptions/speech: voice/text have the same language\n",
    "\n",
    "So we have to build a functionality by ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c560317-ad8d-4f7a-9cbe-cc32446b4121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation chain\n",
    "\n",
    "system_prompt = PromptTemplate.from_template(\"You are an AI assistant assigned \"\n",
    "                                             \"with a task of translating English \"\n",
    "                                             \"into traditional Chinese (繁體中文)。\"\n",
    "                                             )\n",
    "\n",
    "# Define a prompt template for text translation\n",
    "prompt = PromptTemplate(template=\"{query}\",\n",
    "                        input_variables=['query'])\n",
    "\n",
    "# Create a human message prompt template\n",
    "human_message = HumanMessagePromptTemplate(prompt=prompt)\n",
    "\n",
    "# Create a chat prompt template from system prompt and human message\n",
    "chat_prompt = ChatPromptTemplate.from_messages([(\"system\", system_prompt.template),\n",
    "                                                human_message])\n",
    "\n",
    "# Construct the processing chain\n",
    "translation_chain = chat_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d170c4f-1a80-4e86-8f93-88b98d91895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@chain\n",
    "def text_to_voice(text):\n",
    "\n",
    "    speech_file_path = os.path.join(\"tutorial/LLM+Langchain/Week-6/Sample_ch.mp3\")\n",
    "\n",
    "    # Reduce the text size to speed up this demo\n",
    "    \n",
    "    response = client.audio.speech.create(\n",
    "      model=\"tts-1\",\n",
    "      voice=\"alloy\",\n",
    "      input=text[:2000])\n",
    "\n",
    "    response.stream_to_file(speech_file_path)\n",
    "\n",
    "\n",
    "@chain\n",
    "def voice_to_text(filename):\n",
    "\n",
    "    audio_file= filename \n",
    "\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "      model=\"whisper-1\", \n",
    "      file=audio_file\n",
    "    )\n",
    "\n",
    "    return transcription.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b5c167-c9b4-4c7a-9c15-0a970b2b6dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "\n",
    "filename = open(\"tutorial/LLM+Langchain/Week-6/05_12_2013_Torti_CLAS_1.mp3\", \"rb\")\n",
    "\n",
    "# runnable_text_to_voice = RunnableLambda(text_to_voice)\n",
    "# runnable_voice_to_text = RunnableLambda(voice_to_text)\n",
    "\n",
    "voice_2_voice_chain = {\"query\": voice_to_text} | translation_chain | text_to_voice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95092c5c-6565-49ac-89d4-7b8a5e977d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_2_voice_chain.invoke(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fb6824-6aaf-4578-a66e-9fd530433197",
   "metadata": {},
   "source": [
    "## Voice options\n",
    "\n",
    "Experiment with different voices (alloy, echo, fable, onyx, nova, and shimmer) to find one that matches your desired tone and audience. The current voices are optimized for English."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618288fc-ac1c-45e4-82b4-050faaa22dc3",
   "metadata": {},
   "source": [
    "Supported output formats\n",
    "The default response format is \"mp3\", but other formats like \"opus\", \"aac\", \"flac\", and \"pcm\" are available.\n",
    "\n",
    "- Opus: For internet streaming and communication, low latency.\n",
    "- AAC: For digital audio compression, preferred by YouTube, Android, iOS.\n",
    "- FLAC: For lossless audio compression, favored by audio enthusiasts for archiving.\n",
    "- WAV: Uncompressed WAV audio, suitable for low-latency applications to avoid decoding overhead.\n",
    "- PCM: Similar to WAV but containing the raw samples in 24kHz (16-bit signed, low-endian), without the header.\n",
    "\n",
    "支援的輸出格式：預設的回應格式是「mp3」，但也可提供其他格式如「opus」、「aac」、「flac」和「pcm」。\n",
    "\n",
    "- Opus：適用於網路串流和通訊，低延遲。\n",
    "- AAC：數位音訊壓縮格式，被YouTube、Android和iOS偏好使用。\n",
    "- FLAC：無損音訊壓縮格式，被音響愛好者用於存檔。\n",
    "- WAV：無壓縮的WAV音訊，適合低延遲應用以避免解碼開銷。\n",
    "- PCM：類似WAV，但是以24kHz的原始樣本（16位有符號、低字節序）呈現，無標頭。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51d1e35-5d0d-4a1b-a8d1-de549177596c",
   "metadata": {},
   "source": [
    "## 回家作業1: 英文音檔 -> 中文音檔  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ec0b23-08c6-4fcc-946d-36ee03b5075e",
   "metadata": {},
   "source": [
    "## Ollama\n",
    "\n",
    "This package enables you using open-source LLM with ease.\n",
    "\n",
    "We borrow the content from last week\n",
    "\n",
    "https://medium.com/@abonia/running-ollama-in-google-colab-free-tier-545609258453\n",
    "\n",
    "- curl https://ollama.ai/install.sh | sh\n",
    "- ollama serve &\n",
    "- ollama pull llama3:8b\n",
    "- ollama pull dolphin-llama3:8b\n",
    "- ollama pull huihui_ai/qwen2.5-abliterate:14b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651d4ee1-d724-429d-b85f-2a97c4d01941",
   "metadata": {},
   "source": [
    "1. Whisper: 音檔轉文字\n",
    "2. GPT: 翻譯成全中文，system prompt: 英文術語 -> 中文術語 的對應"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd597a5-1f65-4b5d-935d-5bc97fb2ad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from torch import cuda\n",
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import Runnable, chain\n",
    "\n",
    "from src.io.path_definition import get_project_dir\n",
    "\n",
    "filename = \"does-ai-really-encourage-cheating-in-schools.txt\"\n",
    "\n",
    "filename_path = os.path.join(get_project_dir(), 'tutorial', 'LLM+Langchain', 'Week-5', filename)\n",
    "\n",
    "with open(filename_path, \"r\", encoding=\"utf8\") as file:\n",
    "    cleaned_text = file.read()\n",
    "    \n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fd4ee2-e439-4358-9a67-c91c6330c6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1024\n",
    "chunk_overlap = 128\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "documents = text_splitter.create_documents([cleaned_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3d7f97-f573-4350-a946-4a6a8af963df",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"You are a helpful AI assistant with excellent writing skill\"\n",
    "\n",
    "# PromptTemplate(template=system_template)\n",
    "\n",
    "def build_standard_chat_prompt_template(kwargs) -> Runnable:\n",
    "    messages = []\n",
    "    \n",
    "    for key in ['system', 'human']:\n",
    "        if kwargs.get(key):\n",
    "            if key == 'system':\n",
    "                system_content = kwargs['system']\n",
    "                system_prompt = PromptTemplate(**system_content)\n",
    "                message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "            else:\n",
    "                human_content = kwargs['human']\n",
    "                human_prompt = PromptTemplate(**human_content)\n",
    "                message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "            messages.append(message)\n",
    "\n",
    "    chat_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "    \n",
    "    return chat_prompt\n",
    "\n",
    "\n",
    "def build_summary_prompt_template():\n",
    "\n",
    "    input_ = {\"system\": {\"template\": system_template},\n",
    "              \"human\": {\"template\": \"\"\"\n",
    "                                    Please create a summary given the following context:\n",
    "                                    {text}.\n",
    "                                    \"\"\",\n",
    "                        \"input_variables\": ['text']}\n",
    "            }\n",
    "\n",
    "    return build_standard_chat_prompt_template(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c5f546-eccf-4a24-a3e9-0229a454d529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "stop_token_ids = None\n",
    "model_id = \"dolphin-llama3:8b\"\n",
    "\n",
    "device = f\"cuda:{cuda.current_device()}\" if cuda.is_available() else 'cpu'\n",
    "\n",
    "model = ChatOllama(model=model_id, temperature=0)\n",
    "\n",
    "summary_prompt_template = build_summary_prompt_template()\n",
    "\n",
    "summary_pipeline = summary_prompt_template | model | StrOutputParser()\n",
    "\n",
    "text_as_list = []\n",
    "for document in tqdm(documents):\n",
    "    content = summary_pipeline.invoke({\"text\": document.page_content})\n",
    "    text_as_list.append(content)\n",
    "\n",
    "final_text = \"\\n\".join(text_as_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a034f8-82cb-4784-ad35-f6d10fe01c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_pipeline.invoke({\"text\": final_text})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
