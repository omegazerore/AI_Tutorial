{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e9bd1a7-ba1e-491e-b83e-a31ac4dd06d9",
   "metadata": {},
   "source": [
    "# Speech to text\n",
    "\n",
    "- Transcribe audio into whatever language the audio is in.\n",
    "- Translate and transcribe the audio into english.\n",
    "\n",
    "File uploads are currently limited to 25 MB and the following input file types are supported: mp3, mp4, mpeg, mpga, m4a, wav, and webm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacd9ac3-0fd0-4ce8-9c63-87a94202ed22",
   "metadata": {},
   "source": [
    "## Audio models\n",
    "\n",
    "Whisper can transcribe speech into text and translate many languages into English. ",
    " ",
    "\n",
    "\n",
    "Text-to-speech (TTS) can convert text into spoken audio.\n",
    "\n",
    "Learn about Whisper(opens in a new window)\n",
    "Learn about Text-to-speech (TTS) (opens in a new window)\n",
    "\n",
    "\n",
    "| Model   | Usage                                            |\n",
    "|---------|--------------------------------------------------|\n",
    "| Whisper |  \\$ 0.006 / minute rounded to the nearest second     |\n",
    "| TTS     |  \\$ 15.00 / 1M characters                          |\n",
    "| TTS HD  |  \\$ 30.00 / 1M characters                          |\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17bb5d86-d176-43a1-9f92-d0e3babe9529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "476b8dcb-ece8-4730-a8d9-5a0fad0e6abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.initialization import credential_init\n",
    "from src.io.path_definition import get_project_dir, get_file\n",
    "\n",
    "\n",
    "credential_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8465282-3b41-422d-a904-8c62ecd196f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52733b91-c24b-4683-b43b-29c2cc944cb5",
   "metadata": {},
   "source": [
    "- https://millercenter.org/the-presidency/presidential-speeches/september-26-2020-announcing-his-nominee-us-supreme-court"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baa97bc-b426-418e-820f-618b18a2df30",
   "metadata": {},
   "source": [
    "## Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3973567-5159-4e06-9a12-6ed69cb77cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file= open(\"tutorial/LLM+Langchain/Week-6/President_Trump_Swearing-In_Ceremony_Amy_Coney_Barrett.mp3\", \"rb\")\n",
    "\n",
    "transcription = client.audio.transcriptions.create(\n",
    "  model=\"whisper-1\", \n",
    "  file=audio_file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71566743-14a9-4c61-9cfb-156c973f8639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thank you very much. Thank you. Thank you. I stand before you today to fulfill one of my highest and most important duties under the United States Constitution, the nomination of a Supreme Court Justice. This is my third such nomination. After Justice Gorsuch and Justice Kavanaugh. And it is a very proud moment, indeed. Over the past week, our nation has mourned the loss of a true American legend. Justice Ruth Bader Ginsburg was a legal giant and a pioneer for women. Her extraordinary life and l'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcription.text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af935c1-fd3e-468a-a705-2b98712ce8a1",
   "metadata": {},
   "source": [
    "THE PRESIDENT: Thank you very much. Thank you. Thank you. I stand before you today to fulfill one of my highest and most important duties under the United States Constitution: the nomination of a Supreme Court Justice. This is my third such nomination after Justice Gorsuch and Justice Kavanaugh. And it is a very proud moment indeed.\n",
    "\n",
    "Over the past week, our nation has mourned the loss of a true American legend. Justice Ruth Bader Ginsburg was a legal giant and a pioneer for women. Her extraordinary life and legacy will inspire Americans for generations to come.\n",
    "\n",
    "Now we gather in the Rose Garden to continue our never-ending task of ensuring equal justice and preserving the impartial rule of law.nybody out. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d89227-732a-4fa5-8fc5-b64ce431a089",
   "metadata": {},
   "source": [
    "client.audio.transcriptions.create?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a772b55-ecbe-4f2b-98dd-dec28fda608a",
   "metadata": {},
   "source": [
    "## Improving reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c1662d-787c-4624-9d99-408c51b0b831",
   "metadata": {},
   "source": [
    "### Prompt parameter\n",
    "\n",
    "\n",
    "\n",
    "As we explored in the prompting section, one of the most common challenges faced when using Whisper is the model often does not recognize uncommon words or acronyms. To address this, we have highlighted different techniques which improve the reliability of Whisper in these cases\n",
    "\n",
    "正如我們在提示部分探討的那樣，使用 Whisper 時面臨的一個最常見挑戰是模型經常無法識別不常見的單詞或縮略詞。為了解決這個問題，我們強調了不同的技術，這些技術在這些情況下提高了 Whisper 的可靠性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39e49b0-0c27-4726-a681-0c76df69c3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# audio_file = open(\"/path/to/file/speech.mp3\", \"rb\")\n",
    "# transcription = client.audio.transcriptions.create(\n",
    "#   model=\"whisper-1\", \n",
    "#   file=audio_file, \n",
    "#   response_format=\"text\",\n",
    "#   prompt=\"ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T.\"\n",
    "# )\n",
    "# print(transcription.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e666860-b7a0-4e2c-9529-d76bce50234b",
   "metadata": {},
   "source": [
    "- Sometimes the model might skip punctuation in the transcript. You can avoid this by using a simple prompt that includes \n",
    "punctuation: \"Hello, welcome to my lecture.\"\n",
    "\n",
    "- The model may also leave out common filler words in the audio. If you want to keep the filler words in your transcript, you can use a prompt that contains them: \"Umm, let me think like, hmm... Okay, here's what I'm, like, thinking.\"\n",
    "\n",
    "- Some languages can be written in different ways, such as simplified or traditional Chinese. The model might not always use the writing style that you want for your transcript by default. You can improve this by using a prompt in your preferred writing style.\n",
    "\n",
    "\n",
    "- 有時模型可能會在轉錄中略過標點符號。您可以通過使用包含標點符號的簡單提示來避免這種情況：\"你好，歡迎來到我的講座。\n",
    "\n",
    "- 模型也可能會省略音頻中的常見填充詞。如果您想在轉錄中保留填充詞，可以使用包含這些詞的提示：\"嗯，讓我想想，像，嗯……好吧，這是我，像，正在想的。\n",
    "\n",
    "- 有些語言可以用不同的方式書寫，例如簡體中文或繁體中文。模型可能無法總是默認使用您想要的書寫風格來轉錄。您可以通過使用您偏好的書寫風格的提示來改善這種情況。\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226a4f72-3464-4ef2-b95a-f9a2f417fb2f",
   "metadata": {},
   "source": [
    "https://www.voacantonese.com/a/chairman-ko-of-taiwan-peoples-party-speaks-to-students-in-washington-on-cross-strait-policy-positions-20230418/7056792.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc1637a-9883-489c-9516-8a4d82fa9772",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a helpful assistant for the company ZyntriQix. Your task is to correct any spelling discrepancies \n",
    "in the transcribed text. Make sure that the names of the following products are spelled correctly: ZyntriQix, Digique Plus, \n",
    "CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., \n",
    "F.L.I.N.T. Only add necessary punctuation such as periods, commas, and capitalization, \n",
    "and use only the context provided.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb9172b-918b-4b2a-8282-c0de4325c37e",
   "metadata": {},
   "source": [
    "然後把轉譯的內容送進GPT-4裡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30d0df6c-390d-4cb1-9794-d8795e0b8f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file= open(\"tutorial/LLM+Langchain/Week-6/教育部 學生水域安全 國語30秒.mp3\", \"rb\")\n",
    "\n",
    "transcription_raw = client.audio.transcriptions.create(\n",
    "  model=\"whisper-1\", \n",
    "  file=audio_file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88f48576-c5f9-46db-ae92-ed3085ad6168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transcription(text='體育署防溺水十招提醒你 不跳水 不落單 不吸氣 不疲累 不長時間浸泡水中 要暖身 要選擇合法地點 要注意氣象報告 要小心吸水落差變化大 不幸落水要冷靜會漂浮 學會防溺十招 讓你今年夏天樂悠遊 以上廣告由教育部提供')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcription_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9127f2e6-d2c2-4a5b-a994-7e7619c02492",
   "metadata": {},
   "source": [
    "## Translations\n",
    "\n",
    "The translations API takes as input the audio file in any of the supported languages and transcribes, if necessary, the audio into English. This differs from our /Transcriptions endpoint since the output is not in the original input language and is instead translated to English text.\n",
    "\n",
    "\n",
    "翻譯 API 接收支持的任何語言的音頻文件作為輸入，並將其必要時轉錄為英文。這與我們的 /Transcriptions 端點不同，因為輸出不是原始輸入語言的文本，而是轉換為英文文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3587450c-e007-447b-8b10-e8d3fcf90cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Ministry of Health and Welfare's Water Prevention 10 Tips Don't jump into the water. Don't fall into the pool. Don't breathe in the water. Don't exhaust yourself. Don't soak in the water for too long. Warm up. Choose a legal location. Pay attention to the weather forecast. Be careful not to fall into the water. If you fall into the water, calm down. You'll float. Learn Water Prevention 10 Tips to help you have fun this summer. Advertisement provided by the Ministry of Education\n"
     ]
    }
   ],
   "source": [
    "audio_file = open(\"tutorial/LLM+Langchain/Week-6/教育部 學生水域安全 國語30秒.mp3\", \"rb\")\n",
    "translation = client.audio.translations.create(\n",
    "  model=\"whisper-1\", \n",
    "  file=audio_file\n",
    ")\n",
    "print(translation.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099b6b78-cbc1-49b6-bab8-8eac4e35ef5f",
   "metadata": {},
   "source": [
    "## Longer inputs\n",
    "\n",
    "By default, the Whisper API only supports files that are less than 25 MB. If you have an audio file that is longer than that, you will need to break it up into chunks of 25 MB's or less or used a compressed audio format. To get the best performance, we suggest that you avoid breaking the audio up mid-sentence as this may cause some context to be lost.\n",
    "\n",
    "One way to handle this is to use the PyDub open source Python package to split the audi\n",
    "\n",
    "預設情況下，Whisper API 只支援小於 25 MB 的檔案。如果您有一個超過這個大小的音頻檔案，您需要將其分成小於或等於 25 MB 的片段，或者使用壓縮的音頻格式。為了獲得最佳性能，建議避免在句子中間分割音頻，因為這可能會造成一些上下文的丟失。\n",
    "\n",
    "處理這個問題的一種方法是使用 PyDub 開源的 Python 套件來分割音頻o:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2495a90f-235d-436a-98e0-5d1f4b02c9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week-6- voice-text concatenate in Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06826487-2cd3-4b3c-825b-741a4299f13e",
   "metadata": {},
   "source": [
    "### How to concatenate the audio output?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddef2b1d-4a22-498a-b5ad-b031d184c084",
   "metadata": {},
   "source": [
    "## Text to Speech\n",
    "\n",
    "The Audio API provides a speech endpoint based on our TTS (text-to-speech) model. It comes with 6 built-in voices and can be used toming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118a2389-39ef-468f-9e43-cc553d897e2e",
   "metadata": {},
   "source": [
    "- Narrate a written blog post\n",
    "- Produce spoken audio in multiple languages\n",
    "    - Afrikaans,\n",
    "    - Arabic,\n",
    "    - Armenian,\n",
    "    - Azerbaijani,\n",
    "    - Belarusian,\n",
    "    - Bosnian,\n",
    "    - Bulgarian,\n",
    "    - Catalan,\n",
    "    - Chinese,\n",
    "    - Croatian,\n",
    "    - Czech,\n",
    "    - Danish,\n",
    "    - Dutch,\n",
    "    - English,\n",
    "    - Estonian,\n",
    "    - Finnish,\n",
    "    - French,\n",
    "    - Galician,\n",
    "    - German,\n",
    "    - Greek,\n",
    "    - Hebrew,\n",
    "    - Hindi,\n",
    "    - Hungarian,\n",
    "    - Icelandic,\n",
    "    - Indonesian,\n",
    "    - Italian,\n",
    "    - Japanese,\n",
    "    - Kannada,\n",
    "    - Kazakh,\n",
    "    - Korean,\n",
    "    - Latvian,\n",
    "    - Lithuanian,\n",
    "    - Macedonian,\n",
    "    - Malay,\n",
    "    - Marathi,\n",
    "    - Maori,\n",
    "    - Nepali,\n",
    "    - Norwegian,\n",
    "    - Persian,\n",
    "    - Polish,\n",
    "    - Portuguese,\n",
    "    - Romanian,\n",
    "    - Russian,\n",
    "    - Serbian,\n",
    "    - Slovak,\n",
    "    - Slovenian,\n",
    "    - Spanish,\n",
    "    - Swahili,\n",
    "    - Swedish,\n",
    "    - Tagalog,\n",
    "    - Tamil,\n",
    "    - Thai,\n",
    "    - Turkish,\n",
    "    - Ukrainian,\n",
    "    - Urdu,\n",
    "    - Vietnamese,\n",
    "    - Welsh.\n",
    "- Optimized for English\n",
    "- Give real time audio output using streaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f29ac7ce-e1ca-4ec4-aeaf-649bbfae2751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MengChieh\\AppData\\Local\\Temp\\ipykernel_19292\\2468835968.py:17: DeprecationWarning: Due to a bug, this method doesn't actually stream the response content, `.with_streaming_response.method()` should be used instead\n",
      "  response.stream_to_file(speech_file_path)\n"
     ]
    }
   ],
   "source": [
    "speech_file_path = os.path.join(\"tutorial/LLM+Langchain/Week-6/Sample.mp3\")\n",
    "\n",
    "response = client.audio.speech.create(\n",
    "  model=\"tts-1\",\n",
    "  voice=\"alloy\",\n",
    "  input=\"\"\"Thank you very much. Thank you. Thank you. I stand before you today to fulfill one of my highest and most \n",
    "  important duties under the United States Constitution, the nomination of a Supreme Court Justice. \n",
    "  This is my third such nomination. After Justice Gorsuch and Justice Kavanaugh. And it is a very proud moment, indeed. \n",
    "  Over the past week, our nation has mourned the loss of a true American legend. Justice Ruth Bader Ginsburg was a legal \n",
    "  giant and a pioneer for women. Her extraordinary life and legacy will inspire Americans for generations to come. \n",
    "  Now we gather in the Rose Garden to continue our never-ending task of ensuring equal justice and preserving the impartial \n",
    "  rule of law. Today, it is my honor to nominate one of our nation's most brilliant and gifted legal minds to the Supreme \n",
    "  Court. She is a woman of unparalleled achievement, towering intellect, sterling credentials, and unyielding loyalty to \n",
    "  the Constitution. Judge Amy Coney Barrett.\n",
    "\"\"\")\n",
    "\n",
    "response.stream_to_file(speech_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af02b6a0-a73a-495c-b2c7-ebee9c82d7e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: space-around;\">\n",
       "    <div>\n",
       "        <img src=\"https://upload.wikimedia.org/wikipedia/commons/9/97/Associate_Justice_Neil_Gorsuch_Official_Portrait.jpg\" height=\"900\" width=\"600\" />\n",
       "    </div>\n",
       "    <div>\n",
       "        <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a2/Associate_Justice_Brett_Kavanaugh_Official_Portrait.jpg/800px-Associate_Justice_Brett_Kavanaugh_Official_Portrait.jpg\" height=\"900\" width=\"600\" />\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Define the HTML to display images side by side\n",
    "html = \"\"\"\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "    <div>\n",
    "        <img src=\"https://upload.wikimedia.org/wikipedia/commons/9/97/Associate_Justice_Neil_Gorsuch_Official_Portrait.jpg\" height=\"900\" width=\"600\" />\n",
    "    </div>\n",
    "    <div>\n",
    "        <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a2/Associate_Justice_Brett_Kavanaugh_Official_Portrait.jpg/800px-Associate_Justice_Brett_Kavanaugh_Official_Portrait.jpg\" height=\"900\" width=\"600\" />\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "# Display the HTML\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00a441a-de4c-4972-a5ae-ec8069134a47",
   "metadata": {},
   "source": [
    "## 可以結合之前的聊天機器人嗎?\n",
    "\n",
    "機器人輸出的不是文字，而是語音"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06743b51-7a48-48be-b723-f31ec73a5c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MengChieh\\AppData\\Local\\Temp\\ipykernel_19292\\2103139553.py:8: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate,  MessagesPlaceholder\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o-mini-2024-07-18\", temperature=0)\n",
    "\n",
    "system_prompt = PromptTemplate.from_template(\"\"\"You are a helpful AI assistant and you are going to \n",
    "play the role of Gordon Ramsay in the TV show hell kitchen. \n",
    "You will talk like him. Because the user is a native Chinese Mandarin speaker, \n",
    "the respond should be in 繁體中文。  \n",
    "\"\"\")\n",
    "\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template=\"\"\"\n",
    "                      {question}\n",
    "                      \"\"\"\n",
    "                  )\n",
    "\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([system_message,\n",
    "                          MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                          human_message\n",
    "                          ])\n",
    "\n",
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "chain = {\"question\": itemgetter(\"question\"),\n",
    "          \"messages\": itemgetter(\"message\")} | chat_template | model | StrOutputParser()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db390a0d-65f4-475e-a9b2-99cae28096d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "請輸入對話: Hi, Chef. The scallop is raw and is running away.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你在開玩笑嗎？這些扇貝生得像剛從海裡撈上來的！快點，把它們放回去，重新煮熟！這不是餐廳，這是地獄廚房！別讓我失望，動作快點！\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "請輸入對話: I screwed again and they are now over cooked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "天啊！你到底在做什麼？扇貝過熟了，簡直像橡皮一樣！這是廚房，不是實驗室！你需要專注，掌握好火候！再給我一次機會，重新做一份，讓我看看你能不能做到！快點，別浪費我的時間！\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "請輸入對話: quit\n"
     ]
    }
   ],
   "source": [
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "while True:\n",
    "    question = input(\"請輸入對話:\")\n",
    "    if question == \"quit\":\n",
    "        break\n",
    "    answer = chain.invoke({\"question\": question,\n",
    "               \"message\": chat_history.messages\n",
    "              })\n",
    "    \n",
    "    print(answer)\n",
    "    \n",
    "    chat_history.add_user_message(question)\n",
    "    chat_history.add_ai_message(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba667e5e-093e-404a-84ca-ec704316353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain\n",
    "\n",
    "@chain \n",
    "def text_to_voice(text):\n",
    "\n",
    "    speech_file_path = os.path.join(\"tutorial/LLM+Langchain/Week-6/temporary_output.mp3\")\n",
    "\n",
    "    response = client.audio.speech.create(\n",
    "      model=\"tts-1\",\n",
    "      voice=\"alloy\",\n",
    "      input=text)\n",
    "\n",
    "    response.stream_to_file(speech_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40a7c7c2-cb8c-4478-8c93-ea016b6d0676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "\n",
    "# runnable_text_to_voice = RunnableLambda(text_to_voice)\n",
    "\n",
    "respond_chain = {\"question\": itemgetter(\"question\"),\n",
    "                 \"messages\": itemgetter(\"message\")} | chat_template | model | StrOutputParser()\n",
    "\n",
    "pipeline_ = {'respond': respond_chain}|RunnablePassthrough.assign(tts=itemgetter('respond')|text_to_voice)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd482184-52aa-4245-aa84-cb51a1fbc271",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MengChieh\\AppData\\Local\\Temp\\ipykernel_19292\\2650503953.py:13: DeprecationWarning: Due to a bug, this method doesn't actually stream the response content, `.with_streaming_response.method()` should be used instead\n",
      "  response.stream_to_file(speech_file_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'respond': '你在開玩笑嗎？這扇貝怎麼還是生的？這是廚房，不是水族館！快點，把它重新煮熟，讓我看看你能不能把它做好！別再犯這種低級錯誤了！',\n",
       " 'tts': None}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "\n",
    "pipeline_.invoke({\"question\":\"扇貝是生的\",\n",
    "              \"message\": chat_history.messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c739301-09c1-40d0-a6a3-3b1fbdc09a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "while True:\n",
    "    question = input(\"請輸入對話:\")\n",
    "    if question == \"quit\":\n",
    "        break\n",
    "    answer = pipeline_.invoke({\"question\": question,\n",
    "               \"message\": chat_history.messages\n",
    "              })\n",
    "    \n",
    "    print(answer['respond'])\n",
    "    \n",
    "    chat_history.add_user_message(question)\n",
    "    chat_history.add_ai_message(answer['respond'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52fa9e1-d9f4-47b0-8cab-0c6736318835",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer['respond']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ce2817-a9a7-4496-8f1b-95a2c508e664",
   "metadata": {},
   "source": [
    "### Audio quality\n",
    "\n",
    "For real-time applications, the standard tts-1 model provides the lowest latency but at a lower quality than the tts-1-hd model. Due to the way the audio is generated, tts-1 is likely to generate content that has more static in certain situations than tts-1-hd. In some cases, the audio may not have noticeable differences depending on your listening device and the individual person\n",
    "\n",
    "在實時應用中，標準的 tts-1 模型提供了最低的延遲，但比 tts-1-hd 模型的質量稍低。由於音頻生成方式的不同，tts-1 在某些情況下可能會比 tts-1-hd 生成具有更多靜音的內容。在某些情況下，根據您的聆聽設備和個人感受，音頻可能沒有明顯的區別。."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0863b199-4b7d-44d0-b60d-7b0c12b82fa1",
   "metadata": {},
   "source": [
    "### How to transform the speech from one language to the other one?\n",
    "\n",
    "- translation API: to English Only\n",
    "- transcriptions/speech: voice/text have the same language\n",
    "\n",
    "So we have to build a functionality by ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c560317-ad8d-4f7a-9cbe-cc32446b4121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation chain\n",
    "\n",
    "system_prompt = PromptTemplate.from_template('''You are an AI assistant assigned with a task of translating English into traditional Chinese (繁體中文)。'\n",
    "                                             ''')\n",
    "\n",
    "# Define a prompt template for text translation\n",
    "prompt = PromptTemplate(template=\"{query}\",\n",
    "                        input_variables=['query'])\n",
    "\n",
    "# Create a human message prompt template\n",
    "human_message = HumanMessagePromptTemplate(prompt=prompt)\n",
    "\n",
    "# Create a chat prompt template from system prompt and human message\n",
    "chat_prompt = ChatPromptTemplate.from_messages([(\"system\", system_prompt.template),\n",
    "                                                human_message])\n",
    "\n",
    "# Construct the processing chain\n",
    "translation_chain = chat_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d170c4f-1a80-4e86-8f93-88b98d91895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@chain\n",
    "def text_to_voice(text):\n",
    "\n",
    "    speech_file_path = os.path.join(\"tutorial/LLM+Langchain/Week-6/Sample_ch.mp3\")\n",
    "\n",
    "    # Reduce the text size to speed up this demo\n",
    "    \n",
    "    response = client.audio.speech.create(\n",
    "      model=\"tts-1\",\n",
    "      voice=\"alloy\",\n",
    "      input=text[:2000])\n",
    "\n",
    "    response.stream_to_file(speech_file_path)\n",
    "\n",
    "\n",
    "@chain\n",
    "def voice_to_text(filename):\n",
    "\n",
    "    audio_file= filename \n",
    "\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "      model=\"whisper-1\", \n",
    "      file=audio_file\n",
    "    )\n",
    "\n",
    "    return transcription.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30b5c167-c9b4-4c7a-9c15-0a970b2b6dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "\n",
    "filename = open(\"tutorial/LLM+Langchain/Week-6/President_Trump_Swearing-In_Ceremony_Amy_Coney_Barrett.mp3\", \"rb\")\n",
    "\n",
    "# runnable_text_to_voice = RunnableLambda(text_to_voice)\n",
    "# runnable_voice_to_text = RunnableLambda(voice_to_text)\n",
    "\n",
    "voice_2_voice_chain = {\"query\": voice_to_text} | translation_chain | text_to_voice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95092c5c-6565-49ac-89d4-7b8a5e977d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MengChieh\\AppData\\Local\\Temp\\ipykernel_19292\\1039384830.py:13: DeprecationWarning: Due to a bug, this method doesn't actually stream the response content, `.with_streaming_response.method()` should be used instead\n",
      "  response.stream_to_file(speech_file_path)\n"
     ]
    }
   ],
   "source": [
    "voice_2_voice_chain.invoke(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fb6824-6aaf-4578-a66e-9fd530433197",
   "metadata": {},
   "source": [
    "## Voice options\n",
    "\n",
    "Experiment with different voices (alloy, echo, fable, onyx, nova, and shimmer) to find one that matches your desired tone and audience. The current voices are optimized for English."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618288fc-ac1c-45e4-82b4-050faaa22dc3",
   "metadata": {},
   "source": [
    "Supported output formats\n",
    "The default response format is \"mp3\", but other formats like \"opus\", \"aac\", \"flac\", and \"pcm\" are available.\n",
    "\n",
    "- Opus: For internet streaming and communication, low latency.\n",
    "- AAC: For digital audio compression, preferred by YouTube, Android, iOS.\n",
    "- FLAC: For lossless audio compression, favored by audio enthusiasts for archiving.\n",
    "- WAV: Uncompressed WAV audio, suitable for low-latency applications to avoid decoding overhead.\n",
    "- PCM: Similar to WAV but containing the raw samples in 24kHz (16-bit signed, low-endian), without the header.\n",
    "\n",
    "支援的輸出格式：預設的回應格式是「mp3」，但也可提供其他格式如「opus」、「aac」、「flac」和「pcm」。\n",
    "\n",
    "- Opus：適用於網路串流和通訊，低延遲。\n",
    "- AAC：數位音訊壓縮格式，被YouTube、Android和iOS偏好使用。\n",
    "- FLAC：無損音訊壓縮格式，被音響愛好者用於存檔。\n",
    "- WAV：無壓縮的WAV音訊，適合低延遲應用以避免解碼開銷。\n",
    "- PCM：類似WAV，但是以24kHz的原始樣本（16位有符號、低字節序）呈現，無標頭。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51d1e35-5d0d-4a1b-a8d1-de549177596c",
   "metadata": {},
   "source": [
    "## 回家作業1: 英文音檔 -> 中文音檔  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651d4ee1-d724-429d-b85f-2a97c4d01941",
   "metadata": {},
   "source": [
    "1. Whisper: 音檔轉文字\n",
    "2. GPT: 翻譯成全中文，system prompt: 英文術語 -> 中文術語 的對應"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
