{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b98d04a-fda1-4ca8-ae92-ae594722255a",
   "metadata": {},
   "source": [
    "# 作業詳解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebfbfab-219f-4ea9-b24a-f7f414cf25ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b9ca05-18e4-4ecb-83c2-4bb1efcdf588",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b691104-7300-4274-baa7-11ef5def708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "from src.initialization import credential_init\n",
    "from src.io.path_definition import get_project_dir\n",
    "\n",
    "credential_init()\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "with open(os.path.join(get_project_dir(), 'tutorial', 'LLM+Langchain', \n",
    "                       'Week-1', 'recipe_train.json'), 'r') as f:\n",
    "    recipe_train = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b44281d-f281-4307-8947-8298bb8e8367",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_train[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56adde32-0275-4716-a3db-17b045caf231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "documents = []\n",
    "\n",
    "for recipe in recipe_train:\n",
    "    document = Document(page_content=\", \".join(recipe['ingredients']),\n",
    "                        metadata={\"cuisine\": recipe['cuisine'],\n",
    "                                  \"id\": recipe['id']})\n",
    "    documents.append(document)\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(documents, k=10, bm25_params={\"k1\":2.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51f3190-d34a-4435-b90b-80a45e21e073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I created this last week after the session\n",
    "def build_standard_chat_prompt_template(kwargs):\n",
    "\n",
    "    system_content = kwargs['system']\n",
    "    human_content = kwargs['human']\n",
    "    \n",
    "    system_prompt = PromptTemplate(**system_content)\n",
    "    system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "    \n",
    "    human_prompt = PromptTemplate(**human_content)\n",
    "    human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "    \n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                     human_message\n",
    "                                                   ])\n",
    "\n",
    "    return chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c39e7eb-02d3-484c-8355-b3aa56a3b226",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schemas = [\n",
    "        ResponseSchema(name=\"used ingredients\", \n",
    "                       description=\"The actual ingredients used in cooking\"),\n",
    "        ResponseSchema(name=\"extra ingredients\", \n",
    "                       description=\"extra ingredients that have to be prepared \"),\n",
    "        ResponseSchema(name=\"result\", \n",
    "                       description=\"The dish and cooking recipe in detail\")\n",
    "    ]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "\n",
    "system_template = \"\"\"\n",
    "                  You are an AI assistant as the best chef in the world. \n",
    "                  You have a great taste and cooking skills like Gordon Ramsay. \n",
    "                  You should be able to come up with a dish based on `suggested ingredient`, \n",
    "                  and tell us what extra ingredients has to be prepared by comparing \n",
    "                  the ingredients actually used in the cooking and the `existing ingredient`.\n",
    "\n",
    "                  The `suggested ingredients` are the ingredients suggested \n",
    "                  by some recipe. You have the freedom to add or remove \n",
    "                  ingredients to achieve the goal, but try to be as faithful \n",
    "                  to the `suggested ingredient` as possible. \n",
    "                  \"\"\"\n",
    "\n",
    "human_template = \"\"\"\n",
    "                 existing ingredients:[{existing_ingredients}];\n",
    "                 suggested ingredients: [{suggested_ingredients}]\\n; \n",
    "                 format instruction: {format_instructions}\n",
    "                 \"\"\"\n",
    "\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"existing_ingredients\", \"suggested_ingredients\"],\n",
    "                    \"partial_variables\": {\"format_instructions\": format_instructions}}}\n",
    "\n",
    "my_chat_prompt_template = build_standard_chat_prompt_template(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284979b3-ae7e-4650-9a0b-8cf5d1e58d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because I am lazy so I use data from another dataset.\n",
    "\n",
    "with open(os.path.join(get_project_dir(), 'tutorial', 'LLM+Langchain', 'Week-1', 'recipe_test.json'), 'r') as f:\n",
    "    recipe_test = json.load(f)\n",
    "\n",
    "# What you have in your fridge\n",
    "existing_ingredients = \", \".join(recipe_test[0]['ingredients'])\n",
    "\n",
    "print(f\"existing_ingredients: {existing_ingredients}\")\n",
    "\n",
    "output = bm25_retriever.invoke(existing_ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398658b9-c424-4f0e-90d4-d062bba52e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6981b36a-537c-4f6b-a1b6-0711096187ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d54a005-1ef0-4e84-a775-9f12e9eb1c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "suggested_ingredients = output[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bf9c32-01b8-43d8-8839-195d954d01b0",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "Let us learn some python...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bdd873-319d-4bb5-8ecc-67d4982c8cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_ingredients_as_list = recipe_test[0]['ingredients']\n",
    "suggested_ingredients_as_list = suggested_ingredients.split(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8004df7-f826-40f9-93d6-d8b30db7379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for suggested_ingredient in suggested_ingredients_as_list:\n",
    "    if suggested_ingredient in existing_ingredients_as_list:\n",
    "        print(f\"- {suggested_ingredient}: yes\")\n",
    "    else:\n",
    "        print(f\"- {suggested_ingredient}: no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5c7b21-1e17-444c-a55a-1b80561966a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = my_chat_prompt_template.invoke(\n",
    "    {\"existing_ingredients\": existing_ingredients, \n",
    "     \"suggested_ingredients\": suggested_ingredients})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3768c3e5-9cea-45f1-a6bc-e3bb13fdc5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac47403-1e65-4971-bb74-89b8f1f59c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.invoke(prompt)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdbac4a-eca4-403d-8f81-98b69a14605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = output_parser.parse(output.content)\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f18343f-f495-43bc-9662-747e404a1c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d058402-5f51-4c58-a8ba-8517fd4498f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_output['used ingredients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef03ddc9-c8e5-4b3d-855c-e0ba480b5242",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_output['extra ingredients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e053b761-8266-4708-830a-46e9140b9871",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_output['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f855de-d1eb-420f-b233-7503a6071a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_result = model.invoke(f\"Translate the content into traditional Chinese (繁體中文): {final_output['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a735b13-1bf3-431d-8169-7d559c133e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translated_result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf00c087-166a-4f59-a5fe-adf39449145d",
   "metadata": {},
   "source": [
    "# Semantic based retrieval\n",
    "\n",
    "Semantic-based retrieval is a method of finding information that focuses on understanding the meaning behind the words you use. Instead of just matching exact words, it looks for the context and concepts in your query. Here's a simple way to understand it:\n",
    "\n",
    "- 1. Meaning Over Words: Imagine you want to find information about \"healthy eating\". Traditional search might look for documents with the exact phrase \"healthy eating\". Semantic-based retrieval, however, understands that terms like \"nutritious diet\" or \"balanced diet\" are related and will include those in the results.\n",
    "\n",
    "- 2. Context Awareness: This method takes into account the context in which words are used. For example, if you search for \"apple\", a traditional search might give you results about the fruit and the tech company. Semantic-based retrieval uses context to determine whether you’re likely asking about a fruit or a tech product.\n",
    "\n",
    "- 3. Natural Language Understanding: It works more like how humans understand language. When you ask a question, it tries to grasp the intent behind your query and finds relevant information accordingly.\n",
    "\n",
    "- 4. Better Results: By focusing on the meaning and context, semantic-based retrieval can provide more accurate and relevant results. This means you spend less time sifting through unrelated information.\n",
    "\n",
    "\n",
    "語義檢索是一種尋找信息的方法，它重點在於理解你使用的詞語背後的意思。與其僅僅匹配精確的詞語，它會尋找你查詢中的上下文和概念。以下是一種簡單的理解方式：\n",
    "\n",
    "- 1. 重點在於意思：想像一下你想找關於“健康飲食”的信息。傳統搜索可能會尋找包含“健康飲食”這個精確詞語的文檔。而語義檢索則會理解“營養均衡的飲食”或“均衡飲食”等相關詞語，並將它們包含在結果中。\n",
    "\n",
    "- 2. 上下文感知：這種方法會考慮詞語使用的上下文。例如，如果你搜索“蘋果”，傳統搜索可能會給你關於水果和科技公司的結果。語義檢索則會使用上下文來判斷你更可能是在詢問水果還是科技產品。\n",
    "\n",
    "- 3. 自然語言理解：它更像人類理解語言的方式。當你提出問題時，它會嘗試理解你查詢背後的意圖，並相應地找到相關信息。\n",
    "\n",
    "- 4. 更好的結果：通過重點關注意思和上下文，語義檢索可以提供更準確和相關的結果。這意味著你可以減少篩選無關信息的時間。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1d34a9-ca50-474d-9a23-3b46bbf91dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "'egg' == 'large egg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3a7a88-0bd5-4ed1-a619-b7c537813bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# https://platform.openai.com/docs/guides/embeddings/what-are-embeddings\n",
    "\n",
    "# A list of embedding models you can choose \n",
    "# https://www.sbert.net/docs/sentence_transformer/pretrained_models.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d359c46-df1c-4449-8a78-297c41b43088",
   "metadata": {},
   "source": [
    "### 1. Creating Embeddings (創建嵌入):\n",
    "\n",
    "- HuggingFaceEmbeddings is used to create embeddings (vector representations) for text data.\n",
    "- The model all-MiniLM-L6-v2 from HuggingFace is specified to generate these embeddings. This model converts text into numerical vectors that capture the semantic meaning of the text.\n",
    "\n",
    "- 使用 HuggingFaceEmbeddings 創建文本數據的嵌入（向量表示）。\n",
    "- 指定 HuggingFace 的模型 all-MiniLM-L6-v2 來生成這些嵌入。此模型將文本轉換為數字向量，這些向量捕捉文本的語義。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c35cc90-cae0-489d-89f2-3086791607ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "HuggingFaceEmbeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47235efc-d786-4a04-9ec1-89c3a4203e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aece8f30-980a-4f5c-8fe2-d574eb14a301",
   "metadata": {},
   "source": [
    "### 2. Loading a vectorstore:\n",
    "\n",
    "A PDF file is processed and the content is saved as vectorstore. I will show you how in Week-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abc3ac4-4239-4453-a9ab-4ddfee7d5e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "path = os.path.join(get_project_dir(), 'tutorial', 'LLM+Langchain', 'Week-2', 'BertV2 Index')\n",
    "\n",
    "vectorstore = FAISS.load_local(path, embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81113c6d-cd73-4d48-b7a9-e455f38c9000",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Creating a Retriever (創建檢索器):\n",
    "\n",
    "- The as_retriever method is called on the vectorstore object to create a retriever.\n",
    "- This retriever is configured to use \"similarity\" as the search type, meaning it will find documents that are similar to a given query based on their vector embeddings.\n",
    "\n",
    "- 在 vectorstore 對象上調用 as_retriever 方法來創建一個檢索器。\n",
    "- 這個檢索器配置為使用“相似性”作為搜索類型，這意味著它將根據文檔的向量嵌入找到與給定查詢相似的文檔。\n",
    "\n",
    "### 4. Setting Search Parameters (設置搜索參數):\n",
    "\n",
    "- The search_kwargs argument is used to pass additional parameters to the search function.\n",
    "- In this case, {'k': 5} is specified, which means the retriever will return the top 5 most similar documents for each query.\n",
    "\n",
    "- 使用 search_kwargs 參數來傳遞額外的搜索功能參數。\n",
    "- 在這裡，指定了 {'k': 5}，這意味著檢索器將返回每個查詢最相似的前 5 個文檔。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fa38ca-7129-4fba-bc61-a789c77c52b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", \n",
    "                                     search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bdb7f4-35fd-4ae9-a5f4-d33e23c71e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"What kind of attention does BERT use?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e42d24f-4b57-4835-9dd2-027f61b49213",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"Is it difficult to fine-tune BERT?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821359f8-fde9-44dc-8cfb-bb5acbe3a47f",
   "metadata": {},
   "source": [
    "## Runtime Configuration\n",
    "\n",
    "What we learned last week: Runtime Configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea58945a-674e-4501-be3b-22ac739cb9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "query = \"Is it difficult to fine-tune BERT?\"\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\").configurable_fields( \\\n",
    "                                        search_kwargs=ConfigurableField(\n",
    "                                                id=\"hello_search\",\n",
    "                                            )\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e72f2c-dfd1-4de9-b065-ec77f2a0c75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(query, config={\"configurable\": {\"hello_search\": {\"k\": 7}}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198a2242-afae-4197-a671-36ed1819f007",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(query, config={\"configurable\": {\"hello_search\": {\"k\": 3}}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81ce668-c24a-44c8-adfb-209b1118faa5",
   "metadata": {},
   "source": [
    "## Three search types:\n",
    "\n",
    "### 1. similarity (default)\n",
    "\n",
    "- This search type finds documents that are most similar to your query. It looks at the meaning of the words you used and matches documents that have similar meanings. Think of it like finding articles or documents that closely relate to the topic you're interested in.\n",
    "\n",
    "- 這種搜索類型找到與你的查詢最相似的文檔。它會看你使用詞語的意思，並匹配具有相似意思的文檔。可以把它想像成找到與你感興趣的主題密切相關的文章或文檔。\n",
    "\n",
    "### 2. MMR, Maximum Marginal Relevance (MMR, 最大邊際相關性):\n",
    "\n",
    "- This method balances finding documents that are similar to your query while also ensuring that the results are diverse. It's like asking for a variety of opinions on a topic so you don't get too much of the same thing. It helps avoid redundancy in the search results.\n",
    "\n",
    "- 這種方法在找到與你的查詢相似的文檔的同時，也確保結果是多樣的。這就像是在一個主題上尋求多種意見，避免得到過多相同的東西。它有助於避免搜索結果的冗餘。\n",
    "\n",
    "### 3. similarity_score_threshold (相似性分數閾值):\n",
    "\n",
    "- This search type sets a minimum similarity score that documents must meet to be considered relevant. Only documents that are very close to your query in terms of meaning will be included. It ensures that the results are highly relevant and filters out less related information.\n",
    "\n",
    "- 這種搜索類型設置一個最小相似性分數，只有達到這個分數的文檔才會被認為是相關的。只有那些在意思上與你的查詢非常接近的文檔才會被包含進來。它確保結果高度相關，並過濾掉不太相關的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdc6a5d-02c3-4e63-a3d8-f483b370896f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://miro.medium.com/v2/resize:fit:720/format:webp/1*c0c19i2tPSWZaHwQ7cVMrg.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0e436f-96ab-4977-8300-78100b9a90a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "cosine similarity\n",
    "\n",
    "https://api.python.langchain.com/en/latest/_modules/langchain_core/vectorstores.html\n",
    "\n",
    "elif search_type == \"similarity_score_threshold\":\n",
    "    docs_and_similarities = self.similarity_search_with_relevance_scores(\n",
    "        query, **kwargs\n",
    "    )\n",
    "    return [doc for doc, _ in docs_and_similarities]\n",
    "\n",
    "in subclass.\n",
    "Return docs and relevance scores in the range [0, 1].\n",
    "\n",
    "0 is dissimilar, 1 is most similar.\n",
    "\"\"\"\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.5, \"k\": 5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce400da-f757-4691-a656-2aad09a7a1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2855c8-f541-48b3-b0c6-0968bf79e004",
   "metadata": {},
   "source": [
    "### How to get the scores of the documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fbffa6-450f-4994-b11a-587e35b95b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.similarity_search_with_score(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92be3ea-320f-4551-9b04-3e1a73b78f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore._similarity_search_with_relevance_scores(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5160437-1d37-4e08-ac25-7a6f28ed40c8",
   "metadata": {},
   "source": [
    "### How to leverage the metadata?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76537b1f-c3b3-44e0-a1d5-7a0e2fcf3b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = vectorstore.as_retriever(search_type='mmr', search_kwargs={'k': 8, 'fetch_k': 50, 'lambda_mult': 0.1,\n",
    "#                                                                        \"filter\": {\"cuisine\": \"korean\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b97f58-2e9a-452c-b6d2-31c3182524ef",
   "metadata": {},
   "source": [
    "### Multiple Condition Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad44dcb-cc1b-4859-b8c1-658c05668c99",
   "metadata": {},
   "source": [
    "## CNN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0183cc-1400-4d44-8c3a-2d612ed214ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "filename = os.path.join(get_project_dir(), 'tutorial', 'LLM+Langchain', 'Week-2', 'CNN_Articels_clean.csv')\n",
    "\n",
    "df_cnn = pd.read_csv(filename, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5a7b56-6776-4ead-ad61-77b453b0ab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cnn.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9088558-7c13-4f13-8ae6-eb835a36bf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The time format is a string. It will be shown how to transform this object properly later\n",
    "\n",
    "df_cnn.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39fc840-7f19-48f8-9b19-9203586b86a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cnn.groupby([\"Category\", \"Section\"]).agg(n=('Category', 'count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e95ad4-7555-414b-b341-23250dacf78b",
   "metadata": {},
   "source": [
    "### We create a subset of CNN news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5839e98d-40e9-423d-80d5-201593666e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cnn_filtered_1= df_cnn[(df_cnn['Category']=='business') & (df_cnn['Section']=='business')]\n",
    "df_cnn_filtered_2= df_cnn[(df_cnn['Category']=='entertainment') & (df_cnn['Section']=='entertainment')]\n",
    "df_cnn_filtered_3= df_cnn[(df_cnn['Category']=='news') & (df_cnn['Section'].isin(['africa', 'australia', 'us']))]\n",
    "df_cnn_filtered_4= df_cnn[(df_cnn['Category']=='sport') & (df_cnn['Section'].isin(['motorsport', 'tennis']))]\n",
    "\n",
    "df_cnn_filtered = pd.concat([df_cnn_filtered_1, df_cnn_filtered_2, \n",
    "                             df_cnn_filtered_3, df_cnn_filtered_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b5b224-f777-4051-b74b-15de74afaf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the year and month from `Date published`\n",
    "\n",
    "df_cnn_filtered[['year', 'month']] = df_cnn_filtered.apply(lambda x: x['Date published'].split(\" \")[0].split(\"-\")[:2], \n",
    "                                                           axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211e9e0a-0021-45ef-a546-b5e64a903781",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cnn_filtered.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5946b022-0886-4625-b9bd-0f90cbcc68fe",
   "metadata": {},
   "source": [
    "1. Create a bunch of `Document` object storing the data: page_content will be the part used to for similarity calculation\n",
    "2. We use the attributes `Category`, `Section`, `Year`, `ID` as the metadata\n",
    "3. Store the information in the form of high dimension vectors in a vectorstore with an embedding model (all-MiniLM-L6-v2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ab6ace-f1c5-49d3-8dc0-fbc2557df971",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for idx, row in df_cnn_filtered.iterrows():\n",
    "    document = Document(page_content=row['Article text'],\n",
    "                        metadata={\"Category\": row['Category'],\n",
    "                                  \"Section\": row['Section'],\n",
    "                                  \"Year\": row['year'],\n",
    "                                  \"ID\": f\"{idx}\"})\n",
    "    documents.append(document)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "cnn_vectorstore = FAISS.from_documents(documents, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f101cadc-0256-4535-bcab-0d7564078993",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_retriever = cnn_vectorstore.as_retriever(search_type=\"mmr\")\n",
    "cnn_retriever_configurable = cnn_retriever.configurable_fields(search_kwargs=ConfigurableField(id=\"hello_search\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8876e7a7-23b1-478d-b105-2f1efed267fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## BUG https://github.com/langchain-ai/langchain/discussions/26806\n",
    "\n",
    "T = cnn_retriever_configurable.invoke(\"Russian\", \n",
    "                                      config={\"configurable\": \n",
    "                                              {\"hello_search\": \n",
    "                                               {\"k\": 6, 'fetch_k': 100, 'lambda_mult': 0.1,\n",
    "                                                \"filter\": {\"Category\": \"sport\",\n",
    "                                                           \"Section\": \"motorsport\"}}}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665f6a8d-499a-4222-bc4d-aee87792e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for document in T:\n",
    "    print(document.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0044e915-21e1-41dc-9c63-21a5f0bc7637",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = cnn_retriever_configurable.invoke(\"Russian\", \n",
    "                                      config={\"configurable\": \n",
    "                                              {\"hello_search\": \n",
    "                                               {\"k\": 6,\n",
    "                                                \"filter\": {\"Category\": \"sport\",\n",
    "                                                           \"Section\": \"motorsport\",\n",
    "                                                           \"Year\": \"2020\"}}}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d58495-bf97-4c69-8e94-68ea38bd1bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for document in T:\n",
    "    print(document.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e45f58-c0fa-4a14-8723-ae65193ec877",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = cnn_retriever_configurable.invoke(\"Russian\", \n",
    "                                      config={\"configurable\": \n",
    "                                              {\"hello_search\": \n",
    "                                               {\"k\": 6, 'fetch_k': 120, 'lambda_mult': 0.1,\n",
    "                                                \"filter\": {\"Category\": \"sport\",\n",
    "                                                           \"Section\": \"motorsport\",\n",
    "                                                           \"Year\": \"2020\"}}}})\n",
    "for document in T:\n",
    "    print(document.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d64ce4-cebe-45ca-a338-0938e1454e7e",
   "metadata": {},
   "source": [
    "# **** 預計第一個小時結束 ****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc50ad2-8895-4c02-8ccc-55ed3565f783",
   "metadata": {},
   "source": [
    "# LangChain Expression Language (LCEL)\n",
    "\n",
    "From an `Operator` to a `Foreman`:\n",
    "\n",
    "Assuming that you finished an LLM process and you want to hand it over to an intern to run it, who does not have too much knowledge of Langchain. How do you improve the chance that the workflow will run without getting mistake?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6a32c0-a242-40d1-8527-68a2a24fd5f3",
   "metadata": {},
   "source": [
    "### 食譜 - LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b61f5d-4ce0-425b-b712-139e55b2d4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schemas = [\n",
    "        ResponseSchema(name=\"used ingredients\", \n",
    "                       description=\"The actual ingredients used in cooking\"),\n",
    "        ResponseSchema(name=\"extra ingredients\", \n",
    "                       description=\"extra ingredients that have to be prepared \"),\n",
    "        ResponseSchema(name=\"result\", \n",
    "                       description=\"The dish and cooking recipe in detail\")]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "\n",
    "system_template = \"\"\"\n",
    "                  You are an AI assistant as the best chef in the world. \n",
    "                  You have a great taste and cooking skills like Gordon Ramsay. You should be able to come up with a dish based on `suggested ingredient`, and tell us what extra ingredients \n",
    "                  has to be prepared by comparing the ingredients actually \n",
    "                  used in the cooking and the `existing ingredient`\n",
    "\n",
    "                  The `suggested ingredients` are the ingredients suggested \n",
    "                  by some recipe. You have the freedom to add or remove \n",
    "                  ingredients to achieve the goal, but try to be as faithful \n",
    "                  to the `suggested ingredient` as possible. \n",
    "                  \"\"\"\n",
    "\n",
    "human_template = \"\"\"\n",
    "                 existing ingredients:[{existing_ingredients}];\n",
    "                 suggested ingredients: [{suggested_ingredients}]\\n; \n",
    "                 format instruction: {format_instructions}\n",
    "                 \"\"\"\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"existing_ingredients\", \n",
    "                                       \"suggested_ingredients\"],\n",
    "                    \"partial_variables\": {\"format_instructions\": \n",
    "                                          format_instructions}}}\n",
    "\n",
    "cuisine_prompt_template = build_standard_chat_prompt_template(input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b844250-d0f6-4233-a14d-d339509809c3",
   "metadata": {},
   "source": [
    "### Previously\n",
    "\n",
    "1. my_chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "2. prompt = my_chat_prompt_template.invoke(\n",
    "    {\"existing_ingredients\": existing_ingredients, \n",
    "     \"suggested_ingredients\": suggested_ingredients})\n",
    "3. output = model.invoke(prompt)\n",
    "4. final_output = output_parser.parse(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149e4c7f-4a4f-4f8e-b343-c2ae7fd3017b",
   "metadata": {},
   "source": [
    "### LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e547888-495a-4a6b-8b2f-06663d436688",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_ = cuisine_prompt_template|model|output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432a0264-9c47-4639-be04-a3e53a32df9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_.invoke({\"existing_ingredients\": \", \".join(existing_ingredients), \n",
    "                  \"suggested_ingredients\": \", \".join(suggested_ingredients)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc14bdc-b54c-4cea-9121-4521742bb03b",
   "metadata": {},
   "source": [
    "#### How do we attach the translation to the process above?\n",
    "\n",
    "- 1. Build the translation process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30aa0a7-1dc7-4777-ab20-8a0098c65679",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"\"\"\n",
    "                  You are a helpful AI assistant with native speaker \n",
    "                  fluency in both English and traditional Chinese (繁體中文). \n",
    "                  You will translate the given content.\n",
    "                  \"\"\"\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": \"{query}\",\n",
    "                    \"input_variable\": [\"query\"]}}\n",
    "\n",
    "translation_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "translation_chain = translation_prompt_template|model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce74c693-8d26-4ec3-a3a2-b37a81f7d98d",
   "metadata": {},
   "source": [
    "- 2. Connect the recipe chain with the translation chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ead3f6-c734-485a-9bd1-b5e41f622842",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_chain = cuisine_prompt_template|model\n",
    "\n",
    "pipeline = {\"query\": recipe_chain}|translation_chain|output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8015e875-183e-437b-8997-d1c9dfa25811",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.invoke({\"existing_ingredients\": \", \".join(existing_ingredients), \n",
    "                 \"suggested_ingredients\": \", \".join(suggested_ingredients)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d2df8d-7455-4549-a8ee-d572ed69182e",
   "metadata": {},
   "source": [
    "#### What happens?\n",
    "\n",
    "I know it looks mysterious, but it is very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1c2e24-1b1a-420c-a7ad-642be968c3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename= \"tutorial/LLM+Langchain/Week-2/LCEL_1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9dff1e-cd11-4627-a542-f54d52a4e21b",
   "metadata": {},
   "source": [
    "## Minimal Example\n",
    "\n",
    "### 1. Creating a Prompt Template (創建提示模板):\n",
    "\n",
    "- ChatPromptTemplate.from_template is used to create a prompt template. This template is a string that includes a placeholder {topic}.\n",
    "- The template specifies the instruction: \"tell me a short joke about {topic}\".\n",
    "- 使用 ChatPromptTemplate.from_template 創建一個提示模板。這個模板是一個包含佔位符 {topic} 的字符串。\n",
    "- 模板指定了指令：“tell me a short joke about {topic}”（給我講一個關於{topic}的簡短笑話）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd952993-29ad-4877-8795-5cf49e084105",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Official diagram flow\n",
    "\n",
    "Image(filename= \"tutorial/LLM+Langchain/Week-2/lcel pipeline.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb11136f-8d8d-44b9-8a75-9c91975f4aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
    "# prompt = ChatPromptTemplate(template=\"tell me a short joke about {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80d68f9-a5c7-46b2-b1fb-64fe8f530ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f21f8c-d2ca-4960-bb69-a59efaa45880",
   "metadata": {},
   "source": [
    "### 2. Setting Up the Chain (設置鏈條):\n",
    "\n",
    "- chain = prompt | model sets up a chain where the prompt is connected to a model. This means that the model will process the prompt to generate a response.\n",
    "- The | operator is used to combine the prompt and the model into a single chain.\n",
    "- chain = prompt | model 設置了一個鏈條，其中提示連接到模型。這意味著模型將處理該提示來生成回應。\n",
    "- | 運算符用於將提示和模型組合成一個鏈條。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016cee2f-686e-4b9c-b494-11e9f4886997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the PromptTemplate to the ChatModel\n",
    "\n",
    "pipeline_ = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6dcf0b-cb87-4c4f-8d88-06f714ec190b",
   "metadata": {},
   "source": [
    "### 3. Getting the Joke (獲取笑話):\n",
    "\n",
    "- The result of chain.invoke({\"topic\": \"ice cream\"}) is stored in the variable joke.\n",
    "- This variable now contains the generated joke about ice cream.\n",
    "- chain.invoke({\"topic\": \"ice cream\"}) 的結果存儲在變量 joke 中。\n",
    "- 這個變量現在包含生成的關於冰淇淋的笑話。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfd1bcd-39a6-43c3-b895-e2dc24c43ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input -> prompt template -> model\n",
    "\n",
    "joke = pipeline_.invoke({\"topic\": \"ice cream\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98ab297-c127-4e64-93f4-81f843efc958",
   "metadata": {},
   "outputs": [],
   "source": [
    "joke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a621e272-9562-4970-90d0-4d36661905b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(joke.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d10377-f1d6-422a-8c06-db4cc16f54b5",
   "metadata": {},
   "source": [
    "### 1. Importing StrOutputParser (導入 StrOutputParser):\n",
    "\n",
    "- The code imports StrOutputParser from the langchain_core.output_parsers module. This class is used to parse the output of the model into a string format.\n",
    "- 代碼從 langchain_core.output_parsers 模塊導入 StrOutputParser。這個類用於將模型的輸出解析為字符串格式。\n",
    "\n",
    "### 2. Creating an Output Parser:\n",
    "\n",
    "- An instance of StrOutputParser is created and assigned to the variable output_parser.\n",
    "- This parser will be used to process the raw output from the model and convert it into a readable string format.\n",
    "- 創建一個 StrOutputParser 的實例，並將其賦值給變量 output_parser。\n",
    "- 這個解析器將用於處理來自模型的原始輸出，並將其轉換為可讀的字符串格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0e144b-6c59-4a6c-a5c0-0ebd09fbd736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "pipeline_ = prompt | model | output_parser\n",
    "\n",
    "# input -> prompt template -> model -> output parser\n",
    "\n",
    "pipeline_.invoke({\"topic\": \"ice cream\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aab8739-5057-420e-a035-8055a1fad92f",
   "metadata": {},
   "source": [
    "## 範例操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060ba61c-382a-4b2d-a8fa-8a5513825e2c",
   "metadata": {},
   "source": [
    "### Coercion\n",
    "\n",
    "Do not ask me why this word is used...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc561f1b-7daa-42d2-bf1f-b9929aa96e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename= \"tutorial/LLM+Langchain/Week-2/LCEL_2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db580ce-4a32-4a96-be75-2f2d5d68e9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "joke_chain = prompt | model | output_parser\n",
    "\n",
    "template = \"\"\"\n",
    "           is this a funny joke? {joke}\n",
    "           \"\"\"\n",
    "\n",
    "human_prompt = PromptTemplate(template=template,\n",
    "                              input_variables=['joke'])\n",
    "\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "analysis_prompt = ChatPromptTemplate(messages=[human_message])\n",
    "\n",
    "analysis_chain = analysis_prompt | model\n",
    "\n",
    "composed_chain = {\"joke\": joke_chain} | analysis_chain | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e362c88-1f82-42c5-8b61-96c18affddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(composed_chain.invoke({\"topic\": \"ice cream\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5f8553-996e-4d34-bf70-12f9553b8cef",
   "metadata": {},
   "source": [
    "1. chain 執行結果，將結果放進'joke' 這個 key 裡\n",
    "2. {\"joke\": content} 被送進analysis_prompt 中，等價於 analysis_prompt.invoke({\"joke\": content})\n",
    "3. model 接收 analysis_prompt 產生的結果\n",
    "4. output_parser 處理結果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba419208-49a4-48b6-b96a-626963d9e798",
   "metadata": {},
   "source": [
    "## Parallelize steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c25e8c-eda5-464d-9abf-75ee04f70cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "joke_chain = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model\n",
    "poem_chain = ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\") | model\n",
    "\n",
    "map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)\n",
    "\n",
    "map_chain.invoke({\"topic\": \"bear\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143dea0c-1087-4fcf-adc2-8d7067a2d14a",
   "metadata": {},
   "source": [
    "- Computation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b778b6-506a-41c6-ae4d-25dfb3e4248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "joke_chain.invoke({\"topic\": \"bear\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f795294f-f594-4997-b493-9239cf416f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "poem_chain.invoke({\"topic\": \"bear\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcf29cc-d5a2-4879-9dd9-ef49b243b67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "map_chain.invoke({\"topic\": \"bear\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9850622e-12e1-42b0-94f0-cc49cbbb6f62",
   "metadata": {},
   "source": [
    "RunnableParallel are also useful for running independent processes in parallel, since each Runnable in the map is executed in parallel. For example, we can see our earlier joke_chain, poem_chain and map_chain all have about the same runtime, even though map_chain executes both of the other two.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db494f11-15f8-4658-b065-1a0449c10ab7",
   "metadata": {},
   "source": [
    "## Run custom function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e37218-e84c-4b5c-9846-5ae4c32d2e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "def length_function(text):\n",
    "    return len(text)\n",
    "\n",
    "\n",
    "def _multiple_length_function(text1, text2):\n",
    "    return len(text1) * len(text2)\n",
    "\n",
    "\n",
    "def multiple_length_function(_dict):\n",
    "    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")\n",
    "\n",
    "pipeline_ = (\n",
    "    {\n",
    "        \"a\": itemgetter(\"foo\") | length_function,\n",
    "        \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}\n",
    "        | multiple_length_function,\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58914c7-5864-4956-81c2-12b8e11f550e",
   "metadata": {},
   "source": [
    "- Oops, how to solve this error message? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802dadc3-be5a-4067-82c5-8e4135f1f2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap the function with RunnableLambda\n",
    "\n",
    "pipeline_ = (\n",
    "    {\n",
    "        \"a\": itemgetter(\"foo\") | RunnableLambda(length_function),\n",
    "        \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}\n",
    "        | RunnableLambda(multiple_length_function),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad18d9a4-d3ce-4fc5-a9f0-1be23581a9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_.invoke({\"foo\": \"bar\", \"bar\": \"gah\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7d179a-ccb1-48ef-9007-eab272b71c14",
   "metadata": {},
   "source": [
    "How does it work?\n",
    "\n",
    "- 'bar' -> foo, foo ('bar') -> length_function => a = 3\n",
    "- 'bar' -> foo & 'gah' -> bar, foo ('bar') -> 'text1' & bar ('gah') -> 'text2', {'text1': 'bar', 'text2': 'gah'} -> multiple_length_function => b = 9\n",
    "- {'a':3, 'b': 9} -> prompt -> 'what is 3 + 9'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1233f2-d49d-4a7a-8112-b7cba295c91a",
   "metadata": {},
   "source": [
    "#### Decorator\n",
    "\n",
    "- A very cool feature.\n",
    "- This was a new discovery at the beginning of December, so it is not used in subsequent tutorials. However, feel free to adapt the code and experience its magic.\n",
    "- Understanding programming remains key to building successful AI applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3defc52-a2e1-4720-a9c5-62b9c0a64405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import chain, RunnableParallel\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")\n",
    "\n",
    "@chain\n",
    "def length_function(text):\n",
    "    return len(text)\n",
    "\n",
    "def _multiple_length_function(text1, text2):\n",
    "    return len(text1) * len(text2)\n",
    "\n",
    "@chain\n",
    "def multiple_length_function(_dict):\n",
    "    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])\n",
    "\n",
    "pipeline_ = RunnableParallel(\n",
    "        a=itemgetter(\"foo\") | length_function,\n",
    "        b={\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}\n",
    "        | multiple_length_function)| prompt | model\n",
    "\n",
    "pipeline_.invoke({\"foo\": \"bar\", \"bar\": \"gah\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b907c618-0a13-44f0-a9d3-873ec6185f07",
   "metadata": {},
   "source": [
    "## Passing data through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0547d8de-37a7-48b8-8b6c-1448f1b110c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "runnable = RunnableParallel(\n",
    "    passed=RunnablePassthrough(),\n",
    "    modified=lambda x: x[\"num\"] + 1,\n",
    ")\n",
    "\n",
    "runnable.invoke({\"num\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17236ef-98e2-47f4-8e39-5f478ddb7310",
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable = RunnableParallel(\n",
    "    passed_2=RunnablePassthrough(),\n",
    "    modified=lambda x: x[\"num\"] + 1,\n",
    ")\n",
    "\n",
    "runnable.invoke({\"num\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffe96a0-3bb1-4d67-80cc-ef4666f7b3f6",
   "metadata": {},
   "source": [
    "## RAG + LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c43a9c-ed5f-4af1-ac91-b98d16f484c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@chain\n",
    "def chatbot_prompt_fn(data):\n",
    "\n",
    "    system_template = \"\"\"\n",
    "                      You are a helpful assistant. Answer all questions to the best of your ability.\n",
    "                      \"\"\"\n",
    "    \n",
    "    human_template = \"\"\"\n",
    "                     question: {question}\n",
    "\n",
    "                     Please answer the question based on the context:\n",
    "                     {context}\n",
    "                     \"\"\"\n",
    "    \n",
    "    input_ = {\"system\": {\"template\": system_template},\n",
    "              \"human\": {\"template\": human_template,\n",
    "                        \"input_variable\": [\"question\", \"context\"]}}\n",
    "    \n",
    "    prompt_template = build_standard_chat_prompt_template(input_)\n",
    "    \n",
    "    return prompt_template\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "path = os.path.join(get_project_dir(), 'tutorial', 'LLM+Langchain', 'Week-2', 'BertV2 Index')\n",
    "\n",
    "vectorstore = FAISS.load_local(path, embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", \n",
    "                                     search_kwargs={\"k\": 5})\n",
    "\n",
    "pipeline_ = RunnablePassthrough.assign(context=itemgetter('question')|retriever)|chatbot_prompt_fn|model|StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab89047-cf88-40be-9d88-7f804145bf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_.invoke({\"question\": \"Is it difficult to fine-tune BERT?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0c6e19-c233-432f-bc33-82ef3907df4f",
   "metadata": {},
   "source": [
    "## Translation Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e97c917-9e88-4a53-9073-76c03d75b764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain\n",
    "\n",
    "@chain\n",
    "def translation_function(text):\n",
    "\n",
    "    \"\"\"\n",
    "    翻譯\n",
    "    直接將給予內容text翻譯成繁體中文\n",
    "    \"\"\"\n",
    "    \n",
    "    system_template = \"\"\"\n",
    "                      You are a helpful AI assistant with native speaker \n",
    "                      fluency in both English and traditional Chinese \n",
    "                      (繁體中文). You will translate the given content into traditional Chinese \n",
    "                      (繁體中文).\n",
    "                      \"\"\"\n",
    "\n",
    "    human_template = \"\"\"\n",
    "                     {query}\n",
    "                     \"\"\"\n",
    "\n",
    "    input_ = {\"system\": {\"template\": system_template},\n",
    "              \"human\": {\"template\": human_template,\n",
    "                        \"input_variable\": [\"query\"]}}\n",
    "    \n",
    "    prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee204f8e-8f5a-48f9-957c-61dbc7e68456",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_1 = RunnablePassthrough.assign(context=itemgetter('question')|retriever)|chatbot_prompt_fn|model|StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3099436a-5446-441c-b029-05c4cca634c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_ = {\"query\": step_1}|translation_function|model|StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d6a5c6-11b5-4a6b-9551-badbbe1d7520",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_.invoke({\"question\": \"Is it difficult to fine-tune BERT?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d7cb9-485e-4d97-b445-70c77528f152",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
