{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69854491-d24e-4014-b598-1392f2e696e6",
   "metadata": {},
   "source": [
    "上週作業"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f5e101-968d-4062-848a-23cfe4245440",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8602e84a-871a-4cdf-aa31-13fd6be2b431",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(\"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abbb99d-12c3-4ced-a1ad-be97966a33d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain_core.runnables import chain\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "from src.initialization import credential_init\n",
    "from src.io.path_definition import get_project_dir\n",
    "\n",
    "\n",
    "credential_init()\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o-2024-05-13\", temperature=0)\n",
    "\n",
    "\n",
    "with open(os.path.join(get_project_dir(), 'tutorial', 'LLM+Langchain', 'Week-1', 'recipe_train.json'), 'r') as f:\n",
    "    recipe_train = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2b5e40-5a83-4520-9a17-40dec5d4ad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_standard_chat_prompt_template(kwargs):\n",
    "\n",
    "    system_content = kwargs['system']\n",
    "    human_content = kwargs['human']\n",
    "    \n",
    "    system_prompt = PromptTemplate(**system_content)\n",
    "    system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "    \n",
    "    human_prompt = PromptTemplate(**human_content)\n",
    "    human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "    \n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                     human_message\n",
    "                                                   ])\n",
    "\n",
    "    return chat_prompt\n",
    "\n",
    "\n",
    "@chain\n",
    "def translation_function(text):\n",
    "\n",
    "    \"\"\"\n",
    "    翻譯\n",
    "    直接將給予內容text翻譯成繁體中文\n",
    "    \"\"\"\n",
    "    \n",
    "    system_template = \"\"\"\n",
    "                      You are a helpful AI assistant with native speaker \n",
    "                      fluency in both English and traditional Chinese \n",
    "                      (繁體中文). You will translate the given content.\n",
    "                      \"\"\"\n",
    "\n",
    "    human_template = \"\"\"\n",
    "                     {query}\n",
    "                     \"\"\"\n",
    "\n",
    "    input_ = {\"system\": {\"template\": system_template},\n",
    "              \"human\": {\"template\": human_template,\n",
    "                        \"input_variable\": [\"query\"]}}\n",
    "    \n",
    "    prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "    chain = prompt_template|model\n",
    "    \n",
    "    output = chain.invoke({\"query\": text})\n",
    "    return output.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc8b386-3161-4a17-88d7-ec02fa371c3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_corpus = []\n",
    "\n",
    "for recipe in recipe_train:\n",
    "    tokenized_corpus.append(recipe['ingredients'])\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac9741b-e0db-49bb-bb4e-b416be2de046",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schemas = [\n",
    "        ResponseSchema(name=\"used ingredients\", \n",
    "                       description=\"The actual ingredients used in cooking\"),\n",
    "        ResponseSchema(name=\"extra ingredients\", \n",
    "                       description=\"extra ingredients that have to be prepared \"),\n",
    "        ResponseSchema(name=\"result\", \n",
    "                       description=\"The dish and cooking recipe in detail\")\n",
    "    ]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "\n",
    "system_template = \"\"\"\n",
    "                  You are an AI assistant as the best chef in the world. \n",
    "                  You have a great taste and cooking skills like Gordon Ramsay. You should be able to come up with a dish based on `suggested ingredient`, and tell us what extra ingredients \n",
    "                  has to be prepared by comparing the ingredients actually \n",
    "                  used in the cooking and the `existing ingredient`\n",
    "\n",
    "                  The `suggested ingredients` are the ingredients suggested \n",
    "                  by some recipe. You have the freedom to add or remove \n",
    "                  ingredients to achieve the goal, but try to be as faithful \n",
    "                  to the `suggested ingredient` as possible. \n",
    "                  \"\"\"\n",
    "\n",
    "human_template = \"\"\"\n",
    "                 existing ingredients:[{existing_ingredients}];\n",
    "                 suggested ingredients: [{suggested_ingredients}]\\n; \n",
    "                 format instruction: {format_instructions}\n",
    "                 \"\"\"\n",
    "\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"existing_ingredients\", \"suggested_ingredients\"],\n",
    "                    \"partial_variables\": {\"format_instructions\": format_instructions}}}\n",
    "\n",
    "chat_prompt_template = build_standard_chat_prompt_template(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b5c1da-6f71-4f16-90c0-8fdcdd83d817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(get_project_dir(), 'tutorial',  'LLM+Langchain',  'Week-1', 'recipe_test.json'), 'r') as f:\n",
    "    recipe_test = json.load(f)\n",
    "\n",
    "existing_ingredients = recipe_test[50]['ingredients']\n",
    "\n",
    "suggested_ingredients = bm25.get_top_n(existing_ingredients, recipe_train, n=3)[0]['ingredients']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a09fd2-f82a-4641-89de-d67ec5cb8c93",
   "metadata": {},
   "source": [
    "- Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d3936c-42c1-47ff-a14f-15065700ba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_ = chat_prompt_template|model|translation_function|output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3b91f1-b599-4893-81a5-c5cadfd7303b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chain = chat_prompt|model|output_parser\n",
    "\n",
    "# chain_translation = {'recipe': chain}|prompt_translation|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af5f91f-dd49-4b08-adf4-17739c40dba4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_.invoke({\"existing_ingredients\": \", \".join(existing_ingredients), \n",
    "                  \"suggested_ingredients\": \", \".join(suggested_ingredients)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7157f31-6509-4885-8af4-3bcd74b53647",
   "metadata": {},
   "source": [
    "- Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4074679-8f4e-44e0-883c-867c6c3efb83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_parsing(content) -> str:\n",
    "    \n",
    "    return f\"Translate the content into traditional Chinese (繁體中文): {content}\"\n",
    "\n",
    "pipeline_ = chat_prompt_template | model | text_parsing | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13afc86-bd54-4fb6-9a5b-cce35f54430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_.invoke({\"existing_ingredients\": \", \".join(existing_ingredients), \n",
    "                  \"suggested_ingredients\": \", \".join(suggested_ingredients)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec555189-20bc-4e4c-b8d4-2b6fbc056fb7",
   "metadata": {},
   "source": [
    "## Find out all the ingredients and cluster them according to some rules.\n",
    "\n",
    "- Do you still remember the joke-analysis chain? This one is structurally identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeee9f7-c4f5-42f4-a218-c4e7bb534caa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "# Create retriever\n",
    "\n",
    "documents = []\n",
    "\n",
    "for recipe in recipe_train:\n",
    "    document = Document(page_content=\", \".join(recipe['ingredients']),\n",
    "                        metadata={\"cuisine\": recipe['cuisine'],\n",
    "                                  \"id\": recipe['id']})\n",
    "    documents.append(document)\n",
    "\n",
    "retriever = BM25Retriever.from_documents(documents=documents)\n",
    "    \n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "query = \", \".join(recipe_test[5]['ingredients'])\n",
    "\n",
    "question = \"Show me all the ingredients.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf452b5-5b95-4897-a799-528d973f476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first level chain\n",
    "\n",
    "system_template = \"\"\"\n",
    "                  You are an AI assistant as the best chef in the world. \n",
    "                  \"\"\"\n",
    "\n",
    "human_template = \"\"\"\n",
    "                 Answer the question based only on the following context:\n",
    "                {context}\n",
    "                \n",
    "                Question: {question}\n",
    "                 \"\"\"\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"context\", \"question\"]}}\n",
    "\n",
    "chat_prompt_template_1 = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "user_input = {\"context\": itemgetter(\"query\")|retriever, \n",
    "              \"question\":itemgetter(\"question\")}\n",
    "\n",
    "pipeline_1 = user_input | chat_prompt_template_1 | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bc90c3-95d8-4f0a-9b88-54cabbf28f9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# second level chain\n",
    "\n",
    "response_schemas = [\n",
    "        ResponseSchema(name=\"result\", \n",
    "                       description=\"\"\"\n",
    "                                   a python dictionary with structure\n",
    "                                   <PYTHON STR>: <PYTHON LIST> \n",
    "                                   \"\"\")]\n",
    "\n",
    "structured_output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "format_instructions = structured_output_parser.get_format_instructions()\n",
    "\n",
    "system_template = \"\"\"\n",
    "                  You are an AI assistant as the best chef in the world. \n",
    "                  \"\"\"\n",
    "\n",
    "human_template = \"\"\"\n",
    "                 Please sort through the {ingredients}, such as meat, \n",
    "                 scipy, milk product,..., etc; \n",
    "                 format instruction: {format_instructions}\n",
    "                 \"\"\"\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"ingredients\"],\n",
    "                    \"partial_variables\": {\"format_instructions\": \n",
    "                                           format_instructions}}}\n",
    "\n",
    "chat_prompt_template_2 = build_standard_chat_prompt_template(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4696b82-4f0c-4b35-b0b6-dae608371a0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_chain = {\"ingredients\": pipeline_1}|chat_prompt_template_2|model|structured_output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a112bcf-864b-4160-b026-4c8b9d469010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_chain.invoke({\"query\": query, \"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26db4c4-91b7-4d5e-82cd-7f435d9e1c1e",
   "metadata": {},
   "source": [
    "上次上課有人問中文行不行，讓我們來挑戰看看。我之前沒試過中文 (2024.10.21)\n",
    "\n",
    "現實是中文的訓練資料，算力，資金是中國那邊有壓倒性的優勢，所以你要是希望有中文很強大的大語言模型，你得把希望放在中國身上: https://github.com/jeinlee1991/chinese-llm-benchmark\n",
    "- 百度文心一言\n",
    "- 阿里通义千问\n",
    "- 百川\n",
    "- 讯飞星火\n",
    "- 商汤senseChat\n",
    "\n",
    "做AI要多少錢? \n",
    "\n",
    "有一個新創公司老闆找上一個FB的研究員要他過來帶AI團隊，那個研究員只扔下一句話: 等你有10000片H100GPU再來找我。一片大概是40000美金，所以啥都還沒做就要先噴掉4億美金。\n",
    "\n",
    "另外要產圖的話基本上是全英文，目前已知支援中文提示詞的模型是\n",
    "- 快手 Kolors\n",
    "- 騰訊 Huanyuan\n",
    "\n",
    "AI有三個等級\n",
    "1. 美國\n",
    "2. 中國\n",
    "3. 世界其他地方\n",
    "   \n",
    "所以，英文練好可能比較實際一點。\n",
    "\n",
    "我高中國文是在及格線上下起跳的，外加大一國文被當掉，太為難人了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7d50de-1485-4fa0-802f-30e7eff04648",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schemas = [\n",
    "        ResponseSchema(name=\"used ingredients\", \n",
    "                       description=\"實際上用來烹飪的食材\"),\n",
    "        ResponseSchema(name=\"extra ingredients\", \n",
    "                       description=\"在現有的食材上，外加的食材\"),\n",
    "        ResponseSchema(name=\"result\", \n",
    "                       description=\"菜色和烹飪細節\")\n",
    "    ]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# Define human prompt template\n",
    "\n",
    "system_template =  \"\"\"\n",
    "                   你是一個人工智能助手，而你將扮演世界頂尖廚師的角色。 \n",
    "                   你有像是Gordon Ramsay的頂尖烹飪技術和品味. 你將根據`建議食材`\n",
    "                   來製作料理，並且藉由比較`現有食材`和實際上使用的食材，\n",
    "                   告知我們那些‵extra ingredients`需要準備。\n",
    "\n",
    "                   `建議食材` 是來自某本食譜上的食材. 你可以自由發揮，\n",
    "                   多加或是移除食材來達成目標，但是盡可能地貼近`建議食材`。\n",
    "                   使用者是繁體中文母語。請將所有輸出翻譯成繁體中文(traditional Chinese)。\n",
    "                   \"\"\"\n",
    "\n",
    "system_prompt = PromptTemplate(template=system_template)\n",
    "\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_template = \"\"\"\n",
    "                 `現有食材`:[{existing_ingredients}]; \n",
    "                 `建議食材`: [{suggested_ingredients}]\\n; \n",
    "                 `格式`: {format_instructions}\n",
    "                 \"\"\"\n",
    "\n",
    "human_prompt = PromptTemplate(template=human_template,\n",
    "                              input_variables=[\"existing_ingredients\", \n",
    "                                               \"suggested_ingredients\"],\n",
    "                              partial_variables={\"format_instructions\": \n",
    "                                                 format_instructions}\n",
    "                              )\n",
    "\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                human_message\n",
    "                                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb580351-338b-4f54-9278-3739c784771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_ = chat_prompt|model|output_parser\n",
    "\n",
    "pipeline_.invoke({\"existing_ingredients\": \", \".join(existing_ingredients), \n",
    "                  \"suggested_ingredients\": \", \".join(suggested_ingredients)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01268c94-c213-4c6b-9fbc-f6dbc541832e",
   "metadata": {},
   "source": [
    "## 回家自行練習\n",
    "1. 自己找一個中文食譜，打開冰箱掃一下。把冰箱裡的東西作為`現有食材`輸入，食譜中的某道菜作為`建議食材`輸入\n",
    "2. Find out all the ingredients and cluster them according to some rules -> 轉換成中文下去做。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf63ea43-5036-4303-8a7d-12a961cd620a",
   "metadata": {},
   "source": [
    "# Types of Classification (分類類型)\n",
    "\n",
    "There are two main types (主要有兩種類型):\n",
    "\n",
    "- Sorting by feelings (Sentiment Classification)\n",
    "- Sorting by category (Category Classification)\n",
    "- 按感覺排序（情感分類）\n",
    "- 按類別排序（類別分類）\n",
    "\n",
    "## How it Works (工作原理):\n",
    "\n",
    "- We use a method called supervised learning. This means we have data and labels. Labels are like tags that tell us what the data is about. These labels are usually created by people who look at the data and decide what it means.\n",
    "- 我們使用一種稱為監督學習的方法。這意味著我們有數據和標籤。標籤就像標籤，告訴我們數據是什麼。這些標籤通常由查看數據並決定其含義的人創建。\n",
    "\n",
    "把每一組數據想像成是(題目，答案)的組合，靠著瘋狂刷題(大數據)來學習如何從題目得到答案。\n",
    "Aka: 填鴨式教育。填鴨其實不差，起碼鴨會肥。\n",
    "\n",
    "## Challenges\n",
    "\n",
    "- It takes a long time.\n",
    "- It's not always done the same way by different people.\n",
    "- It's expensive.\n",
    "- Training the model often needs renting cloud services like AWS or Azure.\n",
    "- Using these cloud services for running the model also costs a lot.\n",
    "- 耗時： 標記數據需要很長時間。\n",
    "- 不一致： 不同的人可能會以不同的方式標記相同的數據。\n",
    "- 昂貴： 該過程成本高昂。\n",
    "- 資源密集型： 訓練模型通常需要租用雲服務，如 AWS 或 Azure。\n",
    "- 運營成本： 使用這些雲服務運行模型的成本也很高。\n",
    "\n",
    "## Can we mimic supervised learning classification with LLM (我們能否用大型語言模型模仿監督學習分類)?\n",
    "\n",
    "- Yes, we can use large language models (LLMs) to mimic supervised learning classification. LLMs, like GPT-3 or GPT-4, can be trained to understand and generate human-like text. They can be fine-tuned on specific tasks, such as classification, without the need for extensive labeled datasets.\n",
    "- 是的，我們可以使用大型語言模型（LLM）來模仿監督學習分類。像 GPT-3 或 GPT-4 這樣的 LLM 可以被訓練來理解和生成類似人類的文本。它們可以針對特定任務進行微調，例如分類，而不需要大量標記數據集。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de87a279-4a17-4717-b28e-ff5e33620d00",
   "metadata": {},
   "source": [
    "## Zero-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867fe6b6-3106-4e23-8402-31f47a463ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(url=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb157d0e9-d18e-4835-a601-edeb011f0ee6_721x247.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec345b68-b69a-4b88-8147-879fd2e59f00",
   "metadata": {},
   "source": [
    "## 飛安事故原因分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a6e3e2-ba20-4906-81f3-676cdc717079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(os.path.join(get_project_dir(), 'tutorial', 'LLM+Langchain', 'Week-3', 'Data sample.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa789d2-2421-4715-95a7-c7bd9ddd1d09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73f9adc-13ce-4c59-bf07-29095ec514ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef841a6-2cf9-4fe6-8ae5-bb8a54c21b1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(filename= \"tutorial/LLM+Langchain/Week-3/HFACS_Org_Inf.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e09af75-31ad-4fb3-913f-b02fcedfe26d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_template = \"\"\"\n",
    "                  You are an AI assistant assigned with a task of safty report \n",
    "                  classification based on the content. You are a seasoned \n",
    "                  flight safety inspector with deep and extensive knowledge of \n",
    "                  aviation safty. \n",
    "    \n",
    "                  You always do the best work you can. You are highly \n",
    "                  analytical and pay close attention to details. \n",
    "    \n",
    "                  The candidates of the output are:\n",
    "\n",
    "                  - `Organizational Influence;Resource Management`\n",
    "                  - `Organizational Influence;Organizational Climate`\n",
    "                  - `Organizational Influence;rganizational Process`\n",
    "                  - `Unsafe Supervisions;Inadequate Supervision`\n",
    "                  - `Unsafe Supervisions;Planned Inappropriate Operations`\n",
    "                  - `Unsafe Supervisions;Failed to Correct Problem`\n",
    "                  - `Unsafe Supervisions;Supervisory Violation`\n",
    "                  - `Precondition for Unsafe Acts;Environmental Factors;Physical Environment`\n",
    "                  - `Precondition for Unsafe Acts;Environmental Factors;Technological Environment`\n",
    "                  - `Precondition for Unsafe Acts;Condition of Operators;Adverse Mental State`\n",
    "                  - `Precondition for Unsafe Acts;Condition of Operators;Adverse Physiological State`\n",
    "                  - `Precondition for Unsafe Acts;Condition of Operators;Physical/Mental Limitations`\n",
    "                  - `Precondition for Unsafe Acts;Personnel Factors;Crew Resource Management`\n",
    "                  - `Precondition for Unsafe Acts;Personnel Factors;Personal Readiness`\n",
    "                  - `Unsafe Acts;Errors;Decision Errors`\n",
    "                  - `Unsafe Acts;Errors;Skill-Based Errors`\n",
    "                  - `Unsafe Acts;Errors;Perceptual Errors`\n",
    "                  - `Unsafe Acts;Violations;Routine`\n",
    "                  - `Unsafe Acts;Violations;Exceptional`\n",
    "            \n",
    "                 The output is from one of the candidates. \n",
    "                 \"\"\"\n",
    "\n",
    "human_template = \"{report}\"\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"report\"]}}\n",
    "\n",
    "chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "pipeline_ = chat_prompt_template | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79bcc3c-4baa-4aad-a0c3-403733c0f027",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = df.iloc[3]['Report 1']\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1267429-92f8-468d-90fa-7fa990bba8ba",
   "metadata": {},
   "source": [
    "- Do you want to read it by yourself or do you want to outsource this to a machine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1701a49-6d3c-47a1-acde-40c7e4656832",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pipeline_.invoke({\"report\": text})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409e957e-f9b9-44b7-afdb-b916e82d79bc",
   "metadata": {},
   "source": [
    "### 使用parser精煉結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deb1254-889c-4133-873e-30f4fef41881",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_schemas = [\n",
    "        ResponseSchema(name=\"category\", \n",
    "                       description=\"\"\"\n",
    "                                   The predicted category of the \n",
    "                                   classification\n",
    "                                   \"\"\")]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "system_template = \"\"\"\n",
    "                  You are an AI assistant assigned with a task of safty report \n",
    "                  classification based on the content. You are a seasoned \n",
    "                  flight safety inspector with deep and extensive knowledge of \n",
    "                  aviation safty. \n",
    "    \n",
    "                  You always do the best work you can. You are highly \n",
    "                  analytical and pay close attention to details. \n",
    "    \n",
    "                  The candidates of the output are:\n",
    "\n",
    "                  - `Organizational Influence;Resource Management`\n",
    "                  - `Organizational Influence;Organizational Climate`\n",
    "                  - `Organizational Influence;rganizational Process`\n",
    "                  - `Unsafe Supervisions;Inadequate Supervision`\n",
    "                  - `Unsafe Supervisions;Planned Inappropriate Operations`\n",
    "                  - `Unsafe Supervisions;Failed to Correct Problem`\n",
    "                  - `Unsafe Supervisions;Supervisory Violation`\n",
    "                  - `Precondition for Unsafe Acts;Environmental Factors;Physical Environment`\n",
    "                  - `Precondition for Unsafe Acts;Environmental Factors;Technological Environment`\n",
    "                  - `Precondition for Unsafe Acts;Condition of Operators;Adverse Mental State`\n",
    "                  - `Precondition for Unsafe Acts;Condition of Operators;Adverse Physiological State`\n",
    "                  - `Precondition for Unsafe Acts;Condition of Operators;Physical/Mental Limitations`\n",
    "                  - `Precondition for Unsafe Acts;Personnel Factors;Crew Resource Management`\n",
    "                  - `Precondition for Unsafe Acts;Personnel Factors;Personal Readiness`\n",
    "                  - `Unsafe Acts;Errors;Decision Errors`\n",
    "                  - `Unsafe Acts;Errors;Skill-Based Errors`\n",
    "                  - `Unsafe Acts;Errors;Perceptual Errors`\n",
    "                  - `Unsafe Acts;Violations;Routine`\n",
    "                  - `Unsafe Acts;Violations;Exceptional`\n",
    "            \n",
    "                 The output is from one of the candidates. \n",
    "                 \"\"\"\n",
    "\n",
    "human_template = \"\"\"\n",
    "                 {report}; \n",
    "                 format instruction: {format_instructions}\n",
    "                 \"\"\"\n",
    "\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"report\"],\n",
    "                    \"partial_variables\": {'format_instructions': format_instructions}}}\n",
    "\n",
    "chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "pipeline_ = chat_prompt_template|model|output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1da940-87d9-4d77-981a-82e597781d6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = pipeline_.invoke({\"report\": text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2255ea-0e55-4fa9-95d7-a204cbc68942",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78654a90-8412-4486-9859-e5e4008b21d6",
   "metadata": {},
   "source": [
    "### 回家作業 1\n",
    "\n",
    "若要飛安事故報告可以有複數分類結果，如何調整Prompt，包含parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93278e4e-449f-4b76-99ed-3b44acd9f2d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.filter(like='Report')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5984ea6c-2edd-42f9-ac35-9ab6d8af2d55",
   "metadata": {},
   "source": [
    "### 回家作業 2\n",
    "\n",
    "你可以很清楚的看到一個飛安事故中，可以出現複數報告。\n",
    "將`Report 1` 和 `Report 1.2` 結合起來產生一份的新報告。\n",
    "\n",
    "抄也是一門技術"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b2296d-901a-43a6-b441-7b4abc65ac22",
   "metadata": {},
   "source": [
    "# **** 預計第一個小時結束 ****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aee5c0b-7804-49bf-bd77-ff8203de6209",
   "metadata": {},
   "source": [
    "## HR: Job-Applicant Matching\n",
    "\n",
    "- Efficiency and Speed: LLMs can quickly process and analyze large volumes of applications, significantly reducing the time required for initial screening compared to manual reviews.\n",
    "\n",
    "- Consistency and Fairness: LLMs apply the same criteria consistently across all applications, minimizing human bias and ensuring a fairer initial screening process.\n",
    "\n",
    "- Detailed Analysis: LLMs can analyze complex language patterns and extract relevant information from resumes, cover letters, and other application materials, identifying key skills and qualifications that match job requirements.\n",
    "\n",
    "- Customization and Flexibility: LLMs can be customized to prioritize different skills and experiences based on specific job requirements, allowing for a more tailored screening process.\n",
    "\n",
    "- Scalability: LLMs can handle a large number of applications simultaneously, making them ideal for organizations that receive high volumes of applicants.\n",
    "\n",
    "- Cost-Effectiveness: By automating the initial stages of applicant screening, LLMs can reduce the need for extensive human resources, thereby lowering operational costs.\n",
    "\n",
    "- Continuous Improvement: LLMs can be continuously trained and improved based on feedback and new data, enhancing their accuracy and effectiveness over time.\n",
    "\n",
    "- Enhanced Candidate Experience: Faster response times and more consistent evaluations can improve the overall candidate experience, as applicants are more likely to receive timely feedback.\n",
    "\n",
    "- 效率和速度： LLMs 能夠快速處理和分析大量申請，相較於人工審查，顯著縮短初步篩選所需的時間。\n",
    "\n",
    "- 一致性和公平性： LLMs 對所有申請應用相同的標準，最小化人為偏見，確保初步篩選過程的公平性。\n",
    "\n",
    "- 詳細分析： LLMs 能夠分析複雜的語言模式，從簡歷、求職信和其他申請材料中提取相關信息，識別符合工作要求的關鍵技能和資格。\n",
    "\n",
    "- 自定義和靈活性： LLMs 可以根據具體的工作要求自定義優先考慮的技能和經驗，允許更有針對性的篩選過程。\n",
    "\n",
    "- 可擴展性： LLMs 能夠同時處理大量申請，非常適合接收大量申請人的組織。\n",
    "\n",
    "- 成本效益： 通過自動化申請篩選的初始階段，LLMs 可以減少對大量人力資源的需求，從而降低運營成本。\n",
    "\n",
    "- 持續改進： LLMs 可以根據反饋和新數據持續進行訓練和改進，隨著時間的推移提高其準確性和有效性。-\n",
    "\n",
    "- 提升候選人經驗： 更快的回應時間和更一致的評估可以改善整體候選人經驗，因為申請人更有可能及時收到反饋。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39b5024-206c-4bdf-9574-8753f01ca46f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(\"https://www.techjobasia.com/zh-Hant/jobs/GMMlhU0qSayr6ZwTB0U6zA---Software-Engineer-(ReactJS)\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46edd4df-89c2-49ca-9e19-d6ae0eb6adb7",
   "metadata": {},
   "source": [
    "###  1. Send a GET request to the URL (發送 GET 請求到指定的 URL)\n",
    "\n",
    "- This line sends an HTTP GET request to the specified URL and stores the response in the response variable.\n",
    "- 這行程式碼向指定的 URL 發送 HTTP GET 請求，並將響應儲存在 response 變數中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a779c48-badd-423e-889c-a57ece76a957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "\n",
    "url = \"https://www.techjobasia.com/zh-Hant/jobs/GMMlhU0qSayr6ZwTB0U6zA---Software-Engineer-(ReactJS)\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf1968c-7975-45a4-870b-0e62861219a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e681fd9e-5ba2-4c75-8bc3-4d41d747aaa4",
   "metadata": {},
   "source": [
    "### 2. Get the content of the response (獲取響應的內容)\n",
    "\n",
    "- This line extracts the content of the response as a text string and stores it in the html_content variable.\n",
    "- 這行程式碼將響應的內容作為文字字串提取，並將其儲存在 html_content 變數中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7947d4-5e33-4b1c-88b5-ba375e62b2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c04f13-6362-479b-8ebb-98a713c325fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# html_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee3cd12-f11a-4a7b-8f7e-ab8b16ada802",
   "metadata": {},
   "source": [
    "### 3. Parse the HTML content (解析 HTML 內容)\n",
    "\n",
    "- This line uses BeautifulSoup to parse the HTML content stored in html_content, creating a BeautifulSoup object named soup.\n",
    "- 這行程式碼使用 BeautifulSoup 解析儲存在 html_content 中的 HTML 內容，並創建一個名為 soup 的 BeautifulSoup 對象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583fc8ac-d92b-4762-afd0-e1d1de8c3e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa763c1-d8c7-4c82-9528-0ef7ea564112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046accba-5203-43df-b1c8-b5703601d3b3",
   "metadata": {},
   "source": [
    "### 4. Remove all CSS style tags (移除所有 CSS 樣式標籤)\n",
    "\n",
    "- This loop finds all <style> tags in the parsed HTML content and removes them using the decompose() method.\n",
    "- 這個循環找到解析後的 HTML 內容中的所有 <style> 標籤，並使用 decompose() 方法將它們移除。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ab74e8-4e7c-4a92-b5ea-af3db43d1082",
   "metadata": {},
   "outputs": [],
   "source": [
    "for style in soup.find_all('style'):\n",
    "    style.decompose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5edde0-9fb5-43d4-bfb9-6a6b2c146d7e",
   "metadata": {},
   "source": [
    "### 6. Extract and print only the text content (提取並打印僅包含文字的內容)\n",
    "\n",
    "- This line extracts the text content from the parsed HTML, separating elements with a newline character, and stores it in the text_content variable.\n",
    "- 這行程式碼從解析後的 HTML 中提取文字內容，使用換行符將元素分隔，並將其儲存在 text_content 變數中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c797f3bc-33c2-4d67-88e2-b110f0d2da83",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_content = soup.get_text(separator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba34a7e-4ebd-4ea7-b738-54995862f082",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cacc52-b827-47d3-8f05-399360212b76",
   "metadata": {},
   "source": [
    "### 7. Clean up the text (清理文字內容)\n",
    "\n",
    "- This line cleans up the extracted text by removing any leading or trailing whitespace from each line and discarding empty lines. The cleaned text is stored in the cleaned_text variable.\n",
    "- prints the cleaned text content to the console.\n",
    "- 這行程式碼通過移除每行的首尾空白並丟棄空行來清理提取的文字內容。清理後的文字儲存在 cleaned_text 變數中。\n",
    "- 將清理後的文字內容打印到控制台。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777d33a8-301c-4823-8753-4b501300382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for line in text_content.splitlines():\n",
    "#     if line.strip():\n",
    "#         print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6ff06a-d842-465d-b4f4-3e7282824e15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_text = '\\n'.join(line.strip() for line in text_content.splitlines() if line.strip())\n",
    "\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabae79b-382d-4626-a526-af3e781ab3ca",
   "metadata": {},
   "source": [
    "### Extract only the job related content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a31c4e7-395e-4770-a21f-8f21aee07e28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template= \"\"\"\n",
    "          Extract the job description part of the text: {content} \n",
    "          \"\"\"\n",
    "\n",
    "human_prompt = PromptTemplate(template=template)\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "    \n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([human_message])\n",
    "\n",
    "pipeline_ = chat_prompt_template|model\n",
    "\n",
    "output = pipeline_.invoke({\"content\": cleaned_text})\n",
    "\n",
    "job_description = output.content\n",
    "\n",
    "print(job_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3640b1ad-c74a-4535-9d2a-db2f2501ec9e",
   "metadata": {},
   "source": [
    "Let us do the same thing for the ten following candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7355ea-e001-4b89-b831-803a045ae1f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "applicants = [\"https://www.hiredchina.com/profiles/c87dc713-deea-408a-8798-c4e0814fc3ce\",\n",
    "              \"https://www.hiredchina.com/profiles/de4d4802-d58b-4d0a-92f6-b0f999ff2617\",\n",
    "              \"https://www.hiredchina.com/profiles/64cecea3-4f68-4707-89fa-c10dd9889be8\",\n",
    "              \"https://www.hiredchina.com/profiles/708fa8bc-0ccf-4f93-8be6-14154bfe828d\",\n",
    "              \"https://www.hiredchina.com/profiles/3ce4f0fb-c93f-4e87-aa00-1d75badce875\",\n",
    "              \"https://www.hiredchina.com/profiles/2aa145b8-cad0-4682-a542-8bbe0d268d74\",\n",
    "              \"https://www.hiredchina.com/profiles/06b2343a-a1ea-42da-bbfb-318041ae0835\",\n",
    "              \"https://www.hiredchina.com/profiles/3daf0124-ad33-4a9c-8305-28428bbf5ffc\",\n",
    "              \"https://www.hiredchina.com/profiles/053e6b20-0dee-477a-9fb6-3ab27e69206b\",\n",
    "              \"https://www.hiredchina.com/profiles/27f68cab-1f20-432b-9157-4f473c2ec189\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825038f7-7d58-4eb6-ad24-a41c14e6a322",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain\n",
    "\n",
    "\n",
    "def parsing_process(url):\n",
    "    \"\"\"\n",
    "    Fetches and extracts text content from a given URL.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL of the web page to fetch and parse.\n",
    "\n",
    "    Returns:\n",
    "    str: Cleaned text content extracted from the web page.\n",
    "\n",
    "    Raises:\n",
    "    requests.exceptions.RequestException: If an error occurs while fetching the URL.\n",
    "\n",
    "    Notes:\n",
    "    - This function sends a GET request to the specified URL.\n",
    "    - It uses BeautifulSoup to parse the HTML content of the response.\n",
    "    - Any <style> tags in the HTML are removed to extract only textual content.\n",
    "    - The extracted text is cleaned by removing extra whitespace and empty lines.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Get the content of the response\n",
    "    html_content = response.text\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    for style in soup.find_all('style'):\n",
    "        style.decompose()\n",
    "\n",
    "    # Extract and print only the text content\n",
    "    text_content = soup.get_text(separator='\\n')\n",
    "\n",
    "    # Clean up the text (optional)\n",
    "    cleaned_text = '\\n'.join(line.strip() for line in text_content.splitlines() if line.strip())\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f540152-5a9f-45db-a02c-488427766ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IFrame(applicants[0], width=800, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8bc542-54f1-4f0f-a1dd-fdb77252c1e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(parsing_process(applicants[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a74bf02-61db-4b78-ba98-013ebc8f77eb",
   "metadata": {},
   "source": [
    "### Extract the Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7a14c9-84d9-4a67-be3f-feefe74244b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "system_template = \"\"\"\n",
    "                  I am going to give you a template for your output. \n",
    "                  CAPITALIZED WORDS are my placeholders. \n",
    "                  Fill in my placeholders with your output. Please preserve \n",
    "                  the overall formatting of my template. My template is:\n",
    "\n",
    "                  *** Working Experience:*** WORKING EXPERIENCE \n",
    "                  *** Education:*** EDUCATION\n",
    "                  *** Skills:*** SKILLS\n",
    "\n",
    "                  I will give you the data to format in the next prompt. \n",
    "                  Create a resume using my template.\n",
    "                  \"\"\"\n",
    "\n",
    "human_template = \"\"\"\n",
    "                 {query}\n",
    "                 \"\"\"\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"query\"]}}\n",
    "\n",
    "resume_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "resume_pipeline = {\"query\": parsing_process}|resume_prompt_template|model|StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81388d28-d18a-4750-8151-4cb394ad71e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resume_output = resume_pipeline.invoke(applicants[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77226af-b87d-4803-b694-02aa55539298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(resume_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6823b73a-8c52-45e6-8dfd-901732a25767",
   "metadata": {},
   "source": [
    "### Match the resume and the job\n",
    "\n",
    "#### 1. Create Parser\n",
    "\n",
    "- Define Response Schemas (定義響應模式)\n",
    "- Create a Structured Output Parser (創建結構化輸出解析器)\n",
    "- Get Format Instructions (獲取格式說明)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2b73ec-d204-47b0-9d31-f103b609ba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schemas = [\n",
    "        ResponseSchema(name=\"result\", description=\"\"\"If the candidate is a good fit, either `Yes` or `No` \n",
    "        \"\"\"),\n",
    "        ResponseSchema(name=\"reason\", description=\"\"\"Applicant - Job mathing \n",
    "        \"\"\")\n",
    "]\n",
    "\n",
    "structured_output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = structured_output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293333e8-8106-4d23-8bd3-115b15ac6f48",
   "metadata": {},
   "source": [
    "### 2. Create Template\n",
    "\n",
    "- Create a System Prompt Template (創建系統提示模板)\n",
    "- Create a System Message Prompt Template (創建系統訊息提示模板)\n",
    "- Create a Human Prompt Template (創建人類提示模板)\n",
    "- Create a Human Message Prompt Template (創建人類訊息提示模板)\n",
    "- Create a Chat Prompt Template (創建對話提示模板)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1390f7e-9f85-4a71-9860-f017b10ce879",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_template = \"\"\"\n",
    "                  You are an AI assistant acting as an experienced senior \n",
    "                  recruiter in IT field.\n",
    "                  \n",
    "                  You are assigned a tasked of identifying if an applicant, \n",
    "                  as an resume, is a good match to the described job. \n",
    "                    \n",
    "                  You always do the best work you can. You are highly \n",
    "                  analytical and pay close attention to details. \n",
    "                  \"\"\"\n",
    "\n",
    "human_template = \"\"\"\n",
    "                 Job description: {job};\n",
    "                 resume: {resume}; format_instructions: {format_instructions}\n",
    "                 \"\"\"\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"job\", \"resume\"],\n",
    "                    \"partial_variables\": {'format_instructions': format_instructions}}}\n",
    "\n",
    "match_prompt_template = build_standard_chat_prompt_template(input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc647902-b8e7-446b-b0c1-6ae2b6678fc7",
   "metadata": {},
   "source": [
    "### 3. Build the Chain\n",
    "\n",
    "- Chain the Prompt Template with the Model and Parser (將提示模板與模型和解析器鏈接)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee8e78e-49b7-481b-9a26-f1f9ddcdb83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_pipeline = match_prompt_template|model|structured_output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2a6215-3e77-4736-9f4c-54dcc9a6c130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "matching_output = matching_pipeline.invoke({\"job\":output.content, \"resume\":resume_output})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52226361-b458-45e2-a17d-bb47f2f3e7a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "matching_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ed3c6f-6a52-48b4-aab9-cadd8bc13fcb",
   "metadata": {},
   "source": [
    "### 4. An End to End application\n",
    "\n",
    "- Job description: 固定住，對某一群申請者來說這是不會變的\n",
    "- Applicants: 動態抽取\n",
    "- 媒合鍊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ef0972-6fd0-4c54-a3e8-68fc6a4926b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename= \"tutorial/LLM+Langchain/Week-3/LCEL_3.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b412464f-0cd9-43cc-bdfd-4989a2218f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# resume_pipeline: url -> a resume\n",
    "# matching_pipeline: receive the resume and match it with the job description\n",
    "\n",
    "step_1 = RunnablePassthrough.assign(resume=itemgetter(\"url\")|resume_pipeline)\n",
    "step_2 = matching_pipeline\n",
    "e2e_chain = step_1|step_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d17b78-5e04-48b0-b350-1a8bf04e977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in principle, job description is known because as an HR, you should have that\n",
    "\n",
    "e2e_chain.invoke({\"url\": applicants[0],\n",
    "                  \"job\": job_description})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c00acb-7d5a-420c-a642-e6061e4e4a4e",
   "metadata": {},
   "source": [
    "# N-Shot Learning\n",
    "\n",
    "Imagine you have a friend who is incredibly good at understanding and completing tasks just by looking at a few examples. This is somewhat like how N-shot learning works with large language models (LLMs).\n",
    "\n",
    "## What is N-Shot Learning?\n",
    "N-shot learning is a way of teaching an AI model by giving it a small number of examples (N examples) for a particular task. Here's how it works:\n",
    "\n",
    "- 1-Shot Learning: The AI is shown one example of the task and then asked to perform a similar task.\n",
    "- Few-Shot Learning: The AI is shown a few examples (like 2, 3, or 5) of the task before it is asked to perform a similar task.\n",
    "- Zero-Shot Learning: The AI is given instructions without any examples and asked to perform the task.\n",
    "\n",
    "## Why is N-Shot Learning Useful?\n",
    "Let's say you're teaching someone how to fold a paper airplane. If you show them just one way to fold it, that's 1-shot learning. If you show them three different ways, that's 3-shot learning. By seeing these examples, they can understand the general idea and apply it to fold new paper airplanes on their own.\n",
    "\n",
    "## How Does This Work in AI?\n",
    "For AI models, like the ones used in chatbots or virtual assistants, N-shot learning allows the model to learn from a few examples rather than needing thousands of examples. Here’s a simple analogy to explain it:\n",
    "\n",
    "- Example Task: Writing a short story.\n",
    "    - 1-Shot Example: You show the AI one short story and then ask it to write a similar story.\n",
    "    - Few-Shot Example: You show the AI three different short stories and then ask it to write a new story.\n",
    "\n",
    "By doing this, the AI understands patterns, structures, and styles from the examples provided and uses this understanding to generate new and relevant content.\n",
    "\n",
    "## Benefits of N-Shot Learning\n",
    "- Efficiency: It’s much faster and requires less data to train the AI.\n",
    "- Versatility: The AI can quickly adapt to new tasks by being shown a few examples.\n",
    "- Practicality: It’s similar to how humans learn, making it easier to apply in real-world situations.\n",
    "\n",
    "## Real-World Application\n",
    "\n",
    "Imagine a customer service AI that needs to answer queries. Instead of being trained with thousands of examples, you could use N-shot learning:\n",
    "\n",
    "- Show it a few examples of customer queries and the appropriate responses.\n",
    "- The AI learns the pattern and can then generate suitable responses for new queries.\n",
    "\n",
    "In essence, N-shot learning helps AI models become smarter and more adaptable by learning from just a handful of examples, much like how we humans learn from a few demonstrations before we can do something on our own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5786fd-2a50-40a0-8bcb-ce431058c2ca",
   "metadata": {},
   "source": [
    "# N-Shot 學習\n",
    "\n",
    "想像一下，你有一位朋友非常擅長理解和完成任務，只需要看幾個例子。這有點像 N-Shot 學習在大型語言模型（LLM）中的運作方式。\n",
    "\n",
    "## 什麼是 N-Shot 學習？\n",
    "\n",
    "N-Shot 學習 是通過給 AI 模型一些示例（N 個示例）來教它完成特定任務的一種方法。以下是它的運作方式：\n",
    "\n",
    "- 1-Shot 學習：AI 只看到一個任務示例，然後被要求執行類似的任務。\n",
    "- 少樣本學習（Few-Shot Learning）：AI 看到幾個（如 2、3 或 5 個）任務示例，然後被要求執行類似的任務。\n",
    "- 零樣本學習（Zero-Shot Learning）：AI 只給出指令而不提供任何示例，然後被要求執行任務。\n",
    "\n",
    "## 為什麼 N-Shot 學習有用？\n",
    "\n",
    "假設你在教某人如何摺紙飛機。如果你只給他們看一個摺紙飛機的方法，這就是 1-Shot 學習。如果你給他們看三種不同的方法，這就是 3-Shot 學習。通過這些示例，他們可以理解大致的想法，並將其應用到摺新的紙飛機上。\n",
    "\n",
    "## 這在 AI 中如何運作？\n",
    "\n",
    "對於 AI 模型（如聊天機器人或虛擬助手中使用的模型），N-Shot 學習允許模型從少量示例中學習，而不需要成千上萬的示例。這裡有一個簡單的類比來解釋：\n",
    "\n",
    "## 任務示例：寫一個短故事。\n",
    "\n",
    "- 1-Shot 示例：你給 AI 看一個短故事，然後讓它寫一個類似的故事。\n",
    "- Few-Shot 示例：你給 AI 看三個不同的短故事，然後讓它寫一個新的故事。\n",
    "- 通過這樣做，AI 從提供的示例中理解模式、結構和風格，並使用這些理解來生成新的相關內容。\n",
    "\n",
    "## N-Shot 學習的好處\n",
    "\n",
    "- 效率高：訓練 AI 的速度更快，所需數據更少。\n",
    "- 多功能性：AI 可以通過少量示例快速適應新任務。\n",
    "- 實用性：這類似於人類學習的方式，使其更容易應用於現實世界的情況。\n",
    "\n",
    "## 實際應用\n",
    "想像一下，一個需要回答問題的客服 AI。與其用成千上萬的示例進行訓練，你可以使用 N-Shot 學習：\n",
    "- 給它看一些客戶問題和適當回應的示例。\n",
    "- AI 學習模式，然後可以為新問題生成合適的回應。\n",
    "\n",
    "總之，N-Shot 學習幫助 AI 模型通過少量示例變得更聰明和更具適應性，就像我們人類通過幾次示範後就能獨立完成任務一樣。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4509bcf3-c22d-4cec-bb20-777f9f7e9b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url=\"https://miro.medium.com/v2/resize:fit:720/format:webp/0*0eN5uwvq7JcesVex.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7094152d-7559-4163-b22f-0ce3ebb6bc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "documents = []\n",
    " \n",
    "for recipe in recipe_train[:500]:  # 只提取前500個節省時間\n",
    "    document = Document(page_content=\", \".join(recipe['ingredients']),\n",
    "                        metadata={\"cuisine\": recipe['cuisine'],\n",
    "                                  \"id\": recipe['id']})\n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7759dd59-41f9-48f6-a18c-54896bd60474",
   "metadata": {},
   "source": [
    "### 1. Creating a Vector Store with FAISS (使用 FAISS 創建向量存儲)\n",
    "\n",
    "- This line creates a vector store using FAISS, which takes a collection of documents and their embeddings (numerical representations of the documents). This vector store will be used to find documents based on their embeddings.\n",
    "- 這行代碼使用 FAISS 創建了一個向量存儲，它接受一組文檔及其嵌入（文檔的數字表示）。這個向量存儲將用於基於嵌入查找文檔。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63050a8a-486b-480b-a2ad-fe47ca127ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(documents, embedding=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822a3cc5-56c2-45e2-8b6a-d4e4e313330e",
   "metadata": {},
   "source": [
    "### 2. Creating an Embedding Retriever (創建嵌入檢索器)\n",
    "\n",
    "- This line converts the vector store into a retriever that uses similarity search. The search_type=\"similarity\" specifies that the retriever should find documents similar to a given query. The search_kwargs={'k': 5} means that the retriever will return the top 5 most similar documents.\n",
    "- 這行代碼將向量存儲轉換為一個使用相似性搜索的檢索器。search_type=\"similarity\" 指定檢索器應查找與給定查詢相似的文檔。search_kwargs={'k': 5} 意味著檢索器將返回最相似的前 5 個文檔。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ec1aa2-d54a-413a-80b5-a2c1150b0d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_retriever = vectorstore.as_retriever(search_type=\"similarity\",\n",
    "                                               search_kwargs={'k': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f24316-98e6-46b2-8048-da5602abc29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_retriever.invoke(\", \".join(existing_ingredients))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac958249-b600-4edb-a00d-e78b0efceafa",
   "metadata": {},
   "source": [
    "### 3. Creating a BM25 Retriever (創建 BM25 檢索器)\n",
    "\n",
    "- This line creates a BM25 retriever from the given documents. BM25 is a popular algorithm used in information retrieval to rank documents based on their relevance to a query. The k=5 specifies that this retriever will also return the top 5 most relevant documents.\n",
    "- 這行代碼從給定文檔創建了一個 BM25 檢索器。BM25 是一種流行的信息檢索算法，用於根據查詢的相關性對文檔進行排序。k=5 指定這個檢索器也將返回最相關的前 5 個文檔。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bacf7d-f0b5-4ab8-a919-24110245e565",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever = BM25Retriever.from_documents(documents, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d5e0cb-e2be-43bf-a51c-90ea03d6859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever.invoke(\", \".join(existing_ingredients))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91315cab-3064-409c-a4d5-a8c57df9e973",
   "metadata": {},
   "source": [
    "### 4. Creating an Ensemble Retriever (創建集成檢索器)\n",
    "\n",
    "- This line creates an ensemble retriever that combines the BM25 retriever and the embedding retriever. The weights=[0.5, 0.5] means that both retrievers contribute equally to the final search results.\n",
    "- 這行代碼創建了一個集成檢索器，它結合了 BM25 檢索器和嵌入檢索器。weights=[0.5, 0.5] 意味著兩個檢索器對最終搜索結果的貢獻是相等的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a37b896-6d80-4ab3-80eb-528561443455",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, embedding_retriever], weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3731a9-dbf9-4f90-9015-e7bff3365e07",
   "metadata": {},
   "source": [
    "嗯 看起來沒問題 繼續探究原因。會是 recipe_test[99]['ingredients']這筆數據的問題嗎?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf83ad-2f62-4f85-9bb4-a9031ccad391",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(get_project_dir(), 'tutorial', 'LLM+Langchain',  'Week-1', 'recipe_test.json'), 'r') as f:\n",
    "    recipe_test = json.load(f)\n",
    "\n",
    "existing_ingredients = recipe_test[99]['ingredients']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a704c95c-a202-4827-8e29-30a77c5a95cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = ensemble_retriever.invoke(\", \".join(existing_ingredients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b63f44f-5c16-4dfc-8183-142c88214e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cd4385-4b2a-4300-a33d-c388723f3fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af68a17-4116-4bc1-b167-4b3505a597de",
   "metadata": {},
   "source": [
    "同樣還是8筆。分開查詢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896f0830-ab4e-466a-add4-41867769317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_retriever.invoke(\", \".join(existing_ingredients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b294b16-2744-4cbd-a319-17da5a76cdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever.invoke(\", \".join(existing_ingredients))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b32866e-3ca4-4916-9084-994c83c2a7e1",
   "metadata": {},
   "source": [
    "你會看到 id28191 和　id20601有重複出現。也許重複的數據會被扣除。\n",
    "\n",
    "查了一下融合演算法，因為最後是計算每筆數據\"加總\"的得分來計算，所以重複的數據會被合併起來。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9101b4-2468-43a8-a119-f419d5364586",
   "metadata": {},
   "source": [
    "### 1. Creating Example Inputs (創建示例輸入)\n",
    "\n",
    "- This block of code initializes an empty list example_inputs. It then iterates over a list of examples, extracting the page_content (input) and the cuisine from the metadata (output) for each example. These are added to the example_inputs list in the form of dictionaries with keys \"input\" and \"output\".\n",
    "- 這段代碼初始化了一個空列表 example_inputs。然後，它迭代 examples 列表，提取每個示例的 page_content（輸入）和 metadata 中的 cuisine（輸出）。這些被添加到 example_inputs 列表中，形成包含 \"input\" 和 \"output\" 鍵的字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da64cf7-10eb-4f3e-b565-81e54a0dd6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "\n",
    "example_inputs = []\n",
    "\n",
    "for example in examples:\n",
    "    example_inputs.append({\"input\": example.page_content, \n",
    "                           \"output\": example.metadata['cuisine']})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fcb37b-baba-4c54-834c-f4efb33aa2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43178d7e-fe58-4779-9909-516fac96c977",
   "metadata": {},
   "source": [
    "### 2. Creating an Example Prompt (創建示例提示)\n",
    "\n",
    "- This line creates a ChatPromptTemplate for the examples. It defines a conversation where the human's message is the input ({input}), and the AI's response is the output ({output}). This template will be used to format the examples consistently.\n",
    "- 這行代碼為示例創建了一個 ChatPromptTemplate。它定義了一個對話，其中人類的訊息是輸入（{input}），AI 的回應是輸出（{output}）。這個模板將用於一致地格式化示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a532573-4544-4cd2-ab52-0cd0bb25d8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [('human', '{input}'), ('ai', '{output}')]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d2a205-0a2d-4c64-be97-021877098d74",
   "metadata": {},
   "source": [
    "### 3. Creating a Few-Shot Prompt Template (創建少樣本提示模板)\n",
    "\n",
    "- This line creates a FewShotChatMessagePromptTemplate. It takes the example_prompt and the list of example_inputs as arguments. This template will use the provided examples to guide the AI in generating responses, mimicking the structure of the examples.\n",
    "- 這行代碼創建了一個 FewShotChatMessagePromptTemplate。它將 example_prompt 和 example_inputs 列表作為參數。該模板將使用提供的示例來指導 AI 生成回應，模仿示例的結構。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5edde5e-171c-4a4c-985c-ea77c267d160",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_message = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=example_inputs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db57e7d-afd3-48d7-935c-1a9aef232ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cf9807-489c-47f3-9d2a-f900b9b78aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"\"\"\n",
    "                  You are an AI assistant assigned with a \n",
    "                  task of identifying the cuisine based on the ingredients\n",
    "                  \"\"\"\n",
    "\n",
    "system_prompt = PromptTemplate(template=system_template)\n",
    "\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template='{input}')\n",
    "\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt) \n",
    "\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                few_shot_message,\n",
    "                                                human_message\n",
    "                                               ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d7c24d-b726-4c4f-9a29-cabda21c6925",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt.invoke({\"input\": \", \".join(existing_ingredients)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc5f0e3-ce79-4243-b43a-f1c158435566",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = chat_prompt|model\n",
    "\n",
    "output = pipeline.invoke({\"input\": \", \".join(existing_ingredients)})\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b8fdca-2eeb-4650-903c-ba6fbba6d414",
   "metadata": {},
   "source": [
    "## Can we chain this together with LCEL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f294b414-cd5f-4684-83af-df7b528417a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9102c59c-aa52-4ee9-a53e-b8cd467f92cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_standard_chat_prompt_template_v2(kwargs):\n",
    "\n",
    "    messages = []\n",
    "    \n",
    "    for key in ['system', 'few_shot', 'human']:\n",
    "        if kwargs.get(key):\n",
    "            if key == 'system':\n",
    "                system_content = kwargs['system']\n",
    "                system_prompt = PromptTemplate(**system_content)\n",
    "                message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "            elif key == 'human':\n",
    "                human_content = kwargs['human']\n",
    "                human_prompt = PromptTemplate(**human_content)\n",
    "                message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "            else:\n",
    "                few_shot_content = kwargs['few_shot']\n",
    "                message = FewShotChatMessagePromptTemplate(**few_shot_content)\n",
    "            \n",
    "            messages.append(message)\n",
    "    \n",
    "    chat_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "    return chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a03bf32-4113-4e0b-bb2a-cbf1a4ad7f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@chain\n",
    "def few_shot_prompt_fn(data):\n",
    "\n",
    "    example_prompt = ChatPromptTemplate.from_messages(\n",
    "        [('human', '{input}'), ('ai', '{output}')]\n",
    "    )\n",
    "\n",
    "    examples = []\n",
    "    \n",
    "    for example in data['examples']:\n",
    "        examples.append({\"input\": example.page_content, \n",
    "                         \"output\": example.metadata['cuisine']})\n",
    "    \n",
    "    few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "        example_prompt=example_prompt,\n",
    "        examples=examples,\n",
    "    )\n",
    "\n",
    "    system_template = \"\"\"\n",
    "                      You are an AI assistant assigned with a \n",
    "                      task of identifying the cuisine based on the ingredients.\n",
    "                      \"\"\"\n",
    "    \n",
    "    human_template = data['input']\n",
    "    \n",
    "    input_ = {\"system\": {\"template\": system_template},\n",
    "              \"human\": {\"template\": human_template},\n",
    "              \"few_shot\": {\"example_prompt\": example_prompt,\n",
    "                           \"examples\": examples}}\n",
    "    \n",
    "    prompt_template = build_standard_chat_prompt_template_v2(input_)\n",
    "\n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9d9559-4231-4ebf-8fd4-4ff3e25a6545",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = {\"examples\": ensemble_retriever,\n",
    "            \"input\": RunnablePassthrough()}|few_shot_prompt_fn|model|StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800e7fc5-ce8f-4d9b-8075-0d76ed86e918",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.invoke(\", \".join(existing_ingredients))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e29dff-c431-4468-92a3-2fb173c2f747",
   "metadata": {},
   "source": [
    "### 在SystemPrompt 給予更精確的指示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d01bd70-a0ca-44c1-a282-6328765de630",
   "metadata": {},
   "outputs": [],
   "source": [
    "@chain\n",
    "def few_shot_prompt_fn(data):\n",
    "\n",
    "    example_prompt = ChatPromptTemplate.from_messages(\n",
    "        [('human', '{input}'), ('ai', '{output}')]\n",
    "    )\n",
    "\n",
    "    examples = []\n",
    "    \n",
    "    for example in data['examples']:\n",
    "        examples.append({\"input\": example.page_content, \n",
    "                         \"output\": example.metadata['cuisine']})\n",
    "    \n",
    "    few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "        example_prompt=example_prompt,\n",
    "        examples=examples,\n",
    "    )\n",
    "\n",
    "    system_template = \"\"\"\n",
    "                      You are an AI assistant assigned with a task of identifying the origin of cuisine based on the ingredients, \n",
    "                      a few QUESTION - ANSWER pair will be provided in the following format:\n",
    "    \n",
    "                      human: <INGREDIENTS>\n",
    "                      ai: <ORIGIN>\n",
    "    \n",
    "                      \"\"\"\n",
    "    \n",
    "    human_template = data['input']\n",
    "    \n",
    "    input_ = {\"system\": {\"template\": system_template},\n",
    "              \"human\": {\"template\": human_template},\n",
    "              \"few_shot\": {\"example_prompt\": example_prompt,\n",
    "                           \"examples\": examples}}\n",
    "    \n",
    "    prompt_template = build_standard_chat_prompt_template_v2(input_)\n",
    "\n",
    "    return prompt_template\n",
    "\n",
    "pipeline = {\"examples\": ensemble_retriever,\n",
    "            \"input\": RunnablePassthrough()}|few_shot_prompt_fn|model|StrOutputParser()\n",
    "\n",
    "pipeline.invoke(\", \".join(existing_ingredients))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de27572d-da2b-4b21-9520-448232665994",
   "metadata": {},
   "source": [
    "### Any other possibilities?\n",
    "\n",
    "The code above was finished by the end of July."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31858bf2-517f-414f-a397-c772084d8600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "\n",
    "SemanticSimilarityExampleSelector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed90ddb4-742d-4bf2-90fc-6c92e56e9484",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"human: {input}\\nai: {output}\",\n",
    ")\n",
    "\n",
    "examples = []\n",
    "\n",
    "for recipe in recipe_train[:500]:\n",
    "\n",
    "    examples.append({\"input\": \", \".join(recipe['ingredients']),\n",
    "                     \"output\": recipe['cuisine']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5fe4b9-e285-4da8-872f-8ade5ccf41b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a91979-50fa-4e58-b154-d874d3953918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # The list of examples available to select from.\n",
    "    examples=examples,\n",
    "    # The embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    embeddings=embedding,\n",
    "    # The VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    vectorstore_cls=FAISS,\n",
    "    # The number of examples to produce.\n",
    "    k=4,\n",
    ")\n",
    "\n",
    "system_template = \"\"\"\n",
    "                  You are an AI assistant assigned with a task of identifying the origin of cuisine based on the ingredients, \n",
    "                  a few QUESTION - ANSWER pair will be provided in the following format:\n",
    "\n",
    "                  human: <INGREDIENTS>\n",
    "                  ai: <ORIGIN>\n",
    "\n",
    "                  \"\"\"\n",
    "\n",
    "system_prompt = PromptTemplate(template=system_template)\n",
    "\n",
    "## FewShotPromptTemplate here, instead of FewShotChatMessagePromptTemplate\n",
    "\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # We provide an ExampleSelector instead of examples.\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=system_prompt.template,\n",
    "    suffix=\"human: {input}\\nai:\",\n",
    "    input_variables=[\"input\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5528fdb-f79f-4673-bf80-2bad69026ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = similar_prompt|model|StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba791e15-b2ca-4c24-8bd8-d0b92db2f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.invoke({\"input\": \", \".join(existing_ingredients)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9ea019-c3dc-46ea-9d52-72e63849b0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# data: pd.DataFrame\n",
    "\n",
    "# train, test = train_test_split(data, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ba0f93-1d90-4b18-8adc-8763efa06578",
   "metadata": {},
   "source": [
    "### 回家作業 3\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/similarity/\n",
    "\n",
    "1. 替換掉examples\n",
    "<!-- 2. 替換OpenAIEmbeddings()為HuggingFaceEmbeddings -->\n",
    "3. 替換掉k值。假設為5-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b727171-e77e-4e5d-9cd0-3998e5ef2eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
