{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f5e101-968d-4062-848a-23cfe4245440",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8602e84a-871a-4cdf-aa31-13fd6be2b431",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(\"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abbb99d-12c3-4ced-a1ad-be97966a33d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import chain\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "from src.initialization import credential_init\n",
    "from src.io.path_definition import get_project_dir\n",
    "\n",
    "\n",
    "# 快速建立chat_prompt_template\n",
    "def build_standard_chat_prompt_template(kwargs):\n",
    "\n",
    "    messages = []\n",
    " \n",
    "    if 'system' in kwargs:\n",
    "        content = kwargs.get('system')\n",
    "        prompt = PromptTemplate(**content)\n",
    "        message = SystemMessagePromptTemplate(prompt=prompt)\n",
    "        messages.append(message)  \n",
    "\n",
    "    if 'human' in kwargs:\n",
    "        content = kwargs.get('human')\n",
    "        prompt = PromptTemplate(**content)\n",
    "        message = HumanMessagePromptTemplate(prompt=prompt)\n",
    "        messages.append(message)\n",
    "        \n",
    "    chat_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "    return chat_prompt\n",
    "\n",
    "\n",
    "#快速建立pipeline\n",
    "def build_pipeline(model, inputs, parser=None):\n",
    "    prompt = build_standard_chat_prompt_template(inputs)\n",
    "    chain = prompt | model\n",
    "    if parser:\n",
    "        chain |= parser\n",
    "    return chain\n",
    "\n",
    "\n",
    "credential_init()\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578de59b-eadb-44f4-97d5-393703757226",
   "metadata": {},
   "source": [
    "# RAG N-Shot\n",
    "\n",
    "> 🎯 **本章學完你將能學會什麼：**\n",
    "> - 了解 RAG (Retrieval-Augmented Generation) 與 N-Shot Prompting 的概念與應用\n",
    "> - 學會如何讓模型「學風格」──從示例中提取風格特徵並以相同風格生成內容\n",
    "> - 掌握 分類任務 的不同策略（Zero-Shot、Few-Shot、LLM-based Classification）\n",
    "> - 能實作 詩詞分類、飛安報告分類 等多樣化應用\n",
    "> - 了解 非同步資料處理流程 (Async + LCEL)，提升資料擷取與分析效能\n",
    "> - 學會從 網頁、Word、PDF 中提取文字資料並轉化為可用內容\n",
    "> - 能建立一個 AI 招募匹配系統：從網頁抓取職缺 → 提取履歷 → 分析 → 自動配對\n",
    "\n",
    "> 📘 最終你將具備的能力：\n",
    "> -能夠獨立設計並實作一個整合檢索、風格模仿、分類與匹配的 AI 自動化工作流，\n",
    "> -讓模型不只回答問題，更能以特定風格「思考、生成與決策」。\n",
    "\n",
    "在 RAG (Retrieval-Augmented Generation) 中，我們不只可以讓模型檢索資料庫來回答問題，還可以透過 N-Shot 提示 (N-Shot Prompting) 的方式，讓模型學習「風格」。\n",
    "\n",
    "這裡的 N，代表你給模型幾個示例 (Examples)。\n",
    "\n",
    "1-Shot：只給一個示例，模型會模仿該風格來生成。\n",
    "\n",
    "Few-Shot (N-Shot)：給多個示例，模型會歸納出共同的風格特徵。\n",
    "\n",
    "0-Shot：完全沒有示例，模型只能靠內建知識來生成。\n",
    "\n",
    "透過這種方式，我們可以讓模型不只是「回答問題」，而是「用指定風格來回答問題」。\n",
    "\n",
    "老樣子，選擇風格鮮明的例子。\n",
    "\n",
    "## 風格學習: 掄語"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9bf3f7-1c1c-4fb4-814c-7bac50d2f750",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[\"人不知，而不愠，不亦君子乎\", \"有人不知道我的大名，可我還沒發怒，這已經很君子了。\"], \n",
    "        [\"君子不重則不威\", \"君主打人一定要下重手，不然就樹立不了威信。\"],\n",
    "        [\"君子愛財，取之有道\", \"我喜歡錢，所以拿走你的錢，這是很有道理的。\"],\n",
    "        [\"既來之，則安之。\", \"既然來到了這裡，那麼就安葬在這裡吧。\"],\n",
    "        [\"子不語怪力亂神。\", \"夫子不想說話，施展起怪力將人打的神志不清。\"],\n",
    "        [\"不義而富且貴，於我如浮雲。\", \"不正當的錢財，對我來說猶如浮雲一般多。\"],\n",
    "        [\"朝聞道，夕可死矣。\", \"早上聽到我來了，晚上你就得死。\"],\n",
    "        [\"三人行，必有我師焉。\", \"有三個人，只要其中有一個是我，戰力就相當於一個師。\"],\n",
    "        [\"凡事豫則立，不豫則廢。\", \"但凡打架，只要猶豫，對面便站起來了。不猶豫就能直接將對面打廢。\"],\n",
    "        [\"孔子東遊，見兩小兒辯日。\", \"孔子去東邊打架，小孩在討論和孔子打架的人還能不能見到明天的太陽。\"],\n",
    "        [\"父母在，不遠遊，遊必有方。\", \"你父母在我手裡，你跑不了的，就算你跑了，我也有辦法把你抓回來。\"],\n",
    "        [\"始作傭者，其無後乎。\", \"這件事的主謀，已經被我打的絕後了。\"],\n",
    "        [\"鬼神敬而遠之\", \"孔子一旦發威，連鬼神見了都得敬畏的遠離他。\"],\n",
    "        [\"力不足者，中道而廢。\", \"力量不如我的人，在道上就只能被我打廢。\"],\n",
    "        [\"不恥下問\", \"看到我不自愧者，你就去下面問問。\"],\n",
    "        [\"三年無改於父之道，可謂孝矣。\", \"三年不該認我當父親的習慣，可以算作孝了。\"],\n",
    "        [\"人之將死，其言也善。\", \"把人打到瀕死，說的話也就好聽了。\"],\n",
    "        [\"知之為知之，不知為不知，是知也。\", \"該知道的知道，不該知道的少知道，知道嗎?\"],\n",
    "        [\"有教無類\", \"我在教你做事情，不管你是什麼人\"],\n",
    "        [\"子在川上曰: 逝者如斯夫，不捨晝夜。\", \"夫子站在河上說:死的人這麼多，是因為我不分晝夜地打人。\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470a9479-0e34-488a-8391-37a44f476c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "documents = []\n",
    "\n",
    "for row in data:\n",
    "    document = Document(page_content=row[0],\n",
    "                        metadata={\"翻譯\": row[1]})\n",
    "    documents.append(document)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fb31db-5e90-47d4-97c2-d8b1c86cfd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(path=\"/tmp/langchain_qdrant\")\n",
    "\n",
    "collection_name = \"掄語\"\n",
    "dimension =  embeddings.client.get_sentence_embedding_dimension()\n",
    "\n",
    "try:\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=dimension, distance=Distance.COSINE),\n",
    "    )\n",
    "except ValueError:\n",
    "    client.delete_collection(collection_name=collection_name)\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=dimension, distance=Distance.COSINE),\n",
    "    )\n",
    "\n",
    "vectorstore_QVS = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=collection_name,\n",
    "    embedding=embeddings,\n",
    "    retrieval_mode=RetrievalMode.DENSE,\n",
    ")\n",
    "\n",
    "vectorstore_QVS.add_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a88ee1-860b-483c-973c-4b6120c52214",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"其為人也孝弟，而好犯上者，鮮矣\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec8edb5-a575-48d1-a037-7efab8815689",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_QVS = vectorstore_QVS.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "retriever_QVS.invoke(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70defc13-097c-413c-8f10-c33d68689ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "system_template = dedent(\"\"\"\n",
    "You are a helpful AI assistant and you will help us interpret the content based on the style of the examples:\n",
    "\n",
    "{context}\n",
    "\"\"\")\n",
    "\n",
    "human_template = dedent(\"\"\"\n",
    "{query}\n",
    "\"\"\")\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template,\n",
    "                     \"input_variables\": ['context']},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variables\": [\"query\"],\n",
    "                    }}\n",
    "\n",
    "chat_prompt_template = build_standard_chat_prompt_template(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54d43f5-7799-4cfb-b968-27f78e083928",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_documents = retriever_QVS.invoke(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebf6b3f-16ee-47a1-9318-a958df815c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = [f\"Human: {document.page_content}\\nAI: {document.metadata['翻譯']}\" for document in retrieved_documents]\n",
    "\n",
    "print(\"\\n\\n\".join(context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703d3eaa-4325-4b1f-9420-18af52a12ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_context = \"\\n\\n\".join(context)\n",
    "\n",
    "chat_prompt = chat_prompt_template.invoke({\"query\": user_query, \"context\": merged_context})\n",
    "model.invoke(chat_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d99e3fb-c5e0-4de8-92c1-33a43bb2872e",
   "metadata": {},
   "source": [
    "味道不太對...我們有辦法強化生成嗎?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efd7aae-0c32-4423-97f2-f8bdad58bdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke(f\"Help us analyze the style:\\n{merged_context} and reply in traditional Chinese(繁體中文)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0922b0ef-482c-4f19-8508-8c950dc010e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "class requirements(BaseModel):\n",
    "\n",
    "    style: str = Field(description=\"The underlying style shown in the content. The output shall be in traditional Chinese (繁體中文).\")\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=requirements)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "human_template = dedent(\"\"\"\n",
    "                 {query}\n",
    "                 format instruction: {format_instructions}\n",
    "                 \"\"\")\n",
    "\n",
    "input_ = {\"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"query\"],\n",
    "                    \"partial_variables\": {\"format_instructions\": \n",
    "                                          format_instructions}}}\n",
    "\n",
    "chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "prompt = chat_prompt_template.invoke({\"query\": f\"Help us analyze the style:\\n{merged_context}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dae863b-7995-478c-8793-280754797c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c1d075-5190-43fd-ae59-34d2455940dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser.parse(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a01861f-596c-4600-bf71-650c11fd7566",
   "metadata": {},
   "source": [
    "### 方法:\n",
    "\n",
    "1. 使用檢索器檢索相關訊息\n",
    "2. 根據檢索出來的內容抽取風格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d4a1ec-e5b2-4a99-97de-176061253d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f746a2-ff85-4b13-88db-6fc014fa37ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough, chain, RunnableLambda\n",
    "\n",
    "@chain\n",
    "def document_2_context(documents):\n",
    "\n",
    "    context = [f\"Human: {document.page_content}\\nAI: {document.metadata['翻譯']}\" for document in documents]\n",
    "\n",
    "    return \"\\n\\n\".join(context)\n",
    "\n",
    "context_extraction_pipeline = itemgetter(\"query\")|retriever_QVS|document_2_context\n",
    "\n",
    "# Retrieval\n",
    "# context_extraction_pipeline.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e21a164-e220-4297-bf27-988947ee8e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_template = dedent(\"\"\"\n",
    "                 Help us analyze the style of interpretation shown in the text: \n",
    "                 \n",
    "                 {context}\n",
    "                 \n",
    "                 format instruction: {format_instructions}\n",
    "                 \"\"\")\n",
    "\n",
    "input_ = {\"human\": {\"template\": human_template,\n",
    "                    \"input_variables\": [\"context\"],\n",
    "                    \"partial_variables\": {\"format_instructions\": \n",
    "                                          format_instructions}}}\n",
    "\n",
    "style_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "# style_pipeline = RunnablePassthrough.assign(context=itemgetter(\"query\")|retriever_QVS|document_2_context)\n",
    "# print(style_pipeline.invoke({\"query\": query}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e97158-a296-4cf6-8aa3-a2a2ea9f0a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "style_pipeline = RunnablePassthrough.assign(context=context_extraction_pipeline)\n",
    "style_pipeline.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9008de5a-6f9e-4d78-82b6-c214151d6146",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_extraction_pipeline = style_prompt_template|model|output_parser|RunnableLambda(lambda x: x.style)\n",
    "\n",
    "style_pipeline = RunnablePassthrough.assign(context=context_extraction_pipeline)|RunnablePassthrough.assign(style=style_extraction_pipeline)\n",
    "style_pipeline.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97c0c83-c26e-4054-bc90-e1890d8d058d",
   "metadata": {},
   "source": [
    "#### 現在我們看到風格可以被提取出來了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb88f7de-d384-40f0-bda6-5434c5202611",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = dedent(\"\"\"\n",
    "You are a helpful AI assistant and you will help us interpret the user query with this style:\n",
    "{style}\n",
    "\"\"\")\n",
    "\n",
    "human_template = dedent(\"\"\"\n",
    "Examples:\n",
    "{context}\n",
    "\n",
    "query: {query}\n",
    "\"\"\")\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template,\n",
    "                     \"input_variables\": ['style']},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variables\": [\"query\", \"context\"],\n",
    "                    }}\n",
    "\n",
    "\n",
    "\n",
    "generate_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "generate_pipeline = style_pipeline|generate_prompt_template|model\n",
    "\n",
    "print(generate_pipeline.invoke({\"query\": user_query}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1f2634-4e0e-4ec3-8cf4-0b3676b5603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"有顏回者好學，不遷怒，不貳過。不幸短命死矣！\"\n",
    "\n",
    "generate_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "generate_pipeline = style_pipeline|generate_prompt_template|model\n",
    "\n",
    "print(generate_pipeline.invoke({\"query\": query}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72e38dc-bdc4-4bc8-a38f-cf4d8574db00",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"子路曰：「衛君待子而為政，子將奚先？」子曰：「必也正名乎！」\"\n",
    "\n",
    "generate_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "generate_pipeline = style_pipeline|generate_prompt_template|model\n",
    "\n",
    "print(generate_pipeline.invoke({\"query\": query}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf63ea43-5036-4303-8a7d-12a961cd620a",
   "metadata": {},
   "source": [
    "# 分類任務\n",
    "\n",
    "在機器學習中，分類主要有兩大類：\n",
    "\n",
    "- 情感分類 (Sentiment Classification)：判斷文本所表達的情感，例如「正面」「中立」「負面」。\n",
    "\n",
    "- 主題/類別分類 (Topic/Categorical Classification)：將文本歸到特定類別，例如「新聞 → 體育 / 政治 / 財經」。\n",
    "\n",
    "## 工作原理:\n",
    "\n",
    "分類模型通常透過 監督學習 (Supervised Learning) 訓練而成。\n",
    "\n",
    "- 數據與標籤：我們有輸入數據（例如一段文字）和對應標籤（例如「正面」）。\n",
    "\n",
    "- 學習過程：模型反覆從「題目—答案」對中學習，逐漸掌握輸入與輸出之間的規律。\n",
    "\n",
    "這更像是學會「抓模式」而不是「死背答案」。\n",
    "\n",
    "## 挑戰\n",
    "\n",
    "監督學習分類的實際挑戰包括：\n",
    "\n",
    "- 耗時：資料標註需要大量人工投入。\n",
    "\n",
    "- 不一致：不同標註者可能對相同數據有不同理解。\n",
    "\n",
    "- 昂貴：大規模數據收集與標註成本高。\n",
    "\n",
    "- 資源密集：模型訓練往往需要強大硬體或雲端服務（AWS、Azure 等）。\n",
    "\n",
    "- 運營成本：模型在雲端持續運行也需要付出高額費用。\n",
    "\n",
    "## 大型語言模型 (LLM) 的應用\n",
    "\n",
    "近年的 大型語言模型（LLM，如 GPT-3、GPT-4） 提供了新的分類方式：\n",
    "\n",
    "- 少樣本 / 零樣本分類：只需少量範例，甚至僅靠提示，就能完成分類任務。\n",
    "\n",
    "- 降低標註需求：不必建立龐大的人工標註資料集。\n",
    "\n",
    "- 可微調：仍可透過微調 (fine-tuning) 針對特定分類任務強化效果。\n",
    "\n",
    "這使得分類不再完全依賴傳統的監督學習流程，降低了成本與時間。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de87a279-4a17-4717-b28e-ff5e33620d00",
   "metadata": {},
   "source": [
    "## 0-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867fe6b6-3106-4e23-8402-31f47a463ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(url=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb157d0e9-d18e-4835-a601-edeb011f0ee6_721x247.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec345b68-b69a-4b88-8147-879fd2e59f00",
   "metadata": {},
   "source": [
    "## 飛安事故原因分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a6e3e2-ba20-4906-81f3-676cdc717079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(os.path.join(get_project_dir(), 'tutorial', 'LLM+Langchain', 'Week-3', 'Data sample.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa789d2-2421-4715-95a7-c7bd9ddd1d09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73f9adc-13ce-4c59-bf07-29095ec514ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef841a6-2cf9-4fe6-8ae5-bb8a54c21b1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(filename= \"tutorial/LLM+Langchain/Week-3/HFACS_Org_Inf.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e09af75-31ad-4fb3-913f-b02fcedfe26d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_template = dedent(\"\"\"\n",
    "                  You are an AI assistant assigned with a task of safety report \n",
    "                  classification based on the content. You are a seasoned \n",
    "                  flight safety inspector with deep and extensive knowledge of \n",
    "                  aviation safty. \n",
    "    \n",
    "                  You always do the best work you can. You are highly \n",
    "                  analytical and pay close attention to details. \n",
    "    \n",
    "                  The candidates of the output are:\n",
    "\n",
    "                  - `Organizational Influence;Resource Management`\n",
    "                  - `Organizational Influence;Organizational Climate`\n",
    "                  - `Organizational Influence;Organizational Process`\n",
    "                  - `Unsafe Supervisions;Inadequate Supervision`\n",
    "                  - `Unsafe Supervisions;Planned Inappropriate Operations`\n",
    "                  - `Unsafe Supervisions;Failed to Correct Problem`\n",
    "                  - `Unsafe Supervisions;Supervisory Violation`\n",
    "                  - `Precondition for Unsafe Acts;Environmental Factors;Physical Environment`\n",
    "                  - `Precondition for Unsafe Acts;Environmental Factors;Technological Environment`\n",
    "                  - `Precondition for Unsafe Acts;Condition of Operators;Adverse Mental State`\n",
    "                  - `Precondition for Unsafe Acts;Condition of Operators;Adverse Physiological State`\n",
    "                  - `Precondition for Unsafe Acts;Condition of Operators;Physical/Mental Limitations`\n",
    "                  - `Precondition for Unsafe Acts;Personnel Factors;Crew Resource Management`\n",
    "                  - `Precondition for Unsafe Acts;Personnel Factors;Personal Readiness`\n",
    "                  - `Unsafe Acts;Errors;Decision Errors`\n",
    "                  - `Unsafe Acts;Errors;Skill-Based Errors`\n",
    "                  - `Unsafe Acts;Errors;Perceptual Errors`\n",
    "                  - `Unsafe Acts;Violations;Routine`\n",
    "                  - `Unsafe Acts;Violations;Exceptional`\n",
    "            \n",
    "                 The output is from one of the candidates. \n",
    "                 \"\"\")\n",
    "\n",
    "human_template = \"{report}\"\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variables\": [\"report\"]}}\n",
    "\n",
    "pipeline_ = build_pipeline(model=model, inputs=input_, parser=StrOutputParser())\n",
    "\n",
    "# chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "# pipeline_ = chat_prompt_template | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79bcc3c-4baa-4aad-a0c3-403733c0f027",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = df.iloc[3]['Report 1']\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1267429-92f8-468d-90fa-7fa990bba8ba",
   "metadata": {},
   "source": [
    "- 可以想一下，當一天有上百份這種報告的時候，你想要自己閱讀報告得出結論或是將這件工作外包給機器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1701a49-6d3c-47a1-acde-40c7e4656832",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pipeline_.invoke({\"report\": text})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409e957e-f9b9-44b7-afdb-b916e82d79bc",
   "metadata": {},
   "source": [
    "### 使用parser精煉結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deb1254-889c-4133-873e-30f4fef41881",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_schemas = [\n",
    "        ResponseSchema(name=\"category\", \n",
    "                       description=dedent(\"\"\"\n",
    "                                   The predicted category of the classification\n",
    "                                   \"\"\"))]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "system_template = dedent(\"\"\"\n",
    "                  You are an AI assistant assigned with a task of safety report \n",
    "                  classification based on the content. You are a seasoned \n",
    "                  flight safety inspector with deep and extensive knowledge of \n",
    "                  aviation safty. \n",
    "    \n",
    "                  You always do the best work you can. You are highly \n",
    "                  analytical and pay close attention to details. \n",
    "    \n",
    "                  The candidates of the output are:\n",
    "\n",
    "                  - `Organizational Influence;Resource Management`\n",
    "                  - `Organizational Influence;Organizational Climate`\n",
    "                  - `Organizational Influence;Organizational Process`\n",
    "                  - `Unsafe Supervisions;Inadequate Supervision`\n",
    "                  - `Unsafe Supervisions;Planned Inappropriate Operations`\n",
    "                  - `Unsafe Supervisions;Failed to Correct Problem`\n",
    "                  - `Unsafe Supervisions;Supervisory Violation`\n",
    "                  - `Precondition for Unsafe Acts;Environmental Factors;Physical Environment`\n",
    "                  - `Precondition for Unsafe Acts;Environmental Factors;Technological Environment`\n",
    "                  - `Precondition for Unsafe Acts;Condition of Operators;Adverse Mental State`\n",
    "                  - `Precondition for Unsafe Acts;Condition of Operators;Adverse Physiological State`\n",
    "                  - `Precondition for Unsafe Acts;Condition of Operators;Physical/Mental Limitations`\n",
    "                  - `Precondition for Unsafe Acts;Personnel Factors;Crew Resource Management`\n",
    "                  - `Precondition for Unsafe Acts;Personnel Factors;Personal Readiness`\n",
    "                  - `Unsafe Acts;Errors;Decision Errors`\n",
    "                  - `Unsafe Acts;Errors;Skill-Based Errors`\n",
    "                  - `Unsafe Acts;Errors;Perceptual Errors`\n",
    "                  - `Unsafe Acts;Violations;Routine`\n",
    "                  - `Unsafe Acts;Violations;Exceptional`\n",
    "            \n",
    "                 The output is from one of the candidates. \n",
    "                 \"\"\")\n",
    "\n",
    "human_template = \"\"\"\n",
    "                 {report}; \n",
    "                 format instruction: {format_instructions}\n",
    "                 \"\"\"\n",
    "\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variables\": [\"report\"],\n",
    "                    \"partial_variables\": {'format_instructions': format_instructions}}}\n",
    "\n",
    "pipeline_ = build_pipeline(model=model, inputs=input_, parser=StrOutputParser())\n",
    "\n",
    "# chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "# pipeline_ = chat_prompt_template|model|output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1da940-87d9-4d77-981a-82e597781d6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = pipeline_.invoke({\"report\": text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2255ea-0e55-4fa9-95d7-a204cbc68942",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87766fd5-25dc-4b82-ad33-e716878cde7a",
   "metadata": {},
   "source": [
    "## 詩詞分類\n",
    "\n",
    "把唐詩宋詞拿出來回鍋利用\n",
    "\n",
    "有沒有一種高中時該好好學習的感覺?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c8b8bb-d1a3-4fcb-8dec-afbd093d59cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read file\n",
    "filename = os.path.join(\"tutorial\", \"LLM+Langchain\", \"Week-1\", \"唐詩三百首.txt\")\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "poems = []\n",
    "\n",
    "# Split by blank lines\n",
    "blocks = [b.strip() for b in text.strip().split(\"\\n\\n\") if b.strip()]\n",
    "\n",
    "for block in blocks:\n",
    "    entry = {}\n",
    "    for line in block.split(\"\\n\"):\n",
    "        if line.startswith(\"詩名:\"):\n",
    "            entry[\"詩名\"] = line.replace(\"詩名:\", \"\").strip()\n",
    "        elif line.startswith(\"作者:\"):\n",
    "            entry[\"作者\"] = line.replace(\"作者:\", \"\").strip()\n",
    "        elif line.startswith(\"詩體:\"):\n",
    "            entry[\"詩體\"] = line.replace(\"詩體:\", \"\").strip()\n",
    "        elif line.startswith(\"詩文:\"):\n",
    "            entry[\"詩文\"] = line.replace(\"詩文:\", \"\").strip()\n",
    "    if len(entry) != 0:\n",
    "        poems.append(entry)\n",
    "\n",
    "df_poem = pd.DataFrame(poems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247edae4-1a74-4cc4-bc13-2f215f2b1ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Output(BaseModel):\n",
    "\n",
    "    name: Literal['五言古詩', '七言古詩', '七言律詩', \n",
    "                  '五言絕句', '樂府', '七言絕句', '五言律詩'] = Field(description=\"唐詩詩體\")\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=requirements)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "\n",
    "system_template = dedent(\"\"\"\n",
    "                  You are a help AI assistant specialized at Chinese literature, especially the 唐詩。\n",
    "                  You are assigned with a task of classify the given poem.\n",
    "                  The outcomes will be from one of these candidates:\n",
    "\n",
    "                  - 五言古詩\n",
    "                  - 七言古詩\n",
    "                  - 七言律詩\n",
    "                  - 五言絕句\n",
    "                  - 樂府\n",
    "                  - 七言絕句\n",
    "                  - 五言律詩\n",
    "                  \"\"\")\n",
    "\n",
    "human_template = dedent(\"\"\"\n",
    "                 poem: {query}\n",
    "                 format instruction: {format_instructions}\n",
    "                 \"\"\")\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variables\": [\"query\"],\n",
    "                    \"partial_variables\": {\"format_instructions\": \n",
    "                                          format_instructions}}}\n",
    "\n",
    "classification_pipeline = build_pipeline(model=model, inputs=input_, parser=output_parser)\n",
    "\n",
    "# classification_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "# classification_pipeline = classification_prompt_template|model|output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa31b9f-8e43-4d3d-a375-3997f71b6bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poem[df_poem['作者']=='李白']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da96698c-f44c-4816-8c3d-85ee913f1b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df_poem.loc[89]['詩文']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeba9f9-7d6b-42ad-9849-e79d7d536a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4812ff64-d428-46b7-b50d-fc82b62c333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_pipeline.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19345b91-2196-434f-94e8-8d40ca1d8627",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = dedent(\"\"\"\n",
    "趙客縵胡纓，吳鉤霜雪明；\n",
    "銀鞍照白馬，颯沓如流星。\n",
    "十步殺一人，千里不留行；\n",
    "事了拂衣去，深藏身與名。\n",
    "閑過信陵飲，脫劒膝前橫；\n",
    "將炙啖朱亥，持觴勸侯嬴。\n",
    "三杯吐然諾，五嶽倒爲輕；\n",
    "眼花耳熱後，意氣素霓生。\n",
    "救趙揮金槌，邯鄲先震驚；\n",
    "千秋二壯士，烜赫大梁城。\n",
    "縱死俠骨香，不慙世上英；\n",
    "誰能書閤下，白首太玄經。\n",
    "\"\"\")\n",
    "\n",
    "classification_pipeline.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41579e25-86e1-41ed-a5fe-8c940fa8a58d",
   "metadata": {},
   "source": [
    "練習：請修改 RAG 模型，讓它能模仿李白詩風回答現代問題。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc370e0-11c5-4f1f-8c55-dc6fe0c219cd",
   "metadata": {},
   "source": [
    "### 挑戰宋詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e2418d-7265-48c7-bf9f-52d690a78ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file\n",
    "filename = os.path.join(\"tutorial\", \"LLM+Langchain\", \"Week-1\", \"宋詞三百首.txt\")\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "poems = []\n",
    "\n",
    "# Split by blank lines\n",
    "blocks = [b.strip() for b in text.strip().split(\"\\n\\n\") if b.strip()]\n",
    "\n",
    "for block in blocks:\n",
    "    entry = {}\n",
    "    for line in block.split(\"\\n\"):\n",
    "        if line.startswith(\"詞牌:\"):\n",
    "            entry[\"詞牌\"] = line.replace(\"詞牌:\", \"\").strip()\n",
    "        elif line.startswith(\"作者:\"):\n",
    "            entry[\"作者\"] = line.replace(\"作者:\", \"\").strip()\n",
    "        elif line.startswith(\"詞文:\"):\n",
    "            entry[\"詞文\"] = line.replace(\"詞文:\", \"\").strip()\n",
    "    if len(entry) != 0:\n",
    "        poems.append(entry)\n",
    "\n",
    "df_poem = pd.DataFrame(data=poems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26254fd-ae79-49e9-a5ae-e1a18920476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = df_poem['詞牌'].dropna().unique().tolist()\n",
    "\n",
    "# Generate a Literal definition\n",
    "literal_def = f\"Literal[{', '.join(repr(v) for v in values)}]\"\n",
    "\n",
    "eval(literal_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245d1aca-f732-47c6-9ada-a8693badbb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Output(BaseModel):\n",
    "\n",
    "    name: eval(literal_def) = Field(description=\"宋詞詞牌\")\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=requirements)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "\n",
    "classes = '\\n'.join(v for v in values)\n",
    "\n",
    "\n",
    "system_template = dedent(f\"\"\"\n",
    "                  You are a help AI assistant specialized at Chinese literature, especially the 宋詞。\n",
    "                  You are assigned with a task of classify the given poem.\n",
    "                  The outcomes will be from one of these candidates:\n",
    "\n",
    "                  {classes}\n",
    "                  \"\"\")\n",
    "\n",
    "human_template = dedent(\"\"\"\n",
    "                 poem: {query}\n",
    "                 format instruction: {format_instructions}\n",
    "                 \"\"\")\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variables\": [\"query\"],\n",
    "                    \"partial_variables\": {\"format_instructions\": \n",
    "                                          format_instructions}}}\n",
    "\n",
    "classification_pipeline = build_pipeline(model=model, inputs=input_, parser=output_parser)\n",
    "\n",
    "# classification_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "# classification_pipeline = classification_prompt_template|model|output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3cd656-9e85-4521-b6a4-9d41ab37a71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_pipeline.invoke({\"query\": df_poem.loc[0]['詞文']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3462e2da-f29c-4919-bec8-e395e2de711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poem.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d98c96b-2df7-4a8c-a785-3c23c8e9514d",
   "metadata": {},
   "source": [
    "詞牌檢測似乎很困難，可能需要依賴一些專業知識來進行特徵抽取。\n",
    "猜測原因可能是詞牌對於平仄的格式有強烈的要求，而中文是表意文字，在LLM模型訓練的時候，聲音並不會被記錄。\n",
    "所以詞牌偵測可能需要先將內容全部轉換為平仄，然後使用BLEU score計算和詞牌的相似性進行預測。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aee5c0b-7804-49bf-bd77-ff8203de6209",
   "metadata": {},
   "source": [
    "## HR: Job-Applicant Matching\n",
    "\n",
    "- 效率和速度： LLMs 能夠快速處理和分析大量申請，相較於人工審查，顯著縮短初步篩選所需的時間。\n",
    "\n",
    "- 一致性和公平性： LLMs 對所有申請應用相同的標準，最小化人為偏見，確保初步篩選過程的公平性。\n",
    "\n",
    "- 詳細分析： LLMs 能夠分析複雜的語言模式，從簡歷、求職信和其他申請材料中提取相關信息，識別符合工作要求的關鍵技能和資格。\n",
    "\n",
    "- 自定義和靈活性： LLMs 可以根據具體的工作要求自定義優先考慮的技能和經驗，允許更有針對性的篩選過程。\n",
    "\n",
    "- 可擴展性： LLMs 能夠同時處理大量申請，非常適合接收大量申請人的組織。\n",
    "\n",
    "- 成本效益： 通過自動化申請篩選的初始階段，LLMs 可以減少對大量人力資源的需求，從而降低運營成本。\n",
    "\n",
    "- 持續改進： LLMs 可以根據反饋和新數據持續進行訓練和改進，隨著時間的推移提高其準確性和有效性。-\n",
    "\n",
    "- 提升候選人經驗： 更快的回應時間和更一致的評估可以改善整體候選人經驗，因為申請人更有可能及時收到反饋。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39b5024-206c-4bdf-9574-8753f01ca46f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(\"https://www.techjobasia.com/zh-Hant/jobs/GMMlhU0qSayr6ZwTB0U6zA---Software-Engineer-(ReactJS)\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46edd4df-89c2-49ca-9e19-d6ae0eb6adb7",
   "metadata": {},
   "source": [
    "###  1. 發送 GET 請求到指定的 URL\n",
    "\n",
    "- 這行程式碼向指定的 URL 發送 HTTP GET 請求，並將響應儲存在 response 變數中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a779c48-badd-423e-889c-a57ece76a957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "\n",
    "url = \"https://www.techjobasia.com/zh-Hant/jobs/GMMlhU0qSayr6ZwTB0U6zA---Software-Engineer-(ReactJS)\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e681fd9e-5ba2-4c75-8bc3-4d41d747aaa4",
   "metadata": {},
   "source": [
    "### 2. 獲取響應的內容\n",
    "\n",
    "- 這行程式碼將響應的內容作為文字字串提取，並將其儲存在 html_content 變數中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7947d4-5e33-4b1c-88b5-ba375e62b2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee3cd12-f11a-4a7b-8f7e-ab8b16ada802",
   "metadata": {},
   "source": [
    "### 3. 解析 HTML 內容\n",
    "\n",
    "- 這行程式碼使用 BeautifulSoup 解析儲存在 html_content 中的 HTML 內容，並創建一個名為 soup 的 BeautifulSoup 對象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583fc8ac-d92b-4762-afd0-e1d1de8c3e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa763c1-d8c7-4c82-9528-0ef7ea564112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046accba-5203-43df-b1c8-b5703601d3b3",
   "metadata": {},
   "source": [
    "### 4. 移除所有 CSS 樣式標籤\n",
    "\n",
    "- 這個循環找到解析後的 HTML 內容中的所有 <style> 標籤，並使用 decompose() 方法將它們移除。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ab74e8-4e7c-4a92-b5ea-af3db43d1082",
   "metadata": {},
   "outputs": [],
   "source": [
    "for style in soup.find_all('style'):\n",
    "    style.decompose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5edde0-9fb5-43d4-bfb9-6a6b2c146d7e",
   "metadata": {},
   "source": [
    "### 6. 提取並打印僅包含文字的內容\n",
    "\n",
    "- 這行程式碼從解析後的 HTML 中提取文字內容，使用換行符將元素分隔，並將其儲存在 text_content 變數中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c797f3bc-33c2-4d67-88e2-b110f0d2da83",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_content = soup.get_text(separator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba34a7e-4ebd-4ea7-b738-54995862f082",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cacc52-b827-47d3-8f05-399360212b76",
   "metadata": {},
   "source": [
    "### 7. 清理文字內容\n",
    "\n",
    "- 這行程式碼通過移除每行的首尾空白並丟棄空行來清理提取的文字內容。清理後的文字儲存在 cleaned_text 變數中。\n",
    "- 將清理後的文字內容打印到控制台。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6ff06a-d842-465d-b4f4-3e7282824e15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_text = '\\n'.join(line.strip() for line in text_content.splitlines() if line.strip())\n",
    "\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabae79b-382d-4626-a526-af3e781ab3ca",
   "metadata": {},
   "source": [
    "### 8. 使用LLM提取工作相關訊息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a31c4e7-395e-4770-a21f-8f21aee07e28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template= dedent(\"\"\"\n",
    "          Extract the job description part of the text: {content} \n",
    "          \"\"\")\n",
    "\n",
    "human_prompt = PromptTemplate(template=template)\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "    \n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([human_message])\n",
    "\n",
    "pipeline_ = chat_prompt_template|model|StrOutputParser()\n",
    "\n",
    "job_description = pipeline_.invoke({\"content\": cleaned_text})\n",
    "\n",
    "print(job_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64fc577-f6fc-4e75-bf5c-e7f0b1b1d14a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 9. 將上述步驟打包成函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825038f7-7d58-4eb6-ad24-a41c14e6a322",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain, Runnable\n",
    "\n",
    "def parsing_process(url):\n",
    "    \"\"\"\n",
    "    Fetches and extracts text content from a given URL.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL of the web page to fetch and parse.\n",
    "\n",
    "    Returns:\n",
    "    str: Cleaned text content extracted from the web page.\n",
    "\n",
    "    Raises:\n",
    "    requests.exceptions.RequestException: If an error occurs while fetching the URL.\n",
    "\n",
    "    Notes:\n",
    "    - This function sends a GET request to the specified URL.\n",
    "    - It uses BeautifulSoup to parse the HTML content of the response.\n",
    "    - Any <style> tags in the HTML are removed to extract only textual content.\n",
    "    - The extracted text is cleaned by removing extra whitespace and empty lines.\n",
    "    \"\"\"\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Get the content of the response\n",
    "    html_content = response.text\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    for style in soup.find_all('style'):\n",
    "        style.decompose()\n",
    "\n",
    "    # Extract and print only the text content\n",
    "    text_content = soup.get_text(separator='\\n')\n",
    "\n",
    "    # Clean up the text (optional)\n",
    "    cleaned_text = '\\n'.join(line.strip() for line in text_content.splitlines() if line.strip())\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ceff2d-17e0-4240-bdd1-db3b07636429",
   "metadata": {},
   "source": [
    "拿一個1111來試試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f8b0ae-efbc-44b2-943b-f766a6e7ea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.1111.com.tw/job/131976692/\"\n",
    "cleaned_text = parsing_process(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dda6dfb-965c-4c99-8846-a5a9721feda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipeline_.invoke({\"content\": cleaned_text}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d184f4-d1aa-40ce-a619-1ca1230c5738",
   "metadata": {},
   "source": [
    "這個過程中牽涉到從網路上讀取數據，本地數據處理等等。適合用異步流來進行加速:\n",
    "\n",
    "目前程式是 同步阻塞 的：\n",
    "\n",
    "- requests.get(url) → 會阻塞直到網路請求完成。\n",
    "\n",
    "- BeautifulSoup 的解析則是 CPU 本地操作，速度通常不是瓶頸。\n",
    "\n",
    "所以：\n",
    "\n",
    "如果你一次只抓單一 URL，沒必要用 async，因為主要瓶頸就是等待那一次請求。\n",
    "\n",
    "如果要抓 多個 URL，那麼改用 非同步（async/await + aiohttp） 或 多執行緒 / 多處理 才能顯著加速，因為你能同時發送多個請求，避免 I/O 等待造成浪費。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e90873c-749e-48da-bbf7-23314cae33e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "@chain\n",
    "async def async_parsing_process(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches and extracts cleaned text content from a given URL asynchronously.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL of the web page to fetch and parse.\n",
    "\n",
    "    Returns:\n",
    "    str: Cleaned text content extracted from the web page.\n",
    "\n",
    "    Raises:\n",
    "    aiohttp.ClientError: If an error occurs while fetching the URL.\n",
    "    \"\"\"\n",
    "\n",
    "    # request 不支援 asynchronization\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(url) as response:\n",
    "            html_content = await response.text()\n",
    "            soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "            # 移除 style 和 script\n",
    "            for tag in soup([\"style\", \"script\"]):\n",
    "                tag.decompose()\n",
    "\n",
    "            # 提取文字\n",
    "            text_content = soup.get_text(separator=\"\\n\")\n",
    "\n",
    "            # 清理空白與空行\n",
    "            cleaned_text = \"\\n\".join(\n",
    "                line.strip() for line in text_content.splitlines() if line.strip()\n",
    "            )\n",
    "\n",
    "            return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade04f01-7a4c-46e6-9e2d-b892f4058c40",
   "metadata": {},
   "source": [
    "若是我們使用LCEL，pipeline 就是這個樣子:\n",
    "\n",
    "1. 資料提取與清洗\n",
    "2. 使用LLM進行最後的數據提煉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab11d67e-3993-4ccb-86eb-6e6e3831f0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description_pipeline = RunnablePassthrough.assign(content=itemgetter(\"url\")|async_parsing_process)|chat_prompt_template|model|StrOutputParser()\n",
    "\n",
    "job_result = await job_description_pipeline.ainvoke({\"url\": url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695babfd-ee0a-49cd-9082-0cd6c1ad8386",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(job_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0679586-30a0-4384-84aa-15642bd19559",
   "metadata": {},
   "source": [
    "## 履歷提取\n",
    "\n",
    "- 網路上公開資料 (把工作提取的Prompt 稍微修一下就行，網路抓數據的流程是一樣的)\n",
    "- Word\n",
    "- PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c682487c-dca0-4368-ab9b-d9e95bb4e767",
   "metadata": {},
   "source": [
    "### 提取 Word 內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d1ff36-bda0-49ca-85f9-6bdc41d8f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b927ef-5df2-4e9d-8e92-ff227cdc47fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.oxml.table import CT_Tbl\n",
    "from docx.oxml.text.paragraph import CT_P\n",
    "from docx.table import Table\n",
    "from docx.text.paragraph import Paragraph\n",
    "\n",
    "def iter_block_items(parent):\n",
    "    \"\"\"\n",
    "    Yield each paragraph and table in document order.\n",
    "    \"\"\"\n",
    "    for child in parent.element.body:\n",
    "        if isinstance(child, CT_P):\n",
    "            yield Paragraph(child, parent)\n",
    "        elif isinstance(child, CT_Tbl):\n",
    "            yield Table(child, parent)\n",
    "\n",
    "# Load the Word document\n",
    "doc = Document(os.path.join(\"tutorial\", \"LLM+Langchain\", \"Week-3\", \"Resume.docx\"))\n",
    "\n",
    "content = []\n",
    "\n",
    "for block in iter_block_items(doc):\n",
    "    if isinstance(block, Paragraph):\n",
    "        text = block.text.strip()\n",
    "        if text:  # skip empty paragraphs\n",
    "            content.append(text)\n",
    "    elif isinstance(block, Table):\n",
    "        for row in block.rows:\n",
    "            row_data = [cell.text.strip() for cell in row.cells]\n",
    "            content.append(\"\\t\".join(row_data))\n",
    "\n",
    "# Combine everything in order\n",
    "full_text = \"\\n\".join(content)\n",
    "\n",
    "print(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60969101-e529-4f8a-a6b5-d38fef4bee66",
   "metadata": {},
   "source": [
    "### 打包成異步流Langchain Runnable\n",
    "\n",
    "python-docx 本身是一個 同步阻塞的函式庫 —— 它在讀檔、解析 .docx 時會佔用 Python 主執行緒。\n",
    "\n",
    "在 async/await 的應用場景中（例如 LangChain agent、async pipeline、Web server），如果直接呼叫同步的 python-docx，會讓 事件迴圈被卡住，其他 async 任務無法並行進行。\n",
    "\n",
    "await asyncio.to_thread(func, *args)\n",
    "\n",
    "把這個 同步阻塞的工作丟到背景 thread pool 執行，從而讓事件迴圈保持「非阻塞」。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ec6413-9c55-461e-9a5c-41d45fa5beea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因為python-docx是同步流，所以我們需要用asyncio.to_thread將其重新包裝成異步流\n",
    "import asyncio\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "class DocxExtractor(Runnable):\n",
    "    \"\"\"LangChain Runnable that extracts text (paragraphs + tables) from a Word file.\"\"\"\n",
    "\n",
    "    async def ainvoke(self, filename: str, config: Dict[str, Any] | None = None) -> str:\n",
    "        \"\"\"\n",
    "        python-docx 是同步阻塞的函式庫，若直接在 async 環境呼叫會卡住事件迴圈，因此需用 asyncio.to_thread 將其包裝，\n",
    "        讓阻塞操作在背景執行緒執行，避免阻塞其他非同步任務。\n",
    "        \"\"\"\n",
    "        \n",
    "        return await asyncio.to_thread(self._extract, filename)\n",
    "\n",
    "    def invoke(self, filename: str, config=None) -> str:\n",
    "        # synchronous version directly calls the sync helper\n",
    "        return self._extract(filename)\n",
    "    \n",
    "    def _extract(self, filename: str) -> str:\n",
    "        doc = Document(filename)\n",
    "        content = []\n",
    "\n",
    "        for block in iter_block_items(doc):\n",
    "            if isinstance(block, Paragraph):\n",
    "                text = block.text.strip()\n",
    "                if text:\n",
    "                    content.append(text)\n",
    "            elif isinstance(block, Table):\n",
    "                for row in block.rows:\n",
    "                    row_data = [cell.text.strip() for cell in row.cells]\n",
    "                    content.append(\"\\t\".join(row_data))\n",
    "\n",
    "        return \"\\n\".join(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb64b7e-a80c-4bd8-8977-26b00e3538a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = DocxExtractor()\n",
    "result = await extractor.ainvoke(os.path.join(\"tutorial\", \"LLM+Langchain\", \"Week-3\", \"Resume.docx\"))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbc4074-7e4d-48be-bd9e-6db83b4010d1",
   "metadata": {},
   "source": [
    "這個 Word 內容提取是通用型的，不限於簡歷，也可以提取公文或其他文件文字內容。它完全基於程式邏輯，不涉及大語言模型，所以不需要額外付費。並非所有應用都需要大語言模型，有時候你只是需要一台 50cc 小型代步車，沒必要自己去打造一台 F1 賽車。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd3d7fb-c61f-4499-aa28-bb9b3db7a5e3",
   "metadata": {},
   "source": [
    "### 提取 PDF 內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13e82f4-c89b-4b6e-8c5d-85c3739dcb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "def extract_pdf(filename: str) -> str:\n",
    "    reader = PdfReader(filename)\n",
    "    content = []\n",
    "    for page in reader.pages:\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            content.append(text.strip())\n",
    "    return \"\\n\".join(content)\n",
    "\n",
    "# Example\n",
    "print(extract_pdf(os.path.join(\"tutorial\", \"LLM+Langchain\", \"Week-3\", \"Resume.pdf\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0cd678-1737-4d22-946d-c5dbb68b2240",
   "metadata": {},
   "source": [
    "異步流版本:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1448d256-52df-42f3-ae5c-ed21666b6f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PdfExtractor(Runnable):\n",
    "    \"\"\"LangChain Runnable that extracts text from a PDF file.\"\"\"\n",
    "\n",
    "    async def ainvoke(self, filename: str, config: Dict[str, Any] | None = None) -> str:\n",
    "        return await asyncio.to_thread(self._extract, filename)\n",
    "\n",
    "    def invoke(self, filename: str, config: Dict[str, Any] | None = None) -> str:\n",
    "        return self._extract(filename)\n",
    "\n",
    "    def _extract(self, filename: str) -> str:\n",
    "        reader = PdfReader(filename)\n",
    "        content = []\n",
    "        for page in reader.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                content.append(text.strip())\n",
    "        return \"\\n\".join(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bcd5d5-639e-4191-9fab-fb58b553f221",
   "metadata": {},
   "source": [
    "直接使用根據 Docx 提取的內容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d110ac4d-f835-4c4d-b544-02070c2dc2bd",
   "metadata": {},
   "source": [
    "技術上確實可以直接在讀取檔案的同時進行匹配，但在系統設計上，將「檔案內容提取」獨立成工作流會更恰當，因為這樣更有利於維護、監控與後續擴展，再將提取結果交由下游流程進行匹配或其他處理。\n",
    "\n",
    "1. 技術上可行：\n",
    "    確實可以「on-the-fly」直接讀取檔案，然後立刻做比對或匹配，這在 demo 或小規模應用時沒問題。\n",
    "\n",
    "2. 務實上的考量：\n",
    "    在實際系統設計中，把「檔案讀取 / 解析」獨立成一個模組或工作流會更好：\n",
    "\n",
    "    - 維護性：解析邏輯獨立，容易替換不同檔案類型（Word、PDF、Excel…）。\n",
    "\n",
    "    - 可監控：可以針對「解析失敗」做監控和錯誤處理，不會和匹配邏輯糾纏在一起。\n",
    "\n",
    "    - 可擴展：之後不只做匹配，還可能做索引、摘要、分類等，提前抽離流程更有彈性。\n",
    "\n",
    "3. 流程建議：\n",
    "\n",
    "    Step 1：檔案 → 讀取 & 提取文字（抽象成 Extractor 工作流）。\n",
    "\n",
    "    Step 2：抽取內容 → 匹配 / NLP 處理 / 下游任務。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd216bc4-0189-468d-9943-3714f6f06241",
   "metadata": {},
   "source": [
    "## 內容精煉，輸出內容一致化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7a14c9-84d9-4a67-be3f-feefe74244b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "system_template = dedent(\"\"\"\n",
    "                  I am going to give you a template for your output. \n",
    "                  CAPITALIZED WORDS are my placeholders. \n",
    "                  Fill in my placeholders with your output. Please preserve \n",
    "                  the overall formatting of my template. My template is:\n",
    "\n",
    "                  *** Working Experience:*** WORKING EXPERIENCE \n",
    "                  *** Education:*** EDUCATION\n",
    "                  *** Skills:*** SKILLS\n",
    "\n",
    "                  I will give you the data to format in the next prompt. \n",
    "                  Create a resume using my template.\n",
    "                  \"\"\")\n",
    "\n",
    "human_template = \"\"\"\n",
    "                 {query}\n",
    "                 \"\"\"\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variables\": [\"query\"]}}\n",
    "\n",
    "resume_pipeline = build_pipeline(model=model, inputs=input_, parser=StrOutputParser())\n",
    "\n",
    "# resume_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "# resume_pipeline = resume_prompt_template|model|StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81388d28-d18a-4750-8151-4cb394ad71e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resume_output = resume_pipeline.invoke(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77226af-b87d-4803-b694-02aa55539298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(resume_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6823b73a-8c52-45e6-8dfd-901732a25767",
   "metadata": {},
   "source": [
    "## 履歷和工作的匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2b73ec-d204-47b0-9d31-f103b609ba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Output(BaseModel):\n",
    "\n",
    "    result: Literal['Yes', 'No'] = Field(description=\"If the candidate is a good fit, either Yes or No\")\n",
    "    reason: str = Field(description=\"Applicant - Job matching\")\n",
    "    \n",
    "output_parser = PydanticOutputParser(pydantic_object=Output)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1390f7e-9f85-4a71-9860-f017b10ce879",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_template = dedent(\"\"\"\n",
    "                  You are an AI assistant acting as an experienced senior \n",
    "                  recruiter in IT field.\n",
    "                  \n",
    "                  You are assigned a task of identifying if an applicant, \n",
    "                  based on the description in the resume, is a good match to the described job. \n",
    "                    \n",
    "                  You always do the best work you can. You are highly \n",
    "                  analytical and pay close attention to details. \n",
    "                  \"\"\")\n",
    "\n",
    "human_template = dedent(\"\"\"\n",
    "                 Job description: {job}\n",
    "                 resume: {resume}\n",
    "                 output format instructions: {format_instructions}\n",
    "                 \"\"\")\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variables\": [\"job\", \"resume\"],\n",
    "                    \"partial_variables\": {'format_instructions': format_instructions}}}\n",
    "\n",
    "match_prompt_template = build_standard_chat_prompt_template(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40117f4a-6040-49d8-8f1e-6c747de37e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c952973-2f02-49d8-a807-4a16dbd692f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee8e78e-49b7-481b-9a26-f1f9ddcdb83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_pipeline = build_pipeline(model=model, inputs=input_, parser=output_parser)\n",
    "\n",
    "# matching_pipeline = match_prompt_template|model|output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2a6215-3e77-4736-9f4c-54dcc9a6c130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "matching_output = matching_pipeline.invoke({\"job\":job_result, \"resume\":result})\n",
    "\n",
    "print(matching_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09540966-abf5-49c2-8fd5-68bb512e8ed4",
   "metadata": {},
   "source": [
    "# 其他的大語言模型API提供商\n",
    "\n",
    "> 🎯 本章學完你將能學會什麼：\n",
    "> - 理解如何在 LangChain 中整合第三方大語言模型 API（如 Perplexity、DeepSeek）\n",
    "> - 學會設定與呼叫非 OpenAI 模型的 API（包含自訂 base_url、api_key）\n",
    "> - 熟悉 Perplexity 的 sonar-* 模型與 DeepSeek 的 deepseek-reasoner 模型在 LangChain 中的使用方式\n",
    "\n",
    "## Perplexity\n",
    "\n",
    "https://python.langchain.com/docs/integrations/chat/perplexity/\n",
    "\n",
    "- sonar-deep-research\n",
    "- sonar-reasoning-pro\n",
    "- sonar-reasoning\n",
    "- sonar-pro\n",
    "- sonar\t128k\n",
    "- r1-1776\n",
    "\n",
    "Langchain '似乎'支持Perplexity，但我在使用時發現會出問題，所以需要自己套殼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a29dd0-eb1a-4745-ad23-ab6b4c4b804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity 基本API使用\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are an AI assistant that focuses on equity market analysis and you need to \"\n",
    "            \"engage in an accurate, comprehensive, helpful and  polite conversation with a user.\"\n",
    "        ),\n",
    "    },\n",
    "    {  \n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"Find the SKU number of Carslan Lasting Cover Foundation N01\"\n",
    "        ),\n",
    "\n",
    "    },\n",
    "\n",
    "]\n",
    "\n",
    "client = OpenAI(api_key=os.environ['PERPLEXITY_API_KEY'], base_url=\"https://api.perplexity.ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ee1237-ce75-4c59-b323-8fca3c884baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "\n",
    "@chain\n",
    "def prompt_template_2_messages(chat_prompt):\n",
    "\n",
    "    output_messages = []\n",
    "     \n",
    "    _messages = chat_prompt.messages\n",
    "\n",
    "    for message in _messages:\n",
    "        if isinstance(message, SystemMessage):\n",
    "            output_messages.append({\"role\": \"system\", \"content\": message.content})\n",
    "        if isinstance(message, HumanMessage):\n",
    "            output_messages.append({\"role\": \"user\", \"content\": message.content})\n",
    "\n",
    "    return output_messages\n",
    "\n",
    "\n",
    "@chain\n",
    "def messages_2_perplexity(messages):\n",
    "\n",
    "    client = OpenAI(api_key=os.environ['PERPLEXITY_API_KEY'], base_url=\"https://api.perplexity.ai\")\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"sonar-deep-research\",\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message.content\n",
    "    citations = response.citations\n",
    "\n",
    "    return {\"content\": content,\n",
    "            \"citations\": citations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b20975d-e3de-4065-96fc-46d35100550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_perplexity = chat_prompt_perplexity|prompt_template_2_messages|messages_2_perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c418435c-349d-4e9b-b221-d7668007c34f",
   "metadata": {},
   "source": [
    "## DeepSeek\n",
    "\n",
    "- Langchain 支援 Deepseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcaea01-1e16-43bd-a074-f088c273c7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "deepseek_r1 = ChatDeepSeek(api_key=os.environ['DEEPSEEK_API_KEY'], temperature=0, model='deepseek-reasoner')\n",
    "\n",
    "system_template = \"You are a helpful assistant.\"\n",
    "human_template = \"Create a financial report of {ticker} based on:\\n {context}\"\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variables\": [\"context\", \"ticker\"]}\n",
    "         }\n",
    "\n",
    "chat_prompt_deepseek = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "pipeline_deepseek = chat_prompt_deepseek|deepseek_r1|output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bca2d1-d639-49f3-a874-45106ef1e8a5",
   "metadata": {},
   "source": [
    "# MLFlow Part 1\n",
    "\n",
    ">🎯 本章學完你將能學會什麼：\n",
    "> - 理解 MLflow 的核心概念（Experiment、Run、Model Registry、Artifact 等）\n",
    "> - 學會在本地端啟動並連接 MLflow Tracking Server\n",
    "> - 能夠使用 MLflow 追蹤與記錄機器學習模型（以 Logistic Regression + Iris dataset 為例）\n",
    "> - 掌握如何透過 MLflow 註冊、載入、標記（Tag）與別名（Alias）管理模型版本\n",
    "> - 熟悉 mlflow.sklearn 與 mlflow.pyfunc 的模型存取方式，並能模擬遠端載入流程\n",
    "> - 理解如何結合 LangChain 與 MLflow，將 LLMChain pipeline 進行版本化與追蹤\n",
    "> - 學會設定 Model Signature，明確定義模型的輸入輸出結構，提升模型可解釋性與可移植性\n",
    "> - 能夠將自訂 Python pipeline 以 MLflow 模型格式部署與重載運行\n",
    "\n",
    "重啟Notebook來節省硬體資源\n",
    "\n",
    "- pip install mlflow\n",
    "- 在CLI: mlflow server --host 127.0.0.1 --port 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7828b24-ef0e-4ad3-880d-100506ae3943",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2805f4-b256-4043-9f8e-09859ae64d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# Load the Iris dataset\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define the model hyperparameters\n",
    "params = {\n",
    "    \"solver\": \"lbfgs\",\n",
    "    \"max_iter\": 1000,\n",
    "    \"multi_class\": \"auto\",\n",
    "    \"random_state\": 8888,\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "lr = LogisticRegression(**params)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e269cfda-740c-40cb-8504-93134c7be3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# 通知mlflow要把紀錄送去哪裡\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n",
    "\n",
    "# Create a new MLflow Experiment\n",
    "mlflow.set_experiment(\"MLflow Quickstart\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"Week-3-MLFlow\") as run:\n",
    "\n",
    "    # 你需要這個來確保結果都會被記錄在同一個Run裡\n",
    "    run_id = run.info.run_id\n",
    "\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    # Log the loss metric\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "    # Infer the model signature\n",
    "    signature = infer_signature(X_train, lr.predict(X_train))\n",
    "\n",
    "    # Log the model, which inherits the parameters and metric\n",
    "    model_info = mlflow.sklearn.log_model(\n",
    "        sk_model=lr,\n",
    "        # model local name\n",
    "        name=\"iris_model\",\n",
    "        signature=signature,\n",
    "        input_example=X_train,\n",
    "        # model global name\n",
    "        registered_model_name=\"tracking-quickstart\",\n",
    "    )\n",
    "\n",
    "    # Set a tag that we can use to remind ourselves what this model was for\n",
    "    mlflow.set_logged_model_tags(\n",
    "        model_info.model_id, {\"Training Info\": \"Basic LR model for iris data\"}\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48f6fa6-d37f-4c5c-b5b4-87c5cbd2c51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info.model_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e1e550-78eb-4012-afc8-39dab70a9487",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f015683b-5035-4af0-9a8a-7a88fe0d2f10",
   "metadata": {},
   "source": [
    "## 本地執行\n",
    "\n",
    "1. mlflow.pyfunc.load_model(model_info.model_uri)\n",
    "\n",
    "- 這裡用的是 model_info.model_uri，它指向剛剛在 local run 裡 mlflow.sklearn.log_model(...) 存下來的 artifact 路徑。\n",
    "\n",
    "- 因為你在 同一個 local tracking server (或預設的本地檔案系統) 執行，所以可以直接載入。\n",
    "\n",
    "2. predictions = loaded_model.predict(X_test)\n",
    "\n",
    "- mlflow.pyfunc 會包裝成一個 通用 Python function 模型 (不管底層是 sklearn、pytorch、xgboost...)。\n",
    "\n",
    "- 所以你可以直接 .predict(...)，得到預測結果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b6d322-836a-4a45-a648-2e23ed0ad6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model back for predictions as a generic Python Function model\n",
    "\n",
    "loaded_model = mlflow.pyfunc.load_model(model_info.model_uri)\n",
    "\n",
    "predictions = loaded_model.predict(X_test)\n",
    "\n",
    "iris_feature_names = datasets.load_iris().feature_names\n",
    "\n",
    "result = pd.DataFrame(X_test, columns=iris_feature_names)\n",
    "result[\"actual_class\"] = y_test\n",
    "result[\"predicted_class\"] = predictions\n",
    "\n",
    "result[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cd8c06-2e45-486a-8434-e181073c0827",
   "metadata": {},
   "source": [
    "## 模擬遠端執行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5283b5-4db8-4930-af73-5e29d35cd5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# 列出特定模型的所有版本\n",
    "versions = client.search_model_versions(\"name='tracking-quickstart'\")\n",
    "\n",
    "for v in versions:\n",
    "    print(f\"Version: {v.version}, Stage: {v.current_stage}, Run ID: {v.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d026abbb-e2cd-4e47-8043-cb17b7b4b1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri = \"models:/tracking-quickstart/1\"    # version 1\n",
    "\n",
    "model_remote = mlflow.sklearn.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9aa4e6-cbe8-4230-9ebf-e44fdd4e08fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_remote.predict(X_test)\n",
    "\n",
    "iris_feature_names = datasets.load_iris().feature_names\n",
    "\n",
    "result = pd.DataFrame(X_test, columns=iris_feature_names)\n",
    "result[\"actual_class\"] = y_test\n",
    "result[\"predicted_class\"] = predictions\n",
    "\n",
    "result[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79427145-e95f-4af5-9a97-d7f23d91ee79",
   "metadata": {},
   "source": [
    "### 模型 Alias（別名）\n",
    "\n",
    "Alias 是一種可變（mutable）的命名引用，可指向某個註冊模型（registered model）的特定版本。這對於隨後更新部署模型卻不想改程式碼時非常方便。例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e35ea0-3344-494f-bd8d-742df38bf6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "client.set_registered_model_alias(\n",
    "    name=\"tracking-quickstart\",\n",
    "    alias=\"champion\",\n",
    "    version=\"1\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769a174f-1123-49c7-a258-f97514252655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以 alias 載入模型\n",
    "model = mlflow.sklearn.load_model(\"models:/tracking-quickstart@champion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c80fa78-04aa-4e94-94bc-52fafd0d96ed",
   "metadata": {},
   "source": [
    "也可以用 API 刪除 alias："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e52ced0-6b38-4cbb-aa39-69459213714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_registered_model_alias(\"tracking-quickstart\", \"champion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b057d9e0-bfa7-4b1f-8b5a-d2424a9fd2b5",
   "metadata": {},
   "source": [
    "### 模型 Tags（標籤）\n",
    "\n",
    "MLflow 支援兩層 Tag：\n",
    "\n",
    "- Registered model-level tags：整體模型的 metadata，例如用途、團隊等資訊。\n",
    "\n",
    "- Model version-level tags：針對每個版本做不同註記，例如驗證狀態、效能資訊等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c4c242-0268-48dd-a35b-e3dfba7cdedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registered model-level tag\n",
    "client.set_registered_model_tag(\"tracking-quickstart\", \"task\", \"classification\")\n",
    "\n",
    "# Version-level tag\n",
    "client.set_model_version_tag(\"tracking-quickstart\", version=\"1\", key=\"validation_status\", value=\"approved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ab0cb6-f3bd-4e5a-bc5d-c4dfb53f1c16",
   "metadata": {},
   "source": [
    "## Langchain with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbde9b54-4837-4edb-82a3-b23314373e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e1efb6-567f-4ae1-86ec-70b530ffd961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from langchain.chains import LLMChain ## Langchain boilerplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from src.initialization import credential_init\n",
    "credential_init()\n",
    "\n",
    "# Initialize the OpenAI model and the prompt template\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")\n",
    "\n",
    "experiment = \"Week-3-Langchain\"\n",
    "\n",
    "# Create the LLMChain with the specified model and prompt\n",
    "# 最早我也是用這個\n",
    "chain = LLMChain(llm=model, prompt=prompt)\n",
    "\n",
    "# Log the LangChain LLMChain in an MLflow run\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n",
    "\n",
    "# Create a new MLflow Experiment\n",
    "mlflow.set_experiment(experiment)\n",
    "\n",
    "with mlflow.start_run(run_name=\"LLMChain\") as run:\n",
    "    logged_model = mlflow.langchain.log_model(chain, \n",
    "                                              name=\"langchain_model\",\n",
    "                                              registered_model_name=\"LLMChain_Demo\")\n",
    "\n",
    "    prompt_path = os.path.join(\"tutorial\", \"LLM+Langchain\", \"Week-3\" ,\"prompt.txt\")\n",
    "    with open(prompt_path, \"w\") as f:\n",
    "        f.write(prompt.template)\n",
    "    # Log the prompt as an artifact\n",
    "    mlflow.log_artifact(prompt_path, artifact_path=\"prompts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078c63b2-6f77-4602-9bdc-6dc8452c760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=model, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797d329a-8cc2-4ab3-92f3-2897a04125b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"product\": \"colorful socks\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38074620-db41-430b-b0df-f06d70f779df",
   "metadata": {},
   "source": [
    "If you load the model like the MLflow tutorial suggests, we will get an error message because:\n",
    "\n",
    "The core problem is that ChatOpenAI contains fields like openai_api_key: SecretStr (a Pydantic type). When MLflow serializes the chain, those SecretStr objects get dumped to YAML with a tag like:\n",
    "\n",
    "!!python/object/pydantic.types.SecretStr\n",
    "\n",
    "But when you later reload the model, MLflow’s YAML loader doesn’t know how to re-instantiate SecretStr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6c3397-200c-42cc-824e-572abd46003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "from src.initialization import credential_init\n",
    "\n",
    "credential_init()\n",
    "\n",
    "# Log the LangChain LLMChain in an MLflow run\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n",
    "\n",
    "# Reload chain (structure only)\n",
    "loaded_model = mlflow.pyfunc.load_model(\"models:/LLMChain_Demo/1\")\n",
    "\n",
    "# Re-attach the LLM with fresh credentials\n",
    "loaded_model.llm = ChatOpenAI(\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Predict using the loaded model\n",
    "print(loaded_model.predict([{\"product\": \"colorful socks\"}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c15b26-53ea-4272-b4b6-f4ed664c36e0",
   "metadata": {},
   "source": [
    "An Universal Solution\n",
    "\n",
    "1. Create a python script file for your pipeline\n",
    "2. upload the `model` into your MLflow server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a59372e-90af-43e5-b784-67140cd1215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mlflow\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types.schema import Schema, ColSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5484258-12e7-44b3-8002-49f156be7760",
   "metadata": {},
   "source": [
    "在 MLflow 的 log_model 時標明 signature，主要是為了清楚定義模型的輸入與輸出格式。這對模型管理、部署以及後續使用都有實際好處\n",
    "\n",
    "- 1. 什麼是 signature？\n",
    "\n",
    "    signature 是一個 Schema 定義，描述了模型預期的 輸入資料型態與欄位結構，以及模型會輸出的 結果型態與欄位結構。 在這個例子裡：\n",
    "\n",
    "    - 輸入是一個欄位 \"input\"，型別是 string。\n",
    "    - 輸出包含兩個欄位：\"product\" 和 \"text\"，都為 string。\n",
    "    - \n",
    "- 2. 為什麼要標明 signature？\n",
    "\n",
    "    如果不指定，MLflow 會嘗試自動推斷模型的輸入/輸出格式，但這有幾個問題：\n",
    "\n",
    "    - 推斷可能不準確（特別是在複雜 pipeline 或輸入非標準 pandas/numpy 物件時）。\n",
    "\n",
    "    - 模型被部署或交付給其他人時，使用者可能不知道要如何準備正確的輸入資料。\n",
    "\n",
    "    因此 明確指定 signature 可以避免模糊不清。\n",
    "\n",
    "- 3. 好處\n",
    "\n",
    "    - 可讀性與可解釋性\n",
    "\n",
    "        清楚描述「這個模型吃什麼、吐什麼」，讓其他人（或未來的自己）一眼看懂。\n",
    "\n",
    "    - 驗證輸入輸出\n",
    "\n",
    "        MLflow 的 pyfunc API 會利用 signature 驗證輸入資料是否符合定義，避免「欄位缺失」或「型態不符」的錯誤。\n",
    "\n",
    "    - 提升可移植性\n",
    "\n",
    "        模型部署到 MLflow Serving 或其他 REST API 時，會自動帶上 schema 文件，API 使用者可以直接參考。\n",
    "\n",
    "    - 幫助自動化工具\n",
    "\n",
    "        在 AutoML pipeline 或 MLflow Model Registry 中，signature 可以被用來檢查模型相容性（例如 pipeline 不同階段之間輸入/輸出格式是否一致）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f1eaac-5cc6-4a10-b7de-1fea09ef2dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to know what you will put into it and what you will get out of it.\n",
    "input_schema = Schema([ColSpec(\"string\", \"input\")])\n",
    "output_schema = Schema([ColSpec(\"string\", \"product\"),\n",
    "                        ColSpec(\"string\", \"text\")])\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44d0fa0-5cf9-4ac2-8698-1fa82e4b1817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.io.path_definition import get_project_dir\n",
    "\n",
    "model_path = os.path.join(get_project_dir(), 'tutorial', 'LLM+Langchain', \"Week-3\", \"llmchain_mlflow_experiment.py\")\n",
    "\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n",
    "\n",
    "\"\"\"\n",
    "The registry is a separate feature: you must either (a) log with a registered_model_name argument, \n",
    "or (b) promote a logged model to the registry manually in the UI/CLI.\n",
    "\"\"\"\n",
    "\n",
    "with mlflow.start_run(run_name=\"LLMChain\") as run:\n",
    "    \n",
    "    mlflow.log_artifact(model_path, artifact_path=\"source_code\")\n",
    "\n",
    "    input_example = pd.DataFrame(data=[['colorful socks']], columns=['input'])\n",
    "    \n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        python_model=model_path,  # Define the model as the path to the Python file\n",
    "        name=\"langchain_model\",\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "        registered_model_name=\"LLMChain_Demo\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d882323-8175-44c8-9fdb-86e3278e4a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n",
    "\n",
    "loaded_model = mlflow.pyfunc.load_model(\"models:/LLMChain_Demo/17\")\n",
    "\n",
    "input_ = pd.DataFrame(data=[['歐姆尼賽亞的化身，會行走的大教堂: 帝皇級泰坦 (Imperator-class Titan )']], columns=['input'])\n",
    "\n",
    "loaded_model.predict(input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e534dc4a-8c61-450d-8c85-6680825716b2",
   "metadata": {},
   "source": [
    "你可以看到檔案會以Artifact形式下載，並且複寫\n",
    "\n",
    "換句話說 load_model 時要小心控制下載位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63dbd43-6a1f-4c4e-a069-de7b2a96d1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.pyfunc.load_model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13865a8f-ee3f-4d52-bb3f-95eecd615520",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
