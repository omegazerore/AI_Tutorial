{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f5e101-968d-4062-848a-23cfe4245440",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8602e84a-871a-4cdf-aa31-13fd6be2b431",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(\"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abbb99d-12c3-4ced-a1ad-be97966a33d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import chain\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "from src.initialization import credential_init\n",
    "from src.io.path_definition import get_project_dir\n",
    "\n",
    "\n",
    "credential_init()\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "\n",
    "with open(os.path.join(get_project_dir(), 'tutorial', 'LLM+Langchain', 'Week-1', 'recipe_train.json'), 'r') as f:\n",
    "    recipe_train = json.load(f)\n",
    "\n",
    "with open(os.path.join(get_project_dir(), 'tutorial', 'LLM+Langchain', 'Week-1', 'recipe_test.json'), 'r') as f:\n",
    "    recipe_test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2b5e40-5a83-4520-9a17-40dec5d4ad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_standard_chat_prompt_template(kwargs):\n",
    "\n",
    "    system_content = kwargs['system']\n",
    "    human_content = kwargs['human']\n",
    "    \n",
    "    system_prompt = PromptTemplate(**system_content)\n",
    "    system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "    \n",
    "    human_prompt = PromptTemplate(**human_content)\n",
    "    human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "    \n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                     human_message\n",
    "                                                   ])\n",
    "\n",
    "    return chat_prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec555189-20bc-4e4c-b8d4-2b6fbc056fb7",
   "metadata": {},
   "source": [
    "## Find out all the ingredients and cluster them according to some rules.\n",
    "\n",
    "- Do you still remember the joke-analysis chain? This one is structurally identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeee9f7-c4f5-42f4-a218-c4e7bb534caa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for recipe in recipe_train:\n",
    "    document = Document(page_content=\", \".join(recipe['ingredients']),\n",
    "                        metadata={\"cuisine\": recipe['cuisine'],\n",
    "                                  \"id\": recipe['id']})\n",
    "    documents.append(document)\n",
    "\n",
    "retriever = BM25Retriever.from_documents(documents=documents)\n",
    "    \n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "query = \", \".join(recipe_test[5]['ingredients'])\n",
    "\n",
    "question = \"Show me all the ingredients.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf452b5-5b95-4897-a799-528d973f476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first level chain\n",
    "\n",
    "system_template = \"You are an AI assistant as the best chef in the world.\"\n",
    "\n",
    "human_template = (\"Answer the question based only on the following content:\\n\"\n",
    "                  \"{context}\\n\\n\"\n",
    "                  \"Question: {question}\"\n",
    "                 )\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"context\", \"question\"]}}\n",
    "\n",
    "chat_prompt_template_1 = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "user_input = {\"context\": itemgetter(\"query\")|retriever, \n",
    "              \"question\":itemgetter(\"question\")}\n",
    "\n",
    "pipeline_1 = user_input | chat_prompt_template_1 | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3119c3-ea9b-4419-8fcf-079ac0f2a988",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(pipeline_1.invoke({\"question\": question,\n",
    "                         \"query\": query}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bc90c3-95d8-4f0a-9b88-54cabbf28f9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# second level chain\n",
    "\n",
    "response_schemas = [\n",
    "        ResponseSchema(name=\"result\", \n",
    "                       description=\"\"\"\n",
    "                                   a python dictionary with structure\n",
    "                                   <PYTHON STR>: <PYTHON LIST> \n",
    "                                   \"\"\")]\n",
    "\n",
    "structured_output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "format_instructions = structured_output_parser.get_format_instructions()\n",
    "\n",
    "system_template = \"\"\"\n",
    "                  You are an AI assistant as the best chef in the world. \n",
    "                  \"\"\"\n",
    "\n",
    "human_template = \"\"\"\n",
    "                 Please sort through the {ingredients}, such as meat, \n",
    "                 scipy, milk product,..., etc; \n",
    "                 format instruction: {format_instructions}\n",
    "                 \"\"\"\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"ingredients\"],\n",
    "                    \"partial_variables\": {\"format_instructions\": \n",
    "                                           format_instructions}}}\n",
    "\n",
    "chat_prompt_template_2 = build_standard_chat_prompt_template(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4696b82-4f0c-4b35-b0b6-dae608371a0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_ = {\"ingredients\": pipeline_1}|chat_prompt_template_2|model|structured_output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a112bcf-864b-4160-b026-4c8b9d469010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_.invoke({\"query\": query, \"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01268c94-c213-4c6b-9fbc-f6dbc541832e",
   "metadata": {},
   "source": [
    "## 回家自行練習\n",
    "1. 自己找一個中文食譜，打開冰箱掃一下。把冰箱裡的東西作為`現有食材`輸入，食譜中的某道菜作為`建議食材`輸入\n",
    "2. Find out all the ingredients and cluster them according to some rules -> 轉換成中文下去做。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40d2510-dcc6-4835-a2fb-ad1744ed1032",
   "metadata": {},
   "source": [
    "## OpenAI WebSearch Part1\n",
    "\n",
    "- Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c71195-8276-4e73-8922-cae08af0cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "\n",
    "response =  client.responses.create(\n",
    "        model=\"gpt-4o\",\n",
    "        tools=[{\"type\": \"web_search_preview\"}],\n",
    "        input=\"Give me a market wrap on the Hong Kong market for Feb 2025.\"\n",
    "    )\n",
    "    \n",
    "print(response.output_text)\n",
    "\n",
    "# response.output[1].content[0].annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5da8e3-8087-47df-873d-5e1573e0130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def openai_web_search_fn(input_: str, model: str):\n",
    "\n",
    "    assert model in ['gpt-4o', 'gpt-4o-mini']\n",
    "\n",
    "    print(input_)\n",
    "    \n",
    "    response = client.responses.create(\n",
    "        model=model,\n",
    "        tools=[{\"type\": \"web_search_preview\"}],\n",
    "        input=input_\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee71210b-d418-429e-96a4-974f368ac845",
   "metadata": {},
   "source": [
    "- Slightly more advanced way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96cb233-5d3a-4252-884b-431bce7b156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import Optional\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "response_schemas = [\n",
    "        ResponseSchema(name=\"refined_query\", \n",
    "                       description=\"The refined search query\")]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "\n",
    "class OpenAIWebSearch:\n",
    "\n",
    "    def __init__(self, model: str, temperature: Optional[float]=0):\n",
    "\n",
    "        \"\"\"\n",
    "        output: response.output_text\n",
    "        citations: response.output[1].content[0].annotations\n",
    "        \"\"\"\n",
    "\n",
    "        self._model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                                 model_name=model, temperature=temperature)\n",
    "\n",
    "        self.web_search_pipeline = RunnableLambda(partial(openai_web_search_fn, model=model))\n",
    "        self.query_generation_pipeline = self._build_query_generation_pipeline()\n",
    "\n",
    "        self.pipeline = RunnablePassthrough.assign(context=self.query_generation_pipeline|self.web_search_pipeline)\n",
    "\n",
    "    def _build_query_generation_pipeline(self):\n",
    "\n",
    "        input_ = {\"system\": {\"template\": (\"You are a highly intelligent AI assistant specialized in refining and optimizing search queries.\\n\"\n",
    "                                          \"Your goal is to generate precise, well-structured, and effective web search queries that maximize \"\n",
    "                                          \"relevant and accurate results.\")},\n",
    "                  \"human\": {\"template\": (\"Help me generate an optimized web search query based on the following question: {question}.\\n\"\n",
    "                                         \"Ensure the query is clear, specific, and structured to retrieve the most relevant information efficiently.\\n\"\n",
    "                                         \"format instruction: {format_instructions}\"),\n",
    "                            \"input_variables\": ['question'],\n",
    "                            \"partial_variables\": {'format_instructions': format_instructions}}}\n",
    "\n",
    "        chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "        return chat_prompt_template|self._model|output_parser|itemgetter(\"refined_query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f9736f-d202-41eb-81cd-4d6dd9d08772",
   "metadata": {},
   "outputs": [],
   "source": [
    "websearch = OpenAIWebSearch(model='gpt-4o')\n",
    "\n",
    "output = websearch.pipeline.invoke({\"question\": \"Give me a market wrap on the Hong Kong market for Feb 2025.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3edb92-41a3-44c0-a122-88de9f9c4a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9059a4a3-d425-44f9-8c79-ccaa956ea833",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = output['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb20214c-f822-407a-a34d-d685dbb58e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c54fde6-6eae-43a2-83bd-fdad2a705d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response =  client.responses.create(\n",
    "        model=\"gpt-4o\",\n",
    "        tools=[{\"type\": \"web_search_preview\"}],\n",
    "        input=\"Give me a market wrap on the Hong Kong market for Feb 2025.\"\n",
    "    )\n",
    "    \n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a11bf9b-5274-4623-affd-1e1ff0e9922b",
   "metadata": {},
   "source": [
    "## Perplexity\n",
    "\n",
    "https://python.langchain.com/docs/integrations/chat/perplexity/\n",
    "\n",
    "- sonar-deep-research\n",
    "- sonar-reasoning-pro\n",
    "- sonar-reasoning\n",
    "- sonar-pro\n",
    "- sonar\t128k\n",
    "- r1-1776"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25edc99-6ffa-4e43-9e91-ac02c6520db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are an AI assistant that focuses on equity market analysis and you need to \"\n",
    "            \"engage in an accurate, comprehensive, helpful and  polite conversation with a user.\"\n",
    "        ),\n",
    "    },\n",
    "    {  \n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"Give me a market wrap on the Hong Kong market for Feb 2025.\"\n",
    "        ),\n",
    "\n",
    "    },\n",
    "\n",
    "]\n",
    "\n",
    "client = OpenAI(api_key=os.environ['PERPLEXITY_API_KEY'], base_url=\"https://api.perplexity.ai\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"sonar-deep-research\",\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c30fd1-cc54-4241-9c5e-10cbab07954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112e0823-caba-4813-9566-eb21f5c71e5d",
   "metadata": {},
   "source": [
    "- Langchain modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00f868a-190f-483b-b7ec-70156545370a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "@chain\n",
    "def prompt_template_2_messages(chat_prompt):\n",
    "\n",
    "    output_messages = []\n",
    "     \n",
    "    _messages = chat_prompt.messages\n",
    "\n",
    "    for message in _messages:\n",
    "        if isinstance(message, SystemMessage):\n",
    "            output_messages.append({\"role\": \"system\", \"content\": message.content})\n",
    "        if isinstance(message, HumanMessage):\n",
    "            output_messages.append({\"role\": \"user\", \"content\": message.content})\n",
    "\n",
    "    return output_messages\n",
    "\n",
    "\n",
    "@chain\n",
    "def messages_2_perplexity(messages):\n",
    "\n",
    "    client = OpenAI(api_key=os.environ['PERPLEXITY_API_KEY'], base_url=\"https://api.perplexity.ai\")\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"sonar-deep-research\",\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message.content\n",
    "    citations = response.citations\n",
    "\n",
    "    return {\"content\": content,\n",
    "            \"citations\": citations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37e0865-ae3b-4f8e-86a5-bf48c7ce1d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = PromptTemplate(template=(\"You are an AI assistant that focuses on equity market analysis and you need to \"\n",
    "                                \"engage in an accurate, comprehensive, helpful and  polite conversation with a user.\"\n",
    "                               ))\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template=\"find out the report earnings of Bloomberg ticker {ticker}\",\n",
    "                              input_variable=[\"ticker\"])\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "chat_prompt_perplexity = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                           human_message\n",
    "                                                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f6b9d4-628a-429d-a0ee-b37b6dccac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_perplexity = chat_prompt_perplexity|prompt_template_2_messages|messages_2_perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03baefee-1af2-4220-89b4-65f23bdcefc3",
   "metadata": {},
   "source": [
    "## DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760b5173-6f0e-4a7a-9ef5-5a2e2ab39636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_deepseek import ChatDeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65a299b-5cf7-4a2f-8606-557deb17509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek_r1 = ChatDeepSeek(api_key=os.environ['DEEPSEEK_API_KEY'], temperature=0, model='deepseek-reasoner')\n",
    "\n",
    "system_template = \"You are a helpful assistant.\"\n",
    "human_template = \"Create a financial report of {ticker} based on:\\n {context}\"\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"context\", \"ticker\"]}\n",
    "         }\n",
    "\n",
    "chat_prompt_deepseek = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "pipeline_deepseek = chat_prompt_deepseek|deepseek_r1|output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dbff48-5020-4317-b3ca-f2e555f7c681",
   "metadata": {},
   "source": [
    "## Combining the Deep Search of Perplexity and Reasoning of DeepSeek-R1 \n",
    "\n",
    "- Langchain Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9c9b4a-2f28-43ec-b197-7528024992fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "final_pipeline = RunnablePassthrough.assign(context=pipeline_perplexity|itemgetter(\"content\"))|pipeline_deepseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3a5b04-bf42-417a-bc2e-f9e4784049c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_perplexity.invoke({\"ticker\": '2382 HK'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf63ea43-5036-4303-8a7d-12a961cd620a",
   "metadata": {},
   "source": [
    "# Types of Classification (分類類型)\n",
    "\n",
    "There are two main types (主要有兩種類型):\n",
    "\n",
    "- Sorting by feelings (Sentiment Classification)\n",
    "- Sorting by category (Category Classification)\n",
    "- 按感覺排序（情感分類）\n",
    "- 按類別排序（類別分類）\n",
    "\n",
    "## How it Works (工作原理):\n",
    "\n",
    "- We use a method called supervised learning. This means we have data and labels. Labels are like tags that tell us what the data is about. These labels are usually created by people who look at the data and decide what it means.\n",
    "- 我們使用一種稱為監督學習的方法。這意味著我們有數據和標籤。標籤告訴我們數據是什麼。這些標籤通常由查看數據並決定其含義的人創建。\n",
    "\n",
    "把每一組數據想像成是(題目，答案)的組合，靠著瘋狂刷題(大數據)來學習如何從題目得到答案。\n",
    "Aka: 填鴨式教育。填鴨其實不差，起碼鴨會肥。\n",
    "\n",
    "## Challenges\n",
    "\n",
    "- It takes a long time.\n",
    "- It's not always done the same way by different people.\n",
    "- It's expensive.\n",
    "- Training the model often needs renting cloud services like AWS or Azure.\n",
    "- Using these cloud services for running the model also costs a lot.\n",
    "- 耗時： 標記數據需要很長時間。\n",
    "- 不一致： 不同的人可能會以不同的方式標記相同的數據。\n",
    "- 昂貴： 該過程成本高昂。\n",
    "- 資源密集型： 訓練模型通常需要租用雲服務，如 AWS 或 Azure。\n",
    "- 運營成本： 使用這些雲服務運行模型的成本也很高。\n",
    "\n",
    "## Can we mimic supervised learning classification with LLM (我們能否用大型語言模型模仿監督學習分類)?\n",
    "\n",
    "- Yes, we can use large language models (LLMs) to mimic supervised learning classification. LLMs, like GPT-3 or GPT-4, can be trained to understand and generate human-like text. They can be fine-tuned on specific tasks, such as classification, without the need for extensive labeled datasets.\n",
    "- 是的，我們可以使用大型語言模型（LLM）來模仿監督學習分類。像 GPT-3 或 GPT-4 這樣的 LLM 可以被訓練來理解和生成類似人類的文本。它們可以針對特定任務進行微調，例如分類，而不需要大量標記數據集。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de87a279-4a17-4717-b28e-ff5e33620d00",
   "metadata": {},
   "source": [
    "## Zero-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867fe6b6-3106-4e23-8402-31f47a463ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(url=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb157d0e9-d18e-4835-a601-edeb011f0ee6_721x247.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec345b68-b69a-4b88-8147-879fd2e59f00",
   "metadata": {},
   "source": [
    "## 飛安事故原因分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a6e3e2-ba20-4906-81f3-676cdc717079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(os.path.join(get_project_dir(), 'tutorial', 'LLM+Langchain', 'Week-3', 'Data sample.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa789d2-2421-4715-95a7-c7bd9ddd1d09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73f9adc-13ce-4c59-bf07-29095ec514ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef841a6-2cf9-4fe6-8ae5-bb8a54c21b1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(filename= \"tutorial/LLM+Langchain/Week-3/HFACS_Org_Inf.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e09af75-31ad-4fb3-913f-b02fcedfe26d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_template = \"\"\"\n",
    "                  You are an AI assistant assigned with a task of safty report \n",
    "                  classification based on the content. You are a seasoned \n",
    "                  flight safety inspector with deep and extensive knowledge of \n",
    "                  aviation safty. \n",
    "    \n",
    "                  You always do the best work you can. You are highly \n",
    "                  analytical and pay close attention to details. \n",
    "    \n",
    "                  The candidates of the output are:\n",
    "\n",
    "                  - `Organizational Influence;Resource Management`\n",
    "                  - `Organizational Influence;Organizational Climate`\n",
    "                  - `Organizational Influence;rganizational Process`\n",
    "                  - `Unsafe Supervisions;Inadequate Supervision`\n",
    "                  - `Unsafe Supervisions;Planned Inappropriate Operations`\n",
    "                  - `Unsafe Supervisions;Failed to Correct Problem`\n",
    "                  - `Unsafe Supervisions;Supervisory Violation`\n",
    "                  - `Precondition for Unsafe Acts;Environmental Factors;Physical Environment`\n",
    "                  - `Precondition for Unsafe Acts;Environmental Factors;Technological Environment`\n",
    "                  - `Precondition for Unsafe Acts;Condition of Operators;Adverse Mental State`\n",
    "                  - `Precondition for Unsafe Acts;Condition of Operators;Adverse Physiological State`\n",
    "                  - `Precondition for Unsafe Acts;Condition of Operators;Physical/Mental Limitations`\n",
    "                  - `Precondition for Unsafe Acts;Personnel Factors;Crew Resource Management`\n",
    "                  - `Precondition for Unsafe Acts;Personnel Factors;Personal Readiness`\n",
    "                  - `Unsafe Acts;Errors;Decision Errors`\n",
    "                  - `Unsafe Acts;Errors;Skill-Based Errors`\n",
    "                  - `Unsafe Acts;Errors;Perceptual Errors`\n",
    "                  - `Unsafe Acts;Violations;Routine`\n",
    "                  - `Unsafe Acts;Violations;Exceptional`\n",
    "            \n",
    "                 The output is from one of the candidates. \n",
    "                 \"\"\"\n",
    "\n",
    "human_template = \"{report}\"\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"report\"]}}\n",
    "\n",
    "chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "pipeline_ = chat_prompt_template | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79bcc3c-4baa-4aad-a0c3-403733c0f027",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = df.iloc[3]['Report 1']\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1267429-92f8-468d-90fa-7fa990bba8ba",
   "metadata": {},
   "source": [
    "- Do you want to read it by yourself or do you want to outsource this to a machine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1701a49-6d3c-47a1-acde-40c7e4656832",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pipeline_.invoke({\"report\": text})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409e957e-f9b9-44b7-afdb-b916e82d79bc",
   "metadata": {},
   "source": [
    "### 使用parser精煉結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deb1254-889c-4133-873e-30f4fef41881",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_schemas = [\n",
    "        ResponseSchema(name=\"category\", \n",
    "                       description=\"\"\"\n",
    "                                   The predicted category of the classification\n",
    "                                   \"\"\")]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "system_template = \"\"\"\n",
    "                  You are an AI assistant assigned with a task of safty report \n",
    "                  classification based on the content. You are a seasoned \n",
    "                  flight safety inspector with deep and extensive knowledge of \n",
    "                  aviation safty. \n",
    "    \n",
    "                  You always do the best work you can. You are highly \n",
    "                  analytical and pay close attention to details. \n",
    "    \n",
    "                  The candidates of the output are:\n",
    "\n",
    "                  - `Organizational Influence;Resource Management`\n",
    "                  - `Organizational Influence;Organizational Climate`\n",
    "                  - `Organizational Influence;rganizational Process`\n",
    "                  - `Unsafe Supervisions;Inadequate Supervision`\n",
    "                  - `Unsafe Supervisions;Planned Inappropriate Operations`\n",
    "                  - `Unsafe Supervisions;Failed to Correct Problem`\n",
    "                  - `Unsafe Supervisions;Supervisory Violation`\n",
    "                  - `Precondition for Unsafe Acts;Environmental Factors;Physical Environment`\n",
    "                  - `Precondition for Unsafe Acts;Environmental Factors;Technological Environment`\n",
    "                  - `Precondition for Unsafe Acts;Condition of Operators;Adverse Mental State`\n",
    "                  - `Precondition for Unsafe Acts;Condition of Operators;Adverse Physiological State`\n",
    "                  - `Precondition for Unsafe Acts;Condition of Operators;Physical/Mental Limitations`\n",
    "                  - `Precondition for Unsafe Acts;Personnel Factors;Crew Resource Management`\n",
    "                  - `Precondition for Unsafe Acts;Personnel Factors;Personal Readiness`\n",
    "                  - `Unsafe Acts;Errors;Decision Errors`\n",
    "                  - `Unsafe Acts;Errors;Skill-Based Errors`\n",
    "                  - `Unsafe Acts;Errors;Perceptual Errors`\n",
    "                  - `Unsafe Acts;Violations;Routine`\n",
    "                  - `Unsafe Acts;Violations;Exceptional`\n",
    "            \n",
    "                 The output is from one of the candidates. \n",
    "                 \"\"\"\n",
    "\n",
    "human_template = \"\"\"\n",
    "                 {report}; \n",
    "                 format instruction: {format_instructions}\n",
    "                 \"\"\"\n",
    "\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"report\"],\n",
    "                    \"partial_variables\": {'format_instructions': format_instructions}}}\n",
    "\n",
    "chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "pipeline_ = chat_prompt_template|model|output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1da940-87d9-4d77-981a-82e597781d6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = pipeline_.invoke({\"report\": text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2255ea-0e55-4fa9-95d7-a204cbc68942",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78654a90-8412-4486-9859-e5e4008b21d6",
   "metadata": {},
   "source": [
    "### 回家作業 1\n",
    "\n",
    "若要飛安事故報告可以有複數分類結果，如何調整Prompt，包含parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93278e4e-449f-4b76-99ed-3b44acd9f2d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.filter(like='Report')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5984ea6c-2edd-42f9-ac35-9ab6d8af2d55",
   "metadata": {},
   "source": [
    "### 回家作業 2\n",
    "\n",
    "你可以很清楚的看到一個飛安事故中，可以出現複數報告。\n",
    "將`Report 1` 和 `Report 1.2` 結合起來產生一份的新報告。\n",
    "\n",
    "抄也是一門技術"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b2296d-901a-43a6-b441-7b4abc65ac22",
   "metadata": {},
   "source": [
    "# **** 預計第一個小時結束 ****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aee5c0b-7804-49bf-bd77-ff8203de6209",
   "metadata": {},
   "source": [
    "## HR: Job-Applicant Matching\n",
    "\n",
    "- Efficiency and Speed: LLMs can quickly process and analyze large volumes of applications, significantly reducing the time required for initial screening compared to manual reviews.\n",
    "\n",
    "- Consistency and Fairness: LLMs apply the same criteria consistently across all applications, minimizing human bias and ensuring a fairer initial screening process.\n",
    "\n",
    "- Detailed Analysis: LLMs can analyze complex language patterns and extract relevant information from resumes, cover letters, and other application materials, identifying key skills and qualifications that match job requirements.\n",
    "\n",
    "- Customization and Flexibility: LLMs can be customized to prioritize different skills and experiences based on specific job requirements, allowing for a more tailored screening process.\n",
    "\n",
    "- Scalability: LLMs can handle a large number of applications simultaneously, making them ideal for organizations that receive high volumes of applicants.\n",
    "\n",
    "- Cost-Effectiveness: By automating the initial stages of applicant screening, LLMs can reduce the need for extensive human resources, thereby lowering operational costs.\n",
    "\n",
    "- Continuous Improvement: LLMs can be continuously trained and improved based on feedback and new data, enhancing their accuracy and effectiveness over time.\n",
    "\n",
    "- Enhanced Candidate Experience: Faster response times and more consistent evaluations can improve the overall candidate experience, as applicants are more likely to receive timely feedback.\n",
    "\n",
    "- 效率和速度： LLMs 能夠快速處理和分析大量申請，相較於人工審查，顯著縮短初步篩選所需的時間。\n",
    "\n",
    "- 一致性和公平性： LLMs 對所有申請應用相同的標準，最小化人為偏見，確保初步篩選過程的公平性。\n",
    "\n",
    "- 詳細分析： LLMs 能夠分析複雜的語言模式，從簡歷、求職信和其他申請材料中提取相關信息，識別符合工作要求的關鍵技能和資格。\n",
    "\n",
    "- 自定義和靈活性： LLMs 可以根據具體的工作要求自定義優先考慮的技能和經驗，允許更有針對性的篩選過程。\n",
    "\n",
    "- 可擴展性： LLMs 能夠同時處理大量申請，非常適合接收大量申請人的組織。\n",
    "\n",
    "- 成本效益： 通過自動化申請篩選的初始階段，LLMs 可以減少對大量人力資源的需求，從而降低運營成本。\n",
    "\n",
    "- 持續改進： LLMs 可以根據反饋和新數據持續進行訓練和改進，隨著時間的推移提高其準確性和有效性。-\n",
    "\n",
    "- 提升候選人經驗： 更快的回應時間和更一致的評估可以改善整體候選人經驗，因為申請人更有可能及時收到反饋。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39b5024-206c-4bdf-9574-8753f01ca46f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(\"https://www.techjobasia.com/zh-Hant/jobs/GMMlhU0qSayr6ZwTB0U6zA---Software-Engineer-(ReactJS)\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46edd4df-89c2-49ca-9e19-d6ae0eb6adb7",
   "metadata": {},
   "source": [
    "###  1. Send a GET request to the URL (發送 GET 請求到指定的 URL)\n",
    "\n",
    "- This line sends an HTTP GET request to the specified URL and stores the response in the response variable.\n",
    "- 這行程式碼向指定的 URL 發送 HTTP GET 請求，並將響應儲存在 response 變數中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a779c48-badd-423e-889c-a57ece76a957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "\n",
    "url = \"https://www.techjobasia.com/zh-Hant/jobs/GMMlhU0qSayr6ZwTB0U6zA---Software-Engineer-(ReactJS)\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf1968c-7975-45a4-870b-0e62861219a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e681fd9e-5ba2-4c75-8bc3-4d41d747aaa4",
   "metadata": {},
   "source": [
    "### 2. Get the content of the response (獲取響應的內容)\n",
    "\n",
    "- This line extracts the content of the response as a text string and stores it in the html_content variable.\n",
    "- 這行程式碼將響應的內容作為文字字串提取，並將其儲存在 html_content 變數中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7947d4-5e33-4b1c-88b5-ba375e62b2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c04f13-6362-479b-8ebb-98a713c325fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# html_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee3cd12-f11a-4a7b-8f7e-ab8b16ada802",
   "metadata": {},
   "source": [
    "### 3. Parse the HTML content (解析 HTML 內容)\n",
    "\n",
    "- This line uses BeautifulSoup to parse the HTML content stored in html_content, creating a BeautifulSoup object named soup.\n",
    "- 這行程式碼使用 BeautifulSoup 解析儲存在 html_content 中的 HTML 內容，並創建一個名為 soup 的 BeautifulSoup 對象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583fc8ac-d92b-4762-afd0-e1d1de8c3e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa763c1-d8c7-4c82-9528-0ef7ea564112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046accba-5203-43df-b1c8-b5703601d3b3",
   "metadata": {},
   "source": [
    "### 4. Remove all CSS style tags (移除所有 CSS 樣式標籤)\n",
    "\n",
    "- This loop finds all <style> tags in the parsed HTML content and removes them using the decompose() method.\n",
    "- 這個循環找到解析後的 HTML 內容中的所有 <style> 標籤，並使用 decompose() 方法將它們移除。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ab74e8-4e7c-4a92-b5ea-af3db43d1082",
   "metadata": {},
   "outputs": [],
   "source": [
    "for style in soup.find_all('style'):\n",
    "    style.decompose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5edde0-9fb5-43d4-bfb9-6a6b2c146d7e",
   "metadata": {},
   "source": [
    "### 6. Extract and print only the text content (提取並打印僅包含文字的內容)\n",
    "\n",
    "- This line extracts the text content from the parsed HTML, separating elements with a newline character, and stores it in the text_content variable.\n",
    "- 這行程式碼從解析後的 HTML 中提取文字內容，使用換行符將元素分隔，並將其儲存在 text_content 變數中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c797f3bc-33c2-4d67-88e2-b110f0d2da83",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_content = soup.get_text(separator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba34a7e-4ebd-4ea7-b738-54995862f082",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cacc52-b827-47d3-8f05-399360212b76",
   "metadata": {},
   "source": [
    "### 7. Clean up the text (清理文字內容)\n",
    "\n",
    "- This line cleans up the extracted text by removing any leading or trailing whitespace from each line and discarding empty lines. The cleaned text is stored in the cleaned_text variable.\n",
    "- prints the cleaned text content to the console.\n",
    "- 這行程式碼通過移除每行的首尾空白並丟棄空行來清理提取的文字內容。清理後的文字儲存在 cleaned_text 變數中。\n",
    "- 將清理後的文字內容打印到控制台。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777d33a8-301c-4823-8753-4b501300382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for line in text_content.splitlines():\n",
    "#     if line.strip():\n",
    "#         print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6ff06a-d842-465d-b4f4-3e7282824e15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_text = '\\n'.join(line.strip() for line in text_content.splitlines() if line.strip())\n",
    "\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabae79b-382d-4626-a526-af3e781ab3ca",
   "metadata": {},
   "source": [
    "### Extract only the job related content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a31c4e7-395e-4770-a21f-8f21aee07e28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template= \"\"\"\n",
    "          Extract the job description part of the text: {content} \n",
    "          \"\"\"\n",
    "\n",
    "human_prompt = PromptTemplate(template=template)\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "    \n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([human_message])\n",
    "\n",
    "pipeline_ = chat_prompt_template|model|StrOutputParser()\n",
    "\n",
    "job_description = pipeline_.invoke({\"content\": cleaned_text})\n",
    "\n",
    "print(job_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3640b1ad-c74a-4535-9d2a-db2f2501ec9e",
   "metadata": {},
   "source": [
    "Let us do the same thing for the ten following candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7355ea-e001-4b89-b831-803a045ae1f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "applicants = [\"https://www.hiredchina.com/profiles/c87dc713-deea-408a-8798-c4e0814fc3ce\",\n",
    "              \"https://www.hiredchina.com/profiles/de4d4802-d58b-4d0a-92f6-b0f999ff2617\",\n",
    "              \"https://www.hiredchina.com/profiles/64cecea3-4f68-4707-89fa-c10dd9889be8\",\n",
    "              \"https://www.hiredchina.com/profiles/708fa8bc-0ccf-4f93-8be6-14154bfe828d\",\n",
    "              \"https://www.hiredchina.com/profiles/3ce4f0fb-c93f-4e87-aa00-1d75badce875\",\n",
    "              \"https://www.hiredchina.com/profiles/2aa145b8-cad0-4682-a542-8bbe0d268d74\",\n",
    "              \"https://www.hiredchina.com/profiles/06b2343a-a1ea-42da-bbfb-318041ae0835\",\n",
    "              \"https://www.hiredchina.com/profiles/3daf0124-ad33-4a9c-8305-28428bbf5ffc\",\n",
    "              \"https://www.hiredchina.com/profiles/053e6b20-0dee-477a-9fb6-3ab27e69206b\",\n",
    "              \"https://www.hiredchina.com/profiles/27f68cab-1f20-432b-9157-4f473c2ec189\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825038f7-7d58-4eb6-ad24-a41c14e6a322",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain, Runnable\n",
    "\n",
    "\n",
    "def parsing_process(url):\n",
    "    \"\"\"\n",
    "    Fetches and extracts text content from a given URL.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL of the web page to fetch and parse.\n",
    "\n",
    "    Returns:\n",
    "    str: Cleaned text content extracted from the web page.\n",
    "\n",
    "    Raises:\n",
    "    requests.exceptions.RequestException: If an error occurs while fetching the URL.\n",
    "\n",
    "    Notes:\n",
    "    - This function sends a GET request to the specified URL.\n",
    "    - It uses BeautifulSoup to parse the HTML content of the response.\n",
    "    - Any <style> tags in the HTML are removed to extract only textual content.\n",
    "    - The extracted text is cleaned by removing extra whitespace and empty lines.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Get the content of the response\n",
    "    html_content = response.text\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    for style in soup.find_all('style'):\n",
    "        style.decompose()\n",
    "\n",
    "    # Extract and print only the text content\n",
    "    text_content = soup.get_text(separator='\\n')\n",
    "\n",
    "    # Clean up the text (optional)\n",
    "    cleaned_text = '\\n'.join(line.strip() for line in text_content.splitlines() if line.strip())\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f540152-5a9f-45db-a02c-488427766ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IFrame(applicants[0], width=800, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8bc542-54f1-4f0f-a1dd-fdb77252c1e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(parsing_process(applicants[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9b5aab-a9d5-482a-9bbe-5bff77cee41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parsing_process(\"https://www.freelancer.com/u/divyadhakecha1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b861f6-3489-4031-a409-0825f037ed43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a74bf02-61db-4b78-ba98-013ebc8f77eb",
   "metadata": {},
   "source": [
    "### Extract the Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7a14c9-84d9-4a67-be3f-feefe74244b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "system_template = \"\"\"\n",
    "                  I am going to give you a template for your output. \n",
    "                  CAPITALIZED WORDS are my placeholders. \n",
    "                  Fill in my placeholders with your output. Please preserve \n",
    "                  the overall formatting of my template. My template is:\n",
    "\n",
    "                  *** Working Experience:*** WORKING EXPERIENCE \n",
    "                  *** Education:*** EDUCATION\n",
    "                  *** Skills:*** SKILLS\n",
    "\n",
    "                  I will give you the data to format in the next prompt. \n",
    "                  Create a resume using my template.\n",
    "                  \"\"\"\n",
    "\n",
    "human_template = \"\"\"\n",
    "                 {query}\n",
    "                 \"\"\"\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"query\"]}}\n",
    "\n",
    "resume_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "resume_pipeline = {\"query\": parsing_process}|resume_prompt_template|model|StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81388d28-d18a-4750-8151-4cb394ad71e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resume_output = resume_pipeline.invoke(\"https://www.freelancer.com/u/divyadhakecha1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77226af-b87d-4803-b694-02aa55539298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(resume_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e04c4bf-ca8e-4882-9e9e-24a60b6986b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_output = resume_pipeline.invoke(applicants[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d2b5d5-ce24-4260-8356-fb9c0deaa870",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resume_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6823b73a-8c52-45e6-8dfd-901732a25767",
   "metadata": {},
   "source": [
    "### Match the resume and the job\n",
    "\n",
    "#### 1. Create Parser\n",
    "\n",
    "- Define Response Schemas (定義響應模式)\n",
    "- Create a Structured Output Parser (創建結構化輸出解析器)\n",
    "- Get Format Instructions (獲取格式說明)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2b73ec-d204-47b0-9d31-f103b609ba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schemas = [\n",
    "        ResponseSchema(name=\"result\", description=\"\"\"If the candidate is a good fit, \n",
    "        either `Yes` or `No` \n",
    "        \"\"\"),\n",
    "        ResponseSchema(name=\"reason\", description=\"\"\"Applicant - Job matching \n",
    "        \"\"\")\n",
    "]\n",
    "\n",
    "structured_output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = structured_output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293333e8-8106-4d23-8bd3-115b15ac6f48",
   "metadata": {},
   "source": [
    "### 2. Create Template\n",
    "\n",
    "- Create a System Prompt Template (創建系統提示模板)\n",
    "- Create a System Message Prompt Template (創建系統訊息提示模板)\n",
    "- Create a Human Prompt Template (創建人類提示模板)\n",
    "- Create a Human Message Prompt Template (創建人類訊息提示模板)\n",
    "- Create a Chat Prompt Template (創建對話提示模板)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1390f7e-9f85-4a71-9860-f017b10ce879",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_template = \"\"\"\n",
    "                  You are an AI assistant acting as an experienced senior \n",
    "                  recruiter in IT field.\n",
    "                  \n",
    "                  You are assigned a task of identifying if an applicant, \n",
    "                  based on the description in the resume, is a good match to the described job. \n",
    "                    \n",
    "                  You always do the best work you can. You are highly \n",
    "                  analytical and pay close attention to details. \n",
    "                  \"\"\"\n",
    "\n",
    "human_template = \"\"\"\n",
    "                 Job description: {job};\n",
    "                 resume: {resume}; format_instructions: {format_instructions}\n",
    "                 \"\"\"\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"job\", \"resume\"],\n",
    "                    \"partial_variables\": {'format_instructions': format_instructions}}}\n",
    "\n",
    "match_prompt_template = build_standard_chat_prompt_template(input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc647902-b8e7-446b-b0c1-6ae2b6678fc7",
   "metadata": {},
   "source": [
    "### 3. Build the Chain\n",
    "\n",
    "- Chain the Prompt Template with the Model and Parser (將提示模板與模型和解析器鏈接)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee8e78e-49b7-481b-9a26-f1f9ddcdb83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_pipeline = match_prompt_template|model|structured_output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2a6215-3e77-4736-9f4c-54dcc9a6c130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "matching_output = matching_pipeline.invoke({\"job\":job_description, \"resume\":resume_output})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52226361-b458-45e2-a17d-bb47f2f3e7a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "matching_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ed3c6f-6a52-48b4-aab9-cadd8bc13fcb",
   "metadata": {},
   "source": [
    "### 4. An End to End application\n",
    "\n",
    "- Job description: 固定住，對某一群申請者來說這是不會變的\n",
    "- Applicants: 動態抽取\n",
    "- 媒合鍊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ef0972-6fd0-4c54-a3e8-68fc6a4926b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename= \"tutorial/LLM+Langchain/Week-3/LCEL_3.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b412464f-0cd9-43cc-bdfd-4989a2218f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# resume_pipeline: url -> a resume\n",
    "# matching_pipeline: receive the resume and match it with the job description\n",
    "\n",
    "step_1 = RunnablePassthrough.assign(resume=itemgetter(\"url\")|resume_pipeline)\n",
    "step_2 = matching_pipeline\n",
    "e2e_chain = step_1|step_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d17b78-5e04-48b0-b350-1a8bf04e977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in principle, job description is known because as an HR, you should have that\n",
    "\n",
    "e2e_chain.invoke({\"url\": \"https://www.freelancer.com/u/divyadhakecha1\",\n",
    "                  \"job\": job_description})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c00acb-7d5a-420c-a642-e6061e4e4a4e",
   "metadata": {},
   "source": [
    "# N-Shot Learning\n",
    "\n",
    "Imagine you have a friend who is incredibly good at understanding and completing tasks just by looking at a few examples. This is somewhat like how N-shot learning works with large language models (LLMs).\n",
    "\n",
    "## What is N-Shot Learning?\n",
    "N-shot learning is a way of teaching an AI model by giving it a small number of examples (N examples) for a particular task. Here's how it works:\n",
    "\n",
    "- 1-Shot Learning: The AI is shown one example of the task and then asked to perform a similar task.\n",
    "- Few-Shot Learning: The AI is shown a few examples (like 2, 3, or 5) of the task before it is asked to perform a similar task.\n",
    "- Zero-Shot Learning: The AI is given instructions without any examples and asked to perform the task.\n",
    "\n",
    "## Why is N-Shot Learning Useful?\n",
    "Let's say you're teaching someone how to fold a paper airplane. If you show them just one way to fold it, that's 1-shot learning. If you show them three different ways, that's 3-shot learning. By seeing these examples, they can understand the general idea and apply it to fold new paper airplanes on their own.\n",
    "\n",
    "## How Does This Work in AI?\n",
    "For AI models, like the ones used in chatbots or virtual assistants, N-shot learning allows the model to learn from a few examples rather than needing thousands of examples. Here’s a simple analogy to explain it:\n",
    "\n",
    "- Example Task: Writing a short story.\n",
    "    - 1-Shot Example: You show the AI one short story and then ask it to write a similar story.\n",
    "    - Few-Shot Example: You show the AI three different short stories and then ask it to write a new story.\n",
    "\n",
    "By doing this, the AI understands patterns, structures, and styles from the examples provided and uses this understanding to generate new and relevant content.\n",
    "\n",
    "## Benefits of N-Shot Learning\n",
    "- Efficiency: It’s much faster and requires less data to train the AI.\n",
    "- Versatility: The AI can quickly adapt to new tasks by being shown a few examples.\n",
    "- Practicality: It’s similar to how humans learn, making it easier to apply in real-world situations.\n",
    "\n",
    "## Real-World Application\n",
    "\n",
    "Imagine a customer service AI that needs to answer queries. Instead of being trained with thousands of examples, you could use N-shot learning:\n",
    "\n",
    "- Show it a few examples of customer queries and the appropriate responses.\n",
    "- The AI learns the pattern and can then generate suitable responses for new queries.\n",
    "\n",
    "In essence, N-shot learning helps AI models become smarter and more adaptable by learning from just a handful of examples, much like how we humans learn from a few demonstrations before we can do something on our own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5786fd-2a50-40a0-8bcb-ce431058c2ca",
   "metadata": {},
   "source": [
    "# N-Shot 學習\n",
    "\n",
    "想像一下，你有一位朋友非常擅長理解和完成任務，只需要看幾個例子。這有點像 N-Shot 學習在大型語言模型（LLM）中的運作方式。\n",
    "\n",
    "## 什麼是 N-Shot 學習？\n",
    "\n",
    "N-Shot 學習 是通過給 AI 模型一些示例（N 個示例）來教它完成特定任務的一種方法。以下是它的運作方式：\n",
    "\n",
    "- 1-Shot 學習：AI 只看到一個任務示例，然後被要求執行類似的任務。\n",
    "- 少樣本學習（Few-Shot Learning）：AI 看到幾個（如 2、3 或 5 個）任務示例，然後被要求執行類似的任務。\n",
    "- 零樣本學習（Zero-Shot Learning）：AI 只給出指令而不提供任何示例，然後被要求執行任務。\n",
    "\n",
    "## 為什麼 N-Shot 學習有用？\n",
    "\n",
    "假設你在教某人如何摺紙飛機。如果你只給他們看一個摺紙飛機的方法，這就是 1-Shot 學習。如果你給他們看三種不同的方法，這就是 3-Shot 學習。通過這些示例，他們可以理解大致的想法，並將其應用到摺新的紙飛機上。\n",
    "\n",
    "## 這在 AI 中如何運作？\n",
    "\n",
    "對於 AI 模型（如聊天機器人或虛擬助手中使用的模型），N-Shot 學習允許模型從少量示例中學習，而不需要成千上萬的示例。這裡有一個簡單的類比來解釋：\n",
    "\n",
    "## 任務示例：寫一個短故事。\n",
    "\n",
    "- 1-Shot 示例：你給 AI 看一個短故事，然後讓它寫一個類似的故事。\n",
    "- Few-Shot 示例：你給 AI 看三個不同的短故事，然後讓它寫一個新的故事。\n",
    "- 通過這樣做，AI 從提供的示例中理解模式、結構和風格，並使用這些理解來生成新的相關內容。\n",
    "\n",
    "## N-Shot 學習的好處\n",
    "\n",
    "- 效率高：訓練 AI 的速度更快，所需數據更少。\n",
    "- 多功能性：AI 可以通過少量示例快速適應新任務。\n",
    "- 實用性：這類似於人類學習的方式，使其更容易應用於現實世界的情況。\n",
    "\n",
    "## 實際應用\n",
    "想像一下，一個需要回答問題的客服 AI。與其用成千上萬的示例進行訓練，你可以使用 N-Shot 學習：\n",
    "- 給它看一些客戶問題和適當回應的示例。\n",
    "- AI 學習模式，然後可以為新問題生成合適的回應。\n",
    "\n",
    "總之，N-Shot 學習幫助 AI 模型通過少量示例變得更聰明和更具適應性，就像我們人類通過幾次示範後就能獨立完成任務一樣。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4509bcf3-c22d-4cec-bb20-777f9f7e9b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url=\"https://miro.medium.com/v2/resize:fit:720/format:webp/0*0eN5uwvq7JcesVex.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7094152d-7559-4163-b22f-0ce3ebb6bc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from typing import List\n",
    "\n",
    "import tiktoken\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "from src.io.path_definition import get_project_dir\n",
    "\n",
    "\n",
    "encoder = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    # Step 1: Normalize to remove accents\n",
    "    text = unicodedata.normalize('NFD', text)  # Decompose characters into base + accents\n",
    "    text = ''.join([char for char in text if unicodedata.category(char) != 'Mn'])  # Remove accents\n",
    "\n",
    "    text = re.sub(r'[^\\w]', ' ', text)  # This removes all characters that are not word characters or spaces\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def _preprocess_func(text: str) -> List[str]:\n",
    "\n",
    "    text = remove_special_characters(text)\n",
    "\n",
    "    # Lowercase the input text\n",
    "    lowered = text.lower()\n",
    "\n",
    "    # Convert the lowered text into tokens\n",
    "    tokens = encoder.encode(lowered)\n",
    "\n",
    "    # Stringify the tokens\n",
    "    return [str(token) for token in tokens]\n",
    "\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "filename = os.path.join(get_project_dir(), 'tutorial', 'LLM+Langchain', 'Week-3', \"benchmark.csv\")\n",
    "\n",
    "df_cosmetic = pd.read_csv(filename)\n",
    "\n",
    "documents = []\n",
    " \n",
    "for _, row in df_cosmetic.iterrows():\n",
    "    document = Document(page_content=row['Description'],\n",
    "                        metadata={\"compound_category\": row['compound_category']})\n",
    "    documents.append(document)\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents, embedding=embedding)\n",
    "\n",
    "embedding_retriever = vectorstore.as_retriever(search_type=\"similarity\",\n",
    "                                               search_kwargs={'k': 5})\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(documents,\n",
    "                                              k=5, bm25_params={\"k1\": 2.5},\n",
    "                                              preprocess_func=_preprocess_func)\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, embedding_retriever], weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da64cf7-10eb-4f3e-b565-81e54a0dd6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "\n",
    "query = \"Max Factor Colour Elixir Lipstick 837\"\n",
    "\n",
    "examples = ensemble_retriever.invoke(query)\n",
    "\n",
    "example_inputs = []\n",
    "\n",
    "for example in examples[1:]:\n",
    "    example_inputs.append({\"input\": example.page_content, \n",
    "                           \"output\": example.metadata['compound_category']})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fcb37b-baba-4c54-834c-f4efb33aa2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43178d7e-fe58-4779-9909-516fac96c977",
   "metadata": {},
   "source": [
    "### 2. Creating an Example Prompt (創建示例提示)\n",
    "\n",
    "- This line creates a ChatPromptTemplate for the examples. It defines a conversation where the human's message is the input ({input}), and the AI's response is the output ({output}). This template will be used to format the examples consistently.\n",
    "- 這行代碼為示例創建了一個 ChatPromptTemplate。它定義了一個對話，其中人類的訊息是輸入（{input}），AI 的回應是輸出（{output}）。這個模板將用於一致地格式化示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a532573-4544-4cd2-ab52-0cd0bb25d8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [('human', '{input}'), ('ai', '{output}')]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d2a205-0a2d-4c64-be97-021877098d74",
   "metadata": {},
   "source": [
    "### 3. Creating a Few-Shot Prompt Template (創建少樣本提示模板)\n",
    "\n",
    "- This line creates a FewShotChatMessagePromptTemplate. It takes the example_prompt and the list of example_inputs as arguments. This template will use the provided examples to guide the AI in generating responses, mimicking the structure of the examples.\n",
    "- 這行代碼創建了一個 FewShotChatMessagePromptTemplate。它將 example_prompt 和 example_inputs 列表作為參數。該模板將使用提供的示例來指導 AI 生成回應，模仿示例的結構。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5edde5e-171c-4a4c-985c-ea77c267d160",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_message = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=example_inputs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db57e7d-afd3-48d7-935c-1a9aef232ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cf9807-489c-47f3-9d2a-f900b9b78aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schemas = [\n",
    "            ResponseSchema(name=\"category\", description=\"\"\"\n",
    "                                                        product category. It must be from one of the LABEL.\n",
    "                                                        In case you do not know the answer, reply `MinorityHierarchy`\n",
    "                                                     \"\"\"),\n",
    "            ResponseSchema(name=\"reason\", description=\"\"\"\n",
    "                                                    why you make such as decision.`\n",
    "                                                    \"\"\")\n",
    "        ]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "\n",
    "n_shot_system_template = \"\"\"\n",
    "                          You are a helpful assistant. You are assigned with a task of identifying the CATEGORY of a PRODUCT.\n",
    "        \n",
    "                          CATEGORY is from one of the following:\n",
    "        \n",
    "                          - `Colour;Eye;Eye Pencil`\n",
    "                          - `Colour;Eye;Eye Specials`\n",
    "                          - `Colour;Eye;Eyeliner`\n",
    "                          - `Colour;Eye;Eyeshadow`\n",
    "                          - `Colour;Eye;Mascara`\n",
    "                          - `Colour;Face;Blush + Highlighter`\n",
    "                          - `Colour;Face;Body/Other`\n",
    "                          - `Colour;Face;Concealer`\n",
    "                          - `Colour;Face;Make-up`\n",
    "                          - `Colour;Face;Powder + Bronzer`\n",
    "                          - `Colour;Lip;Lip Pencil`\n",
    "                          - `Colour;Lip;Lip Specials / Others`\n",
    "                          - `Colour;Lip;Lipgloss`\n",
    "                          - `Colour;Lip;Lipstick`\n",
    "                          - `Colour;Nail;Base & Top Coats`\n",
    "                          - `Colour;Nail;French Manicure`\n",
    "                          - `Colour;Nail;Nail Art`\n",
    "                          - `Colour;Nail;Nail Care`\n",
    "                          - `Colour;Nail;Nailpolish`\n",
    "                          - `Colour;Nail;Professional Nails`\n",
    "                          - `Fragrance;Eau de toilette`\n",
    "                          - `Hand Cream;Care`\n",
    "                          - `Pure Skin;Cleanser`\n",
    "                \n",
    "                          \"\"\"\n",
    "\n",
    "system_prompt = PromptTemplate(template=n_shot_system_template)\n",
    "\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template=\"\"\"\n",
    "                                       What is the CATEGORY of {input}?\n",
    "\n",
    "                                       format instruction: {format_instructions}\n",
    "                                       \"\"\",\n",
    "                               partial_variables={\"format_instructions\": format_instructions})\n",
    "\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt) \n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                few_shot_message,\n",
    "                                                human_message\n",
    "                                               ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d7c24d-b726-4c4f-9a29-cabda21c6925",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc5f0e3-ce79-4243-b43a-f1c158435566",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = chat_prompt|model|output_parser\n",
    "\n",
    "output = pipeline.invoke({\"input\":query})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b8fdca-2eeb-4650-903c-ba6fbba6d414",
   "metadata": {},
   "source": [
    "## Can we chain this together with LCEL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f294b414-cd5f-4684-83af-df7b528417a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9102c59c-aa52-4ee9-a53e-b8cd467f92cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_standard_chat_prompt_template_v2(kwargs):\n",
    "\n",
    "    messages = []\n",
    "    \n",
    "    for key in ['system', 'few_shot', 'human']:\n",
    "        if kwargs.get(key):\n",
    "            if key == 'system':\n",
    "                system_content = kwargs['system']\n",
    "                system_prompt = PromptTemplate(**system_content)\n",
    "                message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "            elif key == 'human':\n",
    "                human_content = kwargs['human']\n",
    "                human_prompt = PromptTemplate(**human_content)\n",
    "                message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "            else:\n",
    "                few_shot_content = kwargs['few_shot']\n",
    "                message = FewShotChatMessagePromptTemplate(**few_shot_content)\n",
    "            \n",
    "            messages.append(message)\n",
    "    \n",
    "    chat_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "    return chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a03bf32-4113-4e0b-bb2a-cbf1a4ad7f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@chain\n",
    "def few_shot_prompt_fn(data):\n",
    "\n",
    "    example_prompt = ChatPromptTemplate.from_messages(\n",
    "        [('human', '{input}'), ('ai', '{output}')]\n",
    "    )\n",
    "\n",
    "    examples = []\n",
    "    \n",
    "    for example in data['examples']:\n",
    "        examples.append({\"input\": example.page_content, \n",
    "                         \"output\": example.metadata['compound_category']})\n",
    "    \n",
    "    few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "        example_prompt=example_prompt,\n",
    "        examples=examples\n",
    "    )\n",
    "\n",
    "    human_template = \"\"\"\n",
    "                     What is the CATEGORY of {input}?\n",
    "\n",
    "                     format instruction: {format_instructions}\n",
    "                     \"\"\"\n",
    "    \n",
    "    input_ = {\"system\": {\"template\": n_shot_system_template},\n",
    "              \"human\": {\"template\": human_template,\n",
    "                        \"partial_variables\": {\"format_instructions\": format_instructions}},\n",
    "              \"few_shot\": {\"example_prompt\": example_prompt,\n",
    "                           \"examples\": examples}}\n",
    "    \n",
    "    prompt_template = build_standard_chat_prompt_template_v2(input_)\n",
    "\n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9d9559-4231-4ebf-8fd4-4ff3e25a6545",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_ = {\"examples\": ensemble_retriever,\n",
    "             \"input\": RunnablePassthrough()}|few_shot_prompt_fn|model|output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800e7fc5-ce8f-4d9b-8075-0d76ed86e918",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de27572d-da2b-4b21-9520-448232665994",
   "metadata": {},
   "source": [
    "### Any other possibilities?\n",
    "\n",
    "The code above was finished by the end of July."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31858bf2-517f-414f-a397-c772084d8600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "\n",
    "SemanticSimilarityExampleSelector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed90ddb4-742d-4bf2-90fc-6c92e56e9484",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"human: {input}\\nai: {output}\",\n",
    ")\n",
    "\n",
    "examples = []\n",
    "\n",
    "for _, row in df_cosmetic.iterrows():\n",
    "\n",
    "    examples.append({\"input\": row['Description'],\n",
    "                     \"output\": row['compound_category']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a91979-50fa-4e58-b154-d874d3953918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # The list of examples available to select from.\n",
    "    examples=examples,\n",
    "    # The embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    embeddings=embedding,\n",
    "    # The VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    vectorstore_cls=FAISS,\n",
    "    # The number of examples to produce.\n",
    "    k=4,\n",
    ")\n",
    "\n",
    "## FewShotPromptTemplate here, instead of FewShotChatMessagePromptTemplate\n",
    "\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # We provide an ExampleSelector instead of examples.\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=n_shot_system_template,\n",
    "    suffix=\"\"\"\n",
    "           What is the CATEGORY of {input}?\n",
    "\n",
    "           format instruction: {format_instructions}\n",
    "           \"\"\",\n",
    "    input_variables=[\"input\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5528fdb-f79f-4673-bf80-2bad69026ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = similar_prompt|model|output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba791e15-b2ca-4c24-8bd8-d0b92db2f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ba0f93-1d90-4b18-8adc-8763efa06578",
   "metadata": {},
   "source": [
    "### 回家作業 3\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/similarity/\n",
    "\n",
    "1. 替換掉examples\n",
    "<!-- 2. 替換OpenAIEmbeddings()為HuggingFaceEmbeddings -->\n",
    "3. 替換掉k值。假設為5-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b727171-e77e-4e5d-9cd0-3998e5ef2eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
