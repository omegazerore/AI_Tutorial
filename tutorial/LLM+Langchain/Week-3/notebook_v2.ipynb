{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f5e101-968d-4062-848a-23cfe4245440",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8602e84a-871a-4cdf-aa31-13fd6be2b431",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(\"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abbb99d-12c3-4ced-a1ad-be97966a33d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import chain\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "from src.initialization import credential_init\n",
    "from src.io.path_definition import get_project_dir\n",
    "\n",
    "\n",
    "def build_standard_chat_prompt_template(kwargs):\n",
    "\n",
    "    messages = []\n",
    " \n",
    "    if 'system' in kwargs:\n",
    "        content = kwargs.get('system')\n",
    "        prompt = PromptTemplate(**content)\n",
    "        message = SystemMessagePromptTemplate(prompt=prompt)\n",
    "        messages.append(message)  \n",
    "\n",
    "    if 'human' in kwargs:\n",
    "        content = kwargs.get('human')\n",
    "        prompt = PromptTemplate(**content)\n",
    "        message = HumanMessagePromptTemplate(prompt=prompt)\n",
    "        messages.append(message)\n",
    "        \n",
    "    chat_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "    return chat_prompt\n",
    "\n",
    "credential_init()\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578de59b-eadb-44f4-97d5-393703757226",
   "metadata": {},
   "source": [
    "### 風格學習: RAG N-Shot\n",
    "\n",
    "在 RAG (Retrieval-Augmented Generation) 中，我們不只可以讓模型檢索資料庫來回答問題，還可以透過 N-Shot 提示 (N-Shot Prompting) 的方式，讓模型學習「風格」。\n",
    "\n",
    "這裡的 N，代表你給模型幾個示例 (Examples)。\n",
    "\n",
    "1-Shot：只給一個示例，模型會模仿該風格來生成。\n",
    "\n",
    "Few-Shot (N-Shot)：給多個示例，模型會歸納出共同的風格特徵。\n",
    "\n",
    "0-Shot：完全沒有示例，模型只能靠內建知識來生成。\n",
    "\n",
    "透過這種方式，我們可以讓模型不只是「回答問題」，而是「用指定風格來回答問題」。\n",
    "\n",
    "老樣子，選擇風格鮮明的例子。\n",
    "\n",
    "### 掄語"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9bf3f7-1c1c-4fb4-814c-7bac50d2f750",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[\"人不知，而不愠，不亦君子乎\", \"有人不知道我的大名，可我還沒發怒，這已經很君子了。\"], \n",
    "        [\"君子不重則不威\", \"君主打人一定要下重手，不然就樹立不了威信。\"],\n",
    "        [\"君子愛財，取之有道\", \"我喜歡錢，所以拿走你的錢，這是很有道理的。\"],\n",
    "        [\"既來之，則安之。\", \"既然來到了這裡，那麼就安葬在這裡吧。\"],\n",
    "        [\"子不語怪力亂神。\", \"夫子不想說話，施展起怪力將人打的神志不清。\"],\n",
    "        [\"不義而富且貴，於我如浮雲。\", \"不正當的錢財，對我來說猶如浮雲一般多。\"],\n",
    "        [\"朝聞道，夕可死矣。\", \"早上聽到我來了，晚上你就得死。\"],\n",
    "        [\"三人行，必有我師焉。\", \"有三個人，只要其中有一個是我，戰力就相當於一個師。\"],\n",
    "        [\"凡事豫則立，不豫則廢。\", \"但凡打架，只要猶豫，對面便站起來了。不猶豫就能直接將對面打廢。\"],\n",
    "        [\"孔子東遊，見兩小兒辯日。\", \"孔子去東邊打架，小孩在討論和孔子打架的人還能不能見到明天的太陽。\"],\n",
    "        [\"父母在，不遠遊，遊必有方。\", \"你父母在我手裡，你跑不了的，就算你跑了，我也有辦法把你抓回來。\"],\n",
    "        [\"始作傭者，其無後乎。\", \"這件事的主謀，已經被我打的絕後了。\"],\n",
    "        [\"鬼神敬而遠之\", \"孔子一旦發威，連鬼神見了都得敬畏的遠離他。\"],\n",
    "        [\"力不足者，中道而廢。\", \"力量不如我的人，在道上就只能被我打廢。\"],\n",
    "        [\"不恥下問\", \"看到我不自愧者，你就去下面問問。\"],\n",
    "        [\"三年無改於父之道，可謂孝矣。\", \"三年不該認我當父親的習慣，可以算作孝了。\"],\n",
    "        [\"人之將死，其言也善。\", \"把人打到瀕死，說的話也就好聽了。\"],\n",
    "        [\"知之為知之，不知為不知，是知也。\", \"該知道的知道，不該知道的少知道，知道嗎?\"],\n",
    "        [\"有教無類\", \"我在教你做事情，不管你是什麼人\"],\n",
    "        [\"子在川上曰: 逝者如斯夫，不捨晝夜。\", \"夫子站在河上說:死的人這麼多，是因為我不分晝夜地打人。\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470a9479-0e34-488a-8391-37a44f476c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "LitievalMode\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "documents = []\n",
    "\n",
    "for row in data:\n",
    "    document = Document(page_content=row[0],\n",
    "                        metadata={\"翻譯\": row[1]})\n",
    "    documents.append(document)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fb31db-5e90-47d4-97c2-d8b1c86cfd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(path=\"/tmp/langchain_qdrant\")\n",
    "\n",
    "collection_name = \"掄語\"\n",
    "dimension =  embeddings.client.get_sentence_embedding_dimension()\n",
    "\n",
    "try:\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=dimension, distance=Distance.COSINE),\n",
    "    )\n",
    "except ValueError:\n",
    "    client.delete_collection(collection_name=collection_name)\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=dimension, distance=Distance.COSINE),\n",
    "    )\n",
    "\n",
    "vectorstore_QVS = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=collection_name,\n",
    "    embedding=embeddings,\n",
    "    retrieval_mode=RetrievalMode.DENSE,\n",
    ")\n",
    "\n",
    "vectorstore_QVS.add_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec8edb5-a575-48d1-a037-7efab8815689",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_QVS = vectorstore_QVS.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "retriever_QVS.invoke(\"其為人也孝弟，而好犯上者，鮮矣\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70defc13-097c-413c-8f10-c33d68689ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "system_template = dedent(\"\"\"\n",
    "You are a helpful AI assistant and you will help us interpret the content based on the style of the examples:\n",
    "\n",
    "{context}\n",
    "\"\"\")\n",
    "\n",
    "human_template = dedent(\"\"\"\n",
    "{query}\n",
    "\"\"\")\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template,\n",
    "                     \"input_variable\": ['context']},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"query\"],\n",
    "                    }}\n",
    "\n",
    "chat_prompt_template = build_standard_chat_prompt_template(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54d43f5-7799-4cfb-b968-27f78e083928",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_documents = retriever_QVS.invoke(\"其為人也孝弟，而好犯上者，鮮矣\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebf6b3f-16ee-47a1-9318-a958df815c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = [f\"Human: {document.page_content}\\nAI: {document.metadata['翻譯']}\" for document in retrieved_documents]\n",
    "\n",
    "print(\"\\n\\n\".join(context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703d3eaa-4325-4b1f-9420-18af52a12ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_context = \"\\n\\n\".join(context)\n",
    "query = \"其為人也孝弟，而好犯上者，鮮矣\"\n",
    "\n",
    "chat_prompt = chat_prompt_template.invoke({\"query\": query, \"context\": merged_context})\n",
    "model.invoke(chat_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d99e3fb-c5e0-4de8-92c1-33a43bb2872e",
   "metadata": {},
   "source": [
    "味道不太對...我們有辦法強化生成嗎?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efd7aae-0c32-4423-97f2-f8bdad58bdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke(f\"Help us analyze the style:\\n{merged_context} and reply in traditional Chinese(繁體中文)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0922b0ef-482c-4f19-8508-8c950dc010e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "class requirements(BaseModel):\n",
    "\n",
    "    style: str = Field(description=\"The underlying style shown in the content. The output shall be in traditional Chinese (繁體中文).\")\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=requirements)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "human_template = dedent(\"\"\"\n",
    "                 {query}\n",
    "                 format instruction: {format_instructions}\n",
    "                 \"\"\")\n",
    "\n",
    "input_ = {\"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"query\"],\n",
    "                    \"partial_variables\": {\"format_instructions\": \n",
    "                                          format_instructions}}}\n",
    "\n",
    "chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "prompt = chat_prompt_template.invoke({\"query\": f\"Help us analyze the style:\\n{merged_context}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dae863b-7995-478c-8793-280754797c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c1d075-5190-43fd-ae59-34d2455940dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser.parse(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a01861f-596c-4600-bf71-650c11fd7566",
   "metadata": {},
   "source": [
    "### Process:\n",
    "1. 使用檢索器檢索相關訊息\n",
    "2. 根據檢索出來的內容抽取風格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d4a1ec-e5b2-4a99-97de-176061253d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f746a2-ff85-4b13-88db-6fc014fa37ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough, chain, RunnableLambda\n",
    "\n",
    "@chain\n",
    "def document_2_context(documents):\n",
    "\n",
    "    context = [f\"Human: {document.page_content}\\nAI: {document.metadata['翻譯']}\" for document in documents]\n",
    "\n",
    "    return \"\\n\\n\".join(context)\n",
    "\n",
    "context_extraction_pipeline = itemgetter(\"query\")|retriever_QVS|document_2_context\n",
    "\n",
    "# Retrieval\n",
    "# context_extraction_pipeline.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e21a164-e220-4297-bf27-988947ee8e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_template = dedent(\"\"\"\n",
    "                 Help us analyze the style of interpretation shown in the text: \n",
    "                 \n",
    "                 {context}\n",
    "                 \n",
    "                 format instruction: {format_instructions}\n",
    "                 \"\"\")\n",
    "\n",
    "input_ = {\"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"context\"],\n",
    "                    \"partial_variables\": {\"format_instructions\": \n",
    "                                          format_instructions}}}\n",
    "\n",
    "style_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "# style_pipeline = RunnablePassthrough.assign(context=itemgetter(\"query\")|retriever_QVS|document_2_context)\n",
    "# print(style_pipeline.invoke({\"query\": query}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e97158-a296-4cf6-8aa3-a2a2ea9f0a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "style_pipeline = RunnablePassthrough.assign(context=context_extraction_pipeline)#|\\\n",
    "    # RunnablePassthrough.assign(style=style_prompt_template|model|output_parser|RunnableLambda(lambda x: x.style))\n",
    "style_pipeline.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9008de5a-6f9e-4d78-82b6-c214151d6146",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_extraction_pipeline = style_prompt_template|model|output_parser|RunnableLambda(lambda x: x.style)\n",
    "\n",
    "style_pipeline = RunnablePassthrough.assign(context=context_extraction_pipeline)|RunnablePassthrough.assign(style=style_extraction_pipeline)\n",
    "style_pipeline.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97c0c83-c26e-4054-bc90-e1890d8d058d",
   "metadata": {},
   "source": [
    "#### 現在我們看到風格可以被提取出來了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb88f7de-d384-40f0-bda6-5434c5202611",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = dedent(\"\"\"\n",
    "You are a helpful AI assistant and you will help us interpret the user query with this style:\n",
    "{style}\n",
    "\"\"\")\n",
    "\n",
    "human_template = dedent(\"\"\"\n",
    "Examples:\n",
    "{context}\n",
    "\n",
    "query: {query}\n",
    "\"\"\")\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template,\n",
    "                     \"input_variable\": ['style']},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"query\", \"context\"],\n",
    "                    }}\n",
    "\n",
    "query = \"其為人也孝弟，而好犯上者，鮮矣\"\n",
    "\n",
    "generate_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "generate_pipeline = style_pipeline|generate_prompt_template|model\n",
    "\n",
    "print(generate_pipeline.invoke({\"query\": query}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1f2634-4e0e-4ec3-8cf4-0b3676b5603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"有顏回者好學，不遷怒，不貳過。不幸短命死矣！\"\n",
    "\n",
    "generate_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "generate_pipeline = style_pipeline|generate_prompt_template|model\n",
    "\n",
    "print(generate_pipeline.invoke({\"query\": query}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72e38dc-bdc4-4bc8-a38f-cf4d8574db00",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"子路曰：「衛君待子而為政，子將奚先？」子曰：「必也正名乎！」\"\n",
    "\n",
    "generate_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "generate_pipeline = style_pipeline|generate_prompt_template|model\n",
    "\n",
    "print(generate_pipeline.invoke({\"query\": query}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a11bf9b-5274-4623-affd-1e1ff0e9922b",
   "metadata": {},
   "source": [
    "## Perplexity\n",
    "\n",
    "https://python.langchain.com/docs/integrations/chat/perplexity/\n",
    "\n",
    "- sonar-deep-research\n",
    "- sonar-reasoning-pro\n",
    "- sonar-reasoning\n",
    "- sonar-pro\n",
    "- sonar\t128k\n",
    "- r1-1776\n",
    "\n",
    "Langchain '似乎'支持Perplexity，但我在使用時發現會出問題，所以需要自己套殼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25edc99-6ffa-4e43-9e91-ac02c6520db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity 基本API使用\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are an AI assistant that focuses on equity market analysis and you need to \"\n",
    "            \"engage in an accurate, comprehensive, helpful and  polite conversation with a user.\"\n",
    "        ),\n",
    "    },\n",
    "    {  \n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"Find the SKU number of Carslan Lasting Cover Foundation N01\"\n",
    "        ),\n",
    "\n",
    "    },\n",
    "\n",
    "]\n",
    "\n",
    "client = OpenAI(api_key=os.environ['PERPLEXITY_API_KEY'], base_url=\"https://api.perplexity.ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00f868a-190f-483b-b7ec-70156545370a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "\n",
    "@chain\n",
    "def prompt_template_2_messages(chat_prompt):\n",
    "\n",
    "    output_messages = []\n",
    "     \n",
    "    _messages = chat_prompt.messages\n",
    "\n",
    "    for message in _messages:\n",
    "        if isinstance(message, SystemMessage):\n",
    "            output_messages.append({\"role\": \"system\", \"content\": message.content})\n",
    "        if isinstance(message, HumanMessage):\n",
    "            output_messages.append({\"role\": \"user\", \"content\": message.content})\n",
    "\n",
    "    return output_messages\n",
    "\n",
    "\n",
    "@chain\n",
    "def messages_2_perplexity(messages):\n",
    "\n",
    "    client = OpenAI(api_key=os.environ['PERPLEXITY_API_KEY'], base_url=\"https://api.perplexity.ai\")\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"sonar-deep-research\",\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message.content\n",
    "    citations = response.citations\n",
    "\n",
    "    return {\"content\": content,\n",
    "            \"citations\": citations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f6b9d4-628a-429d-a0ee-b37b6dccac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_perplexity = chat_prompt_perplexity|prompt_template_2_messages|messages_2_perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03baefee-1af2-4220-89b4-65f23bdcefc3",
   "metadata": {},
   "source": [
    "## DeepSeek\n",
    "\n",
    "- Langchain 支援 Deepseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65a299b-5cf7-4a2f-8606-557deb17509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "deepseek_r1 = ChatDeepSeek(api_key=os.environ['DEEPSEEK_API_KEY'], temperature=0, model='deepseek-reasoner')\n",
    "\n",
    "system_template = \"You are a helpful assistant.\"\n",
    "human_template = \"Create a financial report of {ticker} based on:\\n {context}\"\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"context\", \"ticker\"]}\n",
    "         }\n",
    "\n",
    "chat_prompt_deepseek = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "pipeline_deepseek = chat_prompt_deepseek|deepseek_r1|output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf63ea43-5036-4303-8a7d-12a961cd620a",
   "metadata": {},
   "source": [
    "# 分類任務\n",
    "\n",
    "在機器學習中，分類主要有兩大類：\n",
    "\n",
    "- 情感分類 (Sentiment Classification)：判斷文本所表達的情感，例如「正面」「中立」「負面」。\n",
    "\n",
    "- 主題/類別分類 (Topic/Categorical Classification)：將文本歸到特定類別，例如「新聞 → 體育 / 政治 / 財經」。\n",
    "\n",
    "## 工作原理:\n",
    "\n",
    "分類模型通常透過 監督學習 (Supervised Learning) 訓練而成。\n",
    "\n",
    "- 數據與標籤：我們有輸入數據（例如一段文字）和對應標籤（例如「正面」）。\n",
    "\n",
    "- 學習過程：模型反覆從「題目—答案」對中學習，逐漸掌握輸入與輸出之間的規律。\n",
    "\n",
    "這更像是學會「抓模式」而不是「死背答案」。\n",
    "\n",
    "## 挑戰\n",
    "\n",
    "監督學習分類的實際挑戰包括：\n",
    "\n",
    "- 耗時：資料標註需要大量人工投入。\n",
    "\n",
    "- 不一致：不同標註者可能對相同數據有不同理解。\n",
    "\n",
    "- 昂貴：大規模數據收集與標註成本高。\n",
    "\n",
    "- 資源密集：模型訓練往往需要強大硬體或雲端服務（AWS、Azure 等）。\n",
    "\n",
    "- 運營成本：模型在雲端持續運行也需要付出高額費用。\n",
    "\n",
    "## 大型語言模型 (LLM) 的應用\n",
    "\n",
    "近年的 大型語言模型（LLM，如 GPT-3、GPT-4） 提供了新的分類方式：\n",
    "\n",
    "- 少樣本 / 零樣本分類：只需少量範例，甚至僅靠提示，就能完成分類任務。\n",
    "\n",
    "- 降低標註需求：不必建立龐大的人工標註資料集。\n",
    "\n",
    "- 可微調：仍可透過微調 (fine-tuning) 針對特定分類任務強化效果。\n",
    "\n",
    "這使得分類不再完全依賴傳統的監督學習流程，降低了成本與時間。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de87a279-4a17-4717-b28e-ff5e33620d00",
   "metadata": {},
   "source": [
    "## Zero-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867fe6b6-3106-4e23-8402-31f47a463ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(url=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb157d0e9-d18e-4835-a601-edeb011f0ee6_721x247.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec345b68-b69a-4b88-8147-879fd2e59f00",
   "metadata": {},
   "source": [
    "## 飛安事故原因分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a6e3e2-ba20-4906-81f3-676cdc717079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(os.path.join(get_project_dir(), 'tutorial', 'LLM+Langchain', 'Week-3', 'Data sample.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa789d2-2421-4715-95a7-c7bd9ddd1d09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73f9adc-13ce-4c59-bf07-29095ec514ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef841a6-2cf9-4fe6-8ae5-bb8a54c21b1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(filename= \"tutorial/LLM+Langchain/Week-3/HFACS_Org_Inf.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e09af75-31ad-4fb3-913f-b02fcedfe26d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_template = dedent(\"\"\"\n",
    "                  You are an AI assistant assigned with a task of safty report \n",
    "                  classification based on the content. You are a seasoned \n",
    "                  flight safety inspector with deep and extensive knowledge of \n",
    "                  aviation safty. \n",
    "    \n",
    "                  You always do the best work you can. You are highly \n",
    "                  analytical and pay close attention to details. \n",
    "    \n",
    "                  The candidates of the output are:\n",
    "\n",
    "                  - `Organizational Influence;Resource Management`\n",
    "                  - `Organizational Influence;Organizational Climate`\n",
    "                  - `Organizational Influence;rganizational Process`\n",
    "                  - `Unsafe Supervisions;Inadequate Supervision`\n",
    "                  - `Unsafe Supervisions;Planned Inappropriate Operations`\n",
    "                  - `Unsafe Supervisions;Failed to Correct Problem`\n",
    "                  - `Unsafe Supervisions;Supervisory Violation`\n",
    "                  - `Precondition for Unsafe Acts;Environmental Factors;Physical Environment`\n",
    "                  - `Precondition for Unsafe Acts;Environmental Factors;Technological Environment`\n",
    "                  - `Precondition for Unsafe Acts;Condition of Operators;Adverse Mental State`\n",
    "                  - `Precondition for Unsafe Acts;Condition of Operators;Adverse Physiological State`\n",
    "                  - `Precondition for Unsafe Acts;Condition of Operators;Physical/Mental Limitations`\n",
    "                  - `Precondition for Unsafe Acts;Personnel Factors;Crew Resource Management`\n",
    "                  - `Precondition for Unsafe Acts;Personnel Factors;Personal Readiness`\n",
    "                  - `Unsafe Acts;Errors;Decision Errors`\n",
    "                  - `Unsafe Acts;Errors;Skill-Based Errors`\n",
    "                  - `Unsafe Acts;Errors;Perceptual Errors`\n",
    "                  - `Unsafe Acts;Violations;Routine`\n",
    "                  - `Unsafe Acts;Violations;Exceptional`\n",
    "            \n",
    "                 The output is from one of the candidates. \n",
    "                 \"\"\")\n",
    "\n",
    "human_template = \"{report}\"\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"report\"]}}\n",
    "\n",
    "chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "pipeline_ = chat_prompt_template | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79bcc3c-4baa-4aad-a0c3-403733c0f027",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = df.iloc[3]['Report 1']\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1267429-92f8-468d-90fa-7fa990bba8ba",
   "metadata": {},
   "source": [
    "- Do you want to read it by yourself or do you want to outsource this to a machine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1701a49-6d3c-47a1-acde-40c7e4656832",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pipeline_.invoke({\"report\": text})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409e957e-f9b9-44b7-afdb-b916e82d79bc",
   "metadata": {},
   "source": [
    "### 使用parser精煉結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deb1254-889c-4133-873e-30f4fef41881",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_schemas = [\n",
    "        ResponseSchema(name=\"category\", \n",
    "                       description=dedent(\"\"\"\n",
    "                                   The predicted category of the classification\n",
    "                                   \"\"\"))]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "system_template = dedent(\"\"\"\n",
    "                  You are an AI assistant assigned with a task of safty report \n",
    "                  classification based on the content. You are a seasoned \n",
    "                  flight safety inspector with deep and extensive knowledge of \n",
    "                  aviation safty. \n",
    "    \n",
    "                  You always do the best work you can. You are highly \n",
    "                  analytical and pay close attention to details. \n",
    "    \n",
    "                  The candidates of the output are:\n",
    "\n",
    "                  - `Organizational Influence;Resource Management`\n",
    "                  - `Organizational Influence;Organizational Climate`\n",
    "                  - `Organizational Influence;rganizational Process`\n",
    "                  - `Unsafe Supervisions;Inadequate Supervision`\n",
    "                  - `Unsafe Supervisions;Planned Inappropriate Operations`\n",
    "                  - `Unsafe Supervisions;Failed to Correct Problem`\n",
    "                  - `Unsafe Supervisions;Supervisory Violation`\n",
    "                  - `Precondition for Unsafe Acts;Environmental Factors;Physical Environment`\n",
    "                  - `Precondition for Unsafe Acts;Environmental Factors;Technological Environment`\n",
    "                  - `Precondition for Unsafe Acts;Condition of Operators;Adverse Mental State`\n",
    "                  - `Precondition for Unsafe Acts;Condition of Operators;Adverse Physiological State`\n",
    "                  - `Precondition for Unsafe Acts;Condition of Operators;Physical/Mental Limitations`\n",
    "                  - `Precondition for Unsafe Acts;Personnel Factors;Crew Resource Management`\n",
    "                  - `Precondition for Unsafe Acts;Personnel Factors;Personal Readiness`\n",
    "                  - `Unsafe Acts;Errors;Decision Errors`\n",
    "                  - `Unsafe Acts;Errors;Skill-Based Errors`\n",
    "                  - `Unsafe Acts;Errors;Perceptual Errors`\n",
    "                  - `Unsafe Acts;Violations;Routine`\n",
    "                  - `Unsafe Acts;Violations;Exceptional`\n",
    "            \n",
    "                 The output is from one of the candidates. \n",
    "                 \"\"\")\n",
    "\n",
    "human_template = \"\"\"\n",
    "                 {report}; \n",
    "                 format instruction: {format_instructions}\n",
    "                 \"\"\"\n",
    "\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"report\"],\n",
    "                    \"partial_variables\": {'format_instructions': format_instructions}}}\n",
    "\n",
    "chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "pipeline_ = chat_prompt_template|model|output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1da940-87d9-4d77-981a-82e597781d6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = pipeline_.invoke({\"report\": text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2255ea-0e55-4fa9-95d7-a204cbc68942",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5984ea6c-2edd-42f9-ac35-9ab6d8af2d55",
   "metadata": {},
   "source": [
    "### 回家作業 2\n",
    "\n",
    "你可以很清楚的看到一個飛安事故中，可以出現複數報告。\n",
    "將`Report 1` 和 `Report 1.2` 結合起來產生一份的新報告。\n",
    "\n",
    "抄也是一門技術"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87766fd5-25dc-4b82-ad33-e716878cde7a",
   "metadata": {},
   "source": [
    "把唐詩宋詞拿出來回鍋利用\n",
    "\n",
    "有沒有一種高中時該好好學習的感覺?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c8b8bb-d1a3-4fcb-8dec-afbd093d59cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read file\n",
    "filename = os.path.join(\"tutorial\", \"LLM+Langchain\", \"Week-1\", \"唐詩三百首.txt\")\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "poems = []\n",
    "\n",
    "# Split by blank lines\n",
    "blocks = [b.strip() for b in text.strip().split(\"\\n\\n\") if b.strip()]\n",
    "\n",
    "for block in blocks:\n",
    "    entry = {}\n",
    "    for line in block.split(\"\\n\"):\n",
    "        if line.startswith(\"詩名:\"):\n",
    "            entry[\"詩名\"] = line.replace(\"詩名:\", \"\").strip()\n",
    "        elif line.startswith(\"作者:\"):\n",
    "            entry[\"作者\"] = line.replace(\"作者:\", \"\").strip()\n",
    "        elif line.startswith(\"詩體:\"):\n",
    "            entry[\"詩體\"] = line.replace(\"詩體:\", \"\").strip()\n",
    "        elif line.startswith(\"詩文:\"):\n",
    "            entry[\"詩文\"] = line.replace(\"詩文:\", \"\").strip()\n",
    "    if len(entry) != 0:\n",
    "        poems.append(entry)\n",
    "\n",
    "df_poem = pd.DataFrame(poems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247edae4-1a74-4cc4-bc13-2f215f2b1ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Output(BaseModel):\n",
    "\n",
    "    name: Literal['五言古詩', '七言古詩', '七言律詩', \n",
    "                  '五言絕句', '樂府', '七言絕句', '五言律詩'] = Field(description=\"唐詩詩體\")\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=requirements)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "\n",
    "system_template = dedent(\"\"\"\n",
    "                  You are a help AI assistant specialized at Chinese literature, especially the 唐詩。\n",
    "                  You are assigned with a task of classify the given poem.\n",
    "                  The outcomes will be from one of these candidates:\n",
    "\n",
    "                  - 五言古詩\n",
    "                  - 七言古詩\n",
    "                  - 七言律詩\n",
    "                  - 五言絕句\n",
    "                  - 樂府\n",
    "                  - 七言絕句\n",
    "                  - 五言律詩\n",
    "                  \"\"\")\n",
    "\n",
    "human_template = dedent(\"\"\"\n",
    "                 poem: {query}\n",
    "                 format instruction: {format_instructions}\n",
    "                 \"\"\")\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"query\"],\n",
    "                    \"partial_variables\": {\"format_instructions\": \n",
    "                                          format_instructions}}}\n",
    "\n",
    "classification_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "classification_pipeline = classification_prompt_template|model|output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa31b9f-8e43-4d3d-a375-3997f71b6bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poem[df_poem['作者']=='李白']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da96698c-f44c-4816-8c3d-85ee913f1b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df_poem.loc[89]['詩文']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeba9f9-7d6b-42ad-9849-e79d7d536a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4812ff64-d428-46b7-b50d-fc82b62c333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_pipeline.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19345b91-2196-434f-94e8-8d40ca1d8627",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = dedent(\"\"\"\n",
    "趙客縵胡纓，吳鉤霜雪明；\n",
    "銀鞍照白馬，颯沓如流星。\n",
    "十步殺一人，千里不留行；\n",
    "事了拂衣去，深藏身與名。\n",
    "閑過信陵飲，脫劒膝前橫；\n",
    "將炙啖朱亥，持觴勸侯嬴。\n",
    "三杯吐然諾，五嶽倒爲輕；\n",
    "眼花耳熱後，意氣素霓生。\n",
    "救趙揮金槌，邯鄲先震驚；\n",
    "千秋二壯士，烜赫大梁城。\n",
    "縱死俠骨香，不慙世上英；\n",
    "誰能書閤下，白首太玄經。\n",
    "\"\"\")\n",
    "\n",
    "classification_pipeline.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e2418d-7265-48c7-bf9f-52d690a78ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file\n",
    "filename = os.path.join(\"tutorial\", \"LLM+Langchain\", \"Week-1\", \"宋詞三百首.txt\")\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "poems = []\n",
    "\n",
    "# Split by blank lines\n",
    "blocks = [b.strip() for b in text.strip().split(\"\\n\\n\") if b.strip()]\n",
    "\n",
    "for block in blocks:\n",
    "    entry = {}\n",
    "    for line in block.split(\"\\n\"):\n",
    "        if line.startswith(\"詞牌:\"):\n",
    "            entry[\"詞牌\"] = line.replace(\"詞牌:\", \"\").strip()\n",
    "        elif line.startswith(\"作者:\"):\n",
    "            entry[\"作者\"] = line.replace(\"作者:\", \"\").strip()\n",
    "        elif line.startswith(\"詞文:\"):\n",
    "            entry[\"詞文\"] = line.replace(\"詞文:\", \"\").strip()\n",
    "    if len(entry) != 0:\n",
    "        poems.append(entry)\n",
    "\n",
    "df_poem = pd.DataFrame(data=poems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26254fd-ae79-49e9-a5ae-e1a18920476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = df_poem['詞牌'].dropna().unique().tolist()\n",
    "\n",
    "# Generate a Literal definition\n",
    "literal_def = f\"Literal[{', '.join(repr(v) for v in values)}]\"\n",
    "\n",
    "eval(literal_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245d1aca-f732-47c6-9ada-a8693badbb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Output(BaseModel):\n",
    "\n",
    "    name: eval(literal_def) = Field(description=\"宋詞詞牌\")\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=requirements)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "\n",
    "classes = '\\n'.join(v for v in values)\n",
    "\n",
    "\n",
    "system_template = dedent(f\"\"\"\n",
    "                  You are a help AI assistant specialized at Chinese literature, especially the 宋詞。\n",
    "                  You are assigned with a task of classify the given poem.\n",
    "                  The outcomes will be from one of these candidates:\n",
    "\n",
    "                  {classes}\n",
    "                  \"\"\")\n",
    "\n",
    "human_template = dedent(\"\"\"\n",
    "                 poem: {query}\n",
    "                 format instruction: {format_instructions}\n",
    "                 \"\"\")\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"query\"],\n",
    "                    \"partial_variables\": {\"format_instructions\": \n",
    "                                          format_instructions}}}\n",
    "\n",
    "classification_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "classification_pipeline = classification_prompt_template|model|output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3cd656-9e85-4521-b6a4-9d41ab37a71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_pipeline.invoke({\"query\": df_poem.loc[0]['詞文']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3462e2da-f29c-4919-bec8-e395e2de711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poem.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d98c96b-2df7-4a8c-a785-3c23c8e9514d",
   "metadata": {},
   "source": [
    "詞牌檢測似乎很困難，可能需要依賴一些專業知識來進行特徵抽取。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b2296d-901a-43a6-b441-7b4abc65ac22",
   "metadata": {},
   "source": [
    "# **** 預計第一個小時結束 ****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aee5c0b-7804-49bf-bd77-ff8203de6209",
   "metadata": {},
   "source": [
    "## HR: Job-Applicant Matching\n",
    "\n",
    "- 效率和速度： LLMs 能夠快速處理和分析大量申請，相較於人工審查，顯著縮短初步篩選所需的時間。\n",
    "\n",
    "- 一致性和公平性： LLMs 對所有申請應用相同的標準，最小化人為偏見，確保初步篩選過程的公平性。\n",
    "\n",
    "- 詳細分析： LLMs 能夠分析複雜的語言模式，從簡歷、求職信和其他申請材料中提取相關信息，識別符合工作要求的關鍵技能和資格。\n",
    "\n",
    "- 自定義和靈活性： LLMs 可以根據具體的工作要求自定義優先考慮的技能和經驗，允許更有針對性的篩選過程。\n",
    "\n",
    "- 可擴展性： LLMs 能夠同時處理大量申請，非常適合接收大量申請人的組織。\n",
    "\n",
    "- 成本效益： 通過自動化申請篩選的初始階段，LLMs 可以減少對大量人力資源的需求，從而降低運營成本。\n",
    "\n",
    "- 持續改進： LLMs 可以根據反饋和新數據持續進行訓練和改進，隨著時間的推移提高其準確性和有效性。-\n",
    "\n",
    "- 提升候選人經驗： 更快的回應時間和更一致的評估可以改善整體候選人經驗，因為申請人更有可能及時收到反饋。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39b5024-206c-4bdf-9574-8753f01ca46f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(\"https://www.techjobasia.com/zh-Hant/jobs/GMMlhU0qSayr6ZwTB0U6zA---Software-Engineer-(ReactJS)\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46edd4df-89c2-49ca-9e19-d6ae0eb6adb7",
   "metadata": {},
   "source": [
    "###  1. 發送 GET 請求到指定的 URL\n",
    "\n",
    "- 這行程式碼向指定的 URL 發送 HTTP GET 請求，並將響應儲存在 response 變數中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a779c48-badd-423e-889c-a57ece76a957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "\n",
    "url = \"https://www.techjobasia.com/zh-Hant/jobs/GMMlhU0qSayr6ZwTB0U6zA---Software-Engineer-(ReactJS)\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e681fd9e-5ba2-4c75-8bc3-4d41d747aaa4",
   "metadata": {},
   "source": [
    "### 2. 獲取響應的內容\n",
    "\n",
    "- 這行程式碼將響應的內容作為文字字串提取，並將其儲存在 html_content 變數中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7947d4-5e33-4b1c-88b5-ba375e62b2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee3cd12-f11a-4a7b-8f7e-ab8b16ada802",
   "metadata": {},
   "source": [
    "### 3. 解析 HTML 內容\n",
    "\n",
    "- 這行程式碼使用 BeautifulSoup 解析儲存在 html_content 中的 HTML 內容，並創建一個名為 soup 的 BeautifulSoup 對象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583fc8ac-d92b-4762-afd0-e1d1de8c3e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa763c1-d8c7-4c82-9528-0ef7ea564112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046accba-5203-43df-b1c8-b5703601d3b3",
   "metadata": {},
   "source": [
    "### 4. 移除所有 CSS 樣式標籤\n",
    "\n",
    "- 這個循環找到解析後的 HTML 內容中的所有 <style> 標籤，並使用 decompose() 方法將它們移除。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ab74e8-4e7c-4a92-b5ea-af3db43d1082",
   "metadata": {},
   "outputs": [],
   "source": [
    "for style in soup.find_all('style'):\n",
    "    style.decompose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5edde0-9fb5-43d4-bfb9-6a6b2c146d7e",
   "metadata": {},
   "source": [
    "### 6. 提取並打印僅包含文字的內容\n",
    "\n",
    "- 這行程式碼從解析後的 HTML 中提取文字內容，使用換行符將元素分隔，並將其儲存在 text_content 變數中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c797f3bc-33c2-4d67-88e2-b110f0d2da83",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_content = soup.get_text(separator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba34a7e-4ebd-4ea7-b738-54995862f082",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cacc52-b827-47d3-8f05-399360212b76",
   "metadata": {},
   "source": [
    "### 7. 清理文字內容\n",
    "\n",
    "- 這行程式碼通過移除每行的首尾空白並丟棄空行來清理提取的文字內容。清理後的文字儲存在 cleaned_text 變數中。\n",
    "- 將清理後的文字內容打印到控制台。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6ff06a-d842-465d-b4f4-3e7282824e15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_text = '\\n'.join(line.strip() for line in text_content.splitlines() if line.strip())\n",
    "\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabae79b-382d-4626-a526-af3e781ab3ca",
   "metadata": {},
   "source": [
    "### 8. 使用LLM提取工作相關訊息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a31c4e7-395e-4770-a21f-8f21aee07e28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template= dedent(\"\"\"\n",
    "          Extract the job description part of the text: {content} \n",
    "          \"\"\")\n",
    "\n",
    "human_prompt = PromptTemplate(template=template)\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "    \n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([human_message])\n",
    "\n",
    "pipeline_ = chat_prompt_template|model|StrOutputParser()\n",
    "\n",
    "job_description = pipeline_.invoke({\"content\": cleaned_text})\n",
    "\n",
    "print(job_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64fc577-f6fc-4e75-bf5c-e7f0b1b1d14a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 9. 將上述步驟打包成函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825038f7-7d58-4eb6-ad24-a41c14e6a322",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain, Runnable\n",
    "\n",
    "def parsing_process(url):\n",
    "    \"\"\"\n",
    "    Fetches and extracts text content from a given URL.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL of the web page to fetch and parse.\n",
    "\n",
    "    Returns:\n",
    "    str: Cleaned text content extracted from the web page.\n",
    "\n",
    "    Raises:\n",
    "    requests.exceptions.RequestException: If an error occurs while fetching the URL.\n",
    "\n",
    "    Notes:\n",
    "    - This function sends a GET request to the specified URL.\n",
    "    - It uses BeautifulSoup to parse the HTML content of the response.\n",
    "    - Any <style> tags in the HTML are removed to extract only textual content.\n",
    "    - The extracted text is cleaned by removing extra whitespace and empty lines.\n",
    "    \"\"\"\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Get the content of the response\n",
    "    html_content = response.text\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    for style in soup.find_all('style'):\n",
    "        style.decompose()\n",
    "\n",
    "    # Extract and print only the text content\n",
    "    text_content = soup.get_text(separator='\\n')\n",
    "\n",
    "    # Clean up the text (optional)\n",
    "    cleaned_text = '\\n'.join(line.strip() for line in text_content.splitlines() if line.strip())\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ceff2d-17e0-4240-bdd1-db3b07636429",
   "metadata": {},
   "source": [
    "拿一個1111來試試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f8b0ae-efbc-44b2-943b-f766a6e7ea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.1111.com.tw/job/131976692/\"\n",
    "cleaned_text = parsing_process(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dda6dfb-965c-4c99-8846-a5a9721feda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipeline_.invoke({\"content\": cleaned_text}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d184f4-d1aa-40ce-a619-1ca1230c5738",
   "metadata": {},
   "source": [
    "這個過程中牽涉到從網路上讀取數據，本地數據處理等等。適合用異步流來進行加速:\n",
    "\n",
    "目前程式是 同步阻塞 的：\n",
    "\n",
    "- requests.get(url) → 會阻塞直到網路請求完成。\n",
    "\n",
    "- BeautifulSoup 的解析則是 CPU 本地操作，速度通常不是瓶頸。\n",
    "\n",
    "所以：\n",
    "\n",
    "如果你一次只抓單一 URL，沒必要用 async，因為主要瓶頸就是等待那一次請求。\n",
    "\n",
    "如果要抓 多個 URL，那麼改用 非同步（async/await + aiohttp） 或 多執行緒 / 多處理 才能顯著加速，因為你能同時發送多個請求，避免 I/O 等待造成浪費。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e90873c-749e-48da-bbf7-23314cae33e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "@chain\n",
    "async def async_parsing_process(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches and extracts cleaned text content from a given URL asynchronously.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL of the web page to fetch and parse.\n",
    "\n",
    "    Returns:\n",
    "    str: Cleaned text content extracted from the web page.\n",
    "\n",
    "    Raises:\n",
    "    aiohttp.ClientError: If an error occurs while fetching the URL.\n",
    "    \"\"\"\n",
    "\n",
    "    # request 不支援 asynchronization\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(url) as response:\n",
    "            html_content = await response.text()\n",
    "            soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "            # 移除 style 和 script\n",
    "            for tag in soup([\"style\", \"script\"]):\n",
    "                tag.decompose()\n",
    "\n",
    "            # 提取文字\n",
    "            text_content = soup.get_text(separator=\"\\n\")\n",
    "\n",
    "            # 清理空白與空行\n",
    "            cleaned_text = \"\\n\".join(\n",
    "                line.strip() for line in text_content.splitlines() if line.strip()\n",
    "            )\n",
    "\n",
    "            return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade04f01-7a4c-46e6-9e2d-b892f4058c40",
   "metadata": {},
   "source": [
    "若是我們使用LCEL，pipeline 就是這個樣子:\n",
    "\n",
    "1. 資料提取與清洗\n",
    "2. 使用LLM進行最後的數據提煉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab11d67e-3993-4ccb-86eb-6e6e3831f0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description_pipeline = RunnablePassthrough.assign(content=itemgetter(\"url\")|async_parsing_process)|chat_prompt_template|model|StrOutputParser()\n",
    "\n",
    "job_result = await job_description_pipeline.ainvoke({\"url\": url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695babfd-ee0a-49cd-9082-0cd6c1ad8386",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(job_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0679586-30a0-4384-84aa-15642bd19559",
   "metadata": {},
   "source": [
    "## 履歷提取\n",
    "\n",
    "- 網路上公開資料 (把工作提取的Prompt 稍微修一下就行，網路抓數據的流程是一樣的)\n",
    "- Word\n",
    "- PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c682487c-dca0-4368-ab9b-d9e95bb4e767",
   "metadata": {},
   "source": [
    "### 提取 Word 內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d1ff36-bda0-49ca-85f9-6bdc41d8f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b927ef-5df2-4e9d-8e92-ff227cdc47fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.oxml.table import CT_Tbl\n",
    "from docx.oxml.text.paragraph import CT_P\n",
    "from docx.table import Table\n",
    "from docx.text.paragraph import Paragraph\n",
    "\n",
    "def iter_block_items(parent):\n",
    "    \"\"\"\n",
    "    Yield each paragraph and table in document order.\n",
    "    \"\"\"\n",
    "    for child in parent.element.body:\n",
    "        if isinstance(child, CT_P):\n",
    "            yield Paragraph(child, parent)\n",
    "        elif isinstance(child, CT_Tbl):\n",
    "            yield Table(child, parent)\n",
    "\n",
    "# Load the Word document\n",
    "doc = Document(os.path.join(\"tutorial\", \"LLM+Langchain\", \"Week-3\", \"Resume.docx\"))\n",
    "\n",
    "content = []\n",
    "\n",
    "for block in iter_block_items(doc):\n",
    "    if isinstance(block, Paragraph):\n",
    "        text = block.text.strip()\n",
    "        if text:  # skip empty paragraphs\n",
    "            content.append(text)\n",
    "    elif isinstance(block, Table):\n",
    "        for row in block.rows:\n",
    "            row_data = [cell.text.strip() for cell in row.cells]\n",
    "            content.append(\"\\t\".join(row_data))\n",
    "\n",
    "# Combine everything in order\n",
    "full_text = \"\\n\".join(content)\n",
    "\n",
    "print(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60969101-e529-4f8a-a6b5-d38fef4bee66",
   "metadata": {},
   "source": [
    "### 打包成異步流Langchain Runnable\n",
    "\n",
    "python-docx 本身是一個 同步阻塞的函式庫 —— 它在讀檔、解析 .docx 時會佔用 Python 主執行緒。\n",
    "\n",
    "在 async/await 的應用場景中（例如 LangChain agent、async pipeline、Web server），如果直接呼叫同步的 python-docx，會讓 事件迴圈被卡住，其他 async 任務無法並行進行。\n",
    "\n",
    "await asyncio.to_thread(func, *args)\n",
    "\n",
    "把這個 同步阻塞的工作丟到背景 thread pool 執行，從而讓事件迴圈保持「非阻塞」。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ec6413-9c55-461e-9a5c-41d45fa5beea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因為python-docx是同步流，所以我們需要用asyncio.to_thread將其重新包裝成異步流\n",
    "import asyncio\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "class DocxExtractor(Runnable):\n",
    "    \"\"\"LangChain Runnable that extracts text (paragraphs + tables) from a Word file.\"\"\"\n",
    "\n",
    "    async def ainvoke(self, filename: str, config: Dict[str, Any] | None = None) -> str:\n",
    "        \"\"\"\n",
    "        python-docx 是同步阻塞的函式庫，若直接在 async 環境呼叫會卡住事件迴圈，因此需用 asyncio.to_thread 將其包裝，\n",
    "        讓阻塞操作在背景執行緒執行，避免阻塞其他非同步任務。\n",
    "        \"\"\"\n",
    "        \n",
    "        return await asyncio.to_thread(self._extract, filename)\n",
    "\n",
    "    def invoke(self, filename: str, config=None) -> str:\n",
    "        # synchronous version directly calls the sync helper\n",
    "        return self._extract(filename)\n",
    "    \n",
    "    def _extract(self, filename: str) -> str:\n",
    "        doc = Document(filename)\n",
    "        content = []\n",
    "\n",
    "        for block in iter_block_items(doc):\n",
    "            if isinstance(block, Paragraph):\n",
    "                text = block.text.strip()\n",
    "                if text:\n",
    "                    content.append(text)\n",
    "            elif isinstance(block, Table):\n",
    "                for row in block.rows:\n",
    "                    row_data = [cell.text.strip() for cell in row.cells]\n",
    "                    content.append(\"\\t\".join(row_data))\n",
    "\n",
    "        return \"\\n\".join(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb64b7e-a80c-4bd8-8977-26b00e3538a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = DocxExtractor()\n",
    "result = await extractor.ainvoke(os.path.join(\"tutorial\", \"LLM+Langchain\", \"Week-3\", \"Resume.docx\"))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbc4074-7e4d-48be-bd9e-6db83b4010d1",
   "metadata": {},
   "source": [
    "這個 Word 內容提取是通用型的，不限於簡歷，也可以提取公文或其他文件文字內容。它完全基於程式邏輯，不涉及大語言模型，所以不需要額外付費。並非所有應用都需要大語言模型，有時候你只是需要一台 50cc 小型代步車，沒必要自己去打造一台 F1 賽車。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd3d7fb-c61f-4499-aa28-bb9b3db7a5e3",
   "metadata": {},
   "source": [
    "### 提取 PDF 內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13e82f4-c89b-4b6e-8c5d-85c3739dcb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "def extract_pdf(filename: str) -> str:\n",
    "    reader = PdfReader(filename)\n",
    "    content = []\n",
    "    for page in reader.pages:\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            content.append(text.strip())\n",
    "    return \"\\n\".join(content)\n",
    "\n",
    "# Example\n",
    "print(extract_pdf(os.path.join(\"tutorial\", \"LLM+Langchain\", \"Week-3\", \"Resume.pdf\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0cd678-1737-4d22-946d-c5dbb68b2240",
   "metadata": {},
   "source": [
    "異步流版本:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1448d256-52df-42f3-ae5c-ed21666b6f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PdfExtractor(Runnable):\n",
    "    \"\"\"LangChain Runnable that extracts text from a PDF file.\"\"\"\n",
    "\n",
    "    async def ainvoke(self, filename: str, config: Dict[str, Any] | None = None) -> str:\n",
    "        return await asyncio.to_thread(self._extract, filename)\n",
    "\n",
    "    def invoke(self, filename: str, config: Dict[str, Any] | None = None) -> str:\n",
    "        return self._extract(filename)\n",
    "\n",
    "    def _extract(self, filename: str) -> str:\n",
    "        reader = PdfReader(filename)\n",
    "        content = []\n",
    "        for page in reader.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                content.append(text.strip())\n",
    "        return \"\\n\".join(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bcd5d5-639e-4191-9fab-fb58b553f221",
   "metadata": {},
   "source": [
    "直接使用根據 Docx 提取的內容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d110ac4d-f835-4c4d-b544-02070c2dc2bd",
   "metadata": {},
   "source": [
    "技術上確實可以直接在讀取檔案的同時進行匹配，但在系統設計上，將「檔案內容提取」獨立成工作流會更恰當，因為這樣更有利於維護、監控與後續擴展，再將提取結果交由下游流程進行匹配或其他處理。\n",
    "\n",
    "1. 技術上可行：\n",
    "    確實可以「on-the-fly」直接讀取檔案，然後立刻做比對或匹配，這在 demo 或小規模應用時沒問題。\n",
    "\n",
    "2. 務實上的考量：\n",
    "    在實際系統設計中，把「檔案讀取 / 解析」獨立成一個模組或工作流會更好：\n",
    "\n",
    "    - 維護性：解析邏輯獨立，容易替換不同檔案類型（Word、PDF、Excel…）。\n",
    "\n",
    "    - 可監控：可以針對「解析失敗」做監控和錯誤處理，不會和匹配邏輯糾纏在一起。\n",
    "\n",
    "    - 可擴展：之後不只做匹配，還可能做索引、摘要、分類等，提前抽離流程更有彈性。\n",
    "\n",
    "3. 流程建議：\n",
    "\n",
    "    Step 1：檔案 → 讀取 & 提取文字（抽象成 Extractor 工作流）。\n",
    "\n",
    "    Step 2：抽取內容 → 匹配 / NLP 處理 / 下游任務。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a74bf02-61db-4b78-ba98-013ebc8f77eb",
   "metadata": {},
   "source": [
    "### Extract the Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7a14c9-84d9-4a67-be3f-feefe74244b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "system_template = dedent(\"\"\"\n",
    "                  I am going to give you a template for your output. \n",
    "                  CAPITALIZED WORDS are my placeholders. \n",
    "                  Fill in my placeholders with your output. Please preserve \n",
    "                  the overall formatting of my template. My template is:\n",
    "\n",
    "                  *** Working Experience:*** WORKING EXPERIENCE \n",
    "                  *** Education:*** EDUCATION\n",
    "                  *** Skills:*** SKILLS\n",
    "\n",
    "                  I will give you the data to format in the next prompt. \n",
    "                  Create a resume using my template.\n",
    "                  \"\"\")\n",
    "\n",
    "human_template = \"\"\"\n",
    "                 {query}\n",
    "                 \"\"\"\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"query\"]}}\n",
    "\n",
    "resume_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "resume_pipeline = resume_prompt_template|model|StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81388d28-d18a-4750-8151-4cb394ad71e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resume_output = resume_pipeline.invoke(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77226af-b87d-4803-b694-02aa55539298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(resume_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6823b73a-8c52-45e6-8dfd-901732a25767",
   "metadata": {},
   "source": [
    "### 履歷和工作的匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2b73ec-d204-47b0-9d31-f103b609ba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Output(BaseModel):\n",
    "\n",
    "    result: Literal['Yes', 'No'] = Field(description=\"If the candidate is a good fit, either Yes or No\")\n",
    "    reason: str = Field(description=\"Applicant - Job matching\")\n",
    "    \n",
    "output_parser = PydanticOutputParser(pydantic_object=Output)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1390f7e-9f85-4a71-9860-f017b10ce879",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_template = dedent(\"\"\"\n",
    "                  You are an AI assistant acting as an experienced senior \n",
    "                  recruiter in IT field.\n",
    "                  \n",
    "                  You are assigned a task of identifying if an applicant, \n",
    "                  based on the description in the resume, is a good match to the described job. \n",
    "                    \n",
    "                  You always do the best work you can. You are highly \n",
    "                  analytical and pay close attention to details. \n",
    "                  \"\"\")\n",
    "\n",
    "human_template = dedent(\"\"\"\n",
    "                 Job description: {job}\n",
    "                 resume: {resume}\n",
    "                 output format instructions: {format_instructions}\n",
    "                 \"\"\")\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"job\", \"resume\"],\n",
    "                    \"partial_variables\": {'format_instructions': format_instructions}}}\n",
    "\n",
    "match_prompt_template = build_standard_chat_prompt_template(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40117f4a-6040-49d8-8f1e-6c747de37e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c952973-2f02-49d8-a807-4a16dbd692f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee8e78e-49b7-481b-9a26-f1f9ddcdb83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_pipeline = match_prompt_template|model|output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2a6215-3e77-4736-9f4c-54dcc9a6c130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "matching_output = matching_pipeline.invoke({\"job\":job_result, \"resume\":result})\n",
    "\n",
    "print(matching_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bca2d1-d639-49f3-a874-45106ef1e8a5",
   "metadata": {},
   "source": [
    "# MLFlow Part 1\n",
    "\n",
    "重啟Notebook來節省硬體資源\n",
    "\n",
    "- pip install mlflow\n",
    "- 在CLI: mlflow server --host 127.0.0.1 --port 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7828b24-ef0e-4ad3-880d-100506ae3943",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef2805f4-b256-4043-9f8e-09859ae64d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omega\\miniconda3\\envs\\aicg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# Load the Iris dataset\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define the model hyperparameters\n",
    "params = {\n",
    "    \"solver\": \"lbfgs\",\n",
    "    \"max_iter\": 1000,\n",
    "    \"multi_class\": \"auto\",\n",
    "    \"random_state\": 8888,\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "lr = LogisticRegression(**params)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e269cfda-740c-40cb-8504-93134c7be3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/20 08:49:39 INFO mlflow.tracking.fluent: Experiment with name 'MLflow Quickstart' does not exist. Creating a new experiment.\n",
      "C:\\Users\\omega\\miniconda3\\envs\\aicg\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading artifacts: 100%|████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 106.02it/s]\n",
      "Successfully registered model 'tracking-quickstart'.\n",
      "2025/09/20 08:50:57 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: tracking-quickstart, version 1\n",
      "Created version '1' of model 'tracking-quickstart'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run Week-3-MLFlow at: http://127.0.0.1:8080/#/experiments/748449508217935171/runs/ec4a35360ebc46949ca2e1195276eff6\n",
      "🧪 View experiment at: http://127.0.0.1:8080/#/experiments/748449508217935171\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "# 通知mlflow要把紀錄送去哪裡\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n",
    "\n",
    "# Create a new MLflow Experiment\n",
    "mlflow.set_experiment(\"MLflow Quickstart\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"Week-3-MLFlow\") as run:\n",
    "\n",
    "    # 你需要這個來確保結果都會被記錄在同一個Run裡\n",
    "    run_id = run.info.run_id\n",
    "\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    # Log the loss metric\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "    # Infer the model signature\n",
    "    signature = infer_signature(X_train, lr.predict(X_train))\n",
    "\n",
    "    # Log the model, which inherits the parameters and metric\n",
    "    model_info = mlflow.sklearn.log_model(\n",
    "        sk_model=lr,\n",
    "        # model local name\n",
    "        name=\"iris_model\",\n",
    "        signature=signature,\n",
    "        input_example=X_train,\n",
    "        # model global name\n",
    "        registered_model_name=\"tracking-quickstart\",\n",
    "    )\n",
    "\n",
    "    # Set a tag that we can use to remind ourselves what this model was for\n",
    "    mlflow.set_logged_model_tags(\n",
    "        model_info.model_id, {\"Training Info\": \"Basic LR model for iris data\"}\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d48f6fa6-d37f-4c5c-b5b4-87c5cbd2c51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models:/m-66070a7f981d41b28c0ae82d674d7fd7'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_info.model_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e1e550-78eb-4012-afc8-39dab70a9487",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f015683b-5035-4af0-9a8a-7a88fe0d2f10",
   "metadata": {},
   "source": [
    "## 本地執行\n",
    "\n",
    "1. mlflow.pyfunc.load_model(model_info.model_uri)\n",
    "\n",
    "- 這裡用的是 model_info.model_uri，它指向剛剛在 local run 裡 mlflow.sklearn.log_model(...) 存下來的 artifact 路徑。\n",
    "\n",
    "- 因為你在 同一個 local tracking server (或預設的本地檔案系統) 執行，所以可以直接載入。\n",
    "\n",
    "2. predictions = loaded_model.predict(X_test)\n",
    "\n",
    "- mlflow.pyfunc 會包裝成一個 通用 Python function 模型 (不管底層是 sklearn、pytorch、xgboost...)。\n",
    "\n",
    "- 所以你可以直接 .predict(...)，得到預測結果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14b6d322-836a-4a45-a648-2e23ed0ad6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:03<00:00,  2.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>actual_class</th>\n",
       "      <th>predicted_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                6.1               2.8                4.7               1.2   \n",
       "1                5.7               3.8                1.7               0.3   \n",
       "2                7.7               2.6                6.9               2.3   \n",
       "3                6.0               2.9                4.5               1.5   \n",
       "\n",
       "   actual_class  predicted_class  \n",
       "0             1                1  \n",
       "1             0                0  \n",
       "2             2                2  \n",
       "3             1                1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model back for predictions as a generic Python Function model\n",
    "\n",
    "loaded_model = mlflow.pyfunc.load_model(model_info.model_uri)\n",
    "\n",
    "predictions = loaded_model.predict(X_test)\n",
    "\n",
    "iris_feature_names = datasets.load_iris().feature_names\n",
    "\n",
    "result = pd.DataFrame(X_test, columns=iris_feature_names)\n",
    "result[\"actual_class\"] = y_test\n",
    "result[\"predicted_class\"] = predictions\n",
    "\n",
    "result[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cd8c06-2e45-486a-8434-e181073c0827",
   "metadata": {},
   "source": [
    "## 模擬遠端執行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c5283b5-4db8-4930-af73-5e29d35cd5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 1, Stage: None, Run ID: ec4a35360ebc46949ca2e1195276eff6\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# 列出特定模型的所有版本\n",
    "versions = client.search_model_versions(\"name='tracking-quickstart'\")\n",
    "\n",
    "for v in versions:\n",
    "    print(f\"Version: {v.version}, Stage: {v.current_stage}, Run ID: {v.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d026abbb-e2cd-4e47-8043-cb17b7b4b1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:04<00:00,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "model_uri = \"models:/tracking-quickstart/1\"    # version 1\n",
    "\n",
    "model_remote = mlflow.sklearn.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a9aa4e6-cbe8-4230-9ebf-e44fdd4e08fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>actual_class</th>\n",
       "      <th>predicted_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                6.1               2.8                4.7               1.2   \n",
       "1                5.7               3.8                1.7               0.3   \n",
       "2                7.7               2.6                6.9               2.3   \n",
       "3                6.0               2.9                4.5               1.5   \n",
       "\n",
       "   actual_class  predicted_class  \n",
       "0             1                1  \n",
       "1             0                0  \n",
       "2             2                2  \n",
       "3             1                1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model_remote.predict(X_test)\n",
    "\n",
    "iris_feature_names = datasets.load_iris().feature_names\n",
    "\n",
    "result = pd.DataFrame(X_test, columns=iris_feature_names)\n",
    "result[\"actual_class\"] = y_test\n",
    "result[\"predicted_class\"] = predictions\n",
    "\n",
    "result[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79427145-e95f-4af5-9a97-d7f23d91ee79",
   "metadata": {},
   "source": [
    "### 模型 Alias（別名）\n",
    "\n",
    "Alias 是一種可變（mutable）的命名引用，可指向某個註冊模型（registered model）的特定版本。這對於隨後更新部署模型卻不想改程式碼時非常方便。例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79e35ea0-3344-494f-bd8d-742df38bf6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "client.set_registered_model_alias(\n",
    "    name=\"tracking-quickstart\",\n",
    "    alias=\"champion\",\n",
    "    version=\"1\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "769a174f-1123-49c7-a258-f97514252655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:01<00:00,  3.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# 以 alias 載入模型\n",
    "model = mlflow.sklearn.load_model(\"models:/tracking-quickstart@champion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c80fa78-04aa-4e94-94bc-52fafd0d96ed",
   "metadata": {},
   "source": [
    "也可以用 API 刪除 alias："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e52ced0-6b38-4cbb-aa39-69459213714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_registered_model_alias(\"tracking-quickstart\", \"champion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b057d9e0-bfa7-4b1f-8b5a-d2424a9fd2b5",
   "metadata": {},
   "source": [
    "### 模型 Tags（標籤）\n",
    "\n",
    "MLflow 支援兩層 Tag：\n",
    "\n",
    "- Registered model-level tags：整體模型的 metadata，例如用途、團隊等資訊。\n",
    "\n",
    "- Model version-level tags：針對每個版本做不同註記，例如驗證狀態、效能資訊等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c4c242-0268-48dd-a35b-e3dfba7cdedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registered model-level tag\n",
    "client.set_registered_model_tag(\"tracking-quickstart\", \"task\", \"classification\")\n",
    "\n",
    "# Version-level tag\n",
    "client.set_model_version_tag(\"tracking-quickstart\", version=\"1\", key=\"validation_status\", value=\"approved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ab0cb6-f3bd-4e5a-bc5d-c4dfb53f1c16",
   "metadata": {},
   "source": [
    "## Langchain with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbde9b54-4837-4edb-82a3-b23314373e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e1efb6-567f-4ae1-86ec-70b530ffd961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from langchain.chains import LLMChain ## Langchain boilerplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from src.initialization import credential_init\n",
    "credential_init()\n",
    "\n",
    "# Initialize the OpenAI model and the prompt template\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")\n",
    "\n",
    "experiment = \"Week-3\"\n",
    "\n",
    "# Create the LLMChain with the specified model and prompt\n",
    "# 最早我也是用這個\n",
    "chain = LLMChain(llm=model, prompt=prompt)\n",
    "\n",
    "# Log the LangChain LLMChain in an MLflow run\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n",
    "\n",
    "# Create a new MLflow Experiment\n",
    "mlflow.set_experiment(experiment)\n",
    "\n",
    "with mlflow.start_run(run_name=\"LLMChain\") as run:\n",
    "    logged_model = mlflow.langchain.log_model(chain, \n",
    "                                              name=\"langchain_model\",\n",
    "                                              registered_model_name=\"LLMChain_Demo\")\n",
    "\n",
    "    prompt_path = os.path.join(\"tutorial\", \"LLM+Langchain\", \"Week-3\" ,\"prompt.txt\")\n",
    "    with open(prompt_path, \"w\") as f:\n",
    "        f.write(prompt.template)\n",
    "    # Log the prompt as an artifact\n",
    "    mlflow.log_artifact(prompt_path, artifact_path=\"prompts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078c63b2-6f77-4602-9bdc-6dc8452c760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=model, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797d329a-8cc2-4ab3-92f3-2897a04125b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"product\": \"colorful socks\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38074620-db41-430b-b0df-f06d70f779df",
   "metadata": {},
   "source": [
    "If you load the model like the MLflow tutorial suggests, we will get an error message because:\n",
    "\n",
    "The core problem is that ChatOpenAI contains fields like openai_api_key: SecretStr (a Pydantic type). When MLflow serializes the chain, those SecretStr objects get dumped to YAML with a tag like:\n",
    "\n",
    "!!python/object/pydantic.types.SecretStr\n",
    "\n",
    "But when you later reload the model, MLflow’s YAML loader doesn’t know how to re-instantiate SecretStr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6c3397-200c-42cc-824e-572abd46003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mlflow\n",
    "\n",
    "# from src.initialization import credential_init\n",
    "\n",
    "# credential_init()\n",
    "\n",
    "# # Log the LangChain LLMChain in an MLflow run\n",
    "# mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n",
    "\n",
    "# # Reload chain (structure only)\n",
    "# loaded_model = mlflow.pyfunc.load_model(\"models:/LLMChain_Demo/3\")\n",
    "\n",
    "# # Re-attach the LLM with fresh credentials\n",
    "# loaded_model.llm = ChatOpenAI(\n",
    "#     openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "#     model_name=\"gpt-4o-mini\",\n",
    "#     temperature=0\n",
    "# )\n",
    "\n",
    "# # Predict using the loaded model\n",
    "# print(loaded_model.predict([{\"product\": \"colorful socks\"}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c15b26-53ea-4272-b4b6-f4ed664c36e0",
   "metadata": {},
   "source": [
    "An Universal Solution\n",
    "\n",
    "1. Create a python script file for your pipeline\n",
    "2. upload the `model` into your MLflow server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44d0fa0-5cf9-4ac2-8698-1fa82e4b1817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mlflow\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "\n",
    "from src.io.path_definition import get_project_dir\n",
    "\n",
    "# You need to know what you will put into it and what you will get out of it.\n",
    "input_schema = Schema([ColSpec(\"string\", \"input\")])\n",
    "output_schema = Schema([ColSpec(\"string\", \"product\"),\n",
    "                        ColSpec(\"string\", \"text\")])\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "\n",
    "model_path = os.path.join(get_project_dir(), 'tutorial', 'LLM+Langchain', \"Week-3\", \"llmchain_mlflow_experiment.py\")\n",
    "\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n",
    "\n",
    "\"\"\"\n",
    "The registry is a separate feature: you must either (a) log with a registered_model_name argument, \n",
    "or (b) promote a logged model to the registry manually in the UI/CLI.\n",
    "\"\"\"\n",
    "\n",
    "with mlflow.start_run(run_name=\"LLMChain\") as run:\n",
    "    \n",
    "    mlflow.log_artifact(model_path, artifact_path=\"source_code\")\n",
    "\n",
    "    input_example = pd.DataFrame(data=[['colorful socks']], columns=['input'])\n",
    "    \n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        python_model=model_path,  # Define the model as the path to the Python file\n",
    "        name=\"langchain_model\",\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "        registered_model_name=\"LLMChain_Demo\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d882323-8175-44c8-9fdb-86e3278e4a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n",
    "\n",
    "loaded_model = mlflow.pyfunc.load_model(\"models:/LLMChain_Demo/17\")\n",
    "\n",
    "input_ = pd.DataFrame(data=[['歐姆尼賽亞的化身，會行走的大教堂: 帝皇級泰坦 (Imperator-class Titan )']], columns=['input'])\n",
    "\n",
    "loaded_model.predict(input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e534dc4a-8c61-450d-8c85-6680825716b2",
   "metadata": {},
   "source": [
    "你可以看到檔案會以Artifact形式下載，並且複寫\n",
    "\n",
    "換句話說 load_model 時要小心控制下載位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63dbd43-6a1f-4c4e-a069-de7b2a96d1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.pyfunc.load_model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13865a8f-ee3f-4d52-bb3f-95eecd615520",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
