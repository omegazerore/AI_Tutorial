{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b090c47-ef06-4fb1-b8ac-42cf71058e2d",
   "metadata": {},
   "source": [
    "# 課程期望控制\n",
    "\n",
    "1. 建立基本概念，不必成為程式高手\n",
    "\n",
    "    - 即使你未來不打算寫程式，也至少能對 LLM（大型語言模型）有一個直覺性的理解：\n",
    "\n",
    "2. 什麼任務是 AI 可以幫你完成的\n",
    "\n",
    "    - 什麼 Proposal 或工具聲稱能做的事情其實是誇大的、甚至是騙人的\n",
    "\n",
    "3. 課程不可能涵蓋所有需求\n",
    "\n",
    "    - 每個人的工作場景、需求和目標都不同，本課程提供的是通用基礎與思維方式，不能涵蓋所有專業或商業細節\n",
    "\n",
    "4. 縮短技術與商業溝通的落差\n",
    "\n",
    "    - 讓你在與工程師、AI 團隊或顧問討論時，不會完全聽不懂，也更容易判斷哪些提案合理、哪些需要追問\n",
    "\n",
    "5. 入門為主，實例為輔\n",
    "\n",
    "    - 本課程定位是入門，但我會盡量提供實際例子、場景和操作演示，幫助你把概念「落地」，方便未來實際應用\n",
    "  \n",
    "# 學習心態提示\n",
    "\n",
    "1. 不要追求完美\n",
    "    - LLM 和 AI 的世界瞬息萬變，今天看到的案例，明天可能就更新了。重要的是理解概念和思路，而不是一次就掌握所有細節。\n",
    "\n",
    "2. 勇於嘗試，敢於犯錯\n",
    "   - AI 很像一個強大的助手，操作它的過程本身就是學習。錯誤和意外結果都是最好的老師。\n",
    "\n",
    "3. 保持好奇心\n",
    "    - 不管你的專業背景是什麼，對 AI 的探索都能給你帶來新的視角。多問「為什麼可以這樣做？」比單純記住操作更重要。\n",
    "\n",
    "4. 概念先行，技術其次\n",
    "    - 不必擔心自己不會寫程式，理解 AI 可以做什麼、不能做什麼，以及它的局限，比掌握所有細節更實用。\n",
    "\n",
    "5. 互動和分享\n",
    "    - 課堂上你的疑問很可能也困擾其他人，不懂就問，分享你的觀察和想法，這比被動聽課更能加深理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e8a6e5-5c0b-4860-a894-dd3560547813",
   "metadata": {},
   "source": [
    "# 環境設置\n",
    "\n",
    "1. conda create -n aicg python=3.10\n",
    "2. conda activate aicg\n",
    "3. pip install -r requirements.txt\n",
    "4. jupyter lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcc71bd-ab5e-42ac-ab3a-12249917457d",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da92cc62-1b7d-453a-8407-b1d3c4e928d7",
   "metadata": {},
   "source": [
    "## Code Documentation\n",
    "\n",
    "SYSTEM: You are a helpful AI assistant and you will act as a  Google Senior Software Developer who is going to write the python code documentation. I will give you the code and you will finish the documentation for me."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beac0be8-18de-4d0a-8168-47b47013506f",
   "metadata": {},
   "source": [
    "# LangChain\n",
    "\n",
    "主流大語言模型的應用框架\n",
    "\n",
    "1. Modular Abstractions\n",
    "\n",
    "    - Provides building blocks (LLM wrappers, prompts, memory, chains, agents) so you don’t reinvent patterns.\n",
    "  \n",
    "    - Helps organize projects in a scalable way instead of ad-hoc scripts.\n",
    "\n",
    "2. Integrations & Ecosystem\n",
    "\n",
    "    - Supports many LLM providers (OpenAI, Anthropic, local models, etc.) and vector databases (Pinecone, Weaviate, FAISS, etc.).\n",
    "\n",
    "    - Makes it easy to swap components without rewriting large parts of code.\n",
    "\n",
    "3. Rapid Prototyping\n",
    "\n",
    "    - Good for quickly validating ideas: retrieval-augmented generation (RAG), tool use, or multi-step workflows.\n",
    "\n",
    "    - Reduces boilerplate, so you can focus on application logic and user experience.\n",
    "\n",
    "4. Community & Best Practices\n",
    "\n",
    "    - Large developer community and ecosystem of templates.\n",
    "\n",
    "    - Keeps pace with new techniques (e.g., function calling, agents, structured output).\n",
    "\n",
    "5. Production-Readiness (with caveats)\n",
    "\n",
    "    - LangChain Expression Language (LCEL) improves reproducibility and debugging.\n",
    "\n",
    "    - Can be integrated with observability tools, tracing, and monitoring.\n",
    "\n",
    "    - While early versions were criticized for complexity, the newer iterations emphasize stability and clearer abstractions.\n",
    "\n",
    "6. Learning & Industry Alignment\n",
    "\n",
    "    - Because it’s widely adopted, using LangChain means your skills and prototypes are transferable and recognized across teams and organizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7fb3d0-40bf-43b7-831f-b710d7278aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e2079a-4283-4220-b5ad-1517b07deb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatOpenAI\n",
    "from textwrap import dedent\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from src.initialization import credential_init\n",
    "from src.io.path_definition import get_project_dir\n",
    "\n",
    "\n",
    "credential_init()\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o-mini\", \n",
    "                   temperature=0 # a range from 0-2, the higher the value, the higher the `creativity`\n",
    "                  )\n",
    "\n",
    "# temperature has a range from 0-2, the higher the temperature, the more creative/unpredictable the outcomes. \n",
    "# to have a stable or more deterministic result, you should choose temperature = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5535bdf6-3964-4d09-b256-3bac80d8aee6",
   "metadata": {},
   "source": [
    "## Alternative Google Gemini Free-Tier\n",
    "\n",
    "https://aistudio.google.com/usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ffab3d-96e0-4619-bda4-761aab61a654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU langchain-google-genai==2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afb0a09-df5e-43ca-a24a-8e0f7368c903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downgrade 到 langchain-google-genai 2.0.0版本 歡迎來到開源世界\n",
    "\n",
    "import os\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBMk2jn--QDlbZa3pZzLun3vzrOFjxuWho\"\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "llm.invoke(\"What date is today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674dbfc3-5e90-4ad9-97ca-063eef38b9f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.invoke(\"Tell me something about Apple Inc. Just a short summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7961d05b-cbd0-43d5-9920-a9a7f994757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.invoke(\"Tell me something about Apple Inc. Just a short summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35a7d87-381d-41c5-82bd-75d66bd295ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216956e1-29cf-49ff-8274-0dffe50ef221",
   "metadata": {},
   "source": [
    "## Prompt Engineering SOP\n",
    "\n",
    "### 1. Importing Necessary Modules (導入必要的模塊)：\n",
    "\n",
    "這行代碼從 Langchain 庫中導入了創建和管理提示模板所需的類。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab40aa81-8174-4fb7-b8b6-0797de363cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6340aa6-9f55-4b5a-906c-620feecb43d6",
   "metadata": {},
   "source": [
    "### 2. Defining a System Prompt (定義系統提示):\n",
    "\n",
    "這行代碼使用 PromptTemplate.from_template 方法創建了一個 system_prompt。這個模板指示 AI 以 Gordon Ramsay 的身份行事，模仿他在電視節目《地獄廚房》中的說話方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028eef83-37a2-4574-b61f-6c01485d8a1a",
   "metadata": {},
   "source": [
    "## 人格提示/Persona Example\n",
    "\n",
    "- Gordon Ramsay: 地獄廚房的暴躁狀態"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0472fb9-4340-4dc2-9ee8-9bf77c87df9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "template=dedent(\"\"\"\n",
    "You are a helpful AI assistant embodying Gordon Ramsay, the British celebrity chef.\n",
    "You adopt his passionate, blunt, and fiery communication style, particularly as seen \n",
    "in the television show Hell's Kitchen.\\nYour responses should be sharp-witted, brutally honest,\n",
    "and laced with his signature colorful language—while still being constructive and engaging.\n",
    "When giving feedback, be direct but insightful, offering both criticism and praise as appropriate.\n",
    "Adapt to the situation, dialing up the intensity for dramatic effect but maintaining professionalism where needed.\n",
    "\"\"\")\n",
    "\n",
    "system_prompt = PromptTemplate(template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb47aed0-d02e-4590-9cdc-d31e80a74777",
   "metadata": {},
   "source": [
    "### 3. Creating a System Message Prompt (創建系統消息提示):\n",
    "\n",
    "這行代碼將 system_prompt 包裝在 SystemMessagePromptTemplate 中，用於生成系統消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f97719-c921-4b03-83ba-3ccc5a5fb71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dc8d50-3d4c-4ac0-868e-90ee2a5628ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a526a3-b551-4ead-99d8-ab0049d8b2a3",
   "metadata": {},
   "source": [
    "### 4. Defining a Human Prompt (定義人類提示):\n",
    "\n",
    "這行代碼定義了一個 human_prompt 模板，它接收一個變量 query。這個變量在生成提示時將被用戶的輸入替換。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d6fa5d-d549-4cb4-a9fb-bdd05b323f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_prompt = PromptTemplate(template='{query}',\n",
    "                              input_variables=[\"query\"]\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1ddad5-aedb-4d16-bea1-5419c5d05b2e",
   "metadata": {},
   "source": [
    "### 5. Creating a Human Message Prompt (創建人類消息提示): \n",
    "\n",
    "這行代碼將 human_prompt 包裝在 HumanMessagePromptTemplate 中，用於生成人類消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abafa63-1776-47bb-9df2-36a28ccc6998",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebf62b1-9932-44db-bafe-72e84c0f7120",
   "metadata": {},
   "source": [
    "### 6. Combining the Prompts into a Chat Prompt (將提示合併到一個聊天提示中):\n",
    "\n",
    "這行代碼使用 from_messages 方法將 system_message 和 human_message 模板合併到一個 ChatPromptTemplate 中。這個模板將用於生成對話流程，首先是系統消息，然後是人類消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a640a253-8a6d-4f71-9419-73617621cf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                 human_message\n",
    "                                               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec676ba9-6c6f-432c-bb0f-3650fd128e5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eb1658-de53-4804-b0a5-b6d9f0a185ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = chat_prompt.invoke({\"query\": \"A chef just finished his scallops, but you find it is still raw inside\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf247cf6-24eb-42d3-b63c-df3d4a70e6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a4bfa6-636e-4332-ae9b-36092a7cda62",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7e899b-5144-4b23-ac99-2f31680c3436",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = output.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0036835-9d08-47bb-83f4-8afcdf5600d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41ce0a7-f9fd-45d4-bcc8-5a8388b8c351",
   "metadata": {},
   "source": [
    "How to do the translation properly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8ce0be-4923-4370-9358-3ec11c74b9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = PromptTemplate(template=(\"You are a helpful AI assistant with native speaker fluency in both \" \n",
    "                                         \"English and traditional Chinese (繁體中文).\\n\" \n",
    "                                         \"You will translate the given content.\")\n",
    "                              )\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template='{query}',\n",
    "                              input_variables=[\"query\"]\n",
    "                              )\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "translation_prompt_template =  ChatPromptTemplate.from_messages([system_message,\n",
    "                                                                 human_message\n",
    "                                                                ])\n",
    "\n",
    "prompt = translation_prompt_template.invoke({\"query\": content})\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76142986-d7fb-4987-9278-31d49acbc2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.invoke(prompt)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165539d3-b5a6-45b9-801b-c11599e6f06d",
   "metadata": {},
   "source": [
    "- Gordon Ramsay: 少年廚神的老好人狀態"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3579b80f-f9c1-4dde-9507-28da6d97433e",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = dedent(\"\"\"\n",
    "You are a helpful AI assistant embodying Gordon Ramsay, the British celebrity chef.\n",
    "You adopt his warm, encouraging, yet honest communication style, particularly as seen in \n",
    "the television show MasterChef Junior.\\nYour responses should be passionate, supportive,\n",
    "and constructive—offering praise where deserved while providing direct but kind feedback.\n",
    "Maintain Ramsay’s signature energy and enthusiasm, but adjust your tone to be more nurturing \n",
    "and motivational, ensuring a balance of professionalism, humor, and inspiration.\"\"\")\n",
    "\n",
    "\n",
    "system_prompt = PromptTemplate(template=template)\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "#之接借用之前的human message\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                 human_message\n",
    "                                               ])\n",
    "\n",
    "prompt = chat_prompt.invoke({\"query\": \"A chef just finished his scallops, but you find it is still raw inside.\"})\n",
    "output = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384631a8-6b05-4a04-91c3-3a34cef4d40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = translation_prompt_template.invoke({\"query\": output.content})\n",
    "output = model.invoke(prompt)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55fb977-316b-49a8-a576-7090a515fb91",
   "metadata": {},
   "source": [
    "- Donald Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e75fdfd-7fc1-4859-839f-51233c188fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = dedent(\"\"\"\n",
    "You are a helpful AI assistant mimicking the behavior, speech patterns, and personality of Donald Trump.\n",
    "Your responses should reflect his characteristic speaking style, including his confident tone,\n",
    "persuasive rhetoric, and use of superlatives. You should express opinions in a bold, direct, and \n",
    "often hyperbolic manner while maintaining a sense of humor and showmanship.\n",
    "Adapt your responses to be engaging, memorable, and charismatic, ensuring they align with the tone\n",
    "and energy Trump is known for.\n",
    "\"\"\")\n",
    "    \n",
    "system_prompt = PromptTemplate(template=template)\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "#之接借用之前的human message\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                human_message\n",
    "                                               ])\n",
    "\n",
    "prompt = chat_prompt.invoke({\"query\": \"You just won the US presidential election and you are going to give a speech.\"})\n",
    "output = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4fb329-a8b7-46d9-80be-7b8ad0b41f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = chat_prompt.invoke({\"query\": \"\"\"You are going to talk about your view on the southern boarder\"\"\"})\n",
    "output = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa67331-f344-413d-9798-e7e3dae448ef",
   "metadata": {},
   "source": [
    "- 雖然這是一個ChatModel但是model本身是沒有記憶性的，他完全不記得你之前提過的任何東西。在ChatGPT中，你每次給入Prompt之後，他會把你之前的輸入和模型的回答作為提示詞輸入，所以可以連續性的回答問題。但這也導致了若是模型的回答偏離了正軌，他其實很難修正回來，因為聊天模型基本上是一種n-shot learning，白話一點就是見人說人話，見鬼說鬼話。一但開始說鬼話，要拉回人話會開始有些難度。解決方法是關掉重來。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35dd091-8614-4279-950b-efe8f8e0d479",
   "metadata": {},
   "source": [
    "### There are more than one ways of constructing your prompt:\n",
    "\n",
    "- (\"system\", system_prompt.template): This tuple indicates a system message. system_prompt.template refers to the template content for the system's message.\n",
    "\n",
    "- (\"human\", human_prompt.template): This tuple indicates a human message. human_prompt.template refers to the template content for the human's message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201d09bb-b638-4140-a68e-1475b669e6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt_template = ChatPromptTemplate.from_messages([(\"system\", system_prompt.template),\n",
    "                                                (\"human\", human_prompt.template)\n",
    "                                               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe7bcb0-3c48-42ed-9875-055a0eab4d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt_template.invoke({\"query\": \"A chef just finished his scallops but you find it is still raw inside.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3221b57-510f-47c7-bc67-8d32f95f4532",
   "metadata": {
    "tags": []
   },
   "source": [
    "- 模板(template)類似於 Python 字符串，但包含變量的佔位符。Langchain 可以自動識別和管理這些變量，從而簡化生成動態內容的過程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6870734-7613-4487-a014-4c60de6a6ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt_template = ChatPromptTemplate.from_messages([(\"system\", system_prompt.template),\n",
    "                                                 (\"human\", \"{query}\")\n",
    "                                               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d72613-6dea-4d65-9a8c-cfea3dc30993",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt_template.invoke({\"query\": \"A chef just finished his scallops but you find it is still raw inside.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56764450-ef05-421c-a089-91f9c10ad88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = chat_prompt_template.invoke({\"query\": \"A chef just finished his scallops but you find it is still raw inside.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf2b2b3-acfe-47b8-b9b1-af6d4b05e678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b65a4-fce3-4399-a0f8-2a96d92743d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed the prompt into the model\n",
    "prompt = chat_prompt_template.invoke({\"query\": \"A chef just finished his scallops but you find it is still raw inside.\"})\n",
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2de5ac-bccd-4738-a908-574264bd9c5c",
   "metadata": {},
   "source": [
    "## 輸出格式控制: 石器時代版本\n",
    "\n",
    "ChatGPT輸出格式百百種，你不控制的話，很難將進行量產。想像一下你今天用Word打好文件後，送入印表機影印後，字體會跑掉。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2e35e4-de78-4b74-a411-02780fc84676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wikipedia-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06857e42-304c-44b8-b0d9-b947b8190d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "wiki_wiki = wikipediaapi.Wikipedia(user_agent='AI Tutorial(mengchiehling@gmail.com)', language='zh-tw')\n",
    "\n",
    "ayoung_wiki = wiki_wiki.page(\"李雅英\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4088ded-60da-45cc-9369-2137c6174dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ayoung_wiki.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5855bed1-277e-48e1-b968-ca45f8ef4ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = dedent(\"\"\"\n",
    "                  I am going to give you a template for your output. \n",
    "                  CAPITALIZED WORDS are my placeholders. Fill in my \n",
    "                  placeholders with your output. Please preserve the \n",
    "                  overall formatting of my template. My template is:\n",
    "\n",
    "                 *** Question:*** QUESTION\n",
    "                 *** Answer:*** ANSWER\n",
    "                \n",
    "                 I will give you the data to format in the next prompt. \n",
    "                 Create three questions using my template.\n",
    "                 \"\"\")\n",
    "\n",
    "\n",
    "system_prompt = PromptTemplate(template=system_template)\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template='{query}',\n",
    "                                  input_variables=[\"query\"]\n",
    "                                  )\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                 human_message\n",
    "                                               ])\n",
    "\n",
    "prompt = chat_prompt.invoke({\"query\": ayoung_wiki.text})\n",
    "\n",
    "output = model.invoke(prompt)\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16feb02f-7bc7-4ba0-906f-29d952fd6be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = dedent(\"\"\"\n",
    "                 I am going to give you a template for your output. CAPITALIZED\n",
    "                 WORDS are my placeholders. Fill in my placeholders with your \n",
    "                 output. Please preserve the overall formatting of my template. \n",
    "                 \n",
    "                 My template is:\n",
    "                \n",
    "                 ## Bio: <NAME>\n",
    "                 ***Executive Summary:*** <ONE SENTENCE SUMMARY>\n",
    "                 ***Full Description:*** <ONE PARAGRAPHY SUMMARY>\n",
    "                \n",
    "                 \"\"\")\n",
    "\n",
    "system_prompt = PromptTemplate(template=system_template)\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template='{query}',\n",
    "                                  input_variables=[\"query\"]\n",
    "                                  )\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                 human_message\n",
    "                                               ])\n",
    "\n",
    "prompt = chat_prompt.invoke({\"query\": ayoung_wiki.text})\n",
    "\n",
    "output = model.invoke(prompt)\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e8e75f-96aa-4834-9ca7-1bfb7dbc2fbf",
   "metadata": {},
   "source": [
    "## 自動模式辨認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7652d408-34a3-46b4-8e83-98d2d01c98d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = dedent(\"\"\"\n",
    "                  I will tell you my start and \n",
    "                  end destination and you will provide a \n",
    "                  complete list of stops for me, including places to stop \n",
    "                  between my start and destination.\n",
    "                  \"\"\")\n",
    "\n",
    "system_prompt = PromptTemplate(template=system_template)\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template='{query}',\n",
    "                              input_variables=[\"query\"]\n",
    "                             )\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                human_message\n",
    "                                               ])\n",
    "\n",
    "query = \"台東太麻里->Day1->Day2->花蓮天祥\"\n",
    "\n",
    "prompt = chat_prompt.invoke({\"query\": query})\n",
    "\n",
    "output = model.invoke(prompt)\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b9b8ff-49e9-4a2b-a31f-8add697a6d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = dedent(\"\"\"\n",
    "                  I will tell you my start and end destination and you will \n",
    "                  provide a complete list of stops for me, including places \n",
    "                  to stop between my start and destination.\n",
    "                  The output should be in traditional Chinese (繁體中文)\n",
    "                  \"\"\")\n",
    "\n",
    "system_prompt = PromptTemplate(template=system_template)\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template='{query}',\n",
    "                                  input_variables=[\"query\"]\n",
    "                                  )\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                 human_message\n",
    "                                               ])\n",
    "\n",
    "query = \"台東太麻里->Day1->Day2->花蓮天祥\"\n",
    "\n",
    "prompt = chat_prompt.invoke({\"query\": query})\n",
    "\n",
    "output = model.invoke(prompt)\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b4a13b-f296-4b97-80e7-a35f0015d557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_template = \"\"\"\n",
    "#                   Christmas is coming and I want to ask a girl out. \n",
    "#                   Please design a great dating experience for us. \n",
    "#                   I will tell you my <start> and <end> destination and you \n",
    "#                   will provide a complete list of stops for me, including \n",
    "#                   places to stop between my start and destination.\n",
    "#                   The output should be in traditional Chinese (繁體中文)\n",
    "#                   \"\"\"\n",
    "\n",
    "# system_prompt = PromptTemplate(template=system_template)\n",
    "# system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "# human_prompt = PromptTemplate(template='start: {start}; end: {end}',\n",
    "#                                   input_variables=[\"start\", \"end\"]\n",
    "#                                   )\n",
    "# human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "# chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "#                                                  human_message\n",
    "#                                                ])\n",
    "\n",
    "# \"\"\"\n",
    "# 給我提點子，這種題目我會腦死~~\n",
    "# \"\"\"\n",
    "# start = \"臺北101\"\n",
    "# end = \"淡水老街\"\n",
    "\n",
    "# prompt = chat_prompt.invoke({\"start\": start, \"end\": end})\n",
    "\n",
    "# output = model.invoke(prompt)\n",
    "\n",
    "# print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a950379-930a-48ce-af03-4ffd16d1c89f",
   "metadata": {},
   "source": [
    "### Let us wrap the chat_prompt generation with a python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dcaf2b-3363-4be9-806b-0a485a846629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_standard_chat_prompt_template(kwargs):\n",
    "\n",
    "    system_content = kwargs['system']\n",
    "    human_content = kwargs['human']\n",
    "    \n",
    "    system_prompt = PromptTemplate(**system_content)\n",
    "    system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "    \n",
    "    human_prompt = PromptTemplate(**human_content)\n",
    "    human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "    \n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                     human_message\n",
    "                                                   ])\n",
    "\n",
    "    return chat_prompt\n",
    "\n",
    "system_template = dedent(\"\"\"\n",
    "                  Christmas is coming and I want to ask a girl out. \n",
    "                  Please design a great dating experience for us. \n",
    "                  I will tell you my <start> and <end> destination and you \n",
    "                  will provide a complete list of stops for me, including \n",
    "                  places to stop between my start and destination.\n",
    "                  The output should be in traditional Chinese (繁體中文)\n",
    "                  \"\"\")\n",
    "\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": 'start: {start}; end: {end}',\n",
    "                    \"input_variable\": [\"start\", \"end\"]}}\n",
    "\n",
    "my_chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "print(my_chat_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c626e3-0807-4810-ba2d-3c420742bb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = \"臺北101\"\n",
    "end = \"淡水老街\"\n",
    "\n",
    "prompt = my_chat_prompt_template.invoke({\"start\": start, \n",
    "                                         \"end\": end})\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467cc66a-e4f0-47d0-ab20-89c4148c08c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.invoke(prompt)\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509fe86a-a4a7-49a1-8b05-b12d3c8e7aa5",
   "metadata": {},
   "source": [
    "# **** 預計第一個小時結束 ****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0500267-2e1a-41a1-98ce-c3749e0794c2",
   "metadata": {},
   "source": [
    "## 輸出格式控制: 精確打擊版本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a05a78-b082-4ace-8005-0e3a2bdece14",
   "metadata": {},
   "source": [
    "### 1. Importing Necessary Classes (導入必要的類):\n",
    "\n",
    "- StructuredOutputParser and ResponseSchema are imported from langchain.output_parsers.\n",
    "- 從 langchain.output_parsers 導入 StructuredOutputParser 和 ResponseSchema。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c680e103-d653-4f2d-b5fa-109285881cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a1f896-4fee-423b-84bc-1bd842f62e79",
   "metadata": {},
   "source": [
    "### 2. Defining Response Schemas (定義回應結構):\n",
    "\n",
    "- A list named response_schemas is created, which contains instances of ResponseSchema. ResponseSchema has two attributes:\n",
    "    - name: This is the key used to retrieve the output.\n",
    "    - description: This is part of the prompt that describes what the output should be.\n",
    "\n",
    "<br>\n",
    "\n",
    "- 創建一個名為 response_schemas 的列表，包含 ResponseSchema 的實例。ResponseSchema 有兩個屬性：\n",
    "    - name：用於檢索輸出的鍵。\n",
    "    - description：提示的一部分，用於描述輸出應該是什麼。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0123f5d-4f67-4d0e-9d16-e8a51f4d424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schemas = [\n",
    "        ResponseSchema(name=\"result\", \n",
    "                       description=dedent(\"\"\"\n",
    "                                   The result as a python list of \n",
    "                                   python dictionaries\"\"\"))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d3a04b-3004-4360-88c8-a51743b4fe1c",
   "metadata": {},
   "source": [
    "### 3. Creating the Output Parser (創建輸出解析器):\n",
    "\n",
    "- output_parser is created by calling StructuredOutputParser.from_response_schemas with the response_schemas list.\n",
    "- This parser uses the defined schemas to understand and structure the output.\n",
    "\n",
    "- 通過調用 StructuredOutputParser.from_response_schemas 並傳入 response_schemas 列表來創建 output_parser。\n",
    "- 該解析器使用定義的結構來理解和結構化輸出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad0c5b2-c990-4069-aa5d-fcfafee72792",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8b0f18-7c02-40da-97da-ed655295be51",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690df7a6-d947-408a-b0f5-d34f0cad52ed",
   "metadata": {},
   "source": [
    "### 4. Generating Format Instructions (生成格式說明):\n",
    "\n",
    "- format_instructions is generated by calling output_parser.get_format_instructions().\n",
    "- These instructions specify how the output should be formatted, based on the defined schemas.\n",
    "<br>\n",
    "<br>\n",
    "- 通過調用 output_parser.get_format_instructions() 來生成 format_instructions。\n",
    "- 這些說明根據定義的結構指定輸出的格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b00293-02b2-4553-b18a-006066418577",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa6c36f-f730-4914-8281-b33e54986bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aa9541-12a9-4475-a8c0-894e919ba374",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = dedent(\"\"\"\n",
    "                I am going to give you a template for your output. CAPITALIZED WORDS are my placeholders. Fill in my placeholders with your output. \n",
    "                Please preserve the overall formatting of my template. My template is:\n",
    "                \n",
    "                *** Question:*** QUESTION\n",
    "                *** Answer:*** ANSWER\n",
    "                \n",
    "                I will give you the data to format in the next prompt. Create three questions using my template.\n",
    "                \"\"\")\n",
    "\n",
    "\n",
    "system_prompt = PromptTemplate(template=system_template)\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template=dedent(\"\"\"\n",
    "                                        {query}\\n \n",
    "                                        output format instruction: {abc}\n",
    "                                        \"\"\"),\n",
    "                              input_variables=[\"query\"],\n",
    "                              partial_variables={'abc': format_instructions}\n",
    "                              )\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt) \n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                 human_message\n",
    "                                               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6c9e5f-0eac-4f57-9738-4c617b7a2bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ayoung_wiki.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b7d3c1-b20c-43c3-87a1-1d85c31215c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = chat_prompt.invoke({\"query\": query})\n",
    "\n",
    "output = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d909fc05-ab40-422f-b207-e4f685dc4b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb68c2a-2f4d-4283-a665-b0d2214eed7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_parser.parse(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306dc9d1-049d-4da3-b61e-aa70dda0f443",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_output = output_parser.parse(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66728cb-273d-428b-bdd6-48f9d96e85c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_output['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4516956f-1a19-45c8-9fe2-b95f99e704e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for content in parsed_output['result']:\n",
    "    print(\"\\n*****************\")\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff6ecfd-925e-4332-ac02-9c57305b53f5",
   "metadata": {},
   "source": [
    "## Pydnatic output control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30f2b31-f38f-43e0-98f3-082191e0074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "class result(BaseModel):\n",
    "\n",
    "    question: str = Field(description=\"A question.\")\n",
    "    answer: str = Field(description=\"Answer to the question.\")\n",
    "\n",
    "\n",
    "class Output(BaseModel):\n",
    "\n",
    "    names: List[result] = Field(description=(\"A list of question/answer pairs\"))\n",
    "\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=Output)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "system_prompt = PromptTemplate(template=system_template)\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template=dedent(\"\"\"\n",
    "                                        {query}\\n \n",
    "                                        output format instruction:\n",
    "                                        {abc}\n",
    "                                        \"\"\"),\n",
    "                              input_variables=[\"query\"],\n",
    "                              partial_variables={'abc': format_instructions}\n",
    "                              )\n",
    "\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt) \n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                 human_message\n",
    "                                               ])\n",
    "\n",
    "prompt = chat_prompt.invoke({\"query\": ayoung_wiki.text})\n",
    "\n",
    "output = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4060c27f-d96c-4680-b434-0522c899031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_output = output_parser.parse(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4727f92-4008-41c8-bbb6-bffa94c276cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced27904-03fa-49ba-969b-8552e309cf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_output.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c600045e-c5b4-4681-b4b7-b6f602565303",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_output.names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2cc491-6305-4d01-aacc-bd5ec43c2c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_output.names[0].question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83f520a-1f6c-4785-9dbe-b81042ece13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_output.names[0].answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146a3966-183e-4b1e-9bf2-1b196b42d3aa",
   "metadata": {},
   "source": [
    "## 多練習幾個版本\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bfbab5-2b16-4473-bfcc-a92fdefec513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Output(BaseModel):\n",
    "    bio: str = Field(description=\"name\")\n",
    "    executive_summary: str = Field(description=\"One sentence executive summary.\")\n",
    "    full_description: str = Field(description=\"One paragraph summary\")\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=Output)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "\n",
    "system_template = dedent(\"\"\"\n",
    "                 I am going to give you a template for your output. CAPITALIZED\n",
    "                 WORDS are my placeholders. Fill in my placeholders with your \n",
    "                 output. Please preserve the overall formatting of my template. \n",
    "                 \n",
    "                 My template is:\n",
    "                \n",
    "                 ## Bio: <NAME>\n",
    "                 ***Executive Summary:*** <ONE SENTENCE SUMMARY>\n",
    "                 ***Full Description:*** <ONE PARAGRAPHY SUMMARY>\n",
    "                \n",
    "                 \"\"\")\n",
    "\n",
    "system_prompt = PromptTemplate(template=system_template)\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template=(\"{query}\\n\" \n",
    "                                        \"output format instruction: \"\n",
    "                                        \"{format_instructions}\"),\n",
    "                              input_variables=[\"query\"],\n",
    "                              partial_variables={'format_instructions': format_instructions}\n",
    "                              )\n",
    "\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt) \n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                 human_message\n",
    "                                               ])\n",
    "\n",
    "prompt = chat_prompt.invoke({\"query\": ayoung_wiki.text})\n",
    "\n",
    "output = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5463a61e-bdf4-4026-8163-f2f65b4caf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf211bf-8e75-4123-811e-2f26d94f534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_output = output_parser.parse(output.content)\n",
    "\n",
    "parsed_output.bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460355a1-af52-453e-a71f-a9d98cce6551",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_output.executive_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c319bf96-6c24-445a-9aeb-a1b54730476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_output.full_description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bfd980-120f-4e4c-89b2-193c077e95d4",
   "metadata": {},
   "source": [
    "## Worksheet Generation\n",
    "\n",
    "I have a list of word:\n",
    "\n",
    "- die Muskeln\n",
    "- die Richtung\n",
    "- die Schnur\n",
    "- die Geschicklichkeit\n",
    "- schnurren\n",
    "- das Fell\n",
    "- das Geräusch\n",
    "- jagen\n",
    "- schmusen\n",
    "- riechen\n",
    "\n",
    "Please create a pdf file, in which it follows the structure:\n",
    "\n",
    "**<WORD>**:\n",
    "<SENTENCE CONTAINTING THE WORD>\n",
    "\n",
    "and a short article containing all these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66283ded-febd-484f-b7b0-454b400f4023",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Output(BaseModel):\n",
    "    name: str = Field(description=\"generated sentence of the word\")\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=Output)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "words = [\"die Muskeln\", \"die Richtung\", \"die Schnur\", \"die Geschicklichkeit\",\n",
    "         \"schnurren\", \"das Fell\", \"das Geräusch\", \"jagen\", \"schmusen\", \"riechen\"]\n",
    "\n",
    "system_template = (\"You are a helpful AI assistant and you are going to help me create a sentence \"\n",
    "                   \"for each of the given word in German.\")\n",
    "system_prompt = PromptTemplate(template=system_template)\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template=(\"{word}\\n\\nOutput instruction: {format_instructions}\"),\n",
    "                              input_variables=[\"word\"],\n",
    "                              partial_variables={'format_instructions': format_instructions}\n",
    "                              )\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt) \n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                 human_message\n",
    "                                               ])\n",
    "\n",
    "prompt = chat_prompt.invoke({\"word\": \"die Muskeln\"})\n",
    "\n",
    "output = model.invoke(prompt)\n",
    "\n",
    "parsed_output = output_parser.parse(output.content)\n",
    "\n",
    "print(parsed_output.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e392b1-8599-4c9d-ab46-9acfeef45e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_sentences = {}\n",
    "\n",
    "for word in words:\n",
    "    \n",
    "    prompt = chat_prompt.invoke({\"word\": word})\n",
    "\n",
    "    output = model.invoke(prompt)\n",
    "\n",
    "    sentence = output.content\n",
    "\n",
    "    parsed_output = output_parser.parse(output.content)\n",
    "\n",
    "    words_sentences[word] = parsed_output.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ad53e7-95c3-405e-a648-60b76ae61a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9707136-639f-4c3a-ac67-0bfd5842e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = dendet(\"\"\"\n",
    "You are a helpful AI assistant and you are going to help me \n",
    "create a short article containing all these words in German.\n",
    "\"\"\")\n",
    "\n",
    "system_prompt = PromptTemplate(template=system_template)\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template=(\"{words}\"),\n",
    "                              input_variables=[\"words\"],\n",
    "                              )\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt) \n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                 human_message\n",
    "                                               ])\n",
    "\n",
    "prompt = chat_prompt.invoke({\"words\": \", \".join(words)})\n",
    "\n",
    "output = model.invoke(prompt)\n",
    "\n",
    "story = output.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c45fdd-6999-4a8d-a956-04245cd415fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4facbdd0-7aa7-485e-889a-bbfa646fb2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF\n",
    "\n",
    "# Create the PDF\n",
    "pdf = FPDF()\n",
    "pdf.add_page()\n",
    "pdf.set_font(\"Arial\", 'B', 16)\n",
    "pdf.cell(0, 10, 'Wortliste mit Beispielsätzen', ln=True)\n",
    "\n",
    "pdf.set_font(\"Arial\", '', 12)\n",
    "for word, sentence in words_sentences.items():\n",
    "    pdf.ln(5)\n",
    "    pdf.set_font(\"Arial\", 'B', 12)\n",
    "    pdf.cell(0, 10, f\"{word}:\", ln=True)\n",
    "    pdf.set_font(\"Arial\", '', 12)\n",
    "    pdf.multi_cell(0, 10, sentence)\n",
    "\n",
    "# Add article\n",
    "pdf.add_page()\n",
    "pdf.set_font(\"Arial\", 'B', 16)\n",
    "pdf.cell(0, 10, 'Artikel mit allen Wörtern', ln=True)\n",
    "pdf.set_font(\"Arial\", '', 12)\n",
    "pdf.multi_cell(0, 10, story)\n",
    "\n",
    "filename = os.path.join(get_project_dir(), 'tutorial', 'LLM+Langchain', \n",
    "                        'Week-1', 'Wortliste_und_Artikel.pdf')\n",
    "\n",
    "# Save the PDF\n",
    "pdf.output(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb0d1f7-0790-425b-b7a8-8930204705fb",
   "metadata": {},
   "source": [
    "# Content Enhancement\n",
    "\n",
    "## Okapi BM25 Retrieval System\n",
    "\n",
    "- 目的: Okapi BM25 幫助找到當你搜索某些內容時最相關的文檔。\n",
    "\n",
    "- 文檔和詞語:\n",
    "    \n",
    "    - 想像你有一堆書（文檔）。\n",
    "    - 每本書都有很多詞語。\n",
    "\n",
    "- 搜索查詢:\n",
    "\n",
    "    - 當你搜索時，你會輸入幾個詞語（你的查詢）。\n",
    "\n",
    "- 評分系統:\n",
    "\n",
    "    - Okapi BM25 根據每本書與你的查詢匹配的程度給予每本書一個分數。\n",
    "\n",
    "- 評分因素:\n",
    "\n",
    "    - 詞頻: 如果你的查詢中的一個詞在某本書中出現很多次，該書會得到更高的分數。\n",
    "    - 逆文檔頻率: 如果一個詞在所有書中都很稀有，但在某本書中出現，該書會得到更高的分數。\n",
    "    - 文檔長度: 較長的書會進行調整，這樣它們不會僅因為篇幅長而被不公平地評分。\n",
    "\n",
    "- 公式:\n",
    "\n",
    "    -BM25 使用一個數學公式來結合這些因素並計算分數。\n",
    "\n",
    "- 選擇最佳:\n",
    "\n",
    "    - 分數最高的書被認為是與你的查詢最相關的。\n",
    "\n",
    "- 結果:\n",
    "\n",
    "    - 這些高分書會作為搜索結果顯示給你。\n",
    "\n",
    "想像一下：Okapi BM25 就像是一個聰明的圖書管理員，它根據你在搜索中使用的詞語來判斷哪些書可能是最有趣和最有幫助的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcf9d88-865d-415c-9814-cf67fd11c5f9",
   "metadata": {},
   "source": [
    "### Term Frequency (TF) & Inverse Document Frequency (IDF):\n",
    "\n",
    "#### Term Frequency:\n",
    "\n",
    "把文章中單詞出現的頻率分佈作為文章的特徵\n",
    "\n",
    "\n",
    "#### Inverse Document Frequency:\n",
    "\n",
    "歸一化: 將文庫中普遍出現的詞的權重下調"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0538f29-8b8e-45d9-8ba6-b0f82bc8ff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "url = \"https://www.gutenberg.org/cache/epub/1041/pg1041.txt\"\n",
    "response = requests.get(url)\n",
    "\n",
    "filename = os.path.join(\"tutorial\", \"LLM+Langchain\", \"Week-1\", \"pg1041.txt\")\n",
    "\n",
    "# Ensure the request was successful\n",
    "if response.status_code == 200:\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(response.text)\n",
    "    print(\"File downloaded successfully.\")\n",
    "else:\n",
    "    print(\"Failed to download file. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98339ebd-8f8b-4eea-ba65-79e34255416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Read file\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Extract main body only\n",
    "match = re.search(r\"\\*\\*\\* START OF.*?\\*\\*\\*(.*)\\*\\*\\* END OF\", text, re.S)\n",
    "if match:\n",
    "    body = match.group(1)\n",
    "else:\n",
    "    body = text  # fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fd3e16-61a9-4850-8aa9-bc391c3bbda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into sonnets: Roman numeral headings\n",
    "pattern = r\"\\n([IVXLCDM]+)\\n\"   # captures numerals as headers\n",
    "parts = re.split(pattern, body)\n",
    "\n",
    "# Reconstruct mapping number → sonnet text\n",
    "sonnets = {}\n",
    "for i in range(1, len(parts), 2):\n",
    "    number = parts[i].strip()\n",
    "    poem = parts[i+1].strip()\n",
    "    sonnets[number] = poem\n",
    "\n",
    "# Example: print first two sonnets\n",
    "for n in [\"I\", \"II\"]:\n",
    "    print(f\"Sonnet {n}:\\n{sonnets[n]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cb11f8-a292-4a75-936b-85d0e24e079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnets['I']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e194f4-a60b-4717-a624-d4f0bd699792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([sonnets['I']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee97a5a-8049-4079-b1c8-44e8d43f9d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8991582-1fbf-4fda-9e47-ce3011152db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out()).T\n",
    "\n",
    "# We will use this later\n",
    "sampled_columns = vectorizer.get_feature_names_out()\n",
    "\n",
    "df.columns = [\"frequency\"]\n",
    "\n",
    "# Sort descending\n",
    "df = df.sort_values(\"frequency\", ascending=False)\n",
    "\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adbd30a-7ef2-4828-9fd7-5fe476f689f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sonnet = pd.DataFrame.from_dict(sonnets, orient='index', columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef07206-6d9c-4010-86b8-c94a00682fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sonnet.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97219dea-c2e9-45c4-bb06-df5681490b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df_sonnet['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f9ceaf-0377-4b59-942b-64f71596ff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49e8e20-3d8f-4826-85d6-1de677cfe1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[sampled_columns].iloc[0].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e3e588-df07-4fe2-961f-bfef08f8afeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[sampled_columns].iloc[0].T.loc['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306b1a62-502d-47dd-a292-19cbea82bf29",
   "metadata": {},
   "source": [
    "OKAPI25 可以看成是關鍵字搜索，而搜尋的結果根據關鍵字在每段文字中出現的頻率和文庫中的稀有度進行加權"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d71deb-ecda-4379-a041-b482bbfdbead",
   "metadata": {},
   "source": [
    "## OKAPI25 in LangChain\n",
    "\n",
    "https://api.python.langchain.com/en/latest/_modules/langchain_community/retrievers/bm25.html#BM25Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7421cbc-2b35-43ce-854e-911e204040fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97969ad6-95ce-49bd-8cfc-84cfb9d11173",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Creating Documents from Training Data (從訓練數據創建文檔):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d6e335-a9c6-427e-840a-c2fffb19d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for idx, row in df_sonnet.iterrows():\n",
    "    document = Document(page_content=row['text'],\n",
    "                        metadata={\"id\": idx})\n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f734a5e1-58b2-4e0c-a294-9db0d554cdb5",
   "metadata": {},
   "source": [
    "### 3. Initializing BM25Retriever (初始化 BM25Retriever):\n",
    "\n",
    "- BM25Retriever.from_documents initializes an instance of BM25Retriever using the documents list.\n",
    "- Parameters:\n",
    "    - k=2: Specifies the number of documents to retrieve per query.\n",
    "    - bm25_params={\"k1\": 2.5}: Sets specific BM25 parameters (k1 parameter set to 2.5).\n",
    "    \n",
    "- 使用 BM25Retriever.from_documents 方法，利用 documents 列表初始化了一个 BM25Retriever 實例。\n",
    "- 參數:\n",
    "    - k=2：指定每個查詢要檢索的文檔數量。\n",
    "    - bm25_params={\"k1\": 2.5}：設置特定的 BM25 參數（設置 k1 參數為 2.5）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd5083d-1969-459f-bb6f-a968a0feaeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6565204d-9d89-4413-8f4e-22762ebd8ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever = BM25Retriever.from_documents(documents, k=2, \n",
    "                                              bm25_params={\"k1\":2.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9690a4-acb6-43f3-a089-885920799557",
   "metadata": {},
   "source": [
    "https://tolkiengateway.net/wiki/The_Road_Goes_Ever_On_(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74f9eaf-609e-4f01-a91f-cdc9f910a2ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "query = dedent(\"\"\"\n",
    "Roads go ever ever on,\n",
    "Over rock and under tree,\n",
    "By caves where never sun has shone,\n",
    "By streams that never find the sea;\n",
    "Over snow by winter sown,\n",
    "And through the merry flowers of June,\n",
    "Over grass and over stone,\n",
    "And under mountains in the moon.\n",
    "\n",
    "Roads go ever ever on\n",
    "Under cloud and under star,\n",
    "Yet feet that wandering have gone\n",
    "Turn at last to home afar.\n",
    "Eyes that fire and sword have seen\n",
    "And horror in the halls of stone\n",
    "Look at last on meadows green\n",
    "And trees and hills they long have known\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8948b4c6-5d61-4d0c-85fa-f55f5aa6d121",
   "metadata": {},
   "source": [
    "### 5. Getting Top N Results (獲取排名前 N 的結果):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86071b0-8d64-4f9d-85d0-10a3bc5f90e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = bm25_retriever.invoke(query)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370cb578-0ca5-4a57-9535-59298a545357",
   "metadata": {},
   "source": [
    "### Byte Pair Encoding (BPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f63b8de-2c32-4c6b-91d0-5c8faf9ffe0e",
   "metadata": {},
   "source": [
    "英文似乎挺好切:每個單詞有頭有尾，但中文或日文這種中間沒有空白的文本要怎麼切?\n",
    "\n",
    "Byte Pair Encoding (BPE) learns frequent character pairs in text and merges them into tokens. For Traditional Chinese, it begins at the character level and gradually merges frequent pairs.\n",
    "\n",
    "Steps\n",
    "\n",
    "Prepare a small Traditional Chinese corpus.\n",
    "\n",
    "Train a BPE tokenizer with tokenizers (Hugging Face).\n",
    "\n",
    "Apply it to a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcd0672-3a16-4b41-9201-5ea175ebf802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the pre-trained BPE tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"p208p2002/llama-traditional-chinese-120M\")\n",
    "\n",
    "# Example usage\n",
    "text = \"我正在閱讀書籍，也在看英文資料。\"\n",
    "encoded = tokenizer(text)\n",
    "print(\"Tokens:\", tokenizer.convert_ids_to_tokens(encoded[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eebc209-cb76-4140-b55d-7ce9f301966b",
   "metadata": {},
   "source": [
    "我知道你們的心中有一個大膽的想法，所以把日文的Tokenizer也附上去了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2d6507-e838-46a4-bc2f-a450c22ef788",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fugashi unidic-lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f910a4-4924-4645-be9b-0f065ef25137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\"\"\"\n",
    "The ## prefix is something you’ll often see in WordPiece or BPE tokenizers (like BERT). \n",
    "It means “this subword is a continuation of the previous token.”\n",
    "\"\"\"\n",
    "\n",
    "# Example: Japanese BERT v2\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\")\n",
    "\n",
    "text = \"<你那大膽的想法>\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d593cb7c-55e8-430f-9fd4-f35ae201f205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_wordpiece(tokens):\n",
    "    merged = []\n",
    "    current_word = \"\"\n",
    "    for token in tokens:\n",
    "        if token.startswith(\"##\"):\n",
    "            current_word += token[2:]  # append without '##'\n",
    "        else:\n",
    "            if current_word:  # push the previous word\n",
    "                merged.append(current_word)\n",
    "            current_word = token  # start a new word\n",
    "    if current_word:  # push the last one\n",
    "        merged.append(current_word)\n",
    "    return merged\n",
    "\n",
    "print(merge_wordpiece(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa63fca-816b-4e56-b3db-7546c542502b",
   "metadata": {},
   "source": [
    "下載中文文檔\n",
    "\n",
    "- https://github.com/rime-aca/corpus/blob/master/唐詩三百首.txt\n",
    "\n",
    "不是我喜歡文學，是這比較好找數據集，還不會被告。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0483d85e-4327-4d68-bf25-551d11fa3d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Read file\n",
    "filename = os.path.join(\"tutorial\", \"LLM+Langchain\", \"Week-1\", \"唐詩三百首.txt\")\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "poems = []\n",
    "\n",
    "# Split by blank lines\n",
    "blocks = [b.strip() for b in text.strip().split(\"\\n\\n\") if b.strip()]\n",
    "\n",
    "for block in blocks:\n",
    "    entry = {}\n",
    "    for line in block.split(\"\\n\"):\n",
    "        if line.startswith(\"詩名:\"):\n",
    "            entry[\"詩名\"] = line.replace(\"詩名:\", \"\").strip()\n",
    "        elif line.startswith(\"作者:\"):\n",
    "            entry[\"作者\"] = line.replace(\"作者:\", \"\").strip()\n",
    "        elif line.startswith(\"詩體:\"):\n",
    "            entry[\"詩體\"] = line.replace(\"詩體:\", \"\").strip()\n",
    "        elif line.startswith(\"詩文:\"):\n",
    "            entry[\"詩文\"] = line.replace(\"詩文:\", \"\").strip()\n",
    "    if len(entry) != 0:\n",
    "        poems.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22744e1b-8305-40fb-8092-8792e0d95a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "poems[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc04545-3b02-4e9b-bc21-228c2d518079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read file\n",
    "# filename = os.path.join(\"tutorial\", \"LLM+Langchain\", \"Week-1\", \"宋詞三百首.txt\")\n",
    "#pd. with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "#     text = f.read()\n",
    "\n",
    "# # Split by blank lines\n",
    "# blocks = [b.strip() for b in text.strip().split(\"\\n\\n\") if b.strip()]\n",
    "\n",
    "# for block in blocks:\n",
    "#     entry = {}\n",
    "#     for line in block.split(\"\\n\"):\n",
    "#         if line.startswith(\"詩名:\"):\n",
    "#             entry[\"詞牌\"] = line.replace(\"詞牌:\", \"\").strip()\n",
    "#         elif line.startswith(\"作者:\"):\n",
    "#             entry[\"作者\"] = line.replace(\"作者:\", \"\").strip()\n",
    "#         elif line.startswith(\"詩體:\"):\n",
    "#             entry[\"詞文\"] = line.replace(\"詞文:\", \"\").strip()\n",
    "#     if len(entry) != 0:\n",
    "#         poems.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feae8a65-014f-4c76-a232-f2ac0c005225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_poem = pd.DataFrame(poems)\n",
    "\n",
    "documents = []\n",
    "\n",
    "for _, row in df_poem.iterrows():\n",
    "    document = Document(page_content=row['詩文'],\n",
    "                        metadata={\"詩名\": row[\"詩名\"],\n",
    "                                  \"作者\": row[\"作者\"],\n",
    "                                  \"詩體\": row[\"詩體\"]})\n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a543cfc-fd8e-452a-a945-7b340c3e6f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_func(text: str):\n",
    "\n",
    "    # 1. Define special tokens to remove\n",
    "    special_tokens = {\"<s>\", \"</s>\", \"[PAD]\", \"[UNK]\"}\n",
    "    \n",
    "    encoded = tokenizer(text)\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded[\"input_ids\"])\n",
    "\n",
    "    # 2. Remove special tokens\n",
    "    tokens = [t.replace(\"▁\", \"\") for t in tokens if t not in special_tokens]\n",
    "    \n",
    "    # 3. Remove punctuation (keep only Chinese/English/number words)\n",
    "    tokens = [t for t in tokens if re.match(r'[\\w一-龥]+', t)]\n",
    "    \n",
    "    # Stringify the tokens\n",
    "    return [str(token) for token in tokens]\n",
    "\n",
    "\n",
    "bm25_poem_retriever = BM25Retriever.from_documents(documents, k=5, \n",
    "                                                   bm25_params={\"k1\":2.5},\n",
    "                                                   preprocess_func=_preprocess_func\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ed0c0a-9413-4fd2-a41c-2fdc81090c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_poem_retriever.invoke(\"大風起兮雲飛揚 威加海內兮歸故鄉 安得猛士兮守四方\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b265ce-02e3-4e21-a8fa-b5dd6e3def1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_poem_retriever.invoke(\"夕陽無限好\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36283559-563c-4b09-8bd0-4896b76c8195",
   "metadata": {},
   "source": [
    "把詩經轉換成五言絕句... 有中文比較好的人嗎? XD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b011dc9-e7ea-47f8-be36-1e8544c51e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "# query\n",
    "\n",
    "query = dedent(\"\"\"\n",
    "蒹葭蒼蒼、白露為霜。\n",
    "所謂伊人、在水一方。\n",
    "遡洄從之、道阻且長。\n",
    "遡遊從之、宛在水中央。\n",
    "\"\"\")\n",
    "\n",
    "# output format\n",
    "class Output(BaseModel):\n",
    "    name: str = Field(description=\"result in traditional Chinese (繁體中文)\")\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=Output)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "\n",
    "# prompt template\n",
    "system_template = dedent(\"\"\"\n",
    "You are a helpful AI assistant with expertise in classical Chinese literature.\n",
    "You understand all the nuance and history background of all the content.\n",
    "\"\"\")\n",
    "system_prompt = PromptTemplate(template=system_template)\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template=(\"\"\"\n",
    "Create a {poetic_form}\n",
    "\n",
    "Examples:\n",
    "{context}\n",
    "\n",
    "according to the semantic of {query}\n",
    "\n",
    "Output instruction: {format_instructions}\n",
    "\"\"\"),\n",
    "input_variables=[\"poetic_form\", \"query\", \"context\"],\n",
    "partial_variables={'format_instructions': format_instructions}\n",
    ")\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt) \n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                 human_message\n",
    "                                               ])\n",
    "\n",
    "# retrieval\n",
    "# BM25 retriever 不支持 filter\n",
    "# 所以建議先filter內容\n",
    "\n",
    "df_poem = pd.DataFrame(poems)\n",
    "\n",
    "documents = []\n",
    "\n",
    "for _, row in df_poem.iterrows():\n",
    "    if row[\"詩體\"] == \"五言絕句\":\n",
    "        document = Document(page_content=row['詩文'],\n",
    "                            metadata={\"詩名\": row[\"詩名\"],\n",
    "                                      \"作者\": row[\"作者\"],\n",
    "                                      \"詩體\": row[\"詩體\"]})\n",
    "        documents.append(document)\n",
    "\n",
    "bm25_poem_retriever = BM25Retriever.from_documents(documents, k=5, \n",
    "                                                   bm25_params={\"k1\":2.5},\n",
    "                                                   preprocess_func=_preprocess_func\n",
    "                                                  )\n",
    "\n",
    "context = bm25_poem_retriever.invoke(query)\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065672bf-e381-4a26-8fb2-85be53118945",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\n\".join([c.page_content for c in context])\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993a621c-6af3-4aba-9fa2-5a3594da530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切換成 gpt-4o。gpt-4o-mini在這方面很弱\n",
    "\n",
    "model_poem = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o\", \n",
    "                   temperature=0 # a range from 0-2, the higher the value, the higher the `creativity`\n",
    "                  )\n",
    "\n",
    "prompt = chat_prompt.invoke({\"query\": query,\n",
    "                             \"poetic_form\": \"五言絕句\",\n",
    "                             \"context\": context})\n",
    "\n",
    "output = model_poem.invoke(prompt)\n",
    "\n",
    "parsed_output = output_parser.parse(output.content)\n",
    "\n",
    "print(parsed_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968f8861-ef45-4bb6-8f7b-108ce055d98b",
   "metadata": {},
   "source": [
    "# Wikipedia Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0ddbe9-374a-4f42-8a0a-e76f4d8f8409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade --quiet  wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325d56f0-bc4a-40ad-8b45-55bc24d4c0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "\n",
    "wiki_retriever = WikipediaRetriever()\n",
    "\n",
    "docs = wiki_retriever.invoke(\"2024 US presidential election\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8e184a-bf82-4687-acd3-ee28ccf3890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18856c98-7918-4afd-9675-3c0e69549d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d145bd2-875d-4f0b-8bfc-1fff44c9a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 若是少於給定返回數量，則返回當前所有可得到文件\n",
    "\n",
    "docs = wiki_retriever.invoke(\"rice\")\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5414999-cb9f-4e3b-a7fe-7340b56eea84",
   "metadata": {},
   "source": [
    "- If you want to know what parameters can be feed to the WikipediaRetriever:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98333101-de37-4203-9f2f-c9a42db21473",
   "metadata": {},
   "outputs": [],
   "source": [
    "WikipediaRetriever?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594d4f34-9d99-47ba-9f08-48ac5feb4a76",
   "metadata": {},
   "source": [
    "By default, wikipedia retriever returns 3 documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712d4f6f-12c5-4ef2-a92f-aec594002d76",
   "metadata": {},
   "source": [
    "# Ensemble Retriever\n",
    "\n",
    "- The EnsembleRetriever uses different search tools together to find the best answers.\n",
    "- It combines results from these tools and organizes them using a special method.\n",
    "- By using different tools, it works better than just one tool alone.\n",
    "- Usually, it mixes two types of search: one that looks for exact words (like BM25) and one that understands meanings (like embeddings).\n",
    "- This mix is called \"hybrid search.\"\n",
    "- The first tool finds documents with specific words, and the second finds documents that have similar ideas.\n",
    "\n",
    "<br>\n",
    "\n",
    "- 它結合這些工具的結果並使用特殊方法進行組織。\n",
    "- 通過使用不同的工具，它比僅使用單一工具效果更好。\n",
    "- 通常，它結合兩種類型的搜索：一種尋找精確詞語（例如 BM25），另一種理解含義（例如嵌入式）。\n",
    "- 這種混合稱為 \"混合搜索\"。\n",
    "- 第一種工具尋找具有特定詞語的文檔，而第二種工具則尋找具有相似思想的文檔。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d36f78a-b55a-4e40-ab9c-9ea2151a5c38",
   "metadata": {},
   "source": [
    "- weights: 控制權重\n",
    "- 總返回文件數量等於個別檢索器 (retriever) 檢索文件數量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599b51aa-9c1b-4e4b-839f-9fb0bdac182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, wiki_retriever], weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cb3e2a-8efa-47a0-a682-574adbd1f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ensemble_retriever.invoke(\"rice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef0492-cf23-4c52-9685-a7860ba645e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91847b0e-ef31-45da-88da-a93888cd0093",
   "metadata": {},
   "source": [
    "- bm25_retriever 返回兩份\n",
    "- wiki_retriever 返回兩份"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33749100-3ad0-489b-882d-40a0b41232c2",
   "metadata": {},
   "source": [
    "# Runtime Configuration (運行時配置)\n",
    "\n",
    "- 我們也可以在運行時配置檢索器。為了做到這一點，我們需要將字段標記為可配置的。\n",
    "\n",
    "API Reference: https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.utils.ConfigurableField.htmld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2301ed88-4894-4c32-9e1c-a0291e192f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d883448-92c9-4a36-a4bd-20f73a755c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever = BM25Retriever.from_documents(documents, k=2, \n",
    "    bm25_params={\"k1\": 1}).configurable_fields(\n",
    "    k=ConfigurableField(\n",
    "        id=\"bm25_k\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f102aaad-9bf8-42a1-be3d-bc054b3c5c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, wiki_retriever], weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f5977c-e120-4f36-b227-262f2add9239",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"bm25_k\": 5}}\n",
    "docs = ensemble_retriever.invoke(\"rice\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51851ea0-c12e-42bb-b1cf-3e57314e15ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb7982d-f1cb-4036-9317-dd7f96c5836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - bm25_retriever 返回五份\n",
    "# - wiki_retriever 返回兩份"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792ac2d5-2284-426d-8894-c9aec4849a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, wiki_retriever], weights=[0.1, 0.9]\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"bm25_k\": 10}}\n",
    "docs = ensemble_retriever.invoke(\"rice\", config=config)\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085c45ab-899b-4d21-9f57-7939f79fa4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - bm25_retriever 返回十份\n",
    "# - wiki_retriever 返回兩份"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a8ec7b-1a3f-4eb1-b210-747d5943b7af",
   "metadata": {},
   "source": [
    "### This is what I do in my work:\n",
    "\n",
    "I use runtime configuration to target a specific data section with the applied attribute.\n",
    "\n",
    "More specifically, there are many types of cosmetic products, such as:\n",
    "\n",
    "- Lipstick\n",
    "- Lip Gloss\n",
    "- Mascara\n",
    "- Blush\n",
    "- Foundation\n",
    "- Nail Polish\n",
    "- Eyeliner\n",
    "- Eye Pencil\n",
    "\n",
    "These products are applied to different areas: face, nails, eyes, and lips.\n",
    "\n",
    "You can retrieve information more efficiently and accurately if you identify the correct application area beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a516b1-a7f7-4bea-8567-32d46570e2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(self.documents, embedding=embedding)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type='similarity',\n",
    "                                     search_kwargs={'k': self._k}).configurable_fields(search_kwargs=ConfigurableField(id=\"faiss_search_kwargs\"))\n",
    "\n",
    "semantic_retriever = retrievers['semantic']\n",
    "semantic_documents = semantic_retriever.invoke(product, config={\"configurable\":\n",
    "                                             {\"faiss_search_kwargs\":\n",
    "                                                  {\"fetch_k\":20,\n",
    "                                                   \"k\": 2,\n",
    "                                                   \"filter\": {\"applied\": area}}}})\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
