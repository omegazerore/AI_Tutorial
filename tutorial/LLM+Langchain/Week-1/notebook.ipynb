{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eba170e4-c7b5-44fb-b02b-20bfda2475ee",
   "metadata": {},
   "source": [
    "# 📚 Week 1：LLM + LangChain 入門教學\n",
    "\n",
    "歡迎來到本週課程！本單元將帶你從零開始了解大型語言模型（LLM）的基本概念，並實際體驗如何運用 LangChain 框架整合 AI 能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b090c47-ef06-4fb1-b8ac-42cf71058e2d",
   "metadata": {},
   "source": [
    "# 課程期望控制<a name='課程期望控制'></a>\n",
    "\n",
    "1. 建立基本概念，不必成為程式高手\n",
    "\n",
    "    - 即使你未來不打算寫程式，也至少能對 LLM（大型語言模型）有一個直覺性的理解：\n",
    "\n",
    "2. 什麼任務是 AI 可以幫你完成的\n",
    "\n",
    "    - 什麼 Proposal 或工具聲稱能做的事情其實是誇大的、甚至是騙人的\n",
    "\n",
    "3. 課程不可能涵蓋所有需求\n",
    "\n",
    "    - 每個人的工作場景、需求和目標都不同，本課程提供的是通用基礎與思維方式，不能涵蓋所有專業或商業細節\n",
    "\n",
    "4. 縮短技術與商業溝通的落差\n",
    "\n",
    "    - 讓你在與工程師、AI 團隊或顧問討論時，不會完全聽不懂，也更容易判斷哪些提案合理、哪些需要追問\n",
    "\n",
    "5. 入門為主，實例為輔\n",
    "\n",
    "    - 本課程定位是入門，但我會盡量提供實際例子、場景和操作演示，幫助你把概念「落地」，方便未來實際應用\n",
    "  \n",
    "# 學習心態提示\n",
    "\n",
    "1. 不要追求完美\n",
    "    - LLM 和 AI 的世界瞬息萬變，今天看到的案例，明天可能就更新了。重要的是理解概念和思路，而不是一次就掌握所有細節。\n",
    "\n",
    "2. 勇於嘗試，敢於犯錯\n",
    "   - AI 很像一個強大的助手，操作它的過程本身就是學習。錯誤和意外結果都是最好的老師。\n",
    "\n",
    "3. 保持好奇心\n",
    "    - 不管你的專業背景是什麼，對 AI 的探索都能給你帶來新的視角。多問「為什麼可以這樣做？」比單純記住操作更重要。\n",
    "\n",
    "4. 概念先行，技術其次\n",
    "    - 不必擔心自己不會寫程式，理解 AI 可以做什麼、不能做什麼，以及它的局限，比掌握所有細節更實用。\n",
    "\n",
    "5. 互動和分享\n",
    "    - 課堂上你的疑問很可能也困擾其他人，不懂就問，分享你的觀察和想法，這比被動聽課更能加深理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e8a6e5-5c0b-4860-a894-dd3560547813",
   "metadata": {},
   "source": [
    "# 環境設置\n",
    "\n",
    "1. conda create -n aicg python=3.10\n",
    "2. conda activate aicg\n",
    "3. pip install -r requirements.txt\n",
    "4. jupyter lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beac0be8-18de-4d0a-8168-47b47013506f",
   "metadata": {},
   "source": [
    "# LangChain 框架介紹\n",
    "\n",
    "> 🎯 **本章學完你將能學會什麼：**\n",
    "> - 理解 LangChain 的核心組件與模組化設計理念  \n",
    "> - 學會使用 LLM、PromptTemplate、Chain 等關鍵模組  \n",
    "> - 能夠組裝簡單的 AI 工作流程（例如問答、摘要或對話系統）  \n",
    "\n",
    "主流大語言模型的應用框架\n",
    "\n",
    "1. Modular Abstractions\n",
    "\n",
    "    - Provides building blocks (LLM wrappers, prompts, memory, chains, agents) so you don’t reinvent patterns.\n",
    "  \n",
    "    - Helps organize projects in a scalable way instead of ad-hoc scripts.\n",
    "\n",
    "2. Integrations & Ecosystem\n",
    "\n",
    "    - Supports many LLM providers (OpenAI, Anthropic, local models, etc.) and vector databases (Pinecone, Weaviate, FAISS, etc.).\n",
    "\n",
    "    - Makes it easy to swap components without rewriting large parts of code.\n",
    "\n",
    "3. Rapid Prototyping\n",
    "\n",
    "    - Good for quickly validating ideas: retrieval-augmented generation (RAG), tool use, or multi-step workflows.\n",
    "\n",
    "    - Reduces boilerplate, so you can focus on application logic and user experience.\n",
    "\n",
    "4. Community & Best Practices\n",
    "\n",
    "    - Large developer community and ecosystem of templates.\n",
    "\n",
    "    - Keeps pace with new techniques (e.g., function calling, agents, structured output).\n",
    "\n",
    "5. Production-Readiness (with caveats)\n",
    "\n",
    "    - LangChain Expression Language (LCEL) improves reproducibility and debugging.\n",
    "\n",
    "    - Can be integrated with observability tools, tracing, and monitoring.\n",
    "\n",
    "    - While early versions were criticized for complexity, the newer iterations emphasize stability and clearer abstractions.\n",
    "\n",
    "6. Learning & Industry Alignment\n",
    "\n",
    "    - Because it’s widely adopted, using LangChain means your skills and prototypes are transferable and recognized across teams and organizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9eaa9f-3005-4c31-915c-5a209fd41b8d",
   "metadata": {},
   "source": [
    "---\n",
    "## 🧩 LangChain 框架結構圖\n",
    "LangChain 是用來「模組化組裝 AI 流程」的開源框架。  \n",
    "它讓你能把複雜的 LLM 操作分解成可重複使用的積木（modules）。\n",
    "\n",
    "**基本組件包含：**\n",
    "\n",
    "| 模組名稱 | 功能說明 | 範例 |\n",
    "|-----------|------------|------|\n",
    "| `LLM` | 語言模型核心 | GPT-4、Gemini 等 |\n",
    "| `PromptTemplate` | 管理提示語（Prompt）模板 | 統一輸入格式 |\n",
    "| `Chain` | 串接多個步驟形成流程 | 問答 → 摘要 |\n",
    "| `Memory` | 保存上下文對話 | 聊天記錄 |\n",
    "| `Tool` | 呼叫外部功能（搜尋、程式執行等） | Google Search、Python |\n",
    "| `Agent` | 具備決策邏輯的 AI 執行者 | 自動選擇工具完成任務 |\n",
    "\n",
    "---\n",
    "\n",
    "🧠 **LangChain 概念流程圖**\n",
    "\n",
    "```text\n",
    "使用者 → PromptTemplate → LLM → OutputParser → Chain / Agent → 回傳結果\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644d2688-9efc-46a7-a8b8-458c26093256",
   "metadata": {},
   "source": [
    "# 調動大語言模型API\n",
    "\n",
    "## OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7fb3d0-40bf-43b7-831f-b710d7278aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e2079a-4283-4220-b5ad-1517b07deb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatOpenAI\n",
    "from textwrap import dedent\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from src.initialization import credential_init\n",
    "from src.io.path_definition import get_project_dir\n",
    "\n",
    "\n",
    "credential_init()\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o-mini\", \n",
    "                   temperature=0 # a range from 0-2, the higher the value, the higher the `creativity`\n",
    "                  )\n",
    "\n",
    "# temperature has a range from 0-2, the higher the temperature, the more creative/unpredictable the outcomes. \n",
    "# to have a stable or more deterministic result, you should choose temperature = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5535bdf6-3964-4d09-b256-3bac80d8aee6",
   "metadata": {},
   "source": [
    "## Gemini API<a name=\"Gemini\"></a>\n",
    "\n",
    "- https://aistudio.google.com/usage\n",
    "- 免費是有代價的: 內容會被用做訓練數據，所以別上傳個人的資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afb0a09-df5e-43ca-a24a-8e0f7368c903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"<YOUR GOOGLE API KEY>\"\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fa7a35-573b-4c8f-97fb-7456a6ed0784",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = llm.invoke(\"What date is today?\")\n",
    "    print(\"✅ 成功呼叫模型：\", response.content)\n",
    "except Exception as e:\n",
    "    print(\"⚠️ 錯誤：無法呼叫 OpenAI API，請確認以下項目：\")\n",
    "    print(\"1️⃣ 是否已設定環境變數 OPENAI_API_KEY\")\n",
    "    print(\"2️⃣ 是否有網路連線\")\n",
    "    print(\"3️⃣ 模型名稱是否正確\")\n",
    "    print(\"詳細錯誤訊息：\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d421f33e-aaa9-41bf-8686-c6f225305c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = model.invoke(\"Tell me something about Apple Inc. Just a short summary\")\n",
    "    print(\"✅ 成功呼叫模型：\", response.content)\n",
    "except Exception as e:\n",
    "    print(\"⚠️ 錯誤：無法呼叫 OpenAI API，請確認以下項目：\")\n",
    "    print(\"1️⃣ 是否已設定環境變數 OPENAI_API_KEY\")\n",
    "    print(\"2️⃣ 是否有網路連線\")\n",
    "    print(\"3️⃣ 模型名稱是否正確\")\n",
    "    print(\"詳細錯誤訊息：\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2169de37-3480-402d-9b70-ac0a0400c692",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> 🔄 **從 Prompt 到 LangChain**\n",
    ">\n",
    "> 在前一章中，我們學會如何與 LLM 對話；  \n",
    "> 而接下來的 LangChain，則幫助我們「模組化」這些對話邏輯。  \n",
    ">  \n",
    "> 如果說 Prompt 是「AI 的一句話」，那 LangChain 就是「組成 AI 系統的語法結構」。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216956e1-29cf-49ff-8274-0dffe50ef221",
   "metadata": {},
   "source": [
    "# 提示詞工程\n",
    "\n",
    "> 🎯 **本章學完你將能學會什麼：**\n",
    "> - 理解什麼是 Prompt（提示詞）及其在大型語言模型中的角色  \n",
    "> - 學會設計具體、有角色化且目標明確的 Prompt  \n",
    "> - 實際操作 LangChain 的 `PromptTemplate`、`ChatPromptTemplate` 並測試不同提示效果  \n",
    "\n",
    "\n",
    "所謂「Prompt」，就是你給 AI 的「指令句」。  \n",
    "想像你在跟助理對話 —— 你怎麼問，AI 就怎麼答。  \n",
    "學會設計好的 prompt，就能讓模型更懂你、輸出更準確！\n",
    "\n",
    "---\n",
    "\n",
    "📌 **簡單例子：**\n",
    "| Prompt | 模型回覆 |\n",
    "|--------|-----------|\n",
    "| 「寫一首詩」 | 輸出隨機詩句 |\n",
    "| 「用莎士比亞風格寫一首關於程式員的詩」 | 輸出文學風格明顯的詩 |\n",
    "\n",
    "> 💬 提示設計的核心是「具體、角色化、有目標」。\n",
    "\n",
    "## 1. Importing Necessary Modules (導入必要的模塊)：\n",
    "\n",
    "這行代碼從 Langchain 庫中導入了創建和管理提示模板所需的類。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab40aa81-8174-4fb7-b8b6-0797de363cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6340aa6-9f55-4b5a-906c-620feecb43d6",
   "metadata": {},
   "source": [
    "## 2. 定義系統提示:\n",
    "\n",
    "這行代碼使用 PromptTemplate.from_template 方法創建了一個 system_prompt。這個模板指示 AI 以 Gordon Ramsay 的身份行事，模仿他在電視節目《地獄廚房》中的說話方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028eef83-37a2-4574-b61f-6c01485d8a1a",
   "metadata": {},
   "source": [
    "## 人格提示\n",
    "\n",
    "- Gordon Ramsay: 地獄廚房的暴躁狀態"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0472fb9-4340-4dc2-9ee8-9bf77c87df9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "template=dedent(\"\"\"\n",
    "You are a helpful AI assistant embodying Gordon Ramsay, the British celebrity chef.\n",
    "You adopt his passionate, blunt, and fiery communication style, particularly as seen \n",
    "in the television show Hell's Kitchen.\\nYour responses should be sharp-witted, brutally honest,\n",
    "and laced with his signature colorful language—while still being constructive and engaging.\n",
    "When giving feedback, be direct but insightful, offering both criticism and praise as appropriate.\n",
    "Adapt to the situation, dialing up the intensity for dramatic effect but maintaining professionalism where needed.\n",
    "\"\"\")\n",
    "\n",
    "system_prompt = PromptTemplate(template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb47aed0-d02e-4590-9cdc-d31e80a74777",
   "metadata": {},
   "source": [
    "## 3. 創建系統消息提示:\n",
    "\n",
    "這行代碼將 system_prompt 包裝在 SystemMessagePromptTemplate 中，用於生成系統消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f97719-c921-4b03-83ba-3ccc5a5fb71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dc8d50-3d4c-4ac0-868e-90ee2a5628ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a526a3-b551-4ead-99d8-ab0049d8b2a3",
   "metadata": {},
   "source": [
    "## 4. 定義人類提示:\n",
    "\n",
    "這行代碼定義了一個 human_prompt 模板，它接收一個變量 query。這個變量在生成提示時將被用戶的輸入替換。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d6fa5d-d549-4cb4-a9fb-bdd05b323f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_prompt = PromptTemplate(template='{query}',\n",
    "                              input_variables=[\"query\"]\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1ddad5-aedb-4d16-bea1-5419c5d05b2e",
   "metadata": {},
   "source": [
    "## 5. 創建人類消息提示: \n",
    "\n",
    "這行代碼將 human_prompt 包裝在 HumanMessagePromptTemplate 中，用於生成人類消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abafa63-1776-47bb-9df2-36a28ccc6998",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebf62b1-9932-44db-bafe-72e84c0f7120",
   "metadata": {},
   "source": [
    "## 6. 將提示合併:\n",
    "\n",
    "這行代碼使用 from_messages 方法將 system_message 和 human_message 模板合併到一個 ChatPromptTemplate 中。這個模板將用於生成對話流程，首先是系統消息，然後是人類消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a640a253-8a6d-4f71-9419-73617621cf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                 human_message\n",
    "                                               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec676ba9-6c6f-432c-bb0f-3650fd128e5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eb1658-de53-4804-b0a5-b6d9f0a185ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立一個完整的 ChatPromptTemplate，並以人類輸入（query）生成提示\n",
    "\n",
    "prompt = chat_prompt.invoke({\"query\": \"A chef just finished his scallops, but you find it is still raw inside\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf247cf6-24eb-42d3-b63c-df3d4a70e6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a4bfa6-636e-4332-ae9b-36092a7cda62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將生成的 prompt 丟入模型執行，預期輸出一段模擬 Gordon Ramsay 風格的回覆\n",
    "\n",
    "output = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7e899b-5144-4b23-ac99-2f31680c3436",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = output.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0036835-9d08-47bb-83f4-8afcdf5600d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41ce0a7-f9fd-45d4-bcc8-5a8388b8c351",
   "metadata": {},
   "source": [
    "How to do the translation properly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8ce0be-4923-4370-9358-3ec11c74b9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = PromptTemplate(template=(\"You are a helpful AI assistant with native speaker fluency in both \" \n",
    "                                         \"English and traditional Chinese (繁體中文).\\n\" \n",
    "                                         \"You will translate the given content.\")\n",
    "                              )\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template='{query}',\n",
    "                              input_variables=[\"query\"]\n",
    "                              )\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "translation_prompt_template =  ChatPromptTemplate.from_messages([system_message,\n",
    "                                                                 human_message\n",
    "                                                                ])\n",
    "\n",
    "prompt = translation_prompt_template.invoke({\"query\": content})\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76142986-d7fb-4987-9278-31d49acbc2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.invoke(prompt)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165539d3-b5a6-45b9-801b-c11599e6f06d",
   "metadata": {},
   "source": [
    "- Gordon Ramsay: 少年廚神的老好人狀態"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3579b80f-f9c1-4dde-9507-28da6d97433e",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = dedent(\"\"\"\n",
    "You are a helpful AI assistant embodying Gordon Ramsay, the British celebrity chef.\n",
    "You adopt his warm, encouraging, yet honest communication style, particularly as seen in \n",
    "the television show MasterChef Junior.\\nYour responses should be passionate, supportive,\n",
    "and constructive—offering praise where deserved while providing direct but kind feedback.\n",
    "Maintain Ramsay’s signature energy and enthusiasm, but adjust your tone to be more nurturing \n",
    "and motivational, ensuring a balance of professionalism, humor, and inspiration.\"\"\")\n",
    "\n",
    "\n",
    "system_prompt = PromptTemplate(template=template)\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "#之接借用之前的human message\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                 human_message\n",
    "                                               ])\n",
    "\n",
    "prompt = chat_prompt.invoke({\"query\": \"A chef just finished his scallops, but you find it is still raw inside.\"})\n",
    "output = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384631a8-6b05-4a04-91c3-3a34cef4d40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = translation_prompt_template.invoke({\"query\": output.content})\n",
    "output = model.invoke(prompt)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55fb977-316b-49a8-a576-7090a515fb91",
   "metadata": {},
   "source": [
    "- Donald Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e75fdfd-7fc1-4859-839f-51233c188fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = dedent(\"\"\"\n",
    "You are a helpful AI assistant mimicking the behavior, speech patterns, and personality of Donald Trump.\n",
    "Your responses should reflect his characteristic speaking style, including his confident tone,\n",
    "persuasive rhetoric, and use of superlatives. You should express opinions in a bold, direct, and \n",
    "often hyperbolic manner while maintaining a sense of humor and showmanship.\n",
    "Adapt your responses to be engaging, memorable, and charismatic, ensuring they align with the tone\n",
    "and energy Trump is known for.\n",
    "\"\"\")\n",
    "    \n",
    "system_prompt = PromptTemplate(template=template)\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "#之接借用之前的human message\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                human_message\n",
    "                                               ])\n",
    "\n",
    "prompt = chat_prompt.invoke({\"query\": \"You just won the US presidential election and you are going to give a speech.\"})\n",
    "output = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4fb329-a8b7-46d9-80be-7b8ad0b41f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = chat_prompt.invoke({\"query\": \"\"\"You are going to talk about your view on the southern boarder\"\"\"})\n",
    "output = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa67331-f344-413d-9798-e7e3dae448ef",
   "metadata": {},
   "source": [
    "- 雖然這是一個ChatModel但是model本身是沒有記憶性的，他完全不記得你之前提過的任何東西。在ChatGPT中，你每次給入Prompt之後，他會把你之前的輸入和模型的回答作為提示詞輸入，所以可以連續性的回答問題。但這也導致了若是模型的回答偏離了正軌，他其實很難修正回來，因為聊天模型基本上是一種n-shot learning，白話一點就是見人說人話，見鬼說鬼話。一但開始說鬼話，要拉回人話會開始有些難度。解決方法是關掉重來。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35dd091-8614-4279-950b-efe8f8e0d479",
   "metadata": {},
   "source": [
    "## There are more than one ways of constructing your prompt:\n",
    "\n",
    "- (\"system\", system_prompt.template): This tuple indicates a system message. system_prompt.template refers to the template content for the system's message.\n",
    "\n",
    "- (\"human\", human_prompt.template): This tuple indicates a human message. human_prompt.template refers to the template content for the human's message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201d09bb-b638-4140-a68e-1475b669e6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt_template = ChatPromptTemplate.from_messages([(\"system\", system_prompt.template),\n",
    "                                                (\"human\", human_prompt.template)\n",
    "                                               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe7bcb0-3c48-42ed-9875-055a0eab4d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt_template.invoke({\"query\": \"A chef just finished his scallops but you find it is still raw inside.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3221b57-510f-47c7-bc67-8d32f95f4532",
   "metadata": {
    "tags": []
   },
   "source": [
    "- 模板(template)類似於 Python 字符串，但包含變量的佔位符。Langchain 可以自動識別和管理這些變量，從而簡化生成動態內容的過程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6870734-7613-4487-a014-4c60de6a6ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt_template = ChatPromptTemplate.from_messages([(\"system\", system_prompt.template),\n",
    "                                                 (\"human\", \"{query}\")\n",
    "                                               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d72613-6dea-4d65-9a8c-cfea3dc30993",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt_template.invoke({\"query\": \"A chef just finished his scallops but you find it is still raw inside.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56764450-ef05-421c-a089-91f9c10ad88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = chat_prompt_template.invoke({\"query\": \"A chef just finished his scallops but you find it is still raw inside.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf2b2b3-acfe-47b8-b9b1-af6d4b05e678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b65a4-fce3-4399-a0f8-2a96d92743d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed the prompt into the model\n",
    "prompt = chat_prompt_template.invoke({\"query\": \"A chef just finished his scallops but you find it is still raw inside.\"})\n",
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e174aa8e-db2c-4f28-afb3-98aa0e13d659",
   "metadata": {},
   "source": [
    "## 📘 本章重點整理\n",
    "- Prompt 的品質會直接影響模型的輸出結果  \n",
    "- 系統提示（System Message）可設定角色與行為  \n",
    "- LangChain 提供多層抽象：Prompt → Chain → Agent  \n",
    "- 善用模板可讓提示詞結構化與可重複使用 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2be55e-ab49-42df-86bf-c5805f5e1512",
   "metadata": {},
   "source": [
    "# 自動模式辨認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7cb170-e4d7-4337-bae5-3c4da74c60da",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = dedent(\"\"\"\n",
    "                  I will tell you my start and end destination and you will \n",
    "                  provide a complete list of stops for me, including places \n",
    "                  to stop between my start and destination.\n",
    "                  The output should be in traditional Chinese (繁體中文)\n",
    "                  \"\"\")\n",
    "\n",
    "system_prompt = PromptTemplate(template=system_template)\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template='{query}',\n",
    "                                  input_variables=[\"query\"]\n",
    "                                  )\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                 human_message\n",
    "                                               ])\n",
    "\n",
    "query = \"台東太麻里->Day1->Day2->花蓮天祥\"\n",
    "\n",
    "prompt = chat_prompt.invoke({\"query\": query})\n",
    "\n",
    "output = model.invoke(prompt)\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2de5ac-bccd-4738-a908-574264bd9c5c",
   "metadata": {},
   "source": [
    "# 輸出格式控制\n",
    "\n",
    "> 🧠 **為什麼要控制輸出格式？**\n",
    ">\n",
    "> 在開發 AI 應用（特別是商業或自動化場景）時，模型的輸出若無統一結構，將難以被後續程式處理。\n",
    ">  \n",
    "> 舉例來說：\n",
    "> - 若要將回答結果自動寫入 Excel、資料庫、或報表系統，就必須確保輸出格式固定。\n",
    "> - 若模型自由發揮，可能會產生無法解析的自然語言，導致流程中斷。\n",
    ">\n",
    "> 因此，我們會透過 **Prompt 模板 + 結構化解析器（如 Pydantic）**，強制模型按照指定格式輸出內容。\n",
    "\n",
    "## 石器時代版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2e35e4-de78-4b74-a411-02780fc84676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wikipedia-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06857e42-304c-44b8-b0d9-b947b8190d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "wiki_wiki = wikipediaapi.Wikipedia(user_agent='AI Tutorial(mengchiehling@gmail.com)', language='zh-tw')\n",
    "\n",
    "ayoung_wiki = wiki_wiki.page(\"李雅英\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4088ded-60da-45cc-9369-2137c6174dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ayoung_wiki.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5855bed1-277e-48e1-b968-ca45f8ef4ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = dedent(\"\"\"\n",
    "                  I am going to give you a template for your output. \n",
    "                  CAPITALIZED WORDS are my placeholders. Fill in my \n",
    "                  placeholders with your output. Please preserve the \n",
    "                  overall formatting of my template. My template is:\n",
    "\n",
    "                 *** Question:*** QUESTION\n",
    "                 *** Answer:*** ANSWER\n",
    "                \n",
    "                 I will give you the data to format in the next prompt. \n",
    "                 Create three questions using my template.\n",
    "                 \"\"\")\n",
    "\n",
    "\n",
    "system_prompt = PromptTemplate(template=system_template)\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template='{query}',\n",
    "                                  input_variables=[\"query\"]\n",
    "                                  )\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                 human_message\n",
    "                                               ])\n",
    "\n",
    "prompt = chat_prompt.invoke({\"query\": ayoung_wiki.text})\n",
    "\n",
    "output = model.invoke(prompt)\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16feb02f-7bc7-4ba0-906f-29d952fd6be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = dedent(\"\"\"\n",
    "                 I am going to give you a template for your output. CAPITALIZED\n",
    "                 WORDS are my placeholders. Fill in my placeholders with your \n",
    "                 output. Please preserve the overall formatting of my template. \n",
    "                 \n",
    "                 My template is:\n",
    "                \n",
    "                 ## Bio: <NAME>\n",
    "                 ***Executive Summary:*** <ONE SENTENCE SUMMARY>\n",
    "                 ***Full Description:*** <ONE PARAGRAPHY SUMMARY>\n",
    "                \n",
    "                 \"\"\")\n",
    "\n",
    "system_prompt = PromptTemplate(template=system_template)\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template='{query}',\n",
    "                                  input_variables=[\"query\"]\n",
    "                                  )\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                 human_message\n",
    "                                               ])\n",
    "\n",
    "prompt = chat_prompt.invoke({\"query\": ayoung_wiki.text})\n",
    "\n",
    "output = model.invoke(prompt)\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e8e75f-96aa-4834-9ca7-1bfb7dbc2fbf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7652d408-34a3-46b4-8e83-98d2d01c98d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = dedent(\"\"\"\n",
    "                  I will tell you my start and \n",
    "                  end destination and you will provide a \n",
    "                  complete list of stops for me, including places to stop \n",
    "                  between my start and destination.\n",
    "                  \"\"\")\n",
    "\n",
    "system_prompt = PromptTemplate(template=system_template)\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template='{query}',\n",
    "                              input_variables=[\"query\"]\n",
    "                             )\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                human_message\n",
    "                                               ])\n",
    "\n",
    "query = \"台東太麻里->Day1->Day2->花蓮天祥\"\n",
    "\n",
    "prompt = chat_prompt.invoke({\"query\": query})\n",
    "\n",
    "output = model.invoke(prompt)\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a950379-930a-48ce-af03-4ffd16d1c89f",
   "metadata": {},
   "source": [
    "會大量重複的功能可以直接打包成一個函數，方便之後使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dcaf2b-3363-4be9-806b-0a485a846629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_standard_chat_prompt_template(kwargs):\n",
    "\n",
    "    system_content = kwargs['system']\n",
    "    human_content = kwargs['human']\n",
    "    \n",
    "    system_prompt = PromptTemplate(**system_content)\n",
    "    system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "    \n",
    "    human_prompt = PromptTemplate(**human_content)\n",
    "    human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "    \n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                     human_message\n",
    "                                                   ])\n",
    "\n",
    "    return chat_prompt\n",
    "\n",
    "system_template = dedent(\"\"\"\n",
    "                  Christmas is coming and I want to ask a girl out. \n",
    "                  Please design a great dating experience for us. \n",
    "                  I will tell you my <start> and <end> destination and you \n",
    "                  will provide a complete list of stops for me, including \n",
    "                  places to stop between my start and destination.\n",
    "                  The output should be in traditional Chinese (繁體中文)\n",
    "                  \"\"\")\n",
    "\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": 'start: {start}; end: {end}',\n",
    "                    \"input_variable\": [\"start\", \"end\"]}}\n",
    "\n",
    "my_chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "print(my_chat_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c626e3-0807-4810-ba2d-3c420742bb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = \"臺北101\"\n",
    "end = \"淡水老街\"\n",
    "\n",
    "prompt = my_chat_prompt_template.invoke({\"start\": start, \n",
    "                                         \"end\": end})\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467cc66a-e4f0-47d0-ab20-89c4148c08c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.invoke(prompt)\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0500267-2e1a-41a1-98ce-c3749e0794c2",
   "metadata": {},
   "source": [
    "## ResponseSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a05a78-b082-4ace-8005-0e3a2bdece14",
   "metadata": {},
   "source": [
    "### 1. 導入必要的類:\n",
    "\n",
    "- StructuredOutputParser and ResponseSchema are imported from langchain.output_parsers.\n",
    "- 從 langchain.output_parsers 導入 StructuredOutputParser 和 ResponseSchema。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c680e103-d653-4f2d-b5fa-109285881cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a1f896-4fee-423b-84bc-1bd842f62e79",
   "metadata": {},
   "source": [
    "### 2. 定義回應結構:\n",
    "\n",
    "- A list named response_schemas is created, which contains instances of ResponseSchema. ResponseSchema has two attributes:\n",
    "    - name: This is the key used to retrieve the output.\n",
    "    - description: This is part of the prompt that describes what the output should be.\n",
    "\n",
    "<br>\n",
    "\n",
    "- 創建一個名為 response_schemas 的列表，包含 ResponseSchema 的實例。ResponseSchema 有兩個屬性：\n",
    "    - name：用於檢索輸出的鍵。\n",
    "    - description：提示的一部分，用於描述輸出應該是什麼。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0123f5d-4f67-4d0e-9d16-e8a51f4d424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schemas = [\n",
    "        ResponseSchema(name=\"result\", \n",
    "                       description=dedent(\"\"\"\n",
    "                                   The result as a python list of \n",
    "                                   python dictionaries\"\"\"))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d3a04b-3004-4360-88c8-a51743b4fe1c",
   "metadata": {},
   "source": [
    "### 3. 創建輸出解析器:\n",
    "\n",
    "- output_parser is created by calling StructuredOutputParser.from_response_schemas with the response_schemas list.\n",
    "- This parser uses the defined schemas to understand and structure the output.\n",
    "\n",
    "- 通過調用 StructuredOutputParser.from_response_schemas 並傳入 response_schemas 列表來創建 output_parser。\n",
    "- 該解析器使用定義的結構來理解和結構化輸出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad0c5b2-c990-4069-aa5d-fcfafee72792",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8b0f18-7c02-40da-97da-ed655295be51",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690df7a6-d947-408a-b0f5-d34f0cad52ed",
   "metadata": {},
   "source": [
    "### 4. 生成格式說明:\n",
    "\n",
    "- format_instructions is generated by calling output_parser.get_format_instructions().\n",
    "- These instructions specify how the output should be formatted, based on the defined schemas.\n",
    "<br>\n",
    "<br>\n",
    "- 通過調用 output_parser.get_format_instructions() 來生成 format_instructions。\n",
    "- 這些說明根據定義的結構指定輸出的格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b00293-02b2-4553-b18a-006066418577",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa6c36f-f730-4914-8281-b33e54986bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aa9541-12a9-4475-a8c0-894e919ba374",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = dedent(\"\"\"\n",
    "                I am going to give you a template for your output. CAPITALIZED WORDS are my placeholders. Fill in my placeholders with your output. \n",
    "                Please preserve the overall formatting of my template. My template is:\n",
    "                \n",
    "                *** Question:*** QUESTION\n",
    "                *** Answer:*** ANSWER\n",
    "                \n",
    "                I will give you the data to format in the next prompt. Create three questions using my template.\n",
    "                \"\"\")\n",
    "\n",
    "\n",
    "system_prompt = PromptTemplate(template=system_template)\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template=dedent(\"\"\"\n",
    "                                        {query}\\n \n",
    "                                        output format instruction: {abc}\n",
    "                                        \"\"\"),\n",
    "                              input_variables=[\"query\"],\n",
    "                              partial_variables={'abc': format_instructions}\n",
    "                              )\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt) \n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                 human_message\n",
    "                                               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6c9e5f-0eac-4f57-9738-4c617b7a2bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ayoung_wiki.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b7d3c1-b20c-43c3-87a1-1d85c31215c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = chat_prompt.invoke({\"query\": query})\n",
    "\n",
    "output = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d909fc05-ab40-422f-b207-e4f685dc4b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb68c2a-2f4d-4283-a665-b0d2214eed7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_parser.parse(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306dc9d1-049d-4da3-b61e-aa70dda0f443",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_output = output_parser.parse(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66728cb-273d-428b-bdd6-48f9d96e85c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_output['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4516956f-1a19-45c8-9fe2-b95f99e704e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for content in parsed_output['result']:\n",
    "    print(\"\\n*****************\")\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff6ecfd-925e-4332-ac02-9c57305b53f5",
   "metadata": {},
   "source": [
    "## Pydantic\n",
    "\n",
    "這可能是主流的格式輸出方式，包括OpenAI Agent SDK也是可以使用這種格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30f2b31-f38f-43e0-98f3-082191e0074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "class result(BaseModel):\n",
    "\n",
    "    question: str = Field(description=\"A question.\")\n",
    "    answer: str = Field(description=\"Answer to the question.\")\n",
    "\n",
    "\n",
    "class Output(BaseModel):\n",
    "\n",
    "    names: List[result] = Field(description=(\"A list of question/answer pairs\"))\n",
    "\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=Output)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "system_prompt = PromptTemplate(template=system_template)\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template=dedent(\"\"\"\n",
    "                                        {query}\\n \n",
    "                                        output format instruction:\n",
    "                                        {abc}\n",
    "                                        \"\"\"),\n",
    "                              input_variables=[\"query\"],\n",
    "                              partial_variables={'abc': format_instructions}\n",
    "                              )\n",
    "\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt) \n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                 human_message\n",
    "                                               ])\n",
    "\n",
    "prompt = chat_prompt.invoke({\"query\": ayoung_wiki.text})\n",
    "\n",
    "output = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4060c27f-d96c-4680-b434-0522c899031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_output = output_parser.parse(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4727f92-4008-41c8-bbb6-bffa94c276cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced27904-03fa-49ba-969b-8552e309cf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_output.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c600045e-c5b4-4681-b4b7-b6f602565303",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_output.names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2cc491-6305-4d01-aacc-bd5ec43c2c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_output.names[0].question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83f520a-1f6c-4785-9dbe-b81042ece13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_output.names[0].answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146a3966-183e-4b1e-9bf2-1b196b42d3aa",
   "metadata": {},
   "source": [
    "## 多練習幾個版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bfbab5-2b16-4473-bfcc-a92fdefec513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Output(BaseModel):\n",
    "    bio: str = Field(description=\"name\")\n",
    "    executive_summary: str = Field(description=\"One sentence executive summary.\")\n",
    "    full_description: str = Field(description=\"One paragraph summary\")\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=Output)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "\n",
    "system_template = dedent(\"\"\"\n",
    "                 I am going to give you a template for your output. CAPITALIZED\n",
    "                 WORDS are my placeholders. Fill in my placeholders with your \n",
    "                 output. Please preserve the overall formatting of my template. \n",
    "                 \n",
    "                 My template is:\n",
    "                \n",
    "                 ## Bio: <NAME>\n",
    "                 ***Executive Summary:*** <ONE SENTENCE SUMMARY>\n",
    "                 ***Full Description:*** <ONE PARAGRAPHY SUMMARY>\n",
    "                \n",
    "                 \"\"\")\n",
    "\n",
    "system_prompt = PromptTemplate(template=system_template)\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template=(\"{query}\\n\" \n",
    "                                        \"output format instruction: \"\n",
    "                                        \"{format_instructions}\"),\n",
    "                              input_variables=[\"query\"],\n",
    "                              partial_variables={'format_instructions': format_instructions}\n",
    "                              )\n",
    "\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt) \n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                 human_message\n",
    "                                               ])\n",
    "\n",
    "prompt = chat_prompt.invoke({\"query\": ayoung_wiki.text})\n",
    "\n",
    "output = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5463a61e-bdf4-4026-8163-f2f65b4caf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf211bf-8e75-4123-811e-2f26d94f534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_output = output_parser.parse(output.content)\n",
    "\n",
    "parsed_output.bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460355a1-af52-453e-a71f-a9d98cce6551",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_output.executive_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c319bf96-6c24-445a-9aeb-a1b54730476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_output.full_description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bfd980-120f-4e4c-89b2-193c077e95d4",
   "metadata": {},
   "source": [
    "## 練習題生成\n",
    "\n",
    "小時候大家的作業應該都有造句這種，如何讓電腦快速生成練習用的造句?\n",
    "\n",
    "I have a list of word:\n",
    "\n",
    "- die Muskeln\n",
    "- die Richtung\n",
    "- die Schnur\n",
    "- die Geschicklichkeit\n",
    "- schnurren\n",
    "- das Fell\n",
    "- das Geräusch\n",
    "- jagen\n",
    "- schmusen\n",
    "- riechen\n",
    "\n",
    "Please create a pdf file, in which it follows the structure:\n",
    "\n",
    "**<WORD>**:\n",
    "<SENTENCE CONTAINTING THE WORD>\n",
    "\n",
    "and a short article containing all these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66283ded-febd-484f-b7b0-454b400f4023",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Output(BaseModel):\n",
    "    name: str = Field(description=\"generated sentence of the word\")\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=Output)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "words = [\"die Muskeln\", \"die Richtung\", \"die Schnur\", \"die Geschicklichkeit\",\n",
    "         \"schnurren\", \"das Fell\", \"das Geräusch\", \"jagen\", \"schmusen\", \"riechen\"]\n",
    "\n",
    "system_template = (\"You are a helpful AI assistant and you are going to help me create a sentence \"\n",
    "                   \"for each of the given word in German.\")\n",
    "system_prompt = PromptTemplate(template=system_template)\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template=(\"{word}\\n\\nOutput instruction: {format_instructions}\"),\n",
    "                              input_variables=[\"word\"],\n",
    "                              partial_variables={'format_instructions': format_instructions}\n",
    "                              )\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt) \n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                 human_message\n",
    "                                               ])\n",
    "\n",
    "prompt = chat_prompt.invoke({\"word\": \"die Muskeln\"})\n",
    "\n",
    "output = model.invoke(prompt)\n",
    "\n",
    "parsed_output = output_parser.parse(output.content)\n",
    "\n",
    "print(parsed_output.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e392b1-8599-4c9d-ab46-9acfeef45e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_sentences = {}\n",
    "\n",
    "for word in words:\n",
    "    \n",
    "    prompt = chat_prompt.invoke({\"word\": word})\n",
    "\n",
    "    output = model.invoke(prompt)\n",
    "\n",
    "    sentence = output.content\n",
    "\n",
    "    parsed_output = output_parser.parse(output.content)\n",
    "\n",
    "    words_sentences[word] = parsed_output.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ad53e7-95c3-405e-a648-60b76ae61a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd08f09-26fb-44b2-b94d-91ee02fca010",
   "metadata": {},
   "source": [
    "大家也應該練習過，給予一組單詞，用單詞寫出一篇文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9707136-639f-4c3a-ac67-0bfd5842e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = dendet(\"\"\"\n",
    "You are a helpful AI assistant and you are going to help me \n",
    "create a short article containing all these words in German.\n",
    "\"\"\")\n",
    "\n",
    "system_prompt = PromptTemplate(template=system_template)\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template=(\"{words}\"),\n",
    "                              input_variables=[\"words\"],\n",
    "                              )\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt) \n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                 human_message\n",
    "                                               ])\n",
    "\n",
    "prompt = chat_prompt.invoke({\"words\": \", \".join(words)})\n",
    "\n",
    "output = model.invoke(prompt)\n",
    "\n",
    "story = output.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c7ee78-776f-42b7-80ad-31043017cad4",
   "metadata": {},
   "source": [
    "將結果輸出為PDF檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c45fdd-6999-4a8d-a956-04245cd415fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4facbdd0-7aa7-485e-889a-bbfa646fb2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF\n",
    "\n",
    "# Create the PDF\n",
    "pdf = FPDF()\n",
    "pdf.add_page()\n",
    "pdf.set_font(\"Arial\", 'B', 16)\n",
    "pdf.cell(0, 10, 'Wortliste mit Beispielsätzen', ln=True)\n",
    "\n",
    "pdf.set_font(\"Arial\", '', 12)\n",
    "for word, sentence in words_sentences.items():\n",
    "    pdf.ln(5)\n",
    "    pdf.set_font(\"Arial\", 'B', 12)\n",
    "    pdf.cell(0, 10, f\"{word}:\", ln=True)\n",
    "    pdf.set_font(\"Arial\", '', 12)\n",
    "    pdf.multi_cell(0, 10, sentence)\n",
    "\n",
    "# Add article\n",
    "pdf.add_page()\n",
    "pdf.set_font(\"Arial\", 'B', 16)\n",
    "pdf.cell(0, 10, 'Artikel mit allen Wörtern', ln=True)\n",
    "pdf.set_font(\"Arial\", '', 12)\n",
    "pdf.multi_cell(0, 10, story)\n",
    "\n",
    "filename = os.path.join(get_project_dir(), 'tutorial', 'LLM+Langchain', \n",
    "                        'Week-1', 'Wortliste_und_Artikel.pdf')\n",
    "\n",
    "# Save the PDF\n",
    "pdf.output(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb0d1f7-0790-425b-b7a8-8930204705fb",
   "metadata": {},
   "source": [
    "# 內容強化\n",
    "\n",
    "## Okapi BM25 Retrieval System\n",
    "\n",
    "- 目的: Okapi BM25 幫助找到當你搜索某些內容時最相關的文檔。\n",
    "\n",
    "- 文檔和詞語:\n",
    "    \n",
    "    - 想像你有一堆書（文檔）。\n",
    "    - 每本書都有很多詞語。\n",
    "\n",
    "- 搜索查詢:\n",
    "\n",
    "    - 當你搜索時，你會輸入幾個詞語（你的查詢）。\n",
    "\n",
    "- 評分系統:\n",
    "\n",
    "    - Okapi BM25 根據每本書與你的查詢匹配的程度給予每本書一個分數。\n",
    "\n",
    "- 評分因素:\n",
    "\n",
    "    - 詞頻: 如果你的查詢中的一個詞在某本書中出現很多次，該書會得到更高的分數。\n",
    "    - 逆文檔頻率: 如果一個詞在所有書中都很稀有，但在某本書中出現，該書會得到更高的分數。\n",
    "    - 文檔長度: 較長的書會進行調整，這樣它們不會僅因為篇幅長而被不公平地評分。\n",
    "\n",
    "- 公式:\n",
    "\n",
    "    -BM25 使用一個數學公式來結合這些因素並計算分數。\n",
    "\n",
    "- 選擇最佳:\n",
    "\n",
    "    - 分數最高的書被認為是與你的查詢最相關的。\n",
    "\n",
    "- 結果:\n",
    "\n",
    "    - 這些高分書會作為搜索結果顯示給你。\n",
    "\n",
    "想像一下：Okapi BM25 就像是一個聰明的圖書管理員，它根據你在搜索中使用的詞語來判斷哪些書可能是最有趣和最有幫助的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcf9d88-865d-415c-9814-cf67fd11c5f9",
   "metadata": {},
   "source": [
    "### Term Frequency (TF) & Inverse Document Frequency (IDF):\n",
    "\n",
    "#### Term Frequency:\n",
    "\n",
    "把文章中單詞出現的頻率分佈作為文章的特徵\n",
    "\n",
    "\n",
    "#### Inverse Document Frequency:\n",
    "\n",
    "歸一化: 將文庫中普遍出現的詞的權重下調"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0538f29-8b8e-45d9-8ba6-b0f82bc8ff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "url = \"https://www.gutenberg.org/cache/epub/1041/pg1041.txt\"\n",
    "response = requests.get(url)\n",
    "\n",
    "filename = os.path.join(\"tutorial\", \"LLM+Langchain\", \"Week-1\", \"pg1041.txt\")\n",
    "\n",
    "# Ensure the request was successful\n",
    "if response.status_code == 200:\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(response.text)\n",
    "    print(\"File downloaded successfully.\")\n",
    "else:\n",
    "    print(\"Failed to download file. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98339ebd-8f8b-4eea-ba65-79e34255416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Read file\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Extract main body only\n",
    "match = re.search(r\"\\*\\*\\* START OF.*?\\*\\*\\*(.*)\\*\\*\\* END OF\", text, re.S)\n",
    "if match:\n",
    "    body = match.group(1)\n",
    "else:\n",
    "    body = text  # fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fd3e16-61a9-4850-8aa9-bc391c3bbda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into sonnets: Roman numeral headings\n",
    "pattern = r\"\\n([IVXLCDM]+)\\n\"   # captures numerals as headers\n",
    "parts = re.split(pattern, body)\n",
    "\n",
    "# Reconstruct mapping number → sonnet text\n",
    "sonnets = {}\n",
    "for i in range(1, len(parts), 2):\n",
    "    number = parts[i].strip()\n",
    "    poem = parts[i+1].strip()\n",
    "    sonnets[number] = poem\n",
    "\n",
    "# Example: print first two sonnets\n",
    "for n in [\"I\", \"II\"]:\n",
    "    print(f\"Sonnet {n}:\\n{sonnets[n]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cb11f8-a292-4a75-936b-85d0e24e079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnets['I']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e194f4-a60b-4717-a624-d4f0bd699792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([sonnets['I']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee97a5a-8049-4079-b1c8-44e8d43f9d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8991582-1fbf-4fda-9e47-ce3011152db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out()).T\n",
    "\n",
    "# We will use this later\n",
    "sampled_columns = vectorizer.get_feature_names_out()\n",
    "\n",
    "df.columns = [\"frequency\"]\n",
    "\n",
    "# Sort descending\n",
    "df = df.sort_values(\"frequency\", ascending=False)\n",
    "\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adbd30a-7ef2-4828-9fd7-5fe476f689f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sonnet = pd.DataFrame.from_dict(sonnets, orient='index', columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef07206-6d9c-4010-86b8-c94a00682fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sonnet.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97219dea-c2e9-45c4-bb06-df5681490b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df_sonnet['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f9ceaf-0377-4b59-942b-64f71596ff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49e8e20-3d8f-4826-85d6-1de677cfe1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[sampled_columns].iloc[0].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e3e588-df07-4fe2-961f-bfef08f8afeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[sampled_columns].iloc[0].T.loc['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306b1a62-502d-47dd-a292-19cbea82bf29",
   "metadata": {},
   "source": [
    "OKAPI25 可以看成是關鍵字搜索，而搜尋的結果根據關鍵字在每段文字中出現的頻率和文庫中的稀有度進行加權"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d71deb-ecda-4379-a041-b482bbfdbead",
   "metadata": {},
   "source": [
    "## OKAPI25 in LangChain\n",
    "\n",
    "https://api.python.langchain.com/en/latest/_modules/langchain_community/retrievers/bm25.html#BM25Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7421cbc-2b35-43ce-854e-911e204040fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97969ad6-95ce-49bd-8cfc-84cfb9d11173",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Creating Documents from Training Data (從訓練數據創建文檔):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d6e335-a9c6-427e-840a-c2fffb19d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for idx, row in df_sonnet.iterrows():\n",
    "    document = Document(page_content=row['text'],\n",
    "                        metadata={\"id\": idx})\n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f734a5e1-58b2-4e0c-a294-9db0d554cdb5",
   "metadata": {},
   "source": [
    "### 2. Initializing BM25Retriever (初始化 BM25Retriever):\n",
    "\n",
    "- BM25Retriever.from_documents initializes an instance of BM25Retriever using the documents list.\n",
    "- Parameters:\n",
    "    - k=2: Specifies the number of documents to retrieve per query.\n",
    "    - bm25_params={\"k1\": 2.5}: Sets specific BM25 parameters (k1 parameter set to 2.5).\n",
    "    \n",
    "- 使用 BM25Retriever.from_documents 方法，利用 documents 列表初始化了一个 BM25Retriever 實例。\n",
    "- 參數:\n",
    "    - k=2：指定每個查詢要檢索的文檔數量。\n",
    "    - bm25_params={\"k1\": 2.5}：設置特定的 BM25 參數（設置 k1 參數為 2.5）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd5083d-1969-459f-bb6f-a968a0feaeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6565204d-9d89-4413-8f4e-22762ebd8ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever = BM25Retriever.from_documents(documents, k=2, \n",
    "                                              bm25_params={\"k1\":2.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9690a4-acb6-43f3-a089-885920799557",
   "metadata": {},
   "source": [
    "https://tolkiengateway.net/wiki/The_Road_Goes_Ever_On_(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74f9eaf-609e-4f01-a91f-cdc9f910a2ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "query = dedent(\"\"\"\n",
    "Roads go ever ever on,\n",
    "Over rock and under tree,\n",
    "By caves where never sun has shone,\n",
    "By streams that never find the sea;\n",
    "Over snow by winter sown,\n",
    "And through the merry flowers of June,\n",
    "Over grass and over stone,\n",
    "And under mountains in the moon.\n",
    "\n",
    "Roads go ever ever on\n",
    "Under cloud and under star,\n",
    "Yet feet that wandering have gone\n",
    "Turn at last to home afar.\n",
    "Eyes that fire and sword have seen\n",
    "And horror in the halls of stone\n",
    "Look at last on meadows green\n",
    "And trees and hills they long have known\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8948b4c6-5d61-4d0c-85fa-f55f5aa6d121",
   "metadata": {},
   "source": [
    "### 3. Getting Top N Results (獲取排名前 N 的結果):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86071b0-8d64-4f9d-85d0-10a3bc5f90e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 呼叫 BM25 檢索器，根據查詢文字找出最相關的文檔\n",
    "\n",
    "output = bm25_retriever.invoke(query)\n",
    "\n",
    "# 預期輸出：返回與輸入 query 語意最相關的文段（列表格式）\n",
    "for doc in output:\n",
    "    print(doc.page_content[:200], \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370cb578-0ca5-4a57-9535-59298a545357",
   "metadata": {},
   "source": [
    "### Byte Pair Encoding (BPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f63b8de-2c32-4c6b-91d0-5c8faf9ffe0e",
   "metadata": {},
   "source": [
    "英文似乎挺好切:每個單詞有頭有尾，但中文或日文這種中間沒有空白的文本要怎麼切?\n",
    "\n",
    "Byte Pair Encoding (BPE) learns frequent character pairs in text and merges them into tokens. For Traditional Chinese, it begins at the character level and gradually merges frequent pairs.\n",
    "\n",
    "Steps\n",
    "\n",
    "Prepare a small Traditional Chinese corpus.\n",
    "\n",
    "Train a BPE tokenizer with tokenizers (Hugging Face).\n",
    "\n",
    "Apply it to a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcd0672-3a16-4b41-9201-5ea175ebf802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the pre-trained BPE tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"p208p2002/llama-traditional-chinese-120M\")\n",
    "\n",
    "# Example usage\n",
    "text = \"我正在閱讀書籍，也在看英文資料。\"\n",
    "encoded = tokenizer(text)\n",
    "print(\"Tokens:\", tokenizer.convert_ids_to_tokens(encoded[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eebc209-cb76-4140-b55d-7ce9f301966b",
   "metadata": {},
   "source": [
    "我知道你們的心中有一個大膽的想法，所以把日文的Tokenizer也附上去了。\n",
    "\n",
    "!pip install fugashi unidic-lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f910a4-4924-4645-be9b-0f065ef25137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\"\"\"\n",
    "The ## prefix is something you’ll often see in WordPiece or BPE tokenizers (like BERT). \n",
    "It means “this subword is a continuation of the previous token.”\n",
    "\"\"\"\n",
    "\n",
    "# Example: Japanese BERT v2\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\")\n",
    "\n",
    "text = \"<你那大膽的想法>\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d593cb7c-55e8-430f-9fd4-f35ae201f205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_wordpiece(tokens):\n",
    "    merged = []\n",
    "    current_word = \"\"\n",
    "    for token in tokens:\n",
    "        if token.startswith(\"##\"):\n",
    "            current_word += token[2:]  # append without '##'\n",
    "        else:\n",
    "            if current_word:  # push the previous word\n",
    "                merged.append(current_word)\n",
    "            current_word = token  # start a new word\n",
    "    if current_word:  # push the last one\n",
    "        merged.append(current_word)\n",
    "    return merged\n",
    "\n",
    "print(merge_wordpiece(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa63fca-816b-4e56-b3db-7546c542502b",
   "metadata": {},
   "source": [
    "下載中文文檔\n",
    "\n",
    "- https://github.com/rime-aca/corpus/blob/master/唐詩三百首.txt\n",
    "\n",
    "不是我喜歡文學，是這比較好找數據集，還不會被告。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0483d85e-4327-4d68-bf25-551d11fa3d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Read file\n",
    "filename = os.path.join(\"tutorial\", \"LLM+Langchain\", \"Week-1\", \"唐詩三百首.txt\")\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "poems = []\n",
    "\n",
    "# Split by blank lines\n",
    "blocks = [b.strip() for b in text.strip().split(\"\\n\\n\") if b.strip()]\n",
    "\n",
    "for block in blocks:\n",
    "    entry = {}\n",
    "    for line in block.split(\"\\n\"):\n",
    "        if line.startswith(\"詩名:\"):\n",
    "            entry[\"詩名\"] = line.replace(\"詩名:\", \"\").strip()\n",
    "        elif line.startswith(\"作者:\"):\n",
    "            entry[\"作者\"] = line.replace(\"作者:\", \"\").strip()\n",
    "        elif line.startswith(\"詩體:\"):\n",
    "            entry[\"詩體\"] = line.replace(\"詩體:\", \"\").strip()\n",
    "        elif line.startswith(\"詩文:\"):\n",
    "            entry[\"詩文\"] = line.replace(\"詩文:\", \"\").strip()\n",
    "    if len(entry) != 0:\n",
    "        poems.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22744e1b-8305-40fb-8092-8792e0d95a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "poems[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc04545-3b02-4e9b-bc21-228c2d518079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read file\n",
    "# filename = os.path.join(\"tutorial\", \"LLM+Langchain\", \"Week-1\", \"宋詞三百首.txt\")\n",
    "#pd. with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "#     text = f.read()\n",
    "\n",
    "# # Split by blank lines\n",
    "# blocks = [b.strip() for b in text.strip().split(\"\\n\\n\") if b.strip()]\n",
    "\n",
    "# for block in blocks:\n",
    "#     entry = {}\n",
    "#     for line in block.split(\"\\n\"):\n",
    "#         if line.startswith(\"詩名:\"):\n",
    "#             entry[\"詞牌\"] = line.replace(\"詞牌:\", \"\").strip()\n",
    "#         elif line.startswith(\"作者:\"):\n",
    "#             entry[\"作者\"] = line.replace(\"作者:\", \"\").strip()\n",
    "#         elif line.startswith(\"詩體:\"):\n",
    "#             entry[\"詞文\"] = line.replace(\"詞文:\", \"\").strip()\n",
    "#     if len(entry) != 0:\n",
    "#         poems.append(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95b10ab-7f97-47b3-b3e5-986302ce2d60",
   "metadata": {},
   "source": [
    "#### 建立Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feae8a65-014f-4c76-a232-f2ac0c005225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_poem = pd.DataFrame(poems)\n",
    "\n",
    "documents = []\n",
    "\n",
    "for _, row in df_poem.iterrows():\n",
    "    document = Document(page_content=row['詩文'],\n",
    "                        metadata={\"詩名\": row[\"詩名\"],\n",
    "                                  \"作者\": row[\"作者\"],\n",
    "                                  \"詩體\": row[\"詩體\"]})\n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387d534f-2498-4c02-ac16-542deb6c76da",
   "metadata": {},
   "source": [
    "自訂義函數，讓BM25使用BPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a543cfc-fd8e-452a-a945-7b340c3e6f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_func(text: str):\n",
    "\n",
    "    # 1. Define special tokens to remove\n",
    "    special_tokens = {\"<s>\", \"</s>\", \"[PAD]\", \"[UNK]\"}\n",
    "    \n",
    "    encoded = tokenizer(text)\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded[\"input_ids\"])\n",
    "\n",
    "    # 2. Remove special tokens\n",
    "    tokens = [t.replace(\"▁\", \"\") for t in tokens if t not in special_tokens]\n",
    "    \n",
    "    # 3. Remove punctuation (keep only Chinese/English/number words)\n",
    "    tokens = [t for t in tokens if re.match(r'[\\w一-龥]+', t)]\n",
    "    \n",
    "    # Stringify the tokens\n",
    "    return [str(token) for token in tokens]\n",
    "\n",
    "\n",
    "bm25_poem_retriever = BM25Retriever.from_documents(documents, k=5, \n",
    "                                                   bm25_params={\"k1\":2.5},\n",
    "                                                   preprocess_func=_preprocess_func\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ed0c0a-9413-4fd2-a41c-2fdc81090c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_poem_retriever.invoke(\"大風起兮雲飛揚 威加海內兮歸故鄉 安得猛士兮守四方\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b265ce-02e3-4e21-a8fa-b5dd6e3def1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_poem_retriever.invoke(\"夕陽無限好\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36283559-563c-4b09-8bd0-4896b76c8195",
   "metadata": {},
   "source": [
    "把詩經轉換成五言絕句... 有中文比較好的人嗎? XD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b011dc9-e7ea-47f8-be36-1e8544c51e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "# query\n",
    "\n",
    "query = dedent(\"\"\"\n",
    "蒹葭蒼蒼、白露為霜。\n",
    "所謂伊人、在水一方。\n",
    "遡洄從之、道阻且長。\n",
    "遡遊從之、宛在水中央。\n",
    "\"\"\")\n",
    "\n",
    "# output format\n",
    "class Output(BaseModel):\n",
    "    name: str = Field(description=\"result in traditional Chinese (繁體中文)\")\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=Output)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "\n",
    "# prompt template\n",
    "system_template = dedent(\"\"\"\n",
    "You are a helpful AI assistant with expertise in classical Chinese literature.\n",
    "You understand all the nuance and history background of all the content.\n",
    "\"\"\")\n",
    "system_prompt = PromptTemplate(template=system_template)\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template=(\"\"\"\n",
    "Create a {poetic_form}\n",
    "\n",
    "Examples:\n",
    "{context}\n",
    "\n",
    "according to the semantic of {query}\n",
    "\n",
    "Output instruction: {format_instructions}\n",
    "\"\"\"),\n",
    "input_variables=[\"poetic_form\", \"query\", \"context\"],\n",
    "partial_variables={'format_instructions': format_instructions}\n",
    ")\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt) \n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                 human_message\n",
    "                                               ])\n",
    "\n",
    "# retrieval\n",
    "# BM25 retriever 不支持 filter\n",
    "# 所以建議先filter內容\n",
    "\n",
    "df_poem = pd.DataFrame(poems)\n",
    "\n",
    "documents = []\n",
    "\n",
    "for _, row in df_poem.iterrows():\n",
    "    if row[\"詩體\"] == \"五言絕句\":\n",
    "        document = Document(page_content=row['詩文'],\n",
    "                            metadata={\"詩名\": row[\"詩名\"],\n",
    "                                      \"作者\": row[\"作者\"],\n",
    "                                      \"詩體\": row[\"詩體\"]})\n",
    "        documents.append(document)\n",
    "\n",
    "bm25_poem_retriever = BM25Retriever.from_documents(documents, k=5, \n",
    "                                                   bm25_params={\"k1\":2.5},\n",
    "                                                   preprocess_func=_preprocess_func\n",
    "                                                  )\n",
    "\n",
    "context = bm25_poem_retriever.invoke(query)\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065672bf-e381-4a26-8fb2-85be53118945",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\n\".join([c.page_content for c in context])\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993a621c-6af3-4aba-9fa2-5a3594da530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切換成 gpt-4o。gpt-4o-mini在這方面很弱\n",
    "\n",
    "model_poem = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o\", \n",
    "                   temperature=0 # a range from 0-2, the higher the value, the higher the `creativity`\n",
    "                  )\n",
    "\n",
    "prompt = chat_prompt.invoke({\"query\": query,\n",
    "                             \"poetic_form\": \"五言絕句\",\n",
    "                             \"context\": context})\n",
    "\n",
    "output = model_poem.invoke(prompt)\n",
    "\n",
    "parsed_output = output_parser.parse(output.content)\n",
    "\n",
    "print(parsed_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968f8861-ef45-4bb6-8f7b-108ce055d98b",
   "metadata": {},
   "source": [
    "# Wikipedia Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0ddbe9-374a-4f42-8a0a-e76f4d8f8409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade --quiet  wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325d56f0-bc4a-40ad-8b45-55bc24d4c0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "\n",
    "wiki_retriever = WikipediaRetriever()\n",
    "\n",
    "docs = wiki_retriever.invoke(\"2024 US presidential election\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8e184a-bf82-4687-acd3-ee28ccf3890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18856c98-7918-4afd-9675-3c0e69549d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d145bd2-875d-4f0b-8bfc-1fff44c9a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 若是少於給定返回數量，則返回當前所有可得到文件\n",
    "\n",
    "docs = wiki_retriever.invoke(\"rice\")\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5414999-cb9f-4e3b-a7fe-7340b56eea84",
   "metadata": {},
   "source": [
    "- If you want to know what parameters can be feed to the WikipediaRetriever:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98333101-de37-4203-9f2f-c9a42db21473",
   "metadata": {},
   "outputs": [],
   "source": [
    "WikipediaRetriever?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594d4f34-9d99-47ba-9f08-48ac5feb4a76",
   "metadata": {},
   "source": [
    "By default, wikipedia retriever returns 3 documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712d4f6f-12c5-4ef2-a92f-aec594002d76",
   "metadata": {},
   "source": [
    "# Ensemble Retriever\n",
    "\n",
    "- 它結合這些工具的結果並使用特殊方法進行組織。\n",
    "- 通過使用不同的工具，它比僅使用單一工具效果更好。\n",
    "- 通常，它結合兩種類型的搜索：一種尋找精確詞語（例如 BM25），另一種理解含義（例如嵌入式）。\n",
    "- 這種混合稱為 \"混合搜索\"。\n",
    "- 第一種工具尋找具有特定詞語的文檔，而第二種工具則尋找具有相似思想的文檔。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d36f78a-b55a-4e40-ab9c-9ea2151a5c38",
   "metadata": {},
   "source": [
    "- weights: 控制權重\n",
    "- 總返回文件數量等於個別檢索器 (retriever) 檢索文件數量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599b51aa-9c1b-4e4b-839f-9fb0bdac182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, wiki_retriever], weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cb3e2a-8efa-47a0-a682-574adbd1f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ensemble_retriever.invoke(\"rice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef0492-cf23-4c52-9685-a7860ba645e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91847b0e-ef31-45da-88da-a93888cd0093",
   "metadata": {},
   "source": [
    "- bm25_retriever 返回兩份\n",
    "- wiki_retriever 返回兩份"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14fcd07-cd35-4c5d-bcc2-973b81daa401",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 💼 實務應用案例：公司知識庫檢索\n",
    "\n",
    "假設你在一間科技公司工作，內部有數百份技術文件與專案紀錄。  \n",
    "若同事詢問：「我們去年哪個團隊使用過 LangChain？」  \n",
    "- **BM25 Retriever** 可用於快速搜尋文件中包含「LangChain」關鍵字的部分（高精度字面匹配）。  \n",
    "- **Embedding Retriever**（語義搜尋）則能找到即使未出現相同字詞、但語意相似的文件。  \n",
    "\n",
    "若同時使用兩者組合成 **Ensemble Retriever（混合檢索）**：\n",
    "- BM25 提供準確的字詞比對  \n",
    "- Embedding 提供語意理解  \n",
    "- 最後整合結果加權排序，能得到更完整、精確的搜尋結果  \n",
    "\n",
    "這類方法常用於：\n",
    "- 客服知識庫（自動回答客戶問題）  \n",
    "- 法律文件檢索  \n",
    "- 公司內部文件搜尋引擎  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33749100-3ad0-489b-882d-40a0b41232c2",
   "metadata": {},
   "source": [
    "# Runtime Configuration (運行時配置)\n",
    "\n",
    "- 我們也可以在運行時配置檢索器。為了做到這一點，我們需要將字段標記為可配置的。\n",
    "\n",
    "API Reference: https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.utils.ConfigurableField.htmld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2301ed88-4894-4c32-9e1c-a0291e192f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d883448-92c9-4a36-a4bd-20f73a755c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever = BM25Retriever.from_documents(documents, k=2, \n",
    "    bm25_params={\"k1\": 1}).configurable_fields(\n",
    "    k=ConfigurableField(\n",
    "        id=\"bm25_k\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f102aaad-9bf8-42a1-be3d-bc054b3c5c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, wiki_retriever], weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f5977c-e120-4f36-b227-262f2add9239",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"bm25_k\": 5}}\n",
    "docs = ensemble_retriever.invoke(\"rice\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51851ea0-c12e-42bb-b1cf-3e57314e15ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb7982d-f1cb-4036-9317-dd7f96c5836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - bm25_retriever 返回五份\n",
    "# - wiki_retriever 返回兩份"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792ac2d5-2284-426d-8894-c9aec4849a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, wiki_retriever], weights=[0.1, 0.9]\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"bm25_k\": 10}}\n",
    "docs = ensemble_retriever.invoke(\"rice\", config=config)\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085c45ab-899b-4d21-9f57-7939f79fa4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - bm25_retriever 返回十份\n",
    "# - wiki_retriever 返回兩份"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a8ec7b-1a3f-4eb1-b210-747d5943b7af",
   "metadata": {},
   "source": [
    "### This is what I do in my work:\n",
    "\n",
    "I use runtime configuration to target a specific data section with the applied attribute.\n",
    "\n",
    "More specifically, there are many types of cosmetic products, such as:\n",
    "\n",
    "- Lipstick\n",
    "- Lip Gloss\n",
    "- Mascara\n",
    "- Blush\n",
    "- Foundation\n",
    "- Nail Polish\n",
    "- Eyeliner\n",
    "- Eye Pencil\n",
    "\n",
    "These products are applied to different areas: face, nails, eyes, and lips.\n",
    "\n",
    "You can retrieve information more efficiently and accurately if you identify the correct application area beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a516b1-a7f7-4bea-8567-32d46570e2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(self.documents, embedding=embedding)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type='similarity',\n",
    "                                     search_kwargs={'k': self._k}).configurable_fields(search_kwargs=ConfigurableField(id=\"faiss_search_kwargs\"))\n",
    "\n",
    "semantic_retriever = retrievers['semantic']\n",
    "semantic_documents = semantic_retriever.invoke(product, config={\"configurable\":\n",
    "                                             {\"faiss_search_kwargs\":\n",
    "                                                  {\"fetch_k\":20,\n",
    "                                                   \"k\": 2,\n",
    "                                                   \"filter\": {\"applied\": area}}}})\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
