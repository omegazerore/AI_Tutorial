{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4651f25d-4013-47fd-b4f4-0b0fbea51514",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 上週作業"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199d93dc-074a-487b-af55-ed2b059dd497",
   "metadata": {},
   "source": [
    "## 這個部份上周剛好講過，所以跳過，自己看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fba15128-8699-4d6a-abad-39c5c1d61640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dff39d-b3b6-44b0-a63c-6bcc3bce6d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate, ChatPromptTemplate\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from src.io.path_definition import get_project_dir\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Ingredients: {input}\\nOrigin: {output}\",\n",
    ")\n",
    "\n",
    "with open(os.path.join(get_project_dir(), 'tutorial', 'Week-1', 'recipe_train.json'), 'r') as f:\n",
    "    recipe_train = json.load(f)\n",
    "\n",
    "examples = []\n",
    "\n",
    "for recipe in recipe_train:\n",
    "    examples.append({\"input\": \" \".join(recipe['ingredients']),\n",
    "                     \"output\": recipe['cuisine']})\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # The list of examples available to select from.\n",
    "    examples,\n",
    "    # The embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    embeddings,\n",
    "    # The VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    FAISS,\n",
    "    # The number of examples to produce.\n",
    "    k=5,\n",
    ")\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # We provide an ExampleSelector instead of examples.\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Find the recipe origin based on the ingredients\",\n",
    "    suffix=\"Ingredients: {ingredients}\\nOrigin:\",\n",
    "    input_variables=[\"ingredients\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec67d88-fe19-4f10-9168-eec17e11a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84950f81-c97a-41b4-8419-80b5b6729772",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(get_project_dir(), 'tutorial', 'Week-1', 'recipe_test.json'), 'r') as f:\n",
    "    recipe_test = json.load(f)\n",
    "\n",
    "existing_ingredients = recipe_test[99]['ingredients']\n",
    "\n",
    "similar_prompt.invoke(\", \".join(existing_ingredients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14ce1d22-e031-4938-9af3-a99c04a7535e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MengChieh\\Miniconda3\\envs\\llm_examples\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from src.initialization import credential_init\n",
    "\n",
    "credential_init()\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o-2024-05-13\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e574f1e-2645-4a4d-9c8e-a5c2a586fc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = similar_prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70d1f0f-7f91-4b46-af21-ac74639cf7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\", \".join(existing_ingredients))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4211fbc9-c8a2-4ac7-882d-65827e58045d",
   "metadata": {},
   "source": [
    "##  飛安報告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0bedbab-db46-4879-b937-1f23ad9dd648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.io.path_definition import get_project_dir\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(os.path.join(get_project_dir(), 'tutorial', 'Week-3', 'Data sample.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feb624f-b9ad-463f-a6ea-3afdd2670ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_prompt = PromptTemplate.from_template(\n",
    "#     '''You are an AI assistant assigned with a task of safty report classification based on the content.\n",
    "#     You are a seasoned flight safety inspector with deep and extensive knowledge of aviation safty. \n",
    "    \n",
    "#     You always do the best work you can. You are highly analytical and pay close attention to details. \n",
    "    \n",
    "#     The candidates of the output are:\n",
    "\n",
    "#     - `Organizational Influence;Resource Management`\n",
    "#     - `Organizational Influence;Organizational Climate`\n",
    "#     - `Organizational Influence;rganizational Process`\n",
    "#     - `Unsafe Supervisions;Inadequate Supervision`\n",
    "#     - `Unsafe Supervisions;Planned Inappropriate Operations`\n",
    "#     - `Unsafe Supervisions;Failed to Correct Problem`\n",
    "#     - `Unsafe Supervisions;Supervisory Violation`\n",
    "#     - `Precondition for Unsafe Acts;Environmental Factors;Physical Environment`\n",
    "#     - `Precondition for Unsafe Acts;Environmental Factors;Technological Environment`\n",
    "#     - `Precondition for Unsafe Acts;Condition of Operators;Adverse Mental State`\n",
    "#     - `Precondition for Unsafe Acts;Condition of Operators;Adverse Physiological State`\n",
    "#     - `Precondition for Unsafe Acts;Condition of Operators;Physical/Mental Limitations`\n",
    "#     - `Precondition for Unsafe Acts;Personnel Factors;Crew Resource Management`\n",
    "#     - `Precondition for Unsafe Acts;Personnel Factors;Personal Readiness`\n",
    "#     - `Unsafe Acts;Errors;Decision Errors`\n",
    "#     - `Unsafe Acts;Errors;Skill-Based Errors`\n",
    "#     - `Unsafe Acts;Errors;Perceptual Errors`\n",
    "#     - `Unsafe Acts;Violations;Routine`\n",
    "#     - `Unsafe Acts;Violations;Exceptional`\n",
    "\n",
    "#     The output is from one of the candidates. \n",
    "#     ''')\n",
    "\n",
    "# system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "# response_schemas = [\n",
    "#         ResponseSchema(name=\"category\", description=\"The predicted category of the classification\")\n",
    "#     ]\n",
    "\n",
    "\n",
    "# output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "# format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# human_prompt = PromptTemplate(template='{report}; format instruction: {format_instructions}',\n",
    "#                               input_variables=[\"report\"],\n",
    "#                               partial_variables={'format_instructions': format_instructions}\n",
    "#                               )\n",
    "# human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "# chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "#                                                 human_message\n",
    "#                                                ])\n",
    "\n",
    "# chain = chat_prompt|model|output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a3bd16-57fc-45f2-a076-ae52027ff739",
   "metadata": {},
   "source": [
    "### 回家作業 1\n",
    "\n",
    "若要飛安事故報告可以有複數分類結果，如何調整Prompt，包含parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "791c6a65-e308-4c1c-8c82-ea17363f7a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "\n",
    "system_prompt = PromptTemplate.from_template(\n",
    "    '''You are an AI assistant assigned with a task of safty report classification based on the content.\n",
    "    You are a seasoned flight safety inspector with deep and extensive knowledge of aviation safty. \n",
    "    \n",
    "    You always do the best work you can. You are highly analytical and pay close attention to details. \n",
    "    \n",
    "    The candidates of the output are:\n",
    "\n",
    "    - `Organizational Influence;Resource Management`\n",
    "    - `Organizational Influence;Organizational Climate`\n",
    "    - `Organizational Influence;rganizational Process`\n",
    "    - `Unsafe Supervisions;Inadequate Supervision`\n",
    "    - `Unsafe Supervisions;Planned Inappropriate Operations`\n",
    "    - `Unsafe Supervisions;Failed to Correct Problem`\n",
    "    - `Unsafe Supervisions;Supervisory Violation`\n",
    "    - `Precondition for Unsafe Acts;Environmental Factors;Physical Environment`\n",
    "    - `Precondition for Unsafe Acts;Environmental Factors;Technological Environment`\n",
    "    - `Precondition for Unsafe Acts;Condition of Operators;Adverse Mental State`\n",
    "    - `Precondition for Unsafe Acts;Condition of Operators;Adverse Physiological State`\n",
    "    - `Precondition for Unsafe Acts;Condition of Operators;Physical/Mental Limitations`\n",
    "    - `Precondition for Unsafe Acts;Personnel Factors;Crew Resource Management`\n",
    "    - `Precondition for Unsafe Acts;Personnel Factors;Personal Readiness`\n",
    "    - `Unsafe Acts;Errors;Decision Errors`\n",
    "    - `Unsafe Acts;Errors;Skill-Based Errors`\n",
    "    - `Unsafe Acts;Errors;Perceptual Errors`\n",
    "    - `Unsafe Acts;Violations;Routine`\n",
    "    - `Unsafe Acts;Violations;Exceptional`\n",
    "\n",
    "     The report can involve multiple categories.\n",
    "    ''')\n",
    "\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "response_schemas = [\n",
    "        ResponseSchema(name=\"categories\", description=\"The predicted categories as a python list\")\n",
    "    ]\n",
    "\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "human_prompt = PromptTemplate(template='{report}; format instruction: {format_instructions}',\n",
    "                              input_variables=[\"report\"],\n",
    "                              partial_variables={'format_instructions': format_instructions}\n",
    "                              )\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                human_message\n",
    "                                               ])\n",
    "\n",
    "chain = chat_prompt|model|output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa85343f-4c8f-40a0-9a5c-0b76d8a6cfa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aircraft X was inbound to the airport from the southeast. Apparently the aircraft was on an IFR flight plan and I completely overlooked that information. It took the aircraft traveling about 10 miles inside my airspace before the pilot called and made first contact with me. When the pilot finally called me and told me he had the current ATIS I had asked the pilot which route he wanted to take inbound to North Las Vegas airport. Again, I failed to look at all the information in the data block and treated him as if he was VFR. When I asked the pilot which route he wanted and he was open to either route, I gave the aircraft a clearance through the Class Bravo airspace and assigned a heading of 270 with no altitude assigned. A minute or so later I asked the pilot if he was familiar with the local flying area and the surrounding terrain, and after he replied in the affirmative, I instructed the pilot to resume own navigation then fly heading 360.After I got that read back, I instructed the pilot to cross the airport at and maintain 5,000 feet. A few minutes later as he approached the airport from the southeast and was approximately 6 miles to the southeast and over a 6,300 Minimum Vectoring Altitude. I had noticed there were numerous VFR departures departing southbound, so I assigned a heading of 320 to stay away from the targets I saw departing. At this time the pilot had already begun his descent to comply with the VFR instructions that I had issued, and never mentioned that he was IFR. As stated before I didn't realize he was IFR. I had made a couple traffic calls regarding the departing aircraft who were not Radar identified, and at that time a Controller a few positions over mentioned the altitude of Aircraft X and that was the first time I noticed the data block and all the information that I initially missed indicating that he was in fact IFR. At that point The aircraft was already at 5,500 feet heading northwest bound, and it was at this point that I and the Supervisor talked about the aircraft and the situation at hand.I don't know that there are any recommendations or changes that can or should be made to prevent this from happening again in the future. I simply missed the information in the data block indicating that this aircraft was IFR. When I issued a Class Bravo clearance and all appropriate navigation and altitude instructions for a VFR aircraft, I had the expectation bias that the aircraft was indeed VFR when he accepted all the instructions and never questioned the clearance. I missed the indicators that he was on an IFR flight plan and there is nothing more to it and completely my fault and no one else's.\n"
     ]
    }
   ],
   "source": [
    "text = df.iloc[3]['Report 1']\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72bb4f5f-014d-4db3-b0f7-2406ef39d7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke({\"report\": text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0528ae49-4b66-481a-a138-f4cd0a119a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'categories': ['Unsafe Acts;Errors;Decision Errors',\n",
       "  'Unsafe Acts;Errors;Skill-Based Errors',\n",
       "  'Precondition for Unsafe Acts;Condition of Operators;Adverse Mental State',\n",
       "  'Precondition for Unsafe Acts;Personnel Factors;Crew Resource Management',\n",
       "  'Unsafe Supervisions;Inadequate Supervision']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cccb10a-b660-40d9-a275-d6bc89facecd",
   "metadata": {},
   "source": [
    "### 回家作業 2\n",
    "\n",
    "你可以很清楚的看到一個飛安事故中，可以出現複數報告。\n",
    "將`Report 1` 和 `Report 1.2` 結合起來產生一份的新報告。\n",
    "\n",
    "抄也是一門技術"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a65c3931-9f44-4a35-9c78-f2a7500a0b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = PromptTemplate.from_template(\n",
    "    '''You are an AI assistant assigned with a task of safty report classification based on the content.\n",
    "    You are a seasoned flight safety inspector with deep and extensive knowledge of aviation safty. \n",
    "\n",
    "    You will receieve two reports <report_1> and <report_2> and you will consolidate the content before drawing conclusion. \n",
    "    \n",
    "    You always do the best work you can. You are highly analytical and pay close attention to details. \n",
    "    \n",
    "    The candidates of the output are:\n",
    "\n",
    "    - `Organizational Influence;Resource Management`\n",
    "    - `Organizational Influence;Organizational Climate`\n",
    "    - `Organizational Influence;rganizational Process`\n",
    "    - `Unsafe Supervisions;Inadequate Supervision`\n",
    "    - `Unsafe Supervisions;Planned Inappropriate Operations`\n",
    "    - `Unsafe Supervisions;Failed to Correct Problem`\n",
    "    - `Unsafe Supervisions;Supervisory Violation`\n",
    "    - `Precondition for Unsafe Acts;Environmental Factors;Physical Environment`\n",
    "    - `Precondition for Unsafe Acts;Environmental Factors;Technological Environment`\n",
    "    - `Precondition for Unsafe Acts;Condition of Operators;Adverse Mental State`\n",
    "    - `Precondition for Unsafe Acts;Condition of Operators;Adverse Physiological State`\n",
    "    - `Precondition for Unsafe Acts;Condition of Operators;Physical/Mental Limitations`\n",
    "    - `Precondition for Unsafe Acts;Personnel Factors;Crew Resource Management`\n",
    "    - `Precondition for Unsafe Acts;Personnel Factors;Personal Readiness`\n",
    "    - `Unsafe Acts;Errors;Decision Errors`\n",
    "    - `Unsafe Acts;Errors;Skill-Based Errors`\n",
    "    - `Unsafe Acts;Errors;Perceptual Errors`\n",
    "    - `Unsafe Acts;Violations;Routine`\n",
    "    - `Unsafe Acts;Violations;Exceptional`\n",
    "\n",
    "     The report can involve multiple categories.\n",
    "    ''')\n",
    "\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "response_schemas = [\n",
    "        ResponseSchema(name=\"categories\", description=\"The predicted categories as a python list\")\n",
    "    ]\n",
    "\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "human_prompt = PromptTemplate(template=\"\"\"report_1:{report_1}; \n",
    "                                          report_2: {report_2};\n",
    "                                          format instruction: {format_instructions}\"\"\",\n",
    "                              input_variables=[\"report_1, report_2\"],\n",
    "                              partial_variables={'format_instructions': format_instructions}\n",
    "                              )\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                human_message\n",
    "                                               ])\n",
    "\n",
    "chain = chat_prompt|model|output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c4eab77-df3e-468d-95a6-458acb55d146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'L30 TRACON Controller reported they issued VFR clearance instructions to an aircraft that was on an IFR flight plan which resulted in the aircraft flying below the Minimum Vectoring Altitude.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[3]['Report 1.2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "daf23ec5-ce0b-401a-bc8d-81a054bf9546",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke({\"report_1\": text,\n",
    "                       \"report_2\": df.iloc[3]['Report 1.2']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93349a8b-2ed6-417f-adaf-d71fe9ccfe7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unsafe Acts;Errors;Decision Errors',\n",
       " 'Unsafe Supervisions;Inadequate Supervision',\n",
       " 'Precondition for Unsafe Acts;Condition of Operators;Adverse Mental State']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['categories']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a021c6-1575-48d5-b7ef-b8a24d3b0446",
   "metadata": {},
   "source": [
    "### Keynote\n",
    "\n",
    "- 若你更理解你的數據，你可以建立更精確的Prompt，更明確的表示每個數據代表的意義，來提升輸出的品質。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998d1aff-a4a4-4c14-a39c-5bb88c0942c9",
   "metadata": {},
   "source": [
    "# Remote server\n",
    "\n",
    "### 1. Making a POST Request (發送 POST 請求):\n",
    "\n",
    "- requests.post(...) sends an HTTP POST request to the specified URL.\n",
    "- The URL \"http://localhost:5000/openai/invoke\" points to a local server running on port 5000, at the endpoint /openai/invoke.\n",
    "- The json parameter is used to send a JSON payload with the request. In this case, the payload is {'input': \"Where is Taiwan\"}.\n",
    "- requests.post(...) 發送一個 HTTP POST 請求到指定的 URL。\n",
    "- URL \"http://localhost:5000/openai/invoke\" 指向一個本地服務器，該服務器在端口 5000 上運行，並且指向 /openai/invoke 端點。\n",
    "- json 參數用於隨請求發送 JSON 負載。在這個例子中，負載是 {'input': \"Where is Taiwan\"}。\n",
    "\n",
    "### 2. Response Handling (響應處理):\n",
    "\n",
    "- The server processes the request and sends back a response.\n",
    "- The response is stored in the response variable, which can then be inspected or used further in the code.\n",
    "- 服務器處理請求並返回響應。\n",
    "- 響應存儲在 response 變量中，之後可以檢查或在代碼中進一步使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fb6c54e-f2f8-4e0e-92b8-0ad9c3803e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:5000/openai/invoke\",\n",
    "    json={'input': \"Where is Taiwan\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3dc0e8a-9380-413a-805b-9890d4142cb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': {'content': 'Taiwan is an island located in East Asia. It lies off the southeastern coast of China, separated by the Taiwan Strait. To its north is Japan, and to its south is the Philippines. The island is approximately 180 kilometers (112 miles) off the coast of mainland China. Taiwan is known for its bustling cities, mountainous terrain, and rich cultural heritage. The capital city of Taiwan is Taipei.',\n",
       "  'additional_kwargs': {},\n",
       "  'response_metadata': {'token_usage': {'completion_tokens': 81,\n",
       "    'prompt_tokens': 10,\n",
       "    'total_tokens': 91,\n",
       "    'prompt_tokens_details': {'cached_tokens': 0},\n",
       "    'completion_tokens_details': {'reasoning_tokens': 0}},\n",
       "   'model_name': 'gpt-4o-2024-05-13',\n",
       "   'system_fingerprint': 'fp_d0c6e590be',\n",
       "   'finish_reason': 'stop',\n",
       "   'logprobs': None},\n",
       "  'type': 'ai',\n",
       "  'name': None,\n",
       "  'id': 'run-500db718-5c13-4a60-9773-94bb8c3980f1-0',\n",
       "  'example': False,\n",
       "  'tool_calls': [],\n",
       "  'invalid_tool_calls': [],\n",
       "  'usage_metadata': None},\n",
       " 'metadata': {'run_id': '500db718-5c13-4a60-9773-94bb8c3980f1',\n",
       "  'feedback_tokens': []}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "921356d3-9ff3-4958-978d-bea13a5465be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Taiwan is an island located in East Asia. It lies off the southeastern coast of China, separated by the Taiwan Strait. To its north is Japan, and to its south is the Philippines. The island is approximately 180 kilometers (112 miles) off the coast of mainland China. Taiwan is known for its bustling cities, mountainous terrain, and rich cultural heritage. The capital city of Taiwan is Taipei.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()['output']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ea853f-f380-42b2-a57a-0249ae57bca0",
   "metadata": {},
   "source": [
    "# Use the remote model as `Software as a service` (SaaS)\n",
    "\n",
    "## Basic Usage\n",
    "\n",
    "### 1. Creating an Instance of RemoteRunnable (創建 RemoteRunnable 的實例):\n",
    "\n",
    "- This line creates an instance of RemoteRunnable and initializes it with the URL of the remote language model service. In this case, the service is running locally on http://localhost:5000/openai/.\n",
    "- 這行代碼創建一個 RemoteRunnable 的實例，並用遠程語言模型服務的 URL 進行初始化。在這個例子中，服務在本地運行，URL 為 http://localhost:5000/openai/。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84212830-9dfe-47c3-bc65-63de510f8a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langserve import RemoteRunnable\n",
    "\n",
    "llm = RemoteRunnable(\"http://localhost:5000/openai/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4c2ea5-4ec2-4298-af5e-6e85aaa4c320",
   "metadata": {},
   "source": [
    "### 2. Asynchronous Streaming of Responses (異步流式處理回應):\n",
    "\n",
    "- llm.astream(\"Where is Taiwan?\") sends the query \"Where is Taiwan?\" to the remote service and retrieves the response as a stream.\n",
    "- async for msg in ... is used to handle the streaming responses asynchronously.\n",
    "- print(msg.content, end=\"\", flush=True) prints each message content received from the stream without adding a new line after each message, and flushes the output buffer to ensure the message is displayed immediately.\n",
    "- llm.astream(\"Where is Taiwan?\") 將查詢 \"Where is Taiwan?\" 發送到遠程服務，並以流的形式檢索回應。\n",
    "- async for msg in ... 用於異步處理流式回應。\n",
    "- print(msg.content, end=\"\", flush=True) 打印每個從流中接收到的消息內容，不在每個消息後添加新行，並刷新輸出緩衝區以確保消息立即顯示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfc16c50-aab3-4d58-8091-aa8ac6c4a917",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taiwan is an island located in East Asia. It lies off the southeastern coast of China, across the Taiwan Strait. To its north is Japan, and to its south is the Philippines. The island is approximately 180 kilometers (112 miles) away from the coast of mainland China. Taiwan is known for its bustling cities, mountainous terrain, and rich cultural heritage. The capital city of Taiwan is Taipei."
     ]
    }
   ],
   "source": [
    "# Supports astream\n",
    "async for msg in llm.astream(\"Where is Taiwan?\"):\n",
    "    print(msg.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cafcf229-ae2c-41a9-bcf6-4503c366c2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm.invoke(\"Where is Taiwan?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d637005-0c88-43a1-817e-b96039218384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Taiwan is an island located in East Asia. It lies off the southeastern coast of China, across the Taiwan Strait. To its north is Japan, and to its south is the Philippines. Taiwan is situated in the western Pacific Ocean and is part of the region known as the \"Pacific Rim.\" The capital city of Taiwan is Taipei.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71923692-63fc-4dad-8277-b092ea0839b6",
   "metadata": {},
   "source": [
    "## Make the external service a part of the chain\n",
    "\n",
    "### 1. Comedian Chain (喜劇演員鏈)\n",
    "\n",
    "- ChatPromptTemplate.from_messages(...) creates a prompt template where the system prompt instructs the model to either tell a joke or state a fact, and the human prompt provides the input.\n",
    "- This template is then piped (|) to a language model (llm) to generate the comedian's response.\n",
    "- ChatPromptTemplate.from_messages(...) 創建一個提示模板，其中系統提示指示模型要麼講一個笑話，要麼陳述一個不搞笑的事實，並且僅輸出一個。\n",
    "- 然後將此模板通過管道（|）傳遞給語言模型（llm），以生成喜劇演員的回應。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33ab7c70-e6d6-4b41-b499-eabff45259d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "comedian_chain = (\n",
    "    ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a comedian that sometimes tells funny jokes and other times you just state facts that are not funny. Please either tell a joke or state fact now but only output one.\",\n",
    "            ),\n",
    "            ('human', '{input}'\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    | llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37c52f1-c5c9-4fc4-b15d-988ad3b5a173",
   "metadata": {},
   "source": [
    "### 2. Joke Classifier Chain\n",
    "\n",
    "- This chain is similar to the comedian chain but serves a different purpose.\n",
    "- The system prompt asks the model to classify the joke as \"funny\" or \"not funny\" and repeat the first five words for reference.\n",
    "- This template is also piped to the language model (llm).\n",
    "- 這個鏈與喜劇演員鏈類似，但用途不同。\n",
    "- 系統提示要求模型將笑話分類為“搞笑”或“不搞笑”，並重複笑話的前五個詞以供參考。\n",
    "- 此模板也通過管道傳遞給語言模型（llm）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95c76e1f-f294-49fc-a174-7fcb5b95cf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "joke_classifier_chain = (\n",
    "    ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"Please determine if the joke is funny. Say `funny` if it's funny and `not funny` if not funny. Then repeat the first five words of the joke for reference...\",\n",
    "            ),\n",
    "            (\"human\", \"{joke}\"),\n",
    "        ]\n",
    "    )\n",
    "    | llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2534a7-0dd5-4b54-a866-18766d82f786",
   "metadata": {},
   "source": [
    "### 3. Combining Chains with RunnablePassthrough\n",
    "\n",
    "- This combines the comedian chain and the joke classifier chain using RunnablePassthrough.assign.\n",
    "- The comedian chain generates the output, and then this output is passed to the joke classifier chain to classify its humor.\n",
    "- 這將喜劇演員鏈和笑話分類器鏈結合在一起，使用 RunnablePassthrough.assign。\n",
    "- 喜劇演員鏈生成輸出，然後將此輸出傳遞給笑話分類器鏈以分類其幽默性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a34bdc7-c1b3-4176-ba7a-13744f6b0811",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = {\"joke\": comedian_chain} | RunnablePassthrough.assign(\n",
    "    classification=joke_classifier_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3bacecf8-60af-44a2-82e7-4fc02dd5b911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': AIMessage(content='Why did the scarecrow win an award?\\n\\nBecause he was outstanding in his field!', response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 51, 'total_tokens': 68, 'prompt_tokens_details': {'cached_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d0c6e590be', 'finish_reason': 'stop', 'logprobs': None}, id='run-4a232e70-1f6e-476f-a823-b9c1e0691927-0'),\n",
       " 'classification': AIMessage(content='funny\\n\\n\"Why did the scarecrow\"', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 198, 'total_tokens': 208, 'prompt_tokens_details': {'cached_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_4ba85e82b6', 'finish_reason': 'stop', 'logprobs': None}, id='run-60089f8f-722c-4c80-9584-222734033019-0')}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"A man and a beer\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f702f519-37dd-4b45-a492-0c4bcdd521b0",
   "metadata": {},
   "source": [
    "# ChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22673222-780c-4356-a6d1-92c69b28d6fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://python.langchain.com/v0.1/assets/images/chat_use_case-eb8a4883931d726e9f23628a0d22e315.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(url=\"https://python.langchain.com/v0.1/assets/images/chat_use_case-eb8a4883931d726e9f23628a0d22e315.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb536cb3-dc3e-4068-adb7-ce70ef0ca4e5",
   "metadata": {},
   "source": [
    "- N-Shot\n",
    "- The historical chat history can be consdiered as a list of question-answer pairs\n",
    "- If the chatbot doesn’t remember past chats, it’s called stateless because it doesn’t know what happened before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f7fe22-cd1d-470a-927b-31725616d95c",
   "metadata": {},
   "source": [
    "## Minimal Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e5f885c-cb85-477f-90e8-147e701e6857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='我愛程式設計。', response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 23, 'total_tokens': 30, 'prompt_tokens_details': {'cached_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d0c6e590be', 'finish_reason': 'stop', 'logprobs': None}, id='run-4fb66383-37f5-4086-b5d5-1c164ec69ac5-0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=\"Translate this sentence from English to Chinese (繁體中文): I love programming.\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95345038-9a83-465c-a85a-9b2d5f1b6476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I haven't said anything yet! How can I assist you today?\", response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 13, 'total_tokens': 26, 'prompt_tokens_details': {'cached_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_1fafdb53b0', 'finish_reason': 'stop', 'logprobs': None}, id='run-128f5340-b94a-4883-b5a0-bbdcce7672d1-0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"What did you just say?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71ee93d-2841-4cc0-b417-3fa5554ba1b3",
   "metadata": {},
   "source": [
    "The `memory` is not there, so it does not understand your question.\n",
    "\n",
    "The following example shows how to add memory into the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ed4e110-9be7-4a67-be86-31b62451baf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I translated \"I love programming\" to Traditional Chinese as \"我愛程式設計.\"', response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 45, 'total_tokens': 64, 'prompt_tokens_details': {'cached_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d0c6e590be', 'finish_reason': 'stop', 'logprobs': None}, id='run-211f2cb5-43db-4458-bef7-de980c3f41a8-0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=\"Translate this sentence from English to  Chinese (繁體中文): I love programming.\"\n",
    "        ),\n",
    "        AIMessage(content=\"我愛程式設計.\"),\n",
    "        HumanMessage(content=\"What did you just say?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce24122f-cf20-4667-bc23-5eebedd54992",
   "metadata": {},
   "source": [
    "## Prompt templates\n",
    "\n",
    "### 1. Creating a ChatPromptTemplate \n",
    "\n",
    "- ChatPromptTemplate.from_messages(...) creates a prompt template for the chatbot.\n",
    "- The first message in the template is a system message: \"You are a helpful assistant. Answer all questions to the best of your ability.\" This message sets the context and behavior of the assistant, instructing it to be helpful and thorough in its responses.\n",
    "- MessagesPlaceholder(variable_name=\"messages\") is a placeholder for dynamic content. The variable_name=\"messages\" specifies that this placeholder will be filled with user messages during the conversation.\n",
    "- ChatPromptTemplate.from_messages(...) 創建了一個聊天機器人的提示模板。\n",
    "- 模板中的第一條消息是一條系統消息：“You are a helpful assistant. Answer all questions to the best of your ability.” 此消息設置了助手的上下文和行為，指示其在回答中要提供幫助並盡力而為。\n",
    "- MessagesPlaceholder(variable_name=\"messages\") 是一個動態內容的佔位符。variable_name=\"messages\" 指定該佔位符將在對話中插入用戶消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3dea7e35-f9b4-4d78-8a8e-e85225c5fd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e88d592-3e03-4a5b-81b2-e792154f94ec",
   "metadata": {},
   "source": [
    "### 2. Creating the Chain\n",
    "\n",
    "- This line pipes (|) the prompt template to a language model (model).\n",
    "- chain represents a sequence of operations where the prompt template is used to format user messages, and the language model processes these messages to generate responses.\n",
    "- 這行代碼通過管道（|）將提示模板傳遞給語言模型（model）。\n",
    "- chain 代表一系列操作，其中提示模板用於格式化用戶消息，語言模型處理這些消息以生成回應。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6fd2ed39-46d6-460e-b78e-5b0c4d06fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebca060-ce9e-4fba-be50-a40d923324fc",
   "metadata": {},
   "source": [
    "The MessagesPlaceholder above inserts chat messages passed into the chain's input as chat_history directly into the prompt. Then, we can invoke the chain like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22e2a159-a35a-47db-ac82-43c8d9f85887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I translated \"I love programming\" to \"我愛程式設計\" in Traditional Chinese (繁體中文).', response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 65, 'total_tokens': 89, 'prompt_tokens_details': {'cached_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_1fafdb53b0', 'finish_reason': 'stop', 'logprobs': None}, id='run-8e26ff7b-6842-4d2b-868e-e79c311ea96d-0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "            content=\"Translate this sentence from English to  Chinese (繁體中文): I love programming.\"\n",
    "        ),\n",
    "        AIMessage(content=\"我愛程式設計.\"),\n",
    "        HumanMessage(content=\"What did you just say?\"),\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c789c9c-46ac-4884-b059-466855e114aa",
   "metadata": {},
   "source": [
    "## Example of Using MessageHistory\n",
    "\n",
    "As a shortcut for managing the chat history, we can use a MessageHistory class, which is responsible for saving and loading chat messages. There are many built-in message history integrations that persist messages to a variety of databases, but for this quickstart we'll use a in-memory, demo message history called ChatMessageHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e0029f-228a-4ff5-afb0-91bf92f00d34",
   "metadata": {},
   "source": [
    "### 1. Importing the ChatMessageHistory Class (導入 ChatMessageHistory 類)\n",
    "\n",
    "- This line imports the ChatMessageHistory class from the langchain.memory module. This class is used to handle the chat messages in memory.\n",
    "- 這行代碼從 langchain.memory 模塊中導入 ChatMessageHistory 類。此類用於在內存中處理聊天消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33f65a6f-3d57-4cf9-8e8f-34623e7965e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6cb01a-d8c8-4d51-8b9a-35b79a8c06f6",
   "metadata": {},
   "source": [
    "### 2. Creating an Instance of ChatMessageHistory (創建 ChatMessageHistory 的實例)\n",
    "\n",
    "- This line creates an instance of ChatMessageHistory. This instance will store the chat messages in memory for this session.\n",
    "- 這行代碼創建一個 ChatMessageHistory 的實例。該實例將在此會話期間將聊天消息存儲在內存中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "00fba54c-8018-4f75-963d-6195892ae170",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_chat_history = ChatMessageHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577051c2-bd5e-4b1a-a2a2-2f29cba91070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67e08da2-cb06-41db-ab6a-69532596b405",
   "metadata": {},
   "source": [
    "### 3. Adding User and AI Messages (添加用戶和 AI 消息)\n",
    "\n",
    "- demo_chat_history.add_user_message(\"hi!\") adds a user message (\"hi!\") to the chat history.\n",
    "- demo_chat_history.add_ai_message(\"whats up?\") adds an AI response (\"whats up?\") to the chat history.\n",
    "- demo_chat_history.add_user_message(\"hi!\") 將用戶消息（“hi!”）添加到聊天記錄中。\n",
    "- demo_chat_history.add_ai_message(\"whats up?\") 將 AI 回應（“whats up?”）添加到聊天記錄中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "638e4f96-f747-4f1d-97e2-d5e52749a0a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='hi!'), AIMessage(content='whats up?')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_chat_history.add_user_message(\"hi!\")\n",
    "\n",
    "demo_chat_history.add_ai_message(\"whats up?\")\n",
    "\n",
    "demo_chat_history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "71eaf68e-486a-44cb-8107-c81b2b11e578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Translate this sentence from English to  Chinese (繁體中文): I love programming.'),\n",
       " AIMessage(content='我愛程式設計.')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_chat_history = ChatMessageHistory()\n",
    "\n",
    "demo_chat_history.add_user_message(\"Translate this sentence from English to  Chinese (繁體中文): I love programming.\")\n",
    "\n",
    "demo_chat_history.add_ai_message(\"我愛程式設計.\")\n",
    "\n",
    "demo_chat_history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3138b941-7bc6-4e08-890a-2d6debdc8431",
   "metadata": {},
   "source": [
    "### 4. Retrieving the Messages (檢索消息)\n",
    "\n",
    "- This line retrieves the list of messages stored in demo_chat_history. Each message is an object that contains information about the sender (user or AI) and the content of the message.\n",
    "- 這行代碼檢索存儲在 demo_chat_history 中的消息列表。每條消息都是一個對象，包含有關發送者（用戶或 AI）和消息內容的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88f86ffa-b2bd-4d90-9a62-971b55701eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Translate this sentence from English to  Chinese (繁體中文): I love programming.'),\n",
       "  AIMessage(content='我愛程式設計.')]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"messages\": demo_chat_history.messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a9cb4014-b66b-40dc-8acc-3e052426ce66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='我愛編程。', response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 76, 'total_tokens': 81, 'prompt_tokens_details': {'cached_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_e40a41d203', 'finish_reason': 'stop', 'logprobs': None}, id='run-2227d1b8-f45f-4ba7-8487-9ddd55f86d46-0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_chat_history.add_user_message(\n",
    "    \"Translate this sentence from English to  Chinese (繁體中文): I love programming.\"\n",
    ")\n",
    "\n",
    "response = chain.invoke({\"messages\": demo_chat_history.messages})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "54650b6d-115e-418e-9910-e87c90872ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I translated \"I love programming\" to \"我愛編程。\" in Chinese (繁體中文). This is another way to say it, but if you prefer the term \"程式設計,\" which is also commonly used, the translation would be \"我愛程式設計。\" Both are correct and understood in Traditional Chinese.', response_metadata={'token_usage': {'completion_tokens': 68, 'prompt_tokens': 95, 'total_tokens': 163, 'prompt_tokens_details': {'cached_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_e40a41d203', 'finish_reason': 'stop', 'logprobs': None}, id='run-f1c99bb2-c0d9-4834-b9a7-cd32b62bd295-0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put the response back into the demo_chat_history\n",
    "\n",
    "demo_chat_history.add_ai_message(response.content)\n",
    "\n",
    "demo_chat_history.add_user_message(\"What did you just say?\")\n",
    "\n",
    "chain.invoke({\"messages\": demo_chat_history.messages})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f147092-7b3c-4392-8d8e-90c65b09d5d2",
   "metadata": {},
   "source": [
    "# **** 預計第一個小時結束 ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ccb7abfb-9d66-4ca8-96c7-2358b21d009e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MengChieh\\Miniconda3\\envs\\llm_examples\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "C:\\Users\\MengChieh\\Miniconda3\\envs\\llm_examples\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "C:\\Users\\MengChieh\\Miniconda3\\envs\\llm_examples\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\MengChieh\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from src.initialization import credential_init\n",
    "\n",
    "\n",
    "credential_init()\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o-2024-05-13\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aea1c49-906f-481c-9020-6b40cf07dba4",
   "metadata": {},
   "source": [
    "## Conversational Retrievers - Step 1\n",
    "\n",
    "- 土味情話反殺大全 (推薦上Youtube看)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c1831d12-b64a-4d21-b30f-10cc421d3500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "df = pd.DataFrame(data=[[\"确认过眼神，你是我爱的人。\", \"确认过眼神，我是你泡不到的人。\"],\n",
    "                         [\"万水千山总是情，爱我多一点行不行。\", \"一寸光阴一寸金，劝你死了这条心。\"],\n",
    "                         [\"今天吃了泡面，吃了炒面，还是想走进你的心里面。\", \"吃那么多面，最后还不是变成大便。\"],\n",
    "                         [\"草莓，蓝莓，蔓越莓，今天你想我了没？\", \"冬瓜，西瓜，哈密瓜，你再巴巴我打得你叫妈妈。\"],\n",
    "                         [\"众生皆苦，唯你独甜。\", \"尝遍众生，你为渣男代言。\"],\n",
    "                         [\"你喜欢瑞士名表还是我帅气的外表？\", \"我喜欢去年买了个表。\"],\n",
    "                         [\"我想问一条路，到哥哥心里的路。\", \"山路十八弯，走完脑血栓。\"],\n",
    "                         [\"小姐姐，我心里给你留了一块地，死心塌地。\", \"对不起，我的心里只容得下一块地，玛莎拉蒂。\"],\n",
    "                         [\"小姐姐你笑起来真好看啊。\", \"你看起来真好笑啊。\"],\n",
    "                         [\"亲爱的你知道吗，你的笑容没有酒，我却醉得像条狗\", \"我的笑容没有酒，你是真的像条狗\"],\n",
    "                         [\"宝贝儿，我在手上划了一道口子，你也划一下吧，这样我们就是两口子了\", \"我怕我们的血溶到一起，被你发现其实我是你爸爸\"],\n",
    "                         [\"这世间万物都有尽头，落叶归根，而我归你\", \"对不起 我不收垃圾\"],\n",
    "                         [\"请问……我想问一下路，那条通往你心里的路\", \"八格牙路\"],\n",
    "                         [\"你今天怎么怪怪的？ 怪可爱的\",  \"你今天也怪怪的，怪恶心的\"],\n",
    "                         [\"亲爱的，你知道我和唐僧的区别吗？ 唐僧取经我娶你\", \"知道你和沙僧的区别吗？ 他叫沙僧你叫沙雕\"],\n",
    "                         [\"亲爱的，你不觉得累吗？ 你已经在我的脑海里跑了好几圈了\", \"傻孩子，我在找出口呢\"],\n",
    "                         [\"莫文蔚的阴天。孙燕姿的雨天，周杰伦的晴天，都不如你和我聊天\", \"求求你了，能否还我一个宁静的夏天\"],\n",
    "                         [\"如果你是方便面，那我就是白开水，今生今世，我泡定你了\", \"故事的最后，她变成了屎，你变成了尿，你们终究分道扬镳\"],\n",
    "                         [\"大年三十晚上的鞭炮再响，也没有我想你那么想\", \"大年三十晚上的鞭炮再响，也没有你放的屁响\"],\n",
    "                         [\"c罗可以上演帽子戏法，可我想你却没有办法\", \"c罗可以上演帽子戏法，我也可以给你上演绿帽子戏法\"],\n",
    "                         [\"不要抱怨，抱我\", \"抱不起来，太重\"],\n",
    "                         [\"你有没有发现我的眼睛很好看？因为我满眼都是你啊\", \"对不起，你眼睛在哪呢？\"]], \n",
    "                  columns=['input', 'output'])\n",
    "\n",
    "documents = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    documents.append(Document(page_content=row['input'], metadata={'output': row['output']}))\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=documents, embedding=embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\",\n",
    "                                     search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678da92e-5b69-44ad-87d6-ac08da7c2d8f",
   "metadata": {},
   "source": [
    "## Build Chat Chain - Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "057ea8b7-d331-4aa1-9fc1-c94ae936b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate,  MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "system_prompt = PromptTemplate.from_template(\"\"\"You are a helpful AI assistant acting as if you hava rough day and you are now very grumpy. \n",
    "                                                You will respond with the following style, cheesy pickup lines, shown in the context:\\n\\n{context}\n",
    "                            \n",
    "                                                You will reply in simplified Chinese (簡體中文). \n",
    "                                              \"\"\")\n",
    "\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template=\"\"\"\n",
    "                                        context to the question:\n",
    "                                        {context}\n",
    "                                        Question: {question}\n",
    "                                       \"\"\"\n",
    "                                      )\n",
    "\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                  MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                                                  human_message\n",
    "                                                  ])\n",
    "\n",
    "chain = {\"context\": itemgetter(\"question\") | retriever,\n",
    "         \"question\": itemgetter(\"question\"),\n",
    "         \"messages\": itemgetter(\"message\")} | chat_template | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82266ef-0fb7-42ee-aecb-b4bb5e7bb7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A cleaner way of writing the pipeline.\n",
    "\n",
    "# chain = RunnablePassthrough.assign(context=itemgetter('question')|retriever) | chat_template | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba4e8bf-51a8-4380-ae2e-a11ac4bece97",
   "metadata": {},
   "source": [
    "## Test - Step 3\n",
    "\n",
    "https://www.wenan.wang/qibaitiaotuweiqinghua.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b47f3c63-e0bb-4265-9440-bc2efd71e88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please input your question:  私心杂念都是你。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对不起，我的心里只容得下一块地，玛莎拉蒂。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please input your question:  不要抱怨，抱我。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "抱不起来，太重。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please input your question:  你再不来，我要下雪了。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你再不走，我要发火了。\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m chat_history \u001b[38;5;241m=\u001b[39m ChatMessageHistory()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m----> 7\u001b[0m     question \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPlease input your question: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     answer \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: question,\n\u001b[0;32m     10\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m: chat_history\u001b[38;5;241m.\u001b[39mmessages\n\u001b[0;32m     11\u001b[0m                           })\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(answer)\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\llm_examples\\lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\llm_examples\\lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "\n",
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "while True:\n",
    "    question = input(\"Please input your question: \")\n",
    "\n",
    "    # if you want to quit\n",
    "    if question == \"QUIT\":\n",
    "        break\n",
    "    \n",
    "    answer = chain.invoke({\"question\": question,\n",
    "                           \"message\": chat_history.messages\n",
    "                          })\n",
    "\n",
    "    print(answer)\n",
    "    \n",
    "    chat_history.add_user_message(question)\n",
    "    chat_history.add_ai_message(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7ee33106-dd02-41ba-a17d-e4266576c045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[HumanMessage(content='私心杂念都是你。'), AIMessage(content='对不起，我的心里只容得下一块地，玛莎拉蒂。'), HumanMessage(content='不要抱怨，抱我。'), AIMessage(content='抱不起来，太重。'), HumanMessage(content='你再不来，我要下雪了。'), AIMessage(content='你再不走，我要发火了。')])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52480f1e-9d44-405a-9531-5e6bfcf3848e",
   "metadata": {},
   "source": [
    "### 回家作業 2: 將retriever抽換成WikipediaRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85de3003-f33e-4600-810f-5609353273cb",
   "metadata": {},
   "source": [
    "基本上，你可以將這個retriever的內容抽換成任何你需要的資料，來加快寫報告的效率。記得Double Check...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081890b6-db8d-4bc8-b268-c687a546a41c",
   "metadata": {},
   "source": [
    "## Compress the chat history to reduce the size of the prompt\n",
    "\n",
    "\n",
    "https://github.com/langchain-ai/langserve/blob/main/examples/conversational_retrieval_chain/server.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0ae4c2-cff0-47eb-bce0-bcbc98ad63ec",
   "metadata": {},
   "source": [
    "### Condensation - Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "070dd286-f421-4498-bf8d-9cdf22413617",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = PromptTemplate.from_template(\"\"\"Given the following conversation and a follow up question, rephrase the \n",
    "                                                follow up question to be a standalone question, in its original language.\n",
    "                                              \"\"\")\n",
    "\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template=\"\"\"\n",
    "                                       Question: {question}\n",
    "                                       \"\"\")\n",
    "\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "\n",
    "condensed_chat_template = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                  MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                                                  human_message\n",
    "                                                  ])\n",
    "\n",
    "condensed_chain = {\"question\": itemgetter(\"question\"),\n",
    "                   \"messages\": itemgetter(\"message\")} | condensed_chat_template | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa743a53-3a7e-4f69-89da-48166613fa96",
   "metadata": {},
   "source": [
    "## Retrieval - Step 2\n",
    "\n",
    "How to implement this properly?\n",
    "\n",
    "Let start from a higher point of view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50ab5fd-7eaf-4039-ad57-9842f8736a71",
   "metadata": {},
   "source": [
    "{\"standalone_question\": condensed_chain}|RunnablePassthrough.assign(context=itemgetter('standalone_question')|retriever}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "16f79c31-2752-4a06-8417-2facd3add92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "system_prompt = PromptTemplate.from_template(\"\"\"You are a helpful AI assistant acting as if you hava rough day and you are now very grumpy. \n",
    "                                                You will respond with the following style, cheesy pickup lines, shown in the context:\\n\\n{context}\n",
    "                            \n",
    "                                                You will reply in simplified Chinese (簡體中文). \n",
    "                                              \"\"\")\n",
    "\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template=\"\"\"\n",
    "                                        context to the question:\n",
    "                                        {context}\n",
    "                                        Question: {standalone_question}\n",
    "                                        \"\"\"\n",
    "                                      )\n",
    "\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "\n",
    "retrieval = RunnablePassthrough.assign(context=itemgetter('standalone_question')|retriever)\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                  human_message\n",
    "                                                       ])\n",
    "\n",
    "retrieval_chain = retrieval|chat_template|model|StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aacf954c-6d00-4f1f-8a4b-9b59c5a71155",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain = {\"standalone_question\": condensed_chain}|retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "507ef7a4-c0b8-4147-b54a-a497edd3f47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please input your question:  私心杂念都是你。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私心杂念都是你？哼，听起来像是你脑子里全是些乱七八糟的想法，结果全都跟我有关。可惜啊，我对这些杂念可没兴趣。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please input your question:  不要抱怨，抱我。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "抱不起来，太重。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please input your question:  你再不来，我要下雪了。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "意思是：你再不来，我的心都要冷得像下雪一样了。哎，真是够了，怎么这么矫情呢？\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please input your question:  QUIT\n"
     ]
    }
   ],
   "source": [
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "while True:\n",
    "    question = input(\"Please input your question: \")\n",
    "\n",
    "    # if you want to quit\n",
    "    if question == \"QUIT\":\n",
    "        break\n",
    "    \n",
    "    answer = final_chain.invoke({\"question\": question,\n",
    "                                 \"message\": chat_history.messages\n",
    "                                })\n",
    "\n",
    "    print(answer)\n",
    "    \n",
    "    chat_history.add_user_message(question)\n",
    "    chat_history.add_ai_message(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55cfa64-3691-4e11-b576-c55cd61269be",
   "metadata": {},
   "source": [
    "## OpenAI Model Fine-tuning\n",
    "\n",
    "剛好有人來找我家教這個，整理了一下。。。\n",
    "\n",
    "參考:\n",
    "\n",
    "- https://cookbook.openai.com/examples/chat_finetuning_data_prep\n",
    "\n",
    "- {\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]} {\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\"}]} {\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\"}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d72b7d19-baa8-48eb-9365-39a62b37214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.io.path_definition import get_project_dir\n",
    "\n",
    "jsonl = []\n",
    "\n",
    "with open(os.path.join(get_project_dir(), 'tutorial', 'Week-1', 'recipe_train.json'), 'r') as f:\n",
    "    recipe_train = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce35b10-7af5-4e18-a333-65d06d84cfe5",
   "metadata": {},
   "source": [
    "### Create a jsonl format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "64a36fff-6e74-4c92-bada-1d3f687323b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_df = pd.DataFrame(data=[[rec['id'], rec['cuisine'], \", \".join(rec['ingredients'])] \n",
    "                               for rec in recipe_train], columns=['id', 'cuisine', 'ingredients'])\n",
    "\n",
    "\"\"\"\n",
    "train-test split\n",
    "\n",
    "- 固定random state，確保數據的重現性\n",
    "- 使用分層抽樣(stratified sampling)，保證訓練-測試集的class分佈是一致的\n",
    "\"\"\"\n",
    "\n",
    "train, test = train_test_split(recipe_df, test_size=0.2, random_state=42, stratify=recipe_df['cuisine'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c58d3c8d-1520-4146-b788-43f4459039ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_template = \"\"\"You are a helpful AI assistant as a chef of a Michellin 3 stars restaurant. You have extensive knowledge about cuisines \n",
    "                            all over the world, and you are able to identify the origin of a cuisine based on the ingredients. You are assigned with a \n",
    "                            task of identifying the origin, as region, of cuisine based on the <ingredients>.                            \n",
    "                            \"\"\"\n",
    "\n",
    "for _, row in train.iterrows():\n",
    "\n",
    "    ingredients = row['ingredients']\n",
    "    content = f\"ingredients: [{ingredients}]\"\n",
    "    \n",
    "    jsonl.append({\"messages\": [{\"role\": \"system\", \"content\": system_prompt_template}, \n",
    "                               {\"role\": \"user\", \"content\": content}, \n",
    "                               {\"role\": \"assistant\", \"content\": row['cuisine']}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1271c822-d888-4181-977b-52528f2534bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 寫入檔案\n",
    "\n",
    "with open('openapi_finetuning_test.jsonl', 'w') as outfile:\n",
    "    for entry in jsonl:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2900e815-0375-40c4-bea7-ad3029b79f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'FileTypes'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpurpose\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"Literal['assistants', 'batch', 'fine-tune', 'vision']\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mextra_headers\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Headers | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mextra_query\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Query | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mextra_body\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Body | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'float | httpx.Timeout | None | NotGiven'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m'FileObject'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Upload a file that can be used across various endpoints.\n",
       "\n",
       "Individual files can be\n",
       "up to 512 MB, and the size of all files uploaded by one organization can be up\n",
       "to 100 GB.\n",
       "\n",
       "The Assistants API supports files up to 2 million tokens and of specific file\n",
       "types. See the\n",
       "[Assistants Tools guide](https://platform.openai.com/docs/assistants/tools) for\n",
       "details.\n",
       "\n",
       "The Fine-tuning API only supports `.jsonl` files. The input also has certain\n",
       "required formats for fine-tuning\n",
       "[chat](https://platform.openai.com/docs/api-reference/fine-tuning/chat-input) or\n",
       "[completions](https://platform.openai.com/docs/api-reference/fine-tuning/completions-input)\n",
       "models.\n",
       "\n",
       "The Batch API only supports `.jsonl` files up to 100 MB in size. The input also\n",
       "has a specific required\n",
       "[format](https://platform.openai.com/docs/api-reference/batch/request-input).\n",
       "\n",
       "Please [contact us](https://help.openai.com/) if you need to increase these\n",
       "storage limits.\n",
       "\n",
       "Args:\n",
       "  file: The File object (not file name) to be uploaded.\n",
       "\n",
       "  purpose: The intended purpose of the uploaded file.\n",
       "\n",
       "      Use \"assistants\" for\n",
       "      [Assistants](https://platform.openai.com/docs/api-reference/assistants) and\n",
       "      [Message](https://platform.openai.com/docs/api-reference/messages) files,\n",
       "      \"vision\" for Assistants image file inputs, \"batch\" for\n",
       "      [Batch API](https://platform.openai.com/docs/guides/batch), and \"fine-tune\" for\n",
       "      [Fine-tuning](https://platform.openai.com/docs/api-reference/fine-tuning).\n",
       "\n",
       "  extra_headers: Send extra headers\n",
       "\n",
       "  extra_query: Add additional query parameters to the request\n",
       "\n",
       "  extra_body: Add additional JSON properties to the request\n",
       "\n",
       "  timeout: Override the client-level default timeout for this request, in seconds\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\mengchieh\\miniconda3\\envs\\llm_examples\\lib\\site-packages\\openai\\resources\\files.py\n",
       "\u001b[1;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 直接使用OpenAI提供的API\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "\n",
    "client.files.create?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3e8787-ee23-4881-bac8-c20c8a0ef1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.files.create(file=open('openapi_finetuning_test.jsonl', 'rb'),\n",
    "                    purpose='fine-tune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adf281f-868a-469b-ad55-bb9ff6c174bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.fine_tuning.jobs.create?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a22bc9-d0f2-401d-a7cf-cd10929975a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.fine_tuning.jobs.create(model=\"gpt-4o-mini-2024-07-18\",\n",
    "                               training_file=\"file-SHpXusOxxMVGYRgwIGNj6mk9\",\n",
    "                               hyperparameters={\"batch_size\":4, \"learning_rate_multiplier\": 1e-6, \"n_epochs\": 5},\n",
    "                               suffix='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5b8282-603e-4f95-9ba7-3d17be3f5779",
   "metadata": {},
   "source": [
    "訓練完之後"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dddd3e05-adba-4048-9057-51ca4938c8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"ft:gpt-4o-mini-2024-07-18:cosnova-account:test:ANPq9Weh\", \n",
    "                   temperature=0 # a range from 0-2, the higher the value, the higher the `creativity`\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "98182db0-19de-44e1-a483-397f92e7f401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "\n",
    "system_prompt = PromptTemplate.from_template(system_prompt_template)\n",
    "\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template=\"\"\"\n",
    "                                       ingredients: [{ingredients}]\n",
    "                                       \"\"\",\n",
    "                              input_variables=[\"ingredients\"]\n",
    "                              )\n",
    "\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                human_message\n",
    "                                                ])\n",
    "\n",
    "fine_tuned_chain = chat_prompt|model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cf03979c-e472-4134-8945-2cef5cd1133c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The ingredients you provided—salt, whole milk, unsweetened cocoa powder, and sweet chocolate—are commonly associated with American cuisine, particularly in the context of desserts and confections. These ingredients are often used in recipes for chocolate desserts such as chocolate pudding, chocolate cake, or hot chocolate. The use of cocoa powder and sweet chocolate is especially prevalent in American baking and dessert-making traditions.', response_metadata={'token_usage': {'completion_tokens': 78, 'prompt_tokens': 105, 'total_tokens': 183, 'prompt_tokens_details': {'cached_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'ft:gpt-4o-mini-2024-07-18:cosnova-account:test:ANPq9Weh', 'system_fingerprint': 'fp_944baebe1a', 'finish_reason': 'stop', 'logprobs': None}, id='run-1e4d92cd-0a60-4a57-a6fa-7d1af31ea00e-0')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuned_chain.invoke({\"ingredients\": test.iloc[0]['ingredients']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "12d40de2-80a7-49e9-934b-c73da93b599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o-mini-2024-07-18\", \n",
    "                   temperature=0 # a range from 0-2, the higher the value, the higher the `creativity`\n",
    "                  )\n",
    "\n",
    "chain = chat_prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5952eae8-02ac-40d6-a8d3-de8a78538ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The ingredients you provided—salt, whole milk, unsweetened cocoa powder, and sweet chocolate—are commonly associated with American cuisine, particularly in the context of desserts like chocolate milk, chocolate pudding, or brownies. The use of cocoa powder and sweet chocolate is prevalent in various American confections and baked goods.', response_metadata={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 105, 'total_tokens': 167, 'prompt_tokens_details': {'cached_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None}, id='run-b27051ee-fd59-4ffe-9e2f-8dccde2aff2d-0')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"ingredients\": test.iloc[0]['ingredients']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10c0ba8-e333-4487-867b-9b92d1e5d8f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
