{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c30f7bc9-d9ba-4f35-b95e-7bc03e4f950a",
   "metadata": {},
   "source": [
    "# MLflow Part 2\n",
    "\n",
    "\n",
    "ğŸ¯ **æœ¬ç« å­¸å®Œä½ å°‡èƒ½å­¸æœƒä»€éº¼ï¼š**\n",
    "\n",
    "- æŒæ¡ MLflow CallbackHandler çš„ä½¿ç”¨æ–¹å¼ï¼Œå°‡ LangChain çš„è¼¸å…¥/è¼¸å‡ºèˆ‡æ¨¡å‹ç´€éŒ„è‡ªå‹•åŒ–  \n",
    "- ç†è§£ Autolog æ¨¡å¼ä¸‹çš„è‡ªå‹•è¿½è¹¤åŸç†èˆ‡æ‡‰ç”¨å ´æ™¯  \n",
    "- èƒ½å¤ åœ¨ MLflow UI ä¸­è§€å¯Ÿ LLMChain çš„é‹ä½œéç¨‹ã€è¼¸å…¥è¼¸å‡ºèˆ‡åŸ·è¡Œæ™‚é–“  \n",
    "\n",
    "ğŸ“˜ **æœ€çµ‚ä½ å°‡å…·å‚™çš„èƒ½åŠ›ï¼š**  \n",
    "èƒ½å¤ å»ºç«‹å¯è¿½è¹¤çš„ LLM å¯¦é©—ç’°å¢ƒï¼Œå°‡ LangChain èˆ‡ MLflow æ•´åˆï¼Œç‚ºæ¨¡å‹é–‹ç™¼ã€é™¤éŒ¯èˆ‡å¯¦é©—ç®¡ç†å¥ å®šåŸºç¤ã€‚  \n",
    "\n",
    "\n",
    "mlflow server --host 127.0.0.1 --port 8080\n",
    "\n",
    "## ç´€éŒ„å…§å®¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff0a3f8-6c8c-450d-b161-e50b6ad71400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed358d5-7c46-4e7e-8d98-eb8765f43208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from textwrap import dedent\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_community.callbacks import MlflowCallbackHandler\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from src.initialization import credential_init\n",
    "\n",
    "def build_standard_chat_prompt_template(kwargs):\n",
    "\n",
    "    messages = []\n",
    " \n",
    "    if 'system' in kwargs:\n",
    "        content = kwargs.get('system')\n",
    "        prompt = PromptTemplate(**content)\n",
    "        message = SystemMessagePromptTemplate(prompt=prompt)\n",
    "        messages.append(message)  \n",
    "\n",
    "    if 'human' in kwargs:\n",
    "        content = kwargs.get('human')\n",
    "        prompt = PromptTemplate(**content)\n",
    "        message = HumanMessagePromptTemplate(prompt=prompt)\n",
    "        messages.append(message)\n",
    "        \n",
    "    chat_prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "    \n",
    "    return chat_prompt_template\n",
    "\n",
    "experiment = \"Week-4\"\n",
    "tracking_url = \"http://127.0.0.1:8080\"\n",
    "\n",
    "credential_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aced9425-c3e4-4b37-90fa-ef32ccffb338",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(uri=tracking_url)\n",
    "\n",
    "# Start or get an MLflow run explicitly\n",
    "mlflow.set_experiment(experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b59d40d-378e-4961-98ef-f64636caf775",
   "metadata": {},
   "source": [
    "### MlflowCallbackHandler\n",
    "\n",
    "è¿½è¹¤ä¸¦è¨˜éŒ„èªè¨€æ¨¡å‹çš„è¼¸å…¥å’Œè¼¸å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84faca3b-fe2e-48ad-bb74-bca05a95ea48",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"my-llm-run\") as run:\n",
    "    run_id = run.info.run_id\n",
    "    print(f\"Using run_id={run_id}\")\n",
    "\n",
    "    # Attach the run_id so all logs go into this run\n",
    "    mlflow_cb = MlflowCallbackHandler(\n",
    "        experiment=experiment,\n",
    "        run_id=run_id,\n",
    "        tracking_uri=tracking_url,\n",
    "    )\n",
    "\n",
    "    model = ChatOpenAI(\n",
    "        model_name=\"gpt-4o-mini\",\n",
    "        temperature=0,\n",
    "        callbacks=[mlflow_cb]\n",
    "    )\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"product\"],\n",
    "        template=\"What is a good name for a company that makes {product}?\",\n",
    "    )\n",
    "\n",
    "    chain = LLMChain(llm=model, prompt=prompt)\n",
    "\n",
    "    # First call logs into this run\n",
    "    chain.invoke({\"product\": \"é™½é›»å­æ”»åŸç‚®\"})\n",
    "\n",
    "    # Second call also logs into the SAME run_id\n",
    "    chain.invoke({\"product\": \"æ—‹é¢¨é­šé›· (Warhammer 40k, Exterminatus)\"})\n",
    "\n",
    "    chain.invoke({\"product\": \"äººå½¢MS/Gundam\"})\n",
    "    \n",
    "    # Finally flush once\n",
    "    mlflow_cb.flush_tracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b7fd37-b372-43e9-8e1b-fbaa7a0ff807",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = run_id\n",
    "artifact_path = \"table_session_analysis.html\"   # artifacts å…§çš„ç›¸å°è·¯å¾‘ä½ç½®\n",
    "\n",
    "# Download to a local directory\n",
    "local_dir = mlflow.artifacts.download_artifacts(run_id=run_id, artifact_path=artifact_path,\n",
    "                                                dst_path=\"tutorial/LLM+Langchain/Week-4\", \n",
    "                                                tracking_uri=tracking_url,\n",
    "                                                )\n",
    "\n",
    "print(\"Downloaded to:\", local_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5e3016-761c-4ee3-b99e-8d93d2cfe9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181d118a-09ee-4b59-8659-82aedaca0bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# è¨˜å¾—è¦åŠ ä¸Šencoding='utf-8',å¦å‰‡ä¸­æ–‡æœƒè®Šæˆäº‚ç¢¼\n",
    "df = pd.read_html(\"tutorial/LLM+Langchain/Week-4/table_session_analysis.html\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32120d0e-45c7-469c-92dd-e9e7337eb088",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567f357e-fb1e-4dc4-b3d7-ae9e9ea4d804",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[0].iloc[0]['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22f7f1c-b1fd-4fb5-9ed3-c3c495e56c78",
   "metadata": {},
   "source": [
    "## Autolog\n",
    "\n",
    "æ­¤æ¨¡å¼å®Œå…¨ä¸æœƒå¯«å…¥ä»»ä½• JSON æª”æ¡ˆ â€”â€” ç›¸ååœ°ï¼Œå®ƒæœƒå°‡ä½ çš„ LangChain åŸ·è¡Œéç¨‹ï¼ˆtraces/spansï¼‰æ•æ‰ä¸¦è¨˜éŒ„åˆ° MLflow çš„å¯¦é©—è¿½è¹¤èˆ‡è¿½è¹¤ï¼ˆtracingï¼‰ä»‹é¢ä¸­ã€‚é€™è¡¨ç¤ºä½ å¯ä»¥åœ¨ MLflow çš„ä½¿ç”¨è€…ä»‹é¢ä¸­çœ‹åˆ°è¼¸å…¥/è¼¸å‡ºã€åŸ·è¡Œæ™‚é–“ä»¥åŠå·¢ç‹€çµæ§‹ï¼Œè€Œä¸æ˜¯ä»¥ .json æª”æ¡ˆçš„å½¢å¼å„²å­˜ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09058a8a-aae9-4b86-8b7c-f86dffa5aa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d2a57-e219-46c2-905b-800eb4b43c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autologging â€” this instruments LangChain automatically\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name in traditional Chinese for a company that makes {product}?\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=model, prompt=prompt)\n",
    "\n",
    "# Run the chain\n",
    "mlflow.set_tracking_uri(uri=tracking_url)\n",
    "\n",
    "# Create a new MLflow Experiment\n",
    "mlflow.set_experiment(experiment)\n",
    "\n",
    "with mlflow.start_run(run_id=run_id) as run:\n",
    "    chain.invoke({\"product\": \"å…‰èŒ… (æˆ°æ§Œ40k)\"})\n",
    "    chain.invoke({\"product\": \"æ—‹é¢¨é­šé›· (Warhammer 40k, Exterminatus)\"})\n",
    "    chain.invoke({\"product\": \"äººå½¢MS/Gundam\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa601924-3194-430c-83c8-57a831fa159d",
   "metadata": {},
   "source": [
    "## Reflection + Revision Pipeline\n",
    "\n",
    "ä¸€å€‹æ¨¡å‹ä¸å¤ ï¼Œä½ å¯ä»¥ç”¨å…©å€‹ã€‚ä¸Šé¢å…©å€‹ç¯„ä¾‹éƒ½åªç”¨äº†ä¸€å€‹æ¨¡å‹ï¼Œé‚„æ²’å¤–åŠ RAGä¹‹é¡çš„\n",
    "\n",
    "ğŸ¯ **æœ¬ç« å­¸å®Œä½ å°‡èƒ½å­¸æœƒä»€éº¼ï¼š**\n",
    "\n",
    "- ç†è§£å¦‚ä½•åˆ©ç”¨å¤šå€‹æ¨¡å‹å»ºç«‹åé¥‹ï¼ˆReflectionï¼‰èˆ‡ä¿®è¨‚ï¼ˆRevisionï¼‰æµç¨‹  \n",
    "- å­¸æœƒè¨­è¨ˆçµæ§‹åŒ–è¼¸å‡ºï¼ˆPydanticOutputParserï¼‰ä»¥ç”Ÿæˆå¯è§£æçš„çµæœ  \n",
    "- æŒæ¡ RunnablePassthrough èˆ‡ RunnableLambda çš„æ‡‰ç”¨ï¼Œæ‰“é€ å¯çµ„åˆçš„ LangChain Pipeline  \n",
    "- ç†è§£å¦‚ä½•åœ¨ MLflow ä¸­è¨˜éŒ„å¤šéšæ®µæ¨¡å‹çš„è¿½è¹¤æ•¸æ“š  \n",
    "\n",
    "ğŸ“˜ **æœ€çµ‚ä½ å°‡å…·å‚™çš„èƒ½åŠ›ï¼š**  \n",
    "èƒ½å¤ æ§‹å»ºå…·å‚™åæ€èˆ‡ä¿®è¨‚èƒ½åŠ›çš„è¤‡åˆå¼ LLM Pipelineï¼Œä¸¦åœ¨ MLflow å…§å®Œæ•´è¿½è¹¤å…¶é‹è¡Œæ­·ç¨‹ã€‚  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e679c304-44bd-423a-ba28-795daeae6fba",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "\n",
    "ä½¿ç”¨æŸå¹´çš„å­¸æ¸¬/æŒ‡è€ƒçš„ä½œæ–‡ä½œç‚ºç¯„ä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd48e595-8b29-425c-ac1f-6b5e69cde646",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = dedent(\"\"\"\n",
    "ä¿—è©±èªªï¼šã€Œé¾ç”Ÿä¹å­ï¼Œå„æœ‰ä¸åŒã€‚ã€åœ¨å»£é—Šæµ©ç€šçš„æµ·æ´‹ä¹‹ä¸­ï¼Œå°±æœ‰ä¸€é ­å­¤ç¨çš„é¯¨é­šâ€”â€”äº”åäºŒèµ«èŒ²é¯¨é­šã€‚ç‰ è²éŸ³çš„é »ç‡å¤©ç”Ÿä¾¿æ¯”åŒä¼´é‚„è¦é«˜ï¼Œé€™é …ç‰¹åˆ¥ä¹‹è™•ï¼Œä¹Ÿå°è‡´äº†ç‰ èˆ‡åŒä¼´ç”¢ç”Ÿäº†ç„¡æ³•æºé€šçš„é´»æºã€‚çœ‹è¦‹é€™å‰‡æ•…äº‹çš„æˆ‘ï¼Œä¸ç¦æ€è€ƒï¼Œåœ¨å¦‚æ­¤å¤šå…ƒçš„äººé–“ï¼Œæ˜¯å¦ä¹Ÿæœ‰åƒäº”åäºŒèµ«èŒ²é¯¨é­šä¸€èˆ¬ï¼Œå¤©ç”Ÿä¾¿èˆ‡çœ¾ä¸åŒï¼Ÿ\n",
    "\n",
    "å›é¦–ç«¥å¹´ï¼Œæˆ‘å°è±¡æœ€ç‚ºæ·±åˆ»çš„ä¸€åˆ»ï¼Œæ˜¯åˆè­˜å­—æ™‚ï¼Œèˆ‡æ–‡å­—äº’ç›¸ç†è§£çš„é‚£ä¸€ç¬ã€æ˜¯ç•¶æˆ‘ç¬¬ä¸€æ¬¡è®€å®Œä¸€å€‹å¥å­æ™‚ï¼Œå®ƒå°‡è‡ªèº«çš„æ„ç¾©å‚³å…¥æˆ‘è…¦ä¸­çš„é‚£ä¸€ç¬ã€‚è‡ªæ­¤ï¼Œæˆ‘ä¾¿å°æ–‡å­—ã€èªè¨€æŠ±æœ‰ç‰¹æ®Šçš„æ„Ÿæƒ…ï¼Œä¹Ÿååˆ†äº«å—é–±è®€èˆ‡æœ—èª¦ã€‚é‚£ç¨®å°‡è‡ªèº«èˆ‡æ–‡å­—ç¶“ç”±ä¸€é»ä¸€æ»´ç©ç´¯è€Œé€£æ¥èµ·ä¾†çš„æ„Ÿæƒ…ï¼Œä½¿æˆ‘å¿ƒéˆæ„Ÿåˆ°ååˆ†å¯Œè¶³ã€‚\n",
    "\n",
    "è€Œç•¶æˆ‘æ­¥å…¥æ ¡åœ’æ¥è§¸åŒå„•æ™‚ï¼Œé©€ç„¶é©šè¦ºæˆ‘èˆ‡åˆ¥äººçš„é–±è®€é€Ÿåº¦ååˆ†ä¸åŒã€‚æ¯ç•¶æˆ‘å·²è®€å®Œä¸€ç¯‡æ–‡ç« ï¼Œä½†åŒå­¸å¯èƒ½åªå®Œæˆäº†ä¸€åŠç”šè‡³ä¸‰åˆ†ä¹‹äºŒã€‚åŒæ™‚ï¼Œæˆ‘åœ¨ç”Ÿå­—è®€éŸ³æ–¹é¢ä¹Ÿç•°å¸¸çš„åŸ·è‘—ï¼Œå› æ­¤è¢«åŒå­¸æŠ±æ€¨æœ‰ã€Œæ–‡å­—æ½”ç™–ã€ã€‚é¢å°åŒå„•æŠ±æ€¨çš„æˆ‘ï¼Œä¹Ÿåªå¥½å¼·å¿å°è€³é‚Šæ™‚è€Œå‡ºç¾å­—éŒ¯è®€éŸ³çš„ä¸é©ï¼Œé–‹å§‹åˆ»æ„å¿½ç•¥å¿ƒè£¡å°å®ƒçš„åŸ·å¿µï¼Œåªç‚ºæƒ³è¦èˆ‡åˆ¥äººä¸€æ¨£ï¼Œæƒ³è¦å’Œæœ‹å‹äº’ç›¸ç†è§£ã€‚\n",
    "\n",
    "ç›´åˆ°å¤šå¹´å‰ï¼Œå› ç·£éš›æœƒä¹‹ä¸‹èªè­˜äº†ã€Œäº”åäºŒèµ«èŒ²ã€é€™ç¨ç‰¹çš„å­˜åœ¨ã€‚ç‰ çš„èº«å½±åœ¨æˆ‘å¿ƒä¸­çƒ™ä¸‹ä¸€é“æ·±åˆ»çš„ç—•è·¡ã€‚å› ç‚ºç‰ ï¼Œæˆ‘é–‹å§‹æ¥å—è‡ªå·±èˆ‡ä»–äººçš„ä¸åŒï¼›ä¹Ÿå› ç‚ºç‰ ï¼Œæˆ‘æ˜ç™½äº†ï¼Œæˆ‘å°æ–‡å­—çš„åŸ·è‘—ï¼Œä¸¦ä¸æ˜¯ä¸€ç¨®è² é¢çš„ç‰¹è³ªï¼Œè€Œæ˜¯ä¸Šå¤©è³œäºˆæˆ‘çš„ç¦®ç‰©ï¼Œæˆ‘é–‹å§‹åœ¨å¯«ä½œä¸Šæ®ç‘è‡ªå¦‚ã€‚é€™è®“æˆ‘çŸ¥é“ï¼Œä¸è¦åœ¨ä¸€é–‹å§‹ä¾¿ç”¨å¦å®šçš„çœ¼å…‰çœ‹å¾…è‡ªå·±çš„ç‰¹è³ªã€‚ä¹Ÿè¨±é€™ç‰¹åˆ¥ä¹‹è™•ï¼Œæœƒä½¿æˆ‘å€‘èˆ‡äº”åäºŒèµ«èŒ²é¯¨é­šä¸€èˆ¬å­¤ç¨ï¼Œæœƒä½¿æˆ‘å€‘é­å—ä»–äººçš„ä¸ç†è§£èˆ‡æ’æ–¥ï¼Œä½†ä¹Ÿæœƒè®“æˆ‘å€‘èˆ‡çœ¾ä¸åŒã€‚\n",
    "\n",
    "é—œæ–¼æ­¤ï¼Œæˆ‘æƒ³èªªçš„æ˜¯ï¼Œå‹‡æ•¢åœ°ç¶»æ”¾è‡ªå·±çš„ç‰¹åˆ¥ï¼Œä¹Ÿè®“è‡ªå·±æˆç‚ºè‡ªå·±å’Œä¸–äººçœ¼ä¸­ï¼Œæœ€é–ƒè€€çš„äº”åäºŒèµ«èŒ²é¯¨é­šã€‚\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "def create_feedback_pipeline(mlflow_callback):\n",
    "\n",
    "    ## Teacher LLM\n",
    "    system_template = dedent(\"\"\"\n",
    "    ä½ æ˜¯ä¸€å€‹æ•™å­¸èˆ‡å¯«ä½œç¶“é©—è±å¯Œçš„å°ç£å¤§å­¸ä¸­æ–‡ç³»æ•™æˆï¼Œä½ è¦ä¾†è² è²¬çµ¦äºˆä½œæ–‡è©•åˆ†èˆ‡å›é¥‹ã€‚\n",
    "    \"\"\")\n",
    "    \n",
    "    human_template = dedent(\"\"\"\n",
    "    Title: {title}\n",
    "    \n",
    "    Article:\n",
    "    {article}\n",
    "    \"\"\")\n",
    "    \n",
    "    model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                       model_name=\"gpt-4o-mini\", temperature=0,\n",
    "                       callbacks=[mlflow_callback])\n",
    "    \n",
    "    input_ = {\"system\": {\"template\": system_template},\n",
    "              \"human\": {\"template\": human_template,\n",
    "                        \"input_variable\": [\"title\", \"article\"],\n",
    "                        }}\n",
    "    \n",
    "    chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "    \n",
    "    feedback_pipeline = chat_prompt_template|model|StrOutputParser()\n",
    "\n",
    "    return feedback_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2ca87c-445d-49b4-b4e5-87fc31beff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "class Output(BaseModel):\n",
    "    name: str = Field(description=\"The revised article in traditional Chinese (ç¹é«”ä¸­æ–‡), please do not include the title.\")\n",
    "\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=Output)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "\n",
    "def create_revision_pipeline(mlflow_callback):\n",
    "    ## Generate\n",
    "    system_template = dedent(\"\"\"\n",
    "    ä½ æ˜¯ä¸€å€‹åœ¨æº–å‚™è€ƒè©¦çš„é«˜ä¸­ç”Ÿï¼Œä½ å°‡æ ¹æ“šåé¥‹å¼·åŒ–çš„ä½œæ–‡å…§å®¹ã€‚\n",
    "    \"\"\")\n",
    "    \n",
    "    human_template = dedent(\"\"\"\n",
    "    Title: {title}\n",
    "    \n",
    "    Old Article:\n",
    "    {article}\n",
    "    \n",
    "    Feedback:\n",
    "    {feedback}\n",
    "\n",
    "    Output format instructions: {format_instructions}\n",
    "    \n",
    "    Revised Article:\n",
    "    \"\"\")\n",
    "    \n",
    "    input_ = {\"system\": {\"template\": system_template},\n",
    "              \"human\": {\"template\": human_template,\n",
    "                        \"input_variable\": [\"title\", \"article\", \"feedback\"],\n",
    "                        \"partial_variables\": {'format_instructions': format_instructions}}}\n",
    "    \n",
    "    chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "    model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                       model_name=\"gpt-4o-mini\", temperature=0,\n",
    "                       callbacks=[mlflow_callback])\n",
    "    \n",
    "    revision_pipeline = chat_prompt_template|model|output_parser\n",
    "\n",
    "    return revision_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1967e3e6-157d-4d4a-b201-37f222f9dd77",
   "metadata": {},
   "source": [
    "åœ¨å‘¼å«MLflowå¾Œï¼Œé€²è¡Œ Reflection -> Revision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c1ea5f-8e11-42de-b9d9-d79155618ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "run_name = \"Reflection_Revision\"\n",
    "\n",
    "mlflow.set_tracking_uri(uri=tracking_url)\n",
    "\n",
    "# Start or get an MLflow run explicitly\n",
    "mlflow.set_experiment(experiment)\n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "    run_id = run.info.run_id\n",
    "    print(f\"Using run_id={run_id}\")\n",
    "\n",
    "    # Attach the run_id so all logs go into this run\n",
    "    mlflow_cb = MlflowCallbackHandler(\n",
    "        experiment=experiment,\n",
    "        run_id=run_id,\n",
    "        tracking_uri=tracking_url,\n",
    "    )\n",
    "\n",
    "    feedback_pipeline = create_feedback_pipeline(mlflow_callback=mlflow_cb)\n",
    "    revision_pipeline = create_revision_pipeline(mlflow_callback=mlflow_cb)\n",
    "    \n",
    "    whole_pipeline = RunnablePassthrough.assign(feedback=feedback_pipeline)|revision_pipeline|RunnableLambda(lambda x: x.name)\n",
    "\n",
    "    result = whole_pipeline.invoke({\"article\": query,\n",
    "                                    \"title\": \"é—œæ–¼äº”åäºŒèµ«èŒ²ï¼Œæˆ‘æƒ³èªªçš„æ˜¯â€¦\"},\n",
    "                                    # config={\"callbacks\": [mlflow_cb]} \n",
    "                                  )\n",
    "    \n",
    "    \n",
    "# Finally flush once\n",
    "mlflow_cb.flush_tracker()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b65ed21-8ce2-4f00-a821-428809a28a04",
   "metadata": {},
   "source": [
    "çµåˆä¸Šé€±çš„å…§å®¹ï¼Œå°‡é€™å€‹Pipelineæ‰“åŒ…æˆä¸€å€‹Artifactä¸Šå‚³åˆ°MLflow Serverï¼Œç„¶å¾Œè—‰ç”±MLflowèª¿ç”¨Pipeline\n",
    "\n",
    "## Upload model as a python script\n",
    "\n",
    "ğŸ¯ **æœ¬ç« å­¸å®Œä½ å°‡èƒ½å­¸æœƒä»€éº¼ï¼š**\n",
    "\n",
    "- ç†è§£ MLflow ModelSignature çš„ç”¨é€”èˆ‡ schema å®šç¾©æ–¹æ³•  \n",
    "- å­¸æœƒå¦‚ä½•è¨­å®šæ¨¡å‹çš„è¼¸å…¥èˆ‡è¼¸å‡ºæ ¼å¼ï¼Œç¢ºä¿éƒ¨ç½²æ™‚é¡å‹é©—è­‰æ­£ç¢º  \n",
    "- æŒæ¡å°‡æ¨¡å‹èˆ‡æºä»£ç¢¼ä¸Šå‚³ç‚º Artifact çš„æµç¨‹  \n",
    "- ç­è§£å¦‚ä½•è¨»å†Šèˆ‡è¼‰å…¥ MLflow æ¨¡å‹ä»¥é€²è¡Œæ¨è«–  \n",
    "\n",
    "ğŸ“˜ **æœ€çµ‚ä½ å°‡å…·å‚™çš„èƒ½åŠ›ï¼š**  \n",
    "èƒ½å¤ å®šç¾©ã€å°è£ä¸¦ä¸Šå‚³å¯é‡ç”¨çš„ LLM æ¨¡å‹ï¼Œä¸¦ä»¥ MLflow ä½œç‚ºæ¨¡å‹ç‰ˆæœ¬èˆ‡éƒ¨ç½²çš„æ ¸å¿ƒå¹³å°ã€‚\n",
    "\n",
    "\n",
    "### ModelSignature\n",
    "\n",
    "    ModelSignature åœ¨ MLflow è£¡çš„ä¸»è¦ä½œç”¨ï¼Œæ­£æ˜¯ç”¨ä¾†å®šç¾©èˆ‡é©—è­‰æ¨¡å‹çš„è¼¸å…¥èˆ‡è¼¸å‡ºæ ¼å¼ï¼ˆschemaï¼‰ã€‚\n",
    "    \n",
    "    ModelSignature æ˜¯ MLflow çš„ä¸€å€‹ç‰©ä»¶ï¼Œç”¨ä¾†æè¿°æ¨¡å‹çš„ï¼š\n",
    "    \n",
    "    è¼¸å…¥ï¼ˆinputsï¼‰ï¼šæ¨¡å‹é æœŸæ¥æ”¶çš„è³‡æ–™æ ¼å¼ï¼ˆæ¬„ä½åç¨±ã€è³‡æ–™å‹åˆ¥ç­‰ï¼‰\n",
    "    \n",
    "    è¼¸å‡ºï¼ˆoutputsï¼‰ï¼šæ¨¡å‹é æœŸå›å‚³çš„è³‡æ–™æ ¼å¼\n",
    "    \n",
    "    å®ƒè®“ MLflow èƒ½ï¼š\n",
    "    \n",
    "        - åœ¨ æ¨¡å‹å„²å­˜ï¼ˆlog_model / save_modelï¼‰ æ™‚è¨˜éŒ„é€™äº›è³‡è¨Šã€‚\n",
    "    \n",
    "        - åœ¨ æ¨¡å‹éƒ¨ç½²æˆ–æ¨è«–ï¼ˆpredictï¼‰ æ™‚è‡ªå‹•æª¢æŸ¥è¼¸å…¥è³‡æ–™æ˜¯å¦ç¬¦åˆå®šç¾©ã€‚\n",
    "    \n",
    "        - åœ¨ MLflow UI ä¸­æ¸…æ¥šé¡¯ç¤ºæ¨¡å‹çš„ã€Œè¼¸å…¥/è¼¸å‡ºçµæ§‹ã€\n",
    "\n",
    "\n",
    "é€™å€‹ä¾‹å­ä¸­æ¨¡å‹éœ€è¦å…©å€‹è¼¸å…¥æ¬„ä½ï¼š\n",
    "\n",
    "- titleï¼ˆå‹åˆ¥ï¼šstringï¼‰\n",
    "\n",
    "- articleï¼ˆå‹åˆ¥ï¼šstringï¼‰\n",
    "\n",
    "æ¨¡å‹æœƒè¼¸å‡ºä¸€å€‹æ¬„ä½ï¼š\n",
    "\n",
    "- ç„¡åç¨±ï¼ˆæˆ–é è¨­åç¨±ï¼‰ä½†å‹åˆ¥æ˜¯ stringã€‚\n",
    "\n",
    "æ›å¥è©±èªªï¼Œé€™å€‹ signature æ˜ç¢ºèªªæ˜äº†æ¨¡å‹çš„ è¼¸å…¥çµæ§‹ èˆ‡ è¼¸å‡ºçµæ§‹ï¼Œ\n",
    "å¯å¹«åŠ© MLflow åœ¨ç´€éŒ„æˆ–éƒ¨ç½²æ¨¡å‹æ™‚è‡ªå‹•é€²è¡Œå‹åˆ¥é©—è­‰èˆ‡è¿½è¹¤ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3b45d5-768b-4c3a-9d16-789b9794b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "import pandas as pd\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "\n",
    "from src.io.path_definition import get_project_dir\n",
    "\n",
    "model_path = os.path.join(get_project_dir(), 'tutorial', 'LLM+Langchain', \n",
    "                          \"Week-4\", \"llmchain_mlflow_experiment_tracing.py\")\n",
    "\n",
    "# You need to know what you will put into it and what you will get out of it.\n",
    "input_schema = Schema([ColSpec(\"string\", \"title\"),\n",
    "                       ColSpec(\"string\", \"article\")])\n",
    "output_schema = Schema([ColSpec(\"string\")])\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "\n",
    "\n",
    "query = dedent(\"\"\"\n",
    "ä¿—è©±èªªï¼šã€Œé¾ç”Ÿä¹å­ï¼Œå„æœ‰ä¸åŒã€‚ã€åœ¨å»£é—Šæµ©ç€šçš„æµ·æ´‹ä¹‹ä¸­ï¼Œå°±æœ‰ä¸€é ­å­¤ç¨çš„é¯¨é­šâ€”â€”äº”åäºŒèµ«èŒ²é¯¨é­šã€‚ç‰ è²éŸ³çš„é »ç‡å¤©ç”Ÿä¾¿æ¯”åŒä¼´é‚„è¦é«˜ï¼Œé€™é …ç‰¹åˆ¥ä¹‹è™•ï¼Œä¹Ÿå°è‡´äº†ç‰ èˆ‡åŒä¼´ç”¢ç”Ÿäº†ç„¡æ³•æºé€šçš„é´»æºã€‚çœ‹è¦‹é€™å‰‡æ•…äº‹çš„æˆ‘ï¼Œä¸ç¦æ€è€ƒï¼Œåœ¨å¦‚æ­¤å¤šå…ƒçš„äººé–“ï¼Œæ˜¯å¦ä¹Ÿæœ‰åƒäº”åäºŒèµ«èŒ²é¯¨é­šä¸€èˆ¬ï¼Œå¤©ç”Ÿä¾¿èˆ‡çœ¾ä¸åŒï¼Ÿ\n",
    "\n",
    "å›é¦–ç«¥å¹´ï¼Œæˆ‘å°è±¡æœ€ç‚ºæ·±åˆ»çš„ä¸€åˆ»ï¼Œæ˜¯åˆè­˜å­—æ™‚ï¼Œèˆ‡æ–‡å­—äº’ç›¸ç†è§£çš„é‚£ä¸€ç¬ã€æ˜¯ç•¶æˆ‘ç¬¬ä¸€æ¬¡è®€å®Œä¸€å€‹å¥å­æ™‚ï¼Œå®ƒå°‡è‡ªèº«çš„æ„ç¾©å‚³å…¥æˆ‘è…¦ä¸­çš„é‚£ä¸€ç¬ã€‚è‡ªæ­¤ï¼Œæˆ‘ä¾¿å°æ–‡å­—ã€èªè¨€æŠ±æœ‰ç‰¹æ®Šçš„æ„Ÿæƒ…ï¼Œä¹Ÿååˆ†äº«å—é–±è®€èˆ‡æœ—èª¦ã€‚é‚£ç¨®å°‡è‡ªèº«èˆ‡æ–‡å­—ç¶“ç”±ä¸€é»ä¸€æ»´ç©ç´¯è€Œé€£æ¥èµ·ä¾†çš„æ„Ÿæƒ…ï¼Œä½¿æˆ‘å¿ƒéˆæ„Ÿåˆ°ååˆ†å¯Œè¶³ã€‚\n",
    "\n",
    "è€Œç•¶æˆ‘æ­¥å…¥æ ¡åœ’æ¥è§¸åŒå„•æ™‚ï¼Œé©€ç„¶é©šè¦ºæˆ‘èˆ‡åˆ¥äººçš„é–±è®€é€Ÿåº¦ååˆ†ä¸åŒã€‚æ¯ç•¶æˆ‘å·²è®€å®Œä¸€ç¯‡æ–‡ç« ï¼Œä½†åŒå­¸å¯èƒ½åªå®Œæˆäº†ä¸€åŠç”šè‡³ä¸‰åˆ†ä¹‹äºŒã€‚åŒæ™‚ï¼Œæˆ‘åœ¨ç”Ÿå­—è®€éŸ³æ–¹é¢ä¹Ÿç•°å¸¸çš„åŸ·è‘—ï¼Œå› æ­¤è¢«åŒå­¸æŠ±æ€¨æœ‰ã€Œæ–‡å­—æ½”ç™–ã€ã€‚é¢å°åŒå„•æŠ±æ€¨çš„æˆ‘ï¼Œä¹Ÿåªå¥½å¼·å¿å°è€³é‚Šæ™‚è€Œå‡ºç¾å­—éŒ¯è®€éŸ³çš„ä¸é©ï¼Œé–‹å§‹åˆ»æ„å¿½ç•¥å¿ƒè£¡å°å®ƒçš„åŸ·å¿µï¼Œåªç‚ºæƒ³è¦èˆ‡åˆ¥äººä¸€æ¨£ï¼Œæƒ³è¦å’Œæœ‹å‹äº’ç›¸ç†è§£ã€‚\n",
    "\n",
    "ç›´åˆ°å¤šå¹´å‰ï¼Œå› ç·£éš›æœƒä¹‹ä¸‹èªè­˜äº†ã€Œäº”åäºŒèµ«èŒ²ã€é€™ç¨ç‰¹çš„å­˜åœ¨ã€‚ç‰ çš„èº«å½±åœ¨æˆ‘å¿ƒä¸­çƒ™ä¸‹ä¸€é“æ·±åˆ»çš„ç—•è·¡ã€‚å› ç‚ºç‰ ï¼Œæˆ‘é–‹å§‹æ¥å—è‡ªå·±èˆ‡ä»–äººçš„ä¸åŒï¼›ä¹Ÿå› ç‚ºç‰ ï¼Œæˆ‘æ˜ç™½äº†ï¼Œæˆ‘å°æ–‡å­—çš„åŸ·è‘—ï¼Œä¸¦ä¸æ˜¯ä¸€ç¨®è² é¢çš„ç‰¹è³ªï¼Œè€Œæ˜¯ä¸Šå¤©è³œäºˆæˆ‘çš„ç¦®ç‰©ï¼Œæˆ‘é–‹å§‹åœ¨å¯«ä½œä¸Šæ®ç‘è‡ªå¦‚ã€‚é€™è®“æˆ‘çŸ¥é“ï¼Œä¸è¦åœ¨ä¸€é–‹å§‹ä¾¿ç”¨å¦å®šçš„çœ¼å…‰çœ‹å¾…è‡ªå·±çš„ç‰¹è³ªã€‚ä¹Ÿè¨±é€™ç‰¹åˆ¥ä¹‹è™•ï¼Œæœƒä½¿æˆ‘å€‘èˆ‡äº”åäºŒèµ«èŒ²é¯¨é­šä¸€èˆ¬å­¤ç¨ï¼Œæœƒä½¿æˆ‘å€‘é­å—ä»–äººçš„ä¸ç†è§£èˆ‡æ’æ–¥ï¼Œä½†ä¹Ÿæœƒè®“æˆ‘å€‘èˆ‡çœ¾ä¸åŒã€‚\n",
    "\n",
    "é—œæ–¼æ­¤ï¼Œæˆ‘æƒ³èªªçš„æ˜¯ï¼Œå‹‡æ•¢åœ°ç¶»æ”¾è‡ªå·±çš„ç‰¹åˆ¥ï¼Œä¹Ÿè®“è‡ªå·±æˆç‚ºè‡ªå·±å’Œä¸–äººçœ¼ä¸­ï¼Œæœ€é–ƒè€€çš„äº”åäºŒèµ«èŒ²é¯¨é­šã€‚\n",
    "\"\"\")\n",
    "\n",
    "run_name = \"Reflection_Revision_Py\"\n",
    "\n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "\n",
    "    os.environ['experiment'] = experiment\n",
    "    os.environ['run_id'] = run.info.run_id\n",
    "    os.environ['run_name'] = run_name\n",
    "    \n",
    "    mlflow.log_artifact(model_path, artifact_path=\"source_code\")\n",
    "\n",
    "    input_example = pd.DataFrame(data=[[query, \"é—œæ–¼äº”åäºŒèµ«èŒ²ï¼Œæˆ‘æƒ³èªªçš„æ˜¯â€¦\"]], columns=['article', 'title'])\n",
    "    \n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        python_model=model_path,  # Define the model as the path to the Python file\n",
    "        name=\"langchain_model\",\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "        registered_model_name=\"Generation_Reflection_Demo\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80af810b-845e-465c-88b0-f9ce51f8f45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = dedent(\"\"\"\\\n",
    "é³´å¤§é˜ä¸€æ¬¡ï¼æ¨å‹•æ æ†ï¼Œå•Ÿå‹•æ´»å¡å’Œæ³µ\n",
    "é³´å¤§é˜å…©æ¬¡ï¼æŒ‰ä¸‹æŒ‰éˆ•ï¼Œç™¼å‹•å¼•æ“ï¼Œé»ç‡ƒæ¸¦è¼ªï¼Œæ³¨å…¥ç”Ÿå‘½\n",
    "é³´å¤§é˜ä¸‰æ¬¡ï¼é½Šè²æ­Œå”±ï¼Œè®šç¾è¬æ©Ÿä¹‹ç¥ï¼\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "\n",
    "    os.environ['experiment'] = experiment\n",
    "    os.environ['run_id'] = run.info.run_id\n",
    "    os.environ['run_name'] = run_name\n",
    "\n",
    "    loaded_model = mlflow.pyfunc.load_model(\"models:/Generation_Reflection_Demo/1\")\n",
    "    \n",
    "    input_ = pd.DataFrame(data=[[query, 'é—œæ–¼äº”åäºŒèµ«èŒ²ï¼Œæˆ‘æƒ³èªªçš„æ˜¯â€¦']], columns=['article', 'title'])\n",
    "    \n",
    "    output = loaded_model.predict(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b488e8-37e0-414d-bcc8-61f6e509af00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec06a59-58b7-4955-95f3-3f019f2e36d5",
   "metadata": {},
   "source": [
    "# LangServe API\n",
    "\n",
    "ğŸ¯ **æœ¬ç« å­¸å®Œä½ å°‡èƒ½å­¸æœƒä»€éº¼ï¼š**\n",
    "\n",
    "- ç†è§£ LangServe çš„æ¶æ§‹èˆ‡ API èª¿ç”¨æµç¨‹  \n",
    "- å­¸æœƒå»ºç«‹ç°¡å–®çš„å¾Œç«¯ä¼ºæœå™¨ï¼Œä¸¦å¾å®¢æˆ¶ç«¯ç™¼é€è«‹æ±‚å–å¾—æ¨¡å‹å›æ‡‰  \n",
    "- æŒæ¡ RemoteRunnable çš„æ‡‰ç”¨ï¼Œè®“æ¨¡å‹èƒ½å¤ é ç«¯å‘¼å«  \n",
    "- ç­è§£å¦‚ä½•çµåˆ MLflow æ¨¡å‹èˆ‡ LangServe API é€²è¡Œæ•´åˆéƒ¨ç½²  \n",
    "\n",
    "ğŸ“˜ **æœ€çµ‚ä½ å°‡å…·å‚™çš„èƒ½åŠ›ï¼š**  \n",
    "èƒ½å¤ è¨­è¨ˆä¸€å€‹å…·å‚™å¾Œç«¯ API ä»‹é¢çš„ LLM ç³»çµ±ï¼Œæ”¯æ´é ç«¯æ¨è«–èˆ‡æ¨¡çµ„åŒ–éƒ¨ç½²ã€‚  \n",
    "\n",
    "## 1. å®¢æˆ¶ç«¯ (client) å‘¼å«å¾Œç«¯ API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6eced7-3568-45c6-a80c-feefec6efb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:5000/openai/invoke\",\n",
    "    json={'input': \"Where is Taiwan?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3d191c-3ba3-46d0-a265-2a00dce2e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75d27ef-15fa-4c09-bf68-78f36aebf7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()['output']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82378be9-1287-4410-9bf0-93f1b4e9b49d",
   "metadata": {},
   "source": [
    "åœ¨Windowsçš„CLI(command line interface)ä¸­:\n",
    "\n",
    "curl -X POST \"http://localhost:5000/openai/invoke\" -H \"Content-Type: application/json\" -d \"{\"\"input\"\": \"\"Where is Taiwan?\"\"}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8711bb4e-5f73-4a8c-8f0a-dc7eb0d0a5a1",
   "metadata": {},
   "source": [
    "## 2. çµåˆä¹‹å‰MLflowçš„æ‡‰ç”¨ã€‚å¾MLflow serverä¸Šä¸‹è¼‰æ¨¡å‹ï¼Œç„¶å¾Œå¾å®¢æˆ¶ç«¯å‘¼å«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26bcb69-936a-4175-9b1c-484f4c0b6e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:5000/demo/invoke\",\n",
    "    json={'input': {\"article\": query,\n",
    "                    \"title\": \"é—œæ–¼äº”åäºŒèµ«èŒ²ï¼Œæˆ‘æƒ³èªªçš„æ˜¯â€¦\"}}\n",
    ")\n",
    "\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2a54e9-4f26-46bd-acf7-0735ec3d531d",
   "metadata": {},
   "source": [
    "## 3 RemoteRunnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4050cc8e-2aea-4af5-ab3d-9fb5c84405b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langserve import RemoteRunnable\n",
    "\n",
    "remote_llm = RemoteRunnable(\"http://localhost:5000/openai/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fc385a-fdc5-43e8-8d93-919243977e69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Supports astream\n",
    "async for msg in remote_llm.astream(\"Where is Taiwan?\"):\n",
    "    print(msg.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7c653c-0454-4261-9bde-a3b04217f676",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_llm.invoke(\"èŠ±è“®ç¸£å…‰å¾©é„‰å› ç‚ºé¦¬å¤ªéæºªå °å¡æ¹–æ½°å ¤ï¼Œå°è‡´è¢«æ³¥çŸ³æµæ·¹éã€‚å°±å®‰å…¨çš„è€ƒé‡ï¼Œæ²’æ¥å—éå°ˆæ¥­è¨“ç·´çš„å¹³æ°‘æ˜¯å¦æ‡‰è©²å»èŠ±è“®ç¸£å…‰å¾©é„‰åƒèˆ‡æ•‘ç½ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830eab43-1635-4cb1-aa13-06e5665d2d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f702f519-37dd-4b45-a492-0c4bcdd521b0",
   "metadata": {},
   "source": [
    "# ChatBot æœ¬é«”èˆ‡è¨˜æ†¶æ©Ÿåˆ¶\n",
    "\n",
    "ğŸ¯ **æœ¬ç« å­¸å®Œä½ å°‡èƒ½å­¸æœƒä»€éº¼ï¼š**\n",
    "\n",
    "- äº†è§£ Stateless èˆ‡ Stateful å°è©±ç³»çµ±çš„å·®ç•°  \n",
    "- å­¸æœƒä½¿ç”¨ ChatMessageHistory ç®¡ç†å°è©±è¨˜éŒ„  \n",
    "- æŒæ¡ ChatPromptTemplate èˆ‡ MessagesPlaceholder çš„æ‡‰ç”¨  \n",
    "- èƒ½å¯¦ä½œç°¡å–®çš„å¤šè¼ªå°è©±æ©Ÿåˆ¶ \n",
    "\n",
    "ğŸ“˜ **æœ€çµ‚ä½ å°‡å…·å‚™çš„èƒ½åŠ›ï¼š**  \n",
    "èƒ½å¤ è¨­è¨ˆä¸€å€‹å…·å‚™ã€Œå°è©±è¨˜æ†¶ã€çš„æ™ºèƒ½ ChatBotï¼Œèƒ½å¤ åœ¨ä¸Šä¸‹æ–‡ä¸­å»¶çºŒä¸¦ç†è§£å°è©±èªå¢ƒã€‚  \n",
    "\n",
    "\n",
    "1. N-Shot Learning èˆ‡å°è©±æ­·å²\n",
    "\n",
    "    - æ­·å²å°è©±å¯ä»¥çœ‹æˆä¸€å€‹ Q&A pair åˆ—è¡¨\n",
    "    - ç•¶å‰æ¨¡å‹åœ¨æ¨ç†æ™‚ï¼ŒæœƒæŠŠã€Œä¹‹å‰çš„å°è©±å…§å®¹ã€ä½œç‚º prompt çš„ä¸€éƒ¨åˆ†ï¼Œå†åŠ ä¸Šä½¿ç”¨è€…æœ€æ–°çš„è¼¸å…¥ï¼Œæ•´åˆå¾Œä¸Ÿé€²æ¨¡å‹ã€‚é€™å…¶å¯¦å°±æ˜¯ä¸€ç¨® few-shot / N-shot çš„å­¸ç¿’æ–¹å¼ï¼šæ¨¡å‹å¾ç¯„ä¾‹ä¸­æŠ½å–èªå¢ƒä¾†ç†è§£ã€Œç¾åœ¨è©²æ€éº¼å›ç­”ã€ã€‚\n",
    "\n",
    "2. Stateless vs. Stateful\n",
    "    - Statelessï¼šå¦‚æœæ¯æ¬¡è«‹æ±‚éƒ½å®Œå…¨ç¨ç«‹ï¼Œæ²’æœ‰ä»»ä½•æ­·å²å°è©±è¢«å¸¶å…¥ï¼Œé‚£å°±å«ç„¡ç‹€æ…‹ (stateless)ã€‚\n",
    "    - Statefulï¼šå¦‚æœç³»çµ±æœƒä¿å­˜å°è©±æ­·å²ï¼ˆä¸è«–æ˜¯æŠŠæ­·å²å‚³å›æ¨¡å‹ï¼Œé‚„æ˜¯å¤–éƒ¨è¨˜æ†¶ç³»çµ±å­˜èµ·ä¾†ï¼‰ï¼Œé‚£å°±æ˜¯æœ‰ç‹€æ…‹çš„ (stateful)ã€‚\n",
    "    - æ‰€ä»¥æ˜¯å¦ã€Œèƒ½è¨˜ä½ã€éå»ï¼Œå–æ±ºæ–¼è¨­è¨ˆï¼Œè€Œä¸æ˜¯æ¨¡å‹æœ¬èº«è‡ªå¸¶çš„èƒ½åŠ›ã€‚\n",
    "\n",
    "3. Tools çš„è§’è‰²\n",
    "    - è®“ ChatBot å¼·å¤§çš„æ˜¯ tools\n",
    "    - æ¨¡å‹æœ¬èº«é›–ç„¶èƒ½ç”Ÿæˆèªè¨€ï¼Œä½† çµåˆå¤–éƒ¨å·¥å…·ï¼ˆä¾‹å¦‚è³‡æ–™åº«æŸ¥è©¢ã€è¨ˆç®—å™¨ã€ç¶²è·¯æœå°‹ã€ä»£ç¢¼åŸ·è¡Œã€åœ–ç‰‡ç”Ÿæˆï¼‰å¾Œï¼ŒChatBot æ‰èƒ½çœŸæ­£åšåˆ°ã€Œæœƒæ¨ç† + æœƒè¡Œå‹•ã€ï¼Œä¸å†åªå—é™æ–¼åƒæ•¸å…§çš„çŸ¥è­˜ã€‚\n",
    "\n",
    "    - å¯ä»¥ç†è§£æˆï¼šæ¨¡å‹æ˜¯ã€Œå¤§è…¦ã€ï¼ŒTools æ˜¯ã€Œæ‰‹è…³ã€ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22673222-780c-4356-a6d1-92c69b28d6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(url=\"https://python.langchain.com/v0.1/assets/images/chat_use_case-eb8a4883931d726e9f23628a0d22e315.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e0c8a7-fc37-4778-9dcc-91fd68d2db66",
   "metadata": {},
   "source": [
    "å…ˆå­¸æ€éº¼èª¿å‹•å·¥å…·: æ¨¡å‹å°±åƒæ˜¯ä¸€å€‹è¨“ç·´æœ‰ç´ çš„é˜¿æ–¯å¡”ç‰¹ï¼Œå·¥å…·å°±åƒæ˜¯å‹•åŠ›ç”²ï¼Œå™´å°„èƒŒåŒ…ï¼Œçˆ†å½ˆæ§ï¼Œå’Œéˆé‹¸åŠã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b382daa-6e0c-4426-a04c-054de90c4c01",
   "metadata": {},
   "source": [
    "## å·¥å…·ç¶å®šèˆ‡å·¥å…·å‘¼å« (Tools & ToolMessage)\n",
    "\n",
    "ğŸ¯ **æœ¬ç« å­¸å®Œä½ å°‡èƒ½å­¸æœƒä»€éº¼ï¼š**\n",
    "\n",
    "- ç†è§£ Tool èˆ‡ LLM çš„äº’å‹•æ–¹å¼  \n",
    "- å­¸æœƒä½¿ç”¨ LangChain çš„ @tool èˆ‡ StructuredTool ç¶å®šå¤–éƒ¨å‡½å¼  \n",
    "- æŒæ¡ ToolMessage çš„è¨­è¨ˆèˆ‡å‘¼å«é‚è¼¯  \n",
    "- èƒ½æ•´åˆå¤šå€‹å·¥å…·ï¼ˆå¦‚è¨ˆç®—ã€æŸ¥è©¢ã€WebSearchï¼‰è®“æ¨¡å‹å…·å‚™ã€Œè¡Œå‹•èƒ½åŠ›ã€  \n",
    "\n",
    "ğŸ“˜ **æœ€çµ‚ä½ å°‡å…·å‚™çš„èƒ½åŠ›ï¼š**  \n",
    "èƒ½å¤ æ‰“é€ èƒ½ã€Œæ€è€ƒ + è¡Œå‹•ã€çš„ ChatBotï¼Œçµåˆå¤šç¨®å·¥å…·å®Œæˆè‡ªå‹•åŒ–ä»»å‹™ã€‚  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5c8762-d00d-4796-baf2-426e7374c096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../../\")\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "from src.initialization import credential_init\n",
    "\n",
    "credential_init()\n",
    "\n",
    "# Define a calculator tool\n",
    "@tool\n",
    "def add_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"Adds two numbers together.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "# # Create the LLM\n",
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=6,\n",
    "    disable_streaming=False\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "# Bind the tool to the model\n",
    "llm_with_tools = llm.bind_tools([add_numbers])\n",
    "\n",
    "# Run\n",
    "resp = llm_with_tools.invoke(\"What is 42 + 58?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0983637-cd11-40d2-b763-708542e0d68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158acc75-89e6-4be7-9f6c-96bf6424715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_call = resp.tool_calls[0]\n",
    "print(tool_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1ab833-fe5e-4694-8b51-c47a6b6463ad",
   "metadata": {},
   "source": [
    "æ ¹æ“štool_callè¨ˆç®—çµæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a0d967-880b-4d12-8803-b622d31cee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = eval(tool_call['name'])(tool_call['args'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8185469-c58c-45c0-b144-2fca0a1b7fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecee1c5-c35e-41dc-b72c-7677c8cfc5fe",
   "metadata": {},
   "source": [
    "å»ºç«‹ToolMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d55e641-6d2f-403e-ae95-e15a2e4acc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "tool_msg = ToolMessage(\n",
    "    content=str(result),          # usually a string or simple text\n",
    "    tool_call_id=tool_call['id']    # must match the AIMessage tool_call id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e8b8ec-d462-4376-a882-ce39e90b681d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3407c3f1-03a3-4f76-a1ac-ed7c45f59d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbfb73f-d34d-41a8-bcef-786fb5bbc54a",
   "metadata": {},
   "source": [
    "æœ€å¾Œï¼Œç¶å®šAIMessageå’ŒToolMessageï¼Œåœ¨é€²è¡Œä¸€æ¬¡invokeå¾—åˆ°çµæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131c78f3-e5e9-405d-87db-936384fb4426",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools.invoke([resp, tool_msg])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8003fc29-36fc-414e-acba-45535dd60f36",
   "metadata": {},
   "source": [
    "## OpenAI WebSearch API èˆ‡ç¶²è·¯æœå°‹æ‡‰ç”¨\n",
    "\n",
    "ğŸ¯ **æœ¬ç« å­¸å®Œä½ å°‡èƒ½å­¸æœƒä»€éº¼ï¼š**\n",
    "\n",
    "- å­¸æœƒèª¿ç”¨ OpenAI WebSearch å·¥å…·å–å¾—å³æ™‚è³‡æ–™  \n",
    "- ç†è§£ WebSearch API çš„ actionã€annotationsã€sources åƒæ•¸  \n",
    "- æŒæ¡å¦‚ä½•åœ¨å›æ‡‰ä¸­å¼•ç”¨ä¾†æºä¸¦åˆ†æçµæœ  \n",
    "- äº†è§£ WebSearch çš„å„ªç¼ºé»èˆ‡æ‡‰ç”¨ç­–ç•¥  \n",
    "\n",
    "ğŸ“˜ **æœ€çµ‚ä½ å°‡å…·å‚™çš„èƒ½åŠ›ï¼š**  \n",
    "èƒ½å¤ è¨­è¨ˆå¯å³æ™‚æª¢ç´¢ã€åˆ†æä¸¦ç”Ÿæˆå ±å‘Šçš„ AI åŠ©æ‰‹ï¼Œä¸¦æ ¹æ“šæƒ…å¢ƒé¸æ“‡æœ€ä½³è³‡è¨Šä¾†æºã€‚  \n",
    "\n",
    "\n",
    "### åŸºæœ¬ä½¿ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23060df1-9851-4229-a650-fce282385040",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089aebe8-6418-4abf-a89c-e900940d8ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "         model=\"gpt-4o-mini\",\n",
    "         tools=[{\"type\": \"web_search\"}],\n",
    "         input=\"å¹«æˆ‘æŸ¥è©¢HunterXHunter æœ€æ–°çš„é€²åº¦\"\n",
    "    )\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f00f3a5-4619-4c3d-a844-cb5f1f5897b7",
   "metadata": {},
   "source": [
    "1. A web_search_call output item with the ID of the search call, along with the action taken in web_search_call.action. The action is one of:\n",
    "    - ***search***, which represents a web search. It will usually (but not always) includes the search query and domains which were searched. Search actions incur a tool call cost (see pricing).\n",
    "    - ***open_page***, which represents a page being opened. Supported in reasoning models.\n",
    "    - ***find_in_page***, which represents searching within a page. Supported in reasoning models.\n",
    "2. A message output item containing:\n",
    "    - The text result in message.content[0].text\n",
    "    - Annotations message.content[0].annotations for the cited URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecab106-6a33-4ff4-abde-1ea6f6a21c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f9d764-cf3b-4449-8977-457a73f56904",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.output[1].annotationscontent[0]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db368f73-866a-4031-906e-b13c3ce0cfdb",
   "metadata": {},
   "source": [
    "websearch çµæœ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67476408-b45f-47c4-97cc-c45c2b8d5cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ef0e7e-6b52-4c92-b6e9-519fe54d081c",
   "metadata": {},
   "source": [
    "#### Source åƒæ•¸\n",
    "è‹¥è¦æŸ¥çœ‹åœ¨ç¶²è·¯æœå°‹éç¨‹ä¸­æ“·å–çš„æ‰€æœ‰ç¶²å€ï¼Œå¯ä½¿ç”¨ sources æ¬„ä½ã€‚\n",
    "èˆ‡åªé¡¯ç¤ºæœ€ç›¸é—œåƒè€ƒè³‡æ–™çš„ã€Œè¡Œå…§å¼•ç”¨ï¼ˆinline citationsï¼‰ã€ä¸åŒï¼Œsources æœƒå›å‚³æ¨¡å‹åœ¨ç”Ÿæˆå›æ‡‰æ™‚æ‰€åƒè€ƒçš„å®Œæ•´ç¶²å€æ¸…å–®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02360c51-807b-4372-8e1e-7842b3438a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "         model=\"gpt-4o-mini\",\n",
    "         tools=[{\"type\": \"web_search\"}],\n",
    "         include=[\"web_search_call.action.sources\"],\n",
    "         input=\"å¹«æˆ‘æŸ¥è©¢HunterXHunter æœ€æ–°çš„é€²åº¦\"\n",
    "    )\n",
    "\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fe427b-c397-46d3-bf8f-fb3047d0eeca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response.output[0].action.sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d180b0-80e9-48a8-8d94-c377f135df99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e638b0e-7756-445d-8163-a37fb19ec43a",
   "metadata": {},
   "source": [
    "#### User location\n",
    "To refine search results based on geography, you can specify an approximate user location using country, city, region, and/or timezone.\n",
    "\n",
    "- The city and region fields are free text strings, like Minneapolis and Minnesota respectively.\n",
    "- The country field is a two-letter ISO country code, like US. (ISO 3166-1 alpha-2)\n",
    "- The timezone field is an IANA timezone like America/Chicago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6bb1a5-df6e-47a8-b005-713b38711fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "         model=\"gpt-4o-mini\",\n",
    "         tools=[{\"type\": \"web_search\",\n",
    "                 \"user_location\": {\n",
    "                    \"type\": \"approximate\",\n",
    "                    \"country\": \"US\",\n",
    "                     },\n",
    "                 \"search_context_size\": \"medium\"\n",
    "                }],\n",
    "         include=[\"web_search_call.action.sources\"],\n",
    "         input=\"å¹«æˆ‘æŸ¥è©¢HunterXHunter æœ€æ–°çš„é€²åº¦\"\n",
    "    )\n",
    "                \n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a34144-3180-46d6-a33f-13f5420553b4",
   "metadata": {},
   "source": [
    "å…¶ä»–çš„argument:\n",
    "\n",
    "- Domain filtering (gpt-5 and o-series models only)\n",
    "- reasoning (gpt-5 and o-series models only)\n",
    "    - effort:\n",
    "        - ***minimal***\n",
    "        - ***low***\n",
    "        - ***medium***\n",
    "        - ***high***\n",
    "    -  summary:\n",
    "        - ***auto***\n",
    "        - ***concise***\n",
    "        - ***detailed***  \n",
    "- tool_choice\n",
    "    - ***none*** means the model will not call any tool and instead generates a message.\n",
    "    - ***auto*** means the model can pick between generating a message or calling one or more tools.\n",
    "    - ***required*** means the model must call one or more tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3edf387-37c8-462d-a420-5db82572d39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "  model=\"gpt-5\",\n",
    "  reasoning={\"effort\": \"low\"},\n",
    "  tools=[\n",
    "      {\n",
    "          \"type\": \"web_search\",\n",
    "          \"filters\": {\n",
    "              \"allowed_domains\": [\n",
    "                  \"pubmed.ncbi.nlm.nih.gov\",\n",
    "                  \"clinicaltrials.gov\",\n",
    "                  \"www.who.int\",\n",
    "                  \"www.cdc.gov\",\n",
    "                  \"www.fda.gov\",\n",
    "              ]\n",
    "          },\n",
    "      }\n",
    "  ],\n",
    "  tool_choice=\"auto\",\n",
    "  include=[\"web_search_call.action.sources\"],\n",
    "  input=\"Please perform a web search on how semaglutide is used in the treatment of diabetes.\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc03788-f1b4-4a2b-8cb4-6e7165f50c5d",
   "metadata": {},
   "source": [
    "### å»ºç«‹websearchå·¥å…·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0893d4df-9aea-4051-8f4e-e3df0858ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def websearh_tool(query: str) -> str:\n",
    "    \"\"\"Use this tool to find the latest information or information you are not sure\"\"\"\n",
    "\n",
    "    response = client.responses.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    tools=[\n",
    "                        {\"type\": \"web_search\",}\n",
    "                    ],\n",
    "                    tool_choice=\"auto\",\n",
    "                    input=query)\n",
    "    \n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ce1dcc-df81-4a08-b5c8-bb7197618730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bind the tool to the model\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=6,\n",
    "    disable_streaming=False\n",
    "    # other params...\n",
    ")\n",
    "llm_with_tools = llm.bind_tools([websearh_tool])\n",
    "\n",
    "# Run\n",
    "resp = llm_with_tools.invoke(\"å°ç£2024ç¸½çµ±å¤§é¸çµæœ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99771f07-3b83-45f6-94ce-d9d1a244f195",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91347678-4c9c-4190-a486-0c44cbc122d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_call = resp.tool_calls[0]\n",
    "result = eval(tool_call['name'])(tool_call['args'])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b096b9-1fa3-474c-b41d-b7a7a61b38e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_function(tool_call):\n",
    "    \n",
    "    return eval(tool_call['name'])(tool_call['args'])\n",
    "\n",
    "\n",
    "def follow_up_answer(aimessage):\n",
    "\n",
    "    tool_call = aimessage.tool_calls[0]\n",
    "    \n",
    "    result = call_function(tool_call)\n",
    "\n",
    "    tool_msg = ToolMessage(\n",
    "        content=str(result),          # usually a string or simple text\n",
    "        tool_call_id=tool_call['id']    # must match the AIMessage tool_call id\n",
    "    )\n",
    "    \n",
    "    follow_up = llm_with_tools.invoke([aimessage, tool_msg])\n",
    "\n",
    "    return follow_up\n",
    "\n",
    "follow_up_answer(aimessage=resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f7fe22-cd1d-470a-927b-31725616d95c",
   "metadata": {},
   "source": [
    "## ChatBot æœ¬é«”\n",
    "\n",
    "### LLM æ²’æœ‰è¨˜æ†¶æ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5f885c-cb85-477f-90e8-147e701e6857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=6,\n",
    "    disable_streaming=False\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "message = HumanMessage(\n",
    "            content=\"Translate this sentence from English to Chinese (ç¹é«”ä¸­æ–‡): I love programming.\"\n",
    "        )\n",
    "\n",
    "model.invoke(\n",
    "    [message]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95345038-9a83-465c-a85a-9b2d5f1b6476",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke([HumanMessage(content=\"What did you just say?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71ee93d-2841-4cc0-b417-3fa5554ba1b3",
   "metadata": {},
   "source": [
    "### å¤–éƒ¨è¨˜æ†¶\n",
    "\n",
    "å¦‚ä½•å°‡å¤–éƒ¨è¨˜æ†¶åŠ å…¥?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed4e110-9be7-4a67-be86-31b62451baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=\"Translate this sentence from English to  Chinese (ç¹é«”ä¸­æ–‡): I love programming.\"\n",
    "        ),\n",
    "        AIMessage(content=\"æˆ‘æ„›ç¨‹å¼è¨­è¨ˆ.\"),\n",
    "        HumanMessage(content=\"What did you just say?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce0fbac-247d-4230-849c-b5475b1a4c00",
   "metadata": {},
   "source": [
    "é€é MessagePlaceholderæ¥æ”¶å¤–éƒ¨è¨˜æ†¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dea7e35-f9b4-4d78-8a8e-e85225c5fd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate\n",
    "\n",
    "system_prompt = PromptTemplate(template=(\"You are a helpful assistant. Answer all questions to the best of your \"\n",
    "                                         \"ability.\"))\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_message,\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e88d592-3e03-4a5b-81b2-e792154f94ec",
   "metadata": {},
   "source": [
    "### å»ºç«‹é‚è¼¯éŠæ¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd2ed39-46d6-460e-b78e-5b0c4d06fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_ = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e2a159-a35a-47db-ac82-43c8d9f85887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "pipeline_.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "            content=\"Translate this sentence from English to Chinese (ç¹é«”ä¸­æ–‡): I love programming.\"\n",
    "            ),\n",
    "            AIMessage(content=\"æˆ‘æ„›ç¨‹å¼è¨­è¨ˆ.\"),\n",
    "            HumanMessage(content=\"What did you just say?\"),\n",
    "            ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e0029f-228a-4ff5-afb0-91bf92f00d34",
   "metadata": {},
   "source": [
    "## å°‡å°è©±è¨˜éŒ„å­˜å…¥ChatMessageHistoryè£¡\n",
    "\n",
    "### å°å…¥ä¸¦å‰µå»º ChatMessageHistoryã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f65a6f-3d57-4cf9-8e8f-34623e7965e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "demo_chat_history = ChatMessageHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e08da2-cb06-41db-ab6a-69532596b405",
   "metadata": {},
   "source": [
    "### æ·»åŠ ç”¨æˆ¶å’Œ AI æ¶ˆæ¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eaf68e-486a-44cb-8107-c81b2b11e578",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_chat_history.add_user_message(\"Translate this sentence from English to Chinese (ç¹é«”ä¸­æ–‡): I love programming.\")\n",
    "\n",
    "demo_chat_history.add_ai_message(\"æˆ‘æ„›ç¨‹å¼è¨­è¨ˆ.\")\n",
    "\n",
    "demo_chat_history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cb4014-b66b-40dc-8acc-3e052426ce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_chat_history.add_user_message(\n",
    "    \"What did you just say?\"\n",
    ")\n",
    "\n",
    "response = pipeline_.invoke({\"messages\": demo_chat_history.messages})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dd8248-d495-4798-8d35-7c36862720f4",
   "metadata": {},
   "source": [
    "### æœ€å°ç¯„ä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc02e090-7b0a-45a4-b377-2e37048340df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chat_history = ChatMessageHistory()\n",
    "pipeline_ = prompt|model|StrOutputParser()\n",
    "\n",
    "while True:\n",
    "    question = input(\"What do you want to ask: \")\n",
    "    if question == \"QUIT\":\n",
    "        break\n",
    "    chat_history.add_user_message(question)\n",
    "    response = pipeline_.invoke({\"messages\": chat_history.messages})\n",
    "\n",
    "    print(response)\n",
    "    chat_history.add_ai_message(response)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec25caf-71ec-4064-923c-ff0c5afd4e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.chdir(\"../../../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789cefac-0701-40ee-bbf9-d92c5259117a",
   "metadata": {},
   "source": [
    "## ChatBot + æª¢ç´¢ç³»çµ±æ•´åˆ\n",
    "\n",
    "ğŸ¯ **æœ¬ç« å­¸å®Œä½ å°‡èƒ½å­¸æœƒä»€éº¼ï¼š**\n",
    "\n",
    "- ç†è§£å¦‚ä½•çµåˆ FAISS å‘é‡è³‡æ–™åº«èˆ‡ ChatBot  \n",
    "- å­¸æœƒå»ºç«‹å¯æª¢ç´¢æ–‡æœ¬çš„ RAGï¼ˆRetrieval-Augmented Generationï¼‰å·¥ä½œæµ  \n",
    "- æŒæ¡ StructuredTool èˆ‡ Retriever å·¥å…·çš„å¯¦éš›æ‡‰ç”¨  \n",
    "- èƒ½è®“ ChatBot æ ¹æ“šçŸ¥è­˜åº«å›ç­”ç‰¹å®šå•é¡Œï¼ˆå¦‚å”è©©æª¢ç´¢ï¼‰  \n",
    "\n",
    "ğŸ“˜ **æœ€çµ‚ä½ å°‡å…·å‚™çš„èƒ½åŠ›ï¼š**  \n",
    "èƒ½å¤ æ§‹å»ºä¸€å€‹å…·å‚™ã€Œå¤–éƒ¨è¨˜æ†¶èˆ‡çŸ¥è­˜æª¢ç´¢ã€èƒ½åŠ›çš„æ™ºæ…§ ChatBotï¼Œçµåˆèªæ„æœå°‹èˆ‡ç”Ÿæˆã€‚  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb7abfb-9d66-4ca8-96c7-2358b21d009e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_core.tools import tool\n",
    "from langchain.tools import StructuredTool\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "from src.io.path_definition import get_project_dir\n",
    "from src.initialization import credential_init\n",
    "\n",
    "\n",
    "credential_init()\n",
    "\n",
    "# å¼•å…¥å”è©©å‘é‡æ•¸æ“šåº«\n",
    "filename = os.path.join(get_project_dir(), \"tutorial\", \"LLM+Langchain\", \"Week-2\", \"poem_faiss_index\")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "vectorstore = FAISS.load_local(\n",
    "    filename, embeddings, allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(seearch_type='similarity').configurable_fields(\\\n",
    "        search_kwargs=ConfigurableField(id=\"search_kwargs\")\n",
    "    )\n",
    "\n",
    "\n",
    "class PoemRetrieverArgs(BaseModel):\n",
    "    query: str = Field(description=\"The keyword or phrase to search for Tang poems. ç”¨ä¾†æœå°‹å”è©©çš„é—œéµå­—æˆ–æ˜¯å¥å­\")\n",
    "    k: int = Field(1, description=\"The number of poems to retrieve.\")\n",
    "\n",
    "\n",
    "def _poem_retriever(query: str, k: int):\n",
    "    output = retriever.invoke(query, config={\"configurable\": {\"search_kwargs\": {\"k\": k}}})\n",
    "    return output\n",
    "\n",
    "\n",
    "# ä½¿ç”¨ StructuredToolå»ºç«‹å·¥å…·\n",
    "# ä¸¦ä¸”é€šéargs_schemaä¾†å‘ŠçŸ¥è¼¸å…¥çš„æ ¼å¼\n",
    "\n",
    "poem_retriever = StructuredTool.from_function(\n",
    "    func=_poem_retriever,\n",
    "    args_schema=PoemRetrieverArgs,\n",
    "    description=\"ä½¿ç”¨é€™å€‹å·¥å…·ä¾†æœå°‹å”è©©; Use this tool to search for Tang poems.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cdf253-b43a-45e2-8f92-2667ac5480ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "system_prompt = PromptTemplate(template=dedent(\"\"\"You are a helpful assistant. \n",
    "Answer all questions to the best of your ability.\n",
    "\"\"\"))\n",
    "\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_message,\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "#                    model_name=\"gpt-4o-2024-05-13\", temperature=0)\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=6,\n",
    "    disable_streaming=False\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "model_with_tools = model.bind_tools([poem_retriever])\n",
    "\n",
    "chatbot_pipeline = prompt | model_with_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6d758f-2e4a-40b0-9775-ff872fae19db",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "question = \"å¹«æˆ‘æ‰¾3é¦–é—œæ–¼å°æ–¼äººç”Ÿæ„Ÿå˜†çš„å”è©©\"\n",
    "\n",
    "chat_history.add_user_message(question)\n",
    "\n",
    "output = chatbot_pipeline.invoke({\"messages\": chat_history.messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d0c399-c1ce-45ed-a54c-42bc6ff3e3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d032970-2c15-408f-9931-f54b1301a62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "def call_function(tool_call):\n",
    "    \n",
    "    return eval(tool_call['name'])(**tool_call['args'])\n",
    "\n",
    "\n",
    "def follow_up_answer(aimessage):\n",
    "\n",
    "    tool_call = aimessage.tool_calls[0]\n",
    "    \n",
    "    result = call_function(tool_call)\n",
    "\n",
    "    tool_msg = ToolMessage(\n",
    "        content=str(result),          # usually a string or simple text\n",
    "        tool_call_id=tool_call['id']    # must match the AIMessage tool_call id\n",
    "    )\n",
    "    \n",
    "    follow_up = model_with_tools.invoke([aimessage, tool_msg])\n",
    "\n",
    "    return follow_up\n",
    "\n",
    "output = follow_up_answer(aimessage=output)\n",
    "\n",
    "print(output)\n",
    "# follow_up_answer(human_message=question, ai_message=output.content, additional_kwargs=output.additional_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e077d123-6b59-46c8-8ab3-36c0ea9f7408",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "while True:\n",
    "    question = input(\"What do you want to ask: \")\n",
    "    if question == \"QUIT\":\n",
    "        break\n",
    "    chat_history.add_user_message(question)\n",
    "    output = chatbot_pipeline.invoke({\"messages\": chat_history.messages})\n",
    "\n",
    "    if output.tool_calls != []:\n",
    "        response = follow_up_answer(output).content\n",
    "    else:\n",
    "        response = output.content\n",
    "\n",
    "    print(\"***********************\")\n",
    "    print(response)\n",
    "    print(\"***********************\")\n",
    "    \n",
    "    chat_history.add_ai_message(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3142dd21-ccf2-4bd4-8714-d3995bb5d713",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨ä»£ç¢¼è§£æ±ºæ•¸å­¸å•é¡Œå·¥å…·\n",
    "\n",
    "- OpenAI API: https://platform.openai.com/docs/guides/tools-code-interpreter\n",
    "- è‡ªå·±ç©ç©çœ‹ã€‚ä½†æˆ‘å€‘ä¹Ÿå¯ä»¥è‡ªå·±æ‰‹æ“å¯«ä»£ç¢¼çš„å·¥å…·\n",
    "\n",
    "1. ä»£ç¢¼ç”¢ç”Ÿçš„é‚è¼¯éŠæ¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52de6f44-3c4a-4c8f-827e-561d0c141049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../../\")\n",
    "\n",
    "from src.initialization import credential_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186123ce-d45c-4bf7-bd03-315cbf3eac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from textwrap import dedent\n",
    "\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain.tools import StructuredTool\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from src.initialization import credential_init\n",
    "\n",
    "credential_init()\n",
    "\n",
    "\n",
    "def build_standard_chat_prompt_template(kwargs):\n",
    "\n",
    "    messages = []\n",
    " \n",
    "    if 'system' in kwargs:\n",
    "        content = kwargs.get('system')\n",
    "        prompt = PromptTemplate(**content)\n",
    "        message = SystemMessagePromptTemplate(prompt=prompt)\n",
    "        messages.append(message)  \n",
    "\n",
    "    if 'human' in kwargs:\n",
    "        content = kwargs.get('human')\n",
    "        prompt = PromptTemplate(**content)\n",
    "        message = HumanMessagePromptTemplate(prompt=prompt)\n",
    "        messages.append(message)\n",
    "        \n",
    "    chat_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "    \n",
    "    return chat_prompt\n",
    "\n",
    "\n",
    "@chain\n",
    "def code_execution(code):\n",
    "    \n",
    "    match = re.findall(r\"python\\n(.*?)\\n```\", code, re.DOTALL)\n",
    "    python_code = match[0]\n",
    "    \n",
    "    lines = python_code.strip()#.split('\\n')\n",
    "    # *stmts, last_line = lines\n",
    "\n",
    "    local_vars = {}\n",
    "    exec(lines, local_vars)\n",
    "\n",
    "    return local_vars\n",
    "\n",
    "\n",
    "def call_function(tool_call):\n",
    "    \n",
    "    return eval(tool_call['name'])(**tool_call['args'])\n",
    "\n",
    "\n",
    "def follow_up_answer(aimessage):\n",
    "\n",
    "    tool_call = aimessage.tool_calls[0]\n",
    "    \n",
    "    result = call_function(tool_call)\n",
    "\n",
    "    tool_msg = ToolMessage(\n",
    "        content=str(result),          # usually a string or simple text\n",
    "        tool_call_id=tool_call['id']    # must match the AIMessage tool_call id\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        follow_up = model_with_tools.invoke([aimessage, tool_msg])\n",
    "    except:\n",
    "        raise ValueError(f\"aimessage={aimessage}\\ntool_msg={tool_msg}\")\n",
    "    \n",
    "    return follow_up\n",
    "\n",
    "\n",
    "system_template = (\n",
    "    \"You are a highly skilled Python developer. Your task is to generate Python code strictly based on the user's instructions.\\n\"\n",
    "    \"Leverage statistical and mathematical libraries such as `statsmodels`, `scipy`, and `numpy` where appropriate to solve the problem.\\n\"\n",
    "    \"Your response must contain only the Python code â€” no explanations, comments, or additional text.\\n\\n\"\n",
    ")\n",
    "\n",
    "human_template = dedent(\"\"\"{query}\\n\\n\n",
    "                            Always copy the final answer to a variable `answer`\n",
    "                            Code:\n",
    "                        \"\"\")\n",
    "\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"query\"]}}\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# model = ChatGoogleGenerativeAI(\n",
    "#     model=\"gemini-2.0-flash\",\n",
    "#     temperature=0,\n",
    "#     max_tokens=None,\n",
    "#     timeout=None,\n",
    "#     max_retries=6,\n",
    "#     disable_streaming=False\n",
    "#     # other params...\n",
    "# )\n",
    "\n",
    "\n",
    "chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "code_generation = chat_prompt_template|model|StrOutputParser()\n",
    "\n",
    "code_pipeline = code_generation|code_execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd20ef9-1d19-424e-95a7-aec635a6273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeArgs(BaseModel):\n",
    "    query: str = Field(description=\"User request; ç”¨æˆ¶éœ€æ±‚\")\n",
    "\n",
    "\n",
    "def _calculator(query: str,):\n",
    "    output = code_pipeline.invoke(query)\n",
    "    return output\n",
    "\n",
    "\n",
    "mathematic_tool = StructuredTool.from_function(\n",
    "    func=_calculator,\n",
    "    args_schema=CodeArgs,\n",
    "    description=\"Use this tool to solve mathematic related problem; ä½¿ç”¨é€™å€‹å·¥å…·è§£æ±ºæ•¸å­¸å•é¡Œ\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1c23eb-d494-49a2-b913-d23cdd4d8336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "model_with_tools = model.bind_tools([mathematic_tool])\n",
    "\n",
    "system_prompt = PromptTemplate(template=dedent(\"\"\"You are a helpful assistant. \n",
    "Answer all questions to the best of your ability.\n",
    "\"\"\"))\n",
    "\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_message,\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chatbot_pipeline = prompt | model_with_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd7e55f-75ab-4323-bcdd-a13246308eab",
   "metadata": {},
   "source": [
    "### æŒ‘æˆ°: åŒ—ä¸€å¥³æ®µè€ƒè€ƒé¡Œ\n",
    "\n",
    "https://drive.google.com/file/d/1csHdgvc5WtbJZ4n39eozogVVPIkPWABf/view\n",
    "\n",
    "- Q1: ä»¥ä¸‹æ•˜è¿°æ˜¯å¦æ­£ç¢º: æ»¿è¶³æ–¹ç¨‹å¼ x^2 + y^2 + 2x âˆ’10y + 30 = 0 ä¹‹é»(x, y)çš„åœ–å½¢æ˜¯ä¸€å€‹åœ“\n",
    "- Q2: ä»¥ä¸‹æ•˜è¿°æ˜¯å¦æ­£ç¢º: éä¸‰é» A( 1, âˆ’ 3), B( 2, 6 ), C( 4, 24 )çš„åœ“æ°æœ‰ä¸€å€‹\n",
    "- Q3: ä»¥ä¸‹æ•˜è¿°æ˜¯å¦æ­£ç¢º: ç›´ç·š 3x âˆ’4y + 7 = 0 èˆ‡åœ“ (x âˆ’ 2)^2 + (y + 3)^2 = 5 æ°æœ‰ä¸€äº¤é»\n",
    "- Q4: ä»¥ä¸‹æ•˜è¿°æ˜¯å¦æ­£ç¢º: åœ“(x âˆ’ 2)^2 + (y + 3)^2 = 5 ä¸Šæ°æœ‰äºŒé»èˆ‡ç›´ç·š 3x âˆ’4y âˆ’13= 0 çš„è·é›¢ç­‰æ–¼ 2\n",
    "- Q5: ä»¥ä¸‹æ•˜è¿°æ˜¯å¦æ­£ç¢º: P(a, b) ç‚º åœ“ (x âˆ’ 2)^2 + ( y + 3)^2 = 4 ä¸Šçš„é»,å‰‡ä½¿ (a^2 + b^2)^0.5 ç‚ºæ•´æ•¸çš„é»å…±æœ‰ 8 å€‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacfcc2c-95be-4e57-89cd-ce5303e89230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "chat_history = ChatMessageHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a28d83-46e6-4efc-b743-d0bf3a4eeffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    question = input(\"Your question: \")\n",
    "    if question == 'quit':\n",
    "        break\n",
    "    chat_history.add_user_message(question)\n",
    "    output = chatbot_pipeline.invoke({\"messages\": chat_history.messages})\n",
    "    \n",
    "    try:\n",
    "        if output.tool_calls != []:\n",
    "            print(\"Call tool\")\n",
    "            final_answer = follow_up_answer(aimessage=output)\n",
    "        else:\n",
    "            print(\"No Tool\")\n",
    "            final_answer = output\n",
    "        print(f\"AI: {final_answer.content}\")\n",
    "        chat_history.add_ai_message(final_answer.content)\n",
    "    except KeyError:\n",
    "        print(f\"AI: {output.content}\")\n",
    "        chat_history.add_ai_message(output.content)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02f81e8-0696-42b4-93a8-6c04db9f52f1",
   "metadata": {},
   "source": [
    "### æœ‰è¾¦æ³•åŠ å…¥ä¸€äº›åŸºæœ¬çš„æ©Ÿæ¢°å­¸ç¿’ä¾†é€²è¡Œåˆ†æå—?\n",
    "\n",
    "æˆ‘é‚„ä¸çŸ¥é“ï¼Œæ‡‰è©²æ˜¯æœƒè »æœ‰è¶£çš„\n",
    "\n",
    "åˆ°é€™è£¡ä½ æ‡‰è©²å¯ä»¥èªè­˜åˆ°ï¼Œå¯«ChatBotæœ¬é«”ä¸¦ä¸å›°é›£ï¼Œä½†ä¸€å€‹ChatBotå¥½ä¸å¥½ç”¨æ˜¯ç”±ä»–æ‰€ç¶‘ç¶çš„å·¥å…·æ±ºå®šã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fe397f-7d0c-43d7-bef8-e14510ffb8e0",
   "metadata": {},
   "source": [
    "# Websearch ç­–ç•¥èˆ‡è³‡æ–™ä¾†æºè¨­è¨ˆ\n",
    "\n",
    "ğŸ¯ **æœ¬ç« å­¸å®Œä½ å°‡èƒ½å­¸æœƒä»€éº¼ï¼š**\n",
    "\n",
    "- ç†è§£ Websearch çš„å„ªå‹¢ã€é™åˆ¶èˆ‡å¸¸è¦‹é¢¨éšª  \n",
    "- å­¸æœƒä¾æ“šæ‡‰ç”¨å ´æ™¯é¸æ“‡æ­£ç¢ºçš„è³‡è¨Šä¾†æºï¼ˆWikipediaã€Fandomã€APIï¼‰  \n",
    "- æŒæ¡è³‡æ–™å“è³ªæ§åˆ¶èˆ‡ä¾†æºéæ¿¾ç­–ç•¥  \n",
    "- æŒæ¡åˆ©ç”¨ LLM å‹•æ…‹ç”Ÿæˆ Pydantic Schema çš„æŠ€å·§  \n",
    "- èƒ½å»ºç«‹ä¸€å€‹å¯å¾ Wikipedia / Fandom æŠ½å–çµæ§‹åŒ–è³‡æ–™çš„è‡ªå‹•åŒ– Pipeline  \n",
    "\n",
    "ğŸ“˜ **æœ€çµ‚ä½ å°‡å…·å‚™çš„èƒ½åŠ›ï¼š**  \n",
    "- å…·å‚™ä»¥ LLM ç‚ºæ ¸å¿ƒçš„ç¶²è·¯è³‡è¨Šæ“·å–èˆ‡åˆ†æèƒ½åŠ›ï¼Œèƒ½å»ºç«‹é«˜å“è³ªã€å¤šä¾†æºçš„ AI ç ”ç©¶èˆ‡å…§å®¹ç”Ÿæˆç³»çµ±ã€‚\n",
    "- èƒ½å¤ è¨­è¨ˆçµåˆçˆ¬èŸ²ã€LLM ä»£ç¢¼ç”Ÿæˆèˆ‡çµæ§‹åŒ–è³‡æ–™æ“·å–çš„æ™ºæ…§è³‡æ–™åˆ†æç³»çµ±ã€‚   \n",
    "\n",
    "## Websearch å„ªç¼ºé»èˆ‡æ‡‰ç”¨ç­–ç•¥\n",
    "\n",
    "### å„ªé»\n",
    "- **å¤šå…ƒåŒ–ä¾†æº**ï¼šæ¶µè“‹ç¯„åœå»£ï¼Œèƒ½æä¾›å¤šè§’åº¦è³‡è¨Šã€‚\n",
    "- **å³æ™‚æ€§**ï¼šèƒ½å¿«é€Ÿå–å¾—å…¬é–‹ç¶²é ä¸Šçš„æœ€æ–°å…§å®¹ã€‚\n",
    "- **éˆæ´»æ€§**ï¼šé©åˆéœ€è¦ã€Œå¤šä¾†æºæ¯”å°ã€çš„å•é¡Œã€‚\n",
    "\n",
    "### ç¼ºé»\n",
    "- **ç¢ç‰‡åŒ–**ï¼šè³‡è¨Šåˆ†æ•£ã€æ ¼å¼ä¸ä¸€ï¼Œé›£ä»¥ç›´æ¥ç³»çµ±åŒ–ä½¿ç”¨ã€‚\n",
    "- **å“è³ªåƒå·®ä¸é½Š**ï¼šä¾†æºå¯é åº¦ä¸åŒï¼Œå¯èƒ½å­˜åœ¨éŒ¯èª¤æˆ–éæ™‚è³‡è¨Šã€‚\n",
    "- **é™åˆ¶èˆ‡é¢¨éšª**ï¼šéƒ¨åˆ† API æˆ–æœå°‹éç¨‹å¯èƒ½å› æ”¿ç­–ã€å®‰å…¨æˆ–æˆæ¬Šè€Œé˜»æ“‹ç‰¹å®šå…§å®¹ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ä½•æ™‚é©åˆä½¿ç”¨ Websearch\n",
    "- éœ€è¦ **å¤šè§’åº¦è§€é»**ï¼ˆå¦‚æ–°èã€è«–å£‡ã€ç¤¾ç¾¤è³‡è¨Šï¼‰ã€‚\n",
    "- **é–‹æ”¾æ¢ç´¢**ï¼Œå°ä¾†æºç²¾ç¢ºåº¦è¦æ±‚ä¸é«˜ã€‚\n",
    "- ç„¡æ³•é€éå–®ä¸€å¯é è³‡æ–™åº«æ»¿è¶³éœ€æ±‚æ™‚ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ä½•æ™‚æ›´é©åˆä½¿ç”¨ç‰¹å®šä¾†æº\n",
    "- **å°ˆé–€é ˜åŸŸ**ï¼šå¦‚ Wikipediaã€Fandom Wikiï¼ˆä¾‹å¦‚éŠæˆ²ã€å°èªªã€Warhammer 40kï¼‰ã€‚\n",
    "- **çµæ§‹åŒ–è³‡æ–™éœ€æ±‚**ï¼šä¾†æºæœ‰è¦å‰‡çš„ç¶²å€èˆ‡å…§å®¹çµ„ç¹”ï¼Œä¾¿æ–¼ç¨‹å¼åŒ–æª¢ç´¢ã€‚\n",
    "- **é«˜å¯ä¿¡åº¦éœ€æ±‚**ï¼šæ¸›å°‘è™•ç†éå¤šé›œè¨Šã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### API ä½¿ç”¨æ³¨æ„äº‹é …\n",
    "- **å®‰å…¨å¯©æŸ¥é˜»æ“‹**ï¼šå¯èƒ½å› æ¶‰åŠã€Œä¸å…è¨±æˆ–æ•æ„Ÿå…§å®¹ã€è€Œç„¡æ³•ç²å–å…¬é–‹è³‡æ–™ã€‚\n",
    "- **æˆæ¬Šèˆ‡é™åˆ¶**ï¼šåŒ…å«ä»˜è²»ç‰†ã€Rate Limitã€éš±ç§è¦ç¯„ç­‰ã€‚\n",
    "- **å‚™æ´è§’è‰²**ï¼šWebsearch å¯ä½œç‚ºè£œå……æ–¹æ¡ˆï¼Œä½†ä¸ä¸€å®šæ˜¯è¬èƒ½è§£æ±ºæ–¹å¼ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ç¸½çµ\n",
    "Websearch æä¾›äº† **å»£æ³›è€Œå¤šå…ƒçš„è³‡è¨Š**ï¼Œä½†ä¹Ÿå¸¶ä¾† **ç¢ç‰‡åŒ–èˆ‡å“è³ªå•é¡Œ**ã€‚  \n",
    "è‹¥éœ€æ±‚æ˜ç¢ºã€å¯ä¾é çµæ§‹åŒ–ä¸”å¯ä¿¡çš„ä¾†æºï¼ˆå¦‚ Wikipediaã€Fandomï¼‰ï¼Œæ‡‰å„ªå…ˆé¸ç”¨ã€‚  \n",
    "è‹¥éœ€è¦å¤šè§’åº¦ã€é–‹æ”¾æ¢ç´¢æˆ–ç„¡ç‰¹å®šè³‡æ–™åº«å¯ä¾è³´æ™‚ï¼ŒWebsearch æ‰èƒ½ç™¼æ®æœ€å¤§åƒ¹å€¼ã€‚\n",
    "\n",
    "\n",
    "## è¿”ç„¡ æ­¸ä¸€\n",
    "\n",
    "- å‡è¨­ä½ ç¢ºå®šåœ¨æŸå€‹ä¾†æºè‚¯å®šæœ‰è³‡è¨Šçš„æ™‚å€™ï¼Œå–å¾—è©²ä¾†æºçš„ç¶²å€\n",
    "- ä½¿ç”¨ç¬¬ä¸€å‘¨å’Œç¬¬ä¸‰å‘¨çš„æŠ€å·§çˆ¬å–ç¶²å€çš„å…§å®¹\n",
    "- é€éLLMæå–ä½ è¦çš„è¨Šæ¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a50883-eb4e-4411-a1df-d45f4348f0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from src.initialization import credential_init\n",
    "\n",
    "\n",
    "def build_standard_chat_prompt_template(kwargs):\n",
    "\n",
    "    messages = []\n",
    " \n",
    "    if 'system' in kwargs:\n",
    "        content = kwargs.get('system')\n",
    "        prompt = PromptTemplate(**content)\n",
    "        message = SystemMessagePromptTemplate(prompt=prompt)\n",
    "        messages.append(message)  \n",
    "\n",
    "    if 'human' in kwargs:\n",
    "        content = kwargs.get('human')\n",
    "        prompt = PromptTemplate(**content)\n",
    "        message = HumanMessagePromptTemplate(prompt=prompt)\n",
    "        messages.append(message)\n",
    "        \n",
    "    chat_prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "    \n",
    "    return chat_prompt_template\n",
    "\n",
    "credential_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020d4e82-2653-4ca3-a9ab-f266479ba176",
   "metadata": {},
   "source": [
    "### æŠ½å–Wikipediaçš„å…§å®¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864440a6-581a-44ed-a5bb-c65d1f4c1913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "query = \"çµäººä¸­çš„å¿µèƒ½åŠ›ç³»çµ±\"\n",
    "\n",
    "# Wikipediaèªè¨€é¸æ“‡\n",
    "URL = \"https://ja.wikipedia.org/w/api.php\"\n",
    "\n",
    "\n",
    "PARAMS = {\n",
    "    \"action\": \"parse\",\n",
    "    # Wikipedia é é¢çš„ é—œéµå­—\n",
    "    \"page\": \"HUNTERÃ—HUNTER\",\n",
    "    \"prop\": \"text\",\n",
    "    \"format\": \"json\"\n",
    "}\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"AI Tutorial Bot/1.0 (mengchiehling@gmail.com)\"\n",
    "}\n",
    "\n",
    "response = session.get(url=URL, params=PARAMS, headers=HEADERS)\n",
    "\n",
    "data = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c785982-fc65-41e9-8fde-09286016f9a8",
   "metadata": {},
   "source": [
    "ä½¿ç”¨ bs4 è™•ç†æ•¸æ“š "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b4f0ab-ce4f-4d0f-b03a-5f8ca35d8b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_content = data['parse']['text'][\"*\"]\n",
    "\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# ç§»é™¤ style å’Œ script\n",
    "for tag in soup([\"style\", \"script\"]):\n",
    "    tag.decompose()\n",
    "\n",
    "# æå–æ–‡å­—\n",
    "text_content = soup.get_text(separator=\"\\n\")\n",
    "\n",
    "# æ¸…ç†ç©ºç™½èˆ‡ç©ºè¡Œ\n",
    "cleaned_text = \"\\n\".join(\n",
    "    line.strip() for line in text_content.splitlines() if line.strip()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c483ad-bbe3-4097-8479-a5f69fe48537",
   "metadata": {},
   "source": [
    "æ…¢æ…¢å¯«Pydanticç‰©ä»¶æ˜¯å¯ä»¥å¯¦ç¾çš„ç›®æ¨™çš„ï¼Œä½†æ˜¯åœ¨æ‡‰ç”¨ä¸­æˆ‘å€‘å¸Œæœ›æœ‰æ›´å¥½çš„è‡ªå‹•åŒ–: æ ¹æ“šä½¿ç”¨è€…çš„éœ€æ±‚è‡ªå‹•ç”Ÿæˆç‰©ä»¶\n",
    "\n",
    "æˆ‘å€‘å˜—è©¦çµåˆä»£ç¢¼ç”Ÿæˆä¾†è¼”åŠ©å®Œæˆç›®æ¨™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e7fb5c-1a59-43f5-ba17-c9fa49166e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_example = dedent(\"\"\"\n",
    "    # Example 1: Name extraction using Pydantic and LangChain\n",
    "\n",
    "    from pydantic import BaseModel, Field\n",
    "    from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "    class NameExtractor(BaseModel):\n",
    "        name: str = Field(description=\"The extracted name from the input text\")\n",
    "\n",
    "    output_parser = PydanticOutputParser(pydantic_object=NameExtractor)\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "    ---\n",
    "    # Example 2: Multiple product information extraction using Pydantic and Langchain\n",
    "\n",
    "    from typing import List\n",
    "    \n",
    "    class Product(BaseModel):\n",
    "        name: str = Field(description=\"Product\")\n",
    "        brand: str = Field(description=\"The brand name\")\n",
    "        country_code: str = Field(description=\"ISO 3166-1 alpha-2 of the country of the brand\")\n",
    "\n",
    "    class ProductOutput(BaseModel):\n",
    "        products: List[Product] = Field(description=\"A list of products\")\n",
    "\n",
    "    output_parser = PydanticOutputParser(pydantic_object=NameExtractor)\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "    \n",
    "\"\"\")\n",
    "\n",
    "\n",
    "system_template = dedent(f\"\"\"\n",
    "    You are an expert Python developer specializing in large language models and the LangChain framework.\n",
    "    Your objective is to generate **only valid, executable Python code** that solves the user's request.\n",
    "\n",
    "    Requirements:\n",
    "    - Use Pydantic models when defining structured outputs.\n",
    "    - Ensure imports are correct and minimal.\n",
    "    - Follow PEP 8 formatting standards.\n",
    "    - Do not include any explanations, markdown, comments, or extra text outside the code block.\n",
    "    - You have have the output_parser and the format_instruction of the Pydantic models.\n",
    "\n",
    "    Example structure:\n",
    "    {code_example}\n",
    "\"\"\")\n",
    "\n",
    "human_template = dedent(\"\"\"\n",
    "                        {query}\n",
    "                        Code:\n",
    "                        \"\"\")\n",
    "\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"query\"]}}\n",
    "\n",
    "chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "code_generation = chat_prompt_template|model|StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8449aef9-cf5c-43cb-9a28-6cb8a93bfcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_code = code_generation.invoke({\"query\": query})\n",
    "print(generated_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa98e812-a0be-4a70-82f5-9ac142b66a85",
   "metadata": {},
   "source": [
    "### ä»£ç¢¼åŸ·è¡Œå·¥å…·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3962439f-2aca-4a27-bf91-05aa01db7581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "@chain\n",
    "def code_execution(code):\n",
    "    \n",
    "    match = re.findall(r\"python\\n(.*?)\\n```\", code, re.DOTALL)\n",
    "    python_code = match[0]\n",
    "    \n",
    "    lines = python_code.strip()#.split('\\n')\n",
    "    # *stmts, last_line = lines\n",
    "\n",
    "    local_vars = {}\n",
    "    exec(lines, local_vars)\n",
    "\n",
    "    return local_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf658777-da30-4c71-be1a-85db64f88b45",
   "metadata": {},
   "source": [
    "åŸ·è¡Œç”¢ç”Ÿçš„ä»£ç¢¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcddf59-2aab-418f-80b7-ce3d585eda94",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_vars = code_execution.invoke(generated_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba66780-462e-4840-aab5-891b6f30987b",
   "metadata": {},
   "source": [
    "ç”¢ç”Ÿéœ€è¦çš„pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b0c809-d6c9-4b6e-acbb-cb384380fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_answer_pipeline(output_parser, format_instructions):\n",
    "\n",
    "    human_template = dedent(\"\"\"{query}\n",
    "                           \n",
    "                           context:\n",
    "                           {context}\n",
    "                           output format instruction = {format_instruction} \n",
    "                        \"\"\")\n",
    "\n",
    "\n",
    "    input_ = {\"system\": {\"template\": \"You are a helpful AI assistant.\"},\n",
    "              \"human\": {\"template\": human_template,\n",
    "                        \"input_variable\": [\"query\"],\n",
    "                        \"partial_variables\": {'format_instruction': format_instructions}\n",
    "                        }}\n",
    "    \n",
    "    chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "    \n",
    "    model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    \n",
    "    answer_pipeline = chat_prompt_template|model|output_parser\n",
    "\n",
    "    return answer_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729a689f-cd1c-41b7-8492-9fabc48b4eb9",
   "metadata": {},
   "source": [
    "åŸ·è¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58735891-8732-4fed-9fae-befa8a36e000",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_pipeline = build_answer_pipeline(output_parser=local_vars['output_parser'], format_instructions=local_vars['format_instructions'])\n",
    "\n",
    "output = answer_pipeline.invoke({\"query\": query, \"context\": cleaned_text})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ebdaf7-e0cb-4ea4-83bb-3a78a850c0c8",
   "metadata": {},
   "source": [
    "### Fan Wiki: Titan\n",
    "\n",
    "è¬æ©Ÿä¹‹ç¥æ­å§†å°¼è³½äºçš„åŒ–èº«\n",
    "\n",
    "https://warhammer40k.fandom.com/wiki/Titan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c618afff-8453-43bf-97b3-d485adc4d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def parsing_process(url):\n",
    "    \"\"\"\n",
    "    Fetches and extracts text content from a given URL.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL of the web page to fetch and parse.\n",
    "\n",
    "    Returns:\n",
    "    str: Cleaned text content extracted from the web page.\n",
    "\n",
    "    Raises:\n",
    "    requests.exceptions.RequestException: If an error occurs while fetching the URL.\n",
    "\n",
    "    Notes:\n",
    "    - This function sends a GET request to the specified URL.\n",
    "    - It uses BeautifulSoup to parse the HTML content of the response.\n",
    "    - Any <style> tags in the HTML are removed to extract only textual content.\n",
    "    - The extracted text is cleaned by removing extra whitespace and empty lines.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; MyTest/1.0)\",\n",
    "    \"Accept\": \"text/html,application/json\"\n",
    "    }\n",
    "    \n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Get the content of the response\n",
    "    html_content = response.text\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # ç§»é™¤ style å’Œ script\n",
    "    for tag in soup([\"style\", \"script\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # Extract and print only the text content\n",
    "    text_content = soup.get_text(separator='\\n')\n",
    "\n",
    "    # Clean up the text (optional)\n",
    "    cleaned_text = '\\n'.join(line.strip() for line in text_content.splitlines() if line.strip())\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "url = \"https://warhammer40k.fandom.com/wiki/Titan\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeab876-e8f1-46be-b55e-0c078b632b9e",
   "metadata": {},
   "source": [
    "æå–ç¶²é å…§å®¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "328ef7c0-9650-476e-abf4-fa97941e3304",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = parsing_process(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282b3066-9f2e-4a6a-a83b-6c69ab69cd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_code = code_generation.invoke({\"query\": query})\n",
    "\n",
    "local_vars = code_execution.invoke(generated_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb5a1f6-3a28-4510-8b96-c2d4556ebc7c",
   "metadata": {},
   "source": [
    "å»ºç«‹æ–°çš„pipelineä¸¦ä¸”åŸ·è¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de8f1a4-a199-4f83-bb92-c4ce19974751",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_pipeline = build_answer_pipeline(output_parser=local_vars['output_parser'], format_instructions=local_vars['format_instructions'])\n",
    "\n",
    "output = answer_pipeline.invoke({\"query\": query, \"context\": cleaned_text})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843802cc-f500-4615-aeb8-e0bfd746ab0e",
   "metadata": {},
   "source": [
    "#### è©¦è©¦çœ‹æ›´å…·æœ‰æŒ‘æˆ°çš„å•é¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aa7a2e-a2fc-459a-b1ea-92e115d583fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"å¹«æˆ‘æ‰¾å‡ºæ‰€æœ‰é™£ç‡Ÿçš„æ‰€æœ‰æ³°å¦ç´šåˆ¥\"\n",
    "\n",
    "generated_code = code_generation.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54b98bd-7994-40dc-8a26-0ffb439f4a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_vars = code_execution.invoke(generated_code)\n",
    "\n",
    "answer_pipeline = build_answer_pipeline(output_parser=local_vars['output_parser'], format_instructions=local_vars['format_instructions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a9078d-1acd-4da8-9cb3-3e51acbf6036",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = answer_pipeline.invoke({\"query\": query, \"context\": cleaned_text})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a58350-7baf-45c7-9beb-bcd4c15f8a2c",
   "metadata": {},
   "source": [
    "## åŠ å…¥Callback é€²è¡Œè¿½è¹¤ChatBot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
