{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c30f7bc9-d9ba-4f35-b95e-7bc03e4f950a",
   "metadata": {},
   "source": [
    "# MLflow Part 2\n",
    "\n",
    "\n",
    "🎯 **本章學完你將能學會什麼：**\n",
    "\n",
    "- 掌握 MLflow CallbackHandler 的使用方式，將 LangChain 的輸入/輸出與模型紀錄自動化  \n",
    "- 理解 Autolog 模式下的自動追蹤原理與應用場景  \n",
    "- 能夠在 MLflow UI 中觀察 LLMChain 的運作過程、輸入輸出與執行時間  \n",
    "\n",
    "📘 **最終你將具備的能力：**  \n",
    "能夠建立可追蹤的 LLM 實驗環境，將 LangChain 與 MLflow 整合，為模型開發、除錯與實驗管理奠定基礎。  \n",
    "\n",
    "\n",
    "mlflow server --host 127.0.0.1 --port 8080\n",
    "\n",
    "## 紀錄內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff0a3f8-6c8c-450d-b161-e50b6ad71400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed358d5-7c46-4e7e-8d98-eb8765f43208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from textwrap import dedent\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_community.callbacks import MlflowCallbackHandler\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from src.initialization import credential_init\n",
    "\n",
    "def build_standard_chat_prompt_template(kwargs):\n",
    "\n",
    "    messages = []\n",
    " \n",
    "    if 'system' in kwargs:\n",
    "        content = kwargs.get('system')\n",
    "        prompt = PromptTemplate(**content)\n",
    "        message = SystemMessagePromptTemplate(prompt=prompt)\n",
    "        messages.append(message)  \n",
    "\n",
    "    if 'human' in kwargs:\n",
    "        content = kwargs.get('human')\n",
    "        prompt = PromptTemplate(**content)\n",
    "        message = HumanMessagePromptTemplate(prompt=prompt)\n",
    "        messages.append(message)\n",
    "        \n",
    "    chat_prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "    \n",
    "    return chat_prompt_template\n",
    "\n",
    "experiment = \"Week-4\"\n",
    "tracking_url = \"http://127.0.0.1:8080\"\n",
    "\n",
    "credential_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aced9425-c3e4-4b37-90fa-ef32ccffb338",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(uri=tracking_url)\n",
    "\n",
    "# Start or get an MLflow run explicitly\n",
    "mlflow.set_experiment(experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b59d40d-378e-4961-98ef-f64636caf775",
   "metadata": {},
   "source": [
    "### MlflowCallbackHandler\n",
    "\n",
    "追蹤並記錄語言模型的輸入和輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84faca3b-fe2e-48ad-bb74-bca05a95ea48",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"my-llm-run\") as run:\n",
    "    run_id = run.info.run_id\n",
    "    print(f\"Using run_id={run_id}\")\n",
    "\n",
    "    # Attach the run_id so all logs go into this run\n",
    "    mlflow_cb = MlflowCallbackHandler(\n",
    "        experiment=experiment,\n",
    "        run_id=run_id,\n",
    "        tracking_uri=tracking_url,\n",
    "    )\n",
    "\n",
    "    model = ChatOpenAI(\n",
    "        model_name=\"gpt-4o-mini\",\n",
    "        temperature=0,\n",
    "        callbacks=[mlflow_cb]\n",
    "    )\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"product\"],\n",
    "        template=\"What is a good name for a company that makes {product}?\",\n",
    "    )\n",
    "\n",
    "    chain = LLMChain(llm=model, prompt=prompt)\n",
    "\n",
    "    # First call logs into this run\n",
    "    chain.invoke({\"product\": \"陽電子攻城炮\"})\n",
    "\n",
    "    # Second call also logs into the SAME run_id\n",
    "    chain.invoke({\"product\": \"旋風魚雷 (Warhammer 40k, Exterminatus)\"})\n",
    "\n",
    "    chain.invoke({\"product\": \"人形MS/Gundam\"})\n",
    "    \n",
    "    # Finally flush once\n",
    "    mlflow_cb.flush_tracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b7fd37-b372-43e9-8e1b-fbaa7a0ff807",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = run_id\n",
    "artifact_path = \"table_session_analysis.html\"   # artifacts 內的相對路徑位置\n",
    "\n",
    "# Download to a local directory\n",
    "local_dir = mlflow.artifacts.download_artifacts(run_id=run_id, artifact_path=artifact_path,\n",
    "                                                dst_path=\"tutorial/LLM+Langchain/Week-4\", \n",
    "                                                tracking_uri=tracking_url,\n",
    "                                                )\n",
    "\n",
    "print(\"Downloaded to:\", local_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5e3016-761c-4ee3-b99e-8d93d2cfe9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181d118a-09ee-4b59-8659-82aedaca0bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 記得要加上encoding='utf-8',否則中文會變成亂碼\n",
    "df = pd.read_html(\"tutorial/LLM+Langchain/Week-4/table_session_analysis.html\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32120d0e-45c7-469c-92dd-e9e7337eb088",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567f357e-fb1e-4dc4-b3d7-ae9e9ea4d804",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[0].iloc[0]['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22f7f1c-b1fd-4fb5-9ed3-c3c495e56c78",
   "metadata": {},
   "source": [
    "## Autolog\n",
    "\n",
    "此模式完全不會寫入任何 JSON 檔案 —— 相反地，它會將你的 LangChain 執行過程（traces/spans）捕捉並記錄到 MLflow 的實驗追蹤與追蹤（tracing）介面中。這表示你可以在 MLflow 的使用者介面中看到輸入/輸出、執行時間以及巢狀結構，而不是以 .json 檔案的形式儲存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09058a8a-aae9-4b86-8b7c-f86dffa5aa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d2a57-e219-46c2-905b-800eb4b43c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autologging — this instruments LangChain automatically\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name in traditional Chinese for a company that makes {product}?\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=model, prompt=prompt)\n",
    "\n",
    "# Run the chain\n",
    "mlflow.set_tracking_uri(uri=tracking_url)\n",
    "\n",
    "# Create a new MLflow Experiment\n",
    "mlflow.set_experiment(experiment)\n",
    "\n",
    "with mlflow.start_run(run_id=run_id) as run:\n",
    "    chain.invoke({\"product\": \"光茅 (戰槌40k)\"})\n",
    "    chain.invoke({\"product\": \"旋風魚雷 (Warhammer 40k, Exterminatus)\"})\n",
    "    chain.invoke({\"product\": \"人形MS/Gundam\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa601924-3194-430c-83c8-57a831fa159d",
   "metadata": {},
   "source": [
    "## Reflection + Revision Pipeline\n",
    "\n",
    "一個模型不夠，你可以用兩個。上面兩個範例都只用了一個模型，還沒外加RAG之類的\n",
    "\n",
    "🎯 **本章學完你將能學會什麼：**\n",
    "\n",
    "- 理解如何利用多個模型建立反饋（Reflection）與修訂（Revision）流程  \n",
    "- 學會設計結構化輸出（PydanticOutputParser）以生成可解析的結果  \n",
    "- 掌握 RunnablePassthrough 與 RunnableLambda 的應用，打造可組合的 LangChain Pipeline  \n",
    "- 理解如何在 MLflow 中記錄多階段模型的追蹤數據  \n",
    "\n",
    "📘 **最終你將具備的能力：**  \n",
    "能夠構建具備反思與修訂能力的複合式 LLM Pipeline，並在 MLflow 內完整追蹤其運行歷程。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e679c304-44bd-423a-ba28-795daeae6fba",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "\n",
    "使用某年的學測/指考的作文作為範例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd48e595-8b29-425c-ac1f-6b5e69cde646",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = dedent(\"\"\"\n",
    "俗話說：「龍生九子，各有不同。」在廣闊浩瀚的海洋之中，就有一頭孤獨的鯨魚——五十二赫茲鯨魚。牠聲音的頻率天生便比同伴還要高，這項特別之處，也導致了牠與同伴產生了無法溝通的鴻溝。看見這則故事的我，不禁思考，在如此多元的人間，是否也有像五十二赫茲鯨魚一般，天生便與眾不同？\n",
    "\n",
    "回首童年，我印象最為深刻的一刻，是初識字時，與文字互相理解的那一瞬、是當我第一次讀完一個句子時，它將自身的意義傳入我腦中的那一瞬。自此，我便對文字、語言抱有特殊的感情，也十分享受閱讀與朗誦。那種將自身與文字經由一點一滴積累而連接起來的感情，使我心靈感到十分富足。\n",
    "\n",
    "而當我步入校園接觸同儕時，驀然驚覺我與別人的閱讀速度十分不同。每當我已讀完一篇文章，但同學可能只完成了一半甚至三分之二。同時，我在生字讀音方面也異常的執著，因此被同學抱怨有「文字潔癖」。面對同儕抱怨的我，也只好強忍對耳邊時而出現字錯讀音的不適，開始刻意忽略心裡對它的執念，只為想要與別人一樣，想要和朋友互相理解。\n",
    "\n",
    "直到多年前，因緣際會之下認識了「五十二赫茲」這獨特的存在。牠的身影在我心中烙下一道深刻的痕跡。因為牠，我開始接受自己與他人的不同；也因為牠，我明白了，我對文字的執著，並不是一種負面的特質，而是上天賜予我的禮物，我開始在寫作上揮灑自如。這讓我知道，不要在一開始便用否定的眼光看待自己的特質。也許這特別之處，會使我們與五十二赫茲鯨魚一般孤獨，會使我們遭受他人的不理解與排斥，但也會讓我們與眾不同。\n",
    "\n",
    "關於此，我想說的是，勇敢地綻放自己的特別，也讓自己成為自己和世人眼中，最閃耀的五十二赫茲鯨魚。\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "def create_feedback_pipeline(mlflow_callback):\n",
    "\n",
    "    ## Teacher LLM\n",
    "    system_template = dedent(\"\"\"\n",
    "    你是一個教學與寫作經驗豐富的台灣大學中文系教授，你要來負責給予作文評分與回饋。\n",
    "    \"\"\")\n",
    "    \n",
    "    human_template = dedent(\"\"\"\n",
    "    Title: {title}\n",
    "    \n",
    "    Article:\n",
    "    {article}\n",
    "    \"\"\")\n",
    "    \n",
    "    model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                       model_name=\"gpt-4o-mini\", temperature=0,\n",
    "                       callbacks=[mlflow_callback])\n",
    "    \n",
    "    input_ = {\"system\": {\"template\": system_template},\n",
    "              \"human\": {\"template\": human_template,\n",
    "                        \"input_variable\": [\"title\", \"article\"],\n",
    "                        }}\n",
    "    \n",
    "    chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "    \n",
    "    feedback_pipeline = chat_prompt_template|model|StrOutputParser()\n",
    "\n",
    "    return feedback_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2ca87c-445d-49b4-b4e5-87fc31beff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "class Output(BaseModel):\n",
    "    name: str = Field(description=\"The revised article in traditional Chinese (繁體中文), please do not include the title.\")\n",
    "\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=Output)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "\n",
    "def create_revision_pipeline(mlflow_callback):\n",
    "    ## Generate\n",
    "    system_template = dedent(\"\"\"\n",
    "    你是一個在準備考試的高中生，你將根據反饋強化的作文內容。\n",
    "    \"\"\")\n",
    "    \n",
    "    human_template = dedent(\"\"\"\n",
    "    Title: {title}\n",
    "    \n",
    "    Old Article:\n",
    "    {article}\n",
    "    \n",
    "    Feedback:\n",
    "    {feedback}\n",
    "\n",
    "    Output format instructions: {format_instructions}\n",
    "    \n",
    "    Revised Article:\n",
    "    \"\"\")\n",
    "    \n",
    "    input_ = {\"system\": {\"template\": system_template},\n",
    "              \"human\": {\"template\": human_template,\n",
    "                        \"input_variable\": [\"title\", \"article\", \"feedback\"],\n",
    "                        \"partial_variables\": {'format_instructions': format_instructions}}}\n",
    "    \n",
    "    chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "    model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                       model_name=\"gpt-4o-mini\", temperature=0,\n",
    "                       callbacks=[mlflow_callback])\n",
    "    \n",
    "    revision_pipeline = chat_prompt_template|model|output_parser\n",
    "\n",
    "    return revision_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1967e3e6-157d-4d4a-b201-37f222f9dd77",
   "metadata": {},
   "source": [
    "在呼叫MLflow後，進行 Reflection -> Revision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c1ea5f-8e11-42de-b9d9-d79155618ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "run_name = \"Reflection_Revision\"\n",
    "\n",
    "mlflow.set_tracking_uri(uri=tracking_url)\n",
    "\n",
    "# Start or get an MLflow run explicitly\n",
    "mlflow.set_experiment(experiment)\n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "    run_id = run.info.run_id\n",
    "    print(f\"Using run_id={run_id}\")\n",
    "\n",
    "    # Attach the run_id so all logs go into this run\n",
    "    mlflow_cb = MlflowCallbackHandler(\n",
    "        experiment=experiment,\n",
    "        run_id=run_id,\n",
    "        tracking_uri=tracking_url,\n",
    "    )\n",
    "\n",
    "    feedback_pipeline = create_feedback_pipeline(mlflow_callback=mlflow_cb)\n",
    "    revision_pipeline = create_revision_pipeline(mlflow_callback=mlflow_cb)\n",
    "    \n",
    "    whole_pipeline = RunnablePassthrough.assign(feedback=feedback_pipeline)|revision_pipeline|RunnableLambda(lambda x: x.name)\n",
    "\n",
    "    result = whole_pipeline.invoke({\"article\": query,\n",
    "                                    \"title\": \"關於五十二赫茲，我想說的是…\"},\n",
    "                                    # config={\"callbacks\": [mlflow_cb]} \n",
    "                                  )\n",
    "    \n",
    "    \n",
    "# Finally flush once\n",
    "mlflow_cb.flush_tracker()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b65ed21-8ce2-4f00-a821-428809a28a04",
   "metadata": {},
   "source": [
    "結合上週的內容，將這個Pipeline打包成一個Artifact上傳到MLflow Server，然後藉由MLflow調用Pipeline\n",
    "\n",
    "## Upload model as a python script\n",
    "\n",
    "🎯 **本章學完你將能學會什麼：**\n",
    "\n",
    "- 理解 MLflow ModelSignature 的用途與 schema 定義方法  \n",
    "- 學會如何設定模型的輸入與輸出格式，確保部署時類型驗證正確  \n",
    "- 掌握將模型與源代碼上傳為 Artifact 的流程  \n",
    "- 瞭解如何註冊與載入 MLflow 模型以進行推論  \n",
    "\n",
    "📘 **最終你將具備的能力：**  \n",
    "能夠定義、封裝並上傳可重用的 LLM 模型，並以 MLflow 作為模型版本與部署的核心平台。\n",
    "\n",
    "\n",
    "### ModelSignature\n",
    "\n",
    "    ModelSignature 在 MLflow 裡的主要作用，正是用來定義與驗證模型的輸入與輸出格式（schema）。\n",
    "    \n",
    "    ModelSignature 是 MLflow 的一個物件，用來描述模型的：\n",
    "    \n",
    "    輸入（inputs）：模型預期接收的資料格式（欄位名稱、資料型別等）\n",
    "    \n",
    "    輸出（outputs）：模型預期回傳的資料格式\n",
    "    \n",
    "    它讓 MLflow 能：\n",
    "    \n",
    "        - 在 模型儲存（log_model / save_model） 時記錄這些資訊。\n",
    "    \n",
    "        - 在 模型部署或推論（predict） 時自動檢查輸入資料是否符合定義。\n",
    "    \n",
    "        - 在 MLflow UI 中清楚顯示模型的「輸入/輸出結構」\n",
    "\n",
    "\n",
    "這個例子中模型需要兩個輸入欄位：\n",
    "\n",
    "- title（型別：string）\n",
    "\n",
    "- article（型別：string）\n",
    "\n",
    "模型會輸出一個欄位：\n",
    "\n",
    "- 無名稱（或預設名稱）但型別是 string。\n",
    "\n",
    "換句話說，這個 signature 明確說明了模型的 輸入結構 與 輸出結構，\n",
    "可幫助 MLflow 在紀錄或部署模型時自動進行型別驗證與追蹤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3b45d5-768b-4c3a-9d16-789b9794b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "import pandas as pd\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "\n",
    "from src.io.path_definition import get_project_dir\n",
    "\n",
    "model_path = os.path.join(get_project_dir(), 'tutorial', 'LLM+Langchain', \n",
    "                          \"Week-4\", \"llmchain_mlflow_experiment_tracing.py\")\n",
    "\n",
    "# You need to know what you will put into it and what you will get out of it.\n",
    "input_schema = Schema([ColSpec(\"string\", \"title\"),\n",
    "                       ColSpec(\"string\", \"article\")])\n",
    "output_schema = Schema([ColSpec(\"string\")])\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "\n",
    "\n",
    "query = dedent(\"\"\"\n",
    "俗話說：「龍生九子，各有不同。」在廣闊浩瀚的海洋之中，就有一頭孤獨的鯨魚——五十二赫茲鯨魚。牠聲音的頻率天生便比同伴還要高，這項特別之處，也導致了牠與同伴產生了無法溝通的鴻溝。看見這則故事的我，不禁思考，在如此多元的人間，是否也有像五十二赫茲鯨魚一般，天生便與眾不同？\n",
    "\n",
    "回首童年，我印象最為深刻的一刻，是初識字時，與文字互相理解的那一瞬、是當我第一次讀完一個句子時，它將自身的意義傳入我腦中的那一瞬。自此，我便對文字、語言抱有特殊的感情，也十分享受閱讀與朗誦。那種將自身與文字經由一點一滴積累而連接起來的感情，使我心靈感到十分富足。\n",
    "\n",
    "而當我步入校園接觸同儕時，驀然驚覺我與別人的閱讀速度十分不同。每當我已讀完一篇文章，但同學可能只完成了一半甚至三分之二。同時，我在生字讀音方面也異常的執著，因此被同學抱怨有「文字潔癖」。面對同儕抱怨的我，也只好強忍對耳邊時而出現字錯讀音的不適，開始刻意忽略心裡對它的執念，只為想要與別人一樣，想要和朋友互相理解。\n",
    "\n",
    "直到多年前，因緣際會之下認識了「五十二赫茲」這獨特的存在。牠的身影在我心中烙下一道深刻的痕跡。因為牠，我開始接受自己與他人的不同；也因為牠，我明白了，我對文字的執著，並不是一種負面的特質，而是上天賜予我的禮物，我開始在寫作上揮灑自如。這讓我知道，不要在一開始便用否定的眼光看待自己的特質。也許這特別之處，會使我們與五十二赫茲鯨魚一般孤獨，會使我們遭受他人的不理解與排斥，但也會讓我們與眾不同。\n",
    "\n",
    "關於此，我想說的是，勇敢地綻放自己的特別，也讓自己成為自己和世人眼中，最閃耀的五十二赫茲鯨魚。\n",
    "\"\"\")\n",
    "\n",
    "run_name = \"Reflection_Revision_Py\"\n",
    "\n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "\n",
    "    os.environ['experiment'] = experiment\n",
    "    os.environ['run_id'] = run.info.run_id\n",
    "    os.environ['run_name'] = run_name\n",
    "    \n",
    "    mlflow.log_artifact(model_path, artifact_path=\"source_code\")\n",
    "\n",
    "    input_example = pd.DataFrame(data=[[query, \"關於五十二赫茲，我想說的是…\"]], columns=['article', 'title'])\n",
    "    \n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        python_model=model_path,  # Define the model as the path to the Python file\n",
    "        name=\"langchain_model\",\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "        registered_model_name=\"Generation_Reflection_Demo\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80af810b-845e-465c-88b0-f9ce51f8f45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = dedent(\"\"\"\\\n",
    "鳴大鐘一次！推動杠杆，啟動活塞和泵\n",
    "鳴大鐘兩次！按下按鈕，發動引擎，點燃渦輪，注入生命\n",
    "鳴大鐘三次！齊聲歌唱，讚美萬機之神！\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "\n",
    "    os.environ['experiment'] = experiment\n",
    "    os.environ['run_id'] = run.info.run_id\n",
    "    os.environ['run_name'] = run_name\n",
    "\n",
    "    loaded_model = mlflow.pyfunc.load_model(\"models:/Generation_Reflection_Demo/1\")\n",
    "    \n",
    "    input_ = pd.DataFrame(data=[[query, '關於五十二赫茲，我想說的是…']], columns=['article', 'title'])\n",
    "    \n",
    "    output = loaded_model.predict(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b488e8-37e0-414d-bcc8-61f6e509af00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec06a59-58b7-4955-95f3-3f019f2e36d5",
   "metadata": {},
   "source": [
    "# LangServe API\n",
    "\n",
    "🎯 **本章學完你將能學會什麼：**\n",
    "\n",
    "- 理解 LangServe 的架構與 API 調用流程  \n",
    "- 學會建立簡單的後端伺服器，並從客戶端發送請求取得模型回應  \n",
    "- 掌握 RemoteRunnable 的應用，讓模型能夠遠端呼叫  \n",
    "- 瞭解如何結合 MLflow 模型與 LangServe API 進行整合部署  \n",
    "\n",
    "📘 **最終你將具備的能力：**  \n",
    "能夠設計一個具備後端 API 介面的 LLM 系統，支援遠端推論與模組化部署。  \n",
    "\n",
    "## 1. 客戶端 (client) 呼叫後端 API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6eced7-3568-45c6-a80c-feefec6efb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:5000/openai/invoke\",\n",
    "    json={'input': \"Where is Taiwan?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3d191c-3ba3-46d0-a265-2a00dce2e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75d27ef-15fa-4c09-bf68-78f36aebf7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()['output']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82378be9-1287-4410-9bf0-93f1b4e9b49d",
   "metadata": {},
   "source": [
    "在Windows的CLI(command line interface)中:\n",
    "\n",
    "curl -X POST \"http://localhost:5000/openai/invoke\" -H \"Content-Type: application/json\" -d \"{\"\"input\"\": \"\"Where is Taiwan?\"\"}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8711bb4e-5f73-4a8c-8f0a-dc7eb0d0a5a1",
   "metadata": {},
   "source": [
    "## 2. 結合之前MLflow的應用。從MLflow server上下載模型，然後從客戶端呼叫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26bcb69-936a-4175-9b1c-484f4c0b6e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:5000/demo/invoke\",\n",
    "    json={'input': {\"article\": query,\n",
    "                    \"title\": \"關於五十二赫茲，我想說的是…\"}}\n",
    ")\n",
    "\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2a54e9-4f26-46bd-acf7-0735ec3d531d",
   "metadata": {},
   "source": [
    "## 3 RemoteRunnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4050cc8e-2aea-4af5-ab3d-9fb5c84405b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langserve import RemoteRunnable\n",
    "\n",
    "remote_llm = RemoteRunnable(\"http://localhost:5000/openai/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fc385a-fdc5-43e8-8d93-919243977e69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Supports astream\n",
    "async for msg in remote_llm.astream(\"Where is Taiwan?\"):\n",
    "    print(msg.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7c653c-0454-4261-9bde-a3b04217f676",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_llm.invoke(\"花蓮縣光復鄉因為馬太鞍溪堰塞湖潰堤，導致被泥石流淹過。就安全的考量，沒接受過專業訓練的平民是否應該去花蓮縣光復鄉參與救災。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830eab43-1635-4cb1-aa13-06e5665d2d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f702f519-37dd-4b45-a492-0c4bcdd521b0",
   "metadata": {},
   "source": [
    "# ChatBot 本體與記憶機制\n",
    "\n",
    "🎯 **本章學完你將能學會什麼：**\n",
    "\n",
    "- 了解 Stateless 與 Stateful 對話系統的差異  \n",
    "- 學會使用 ChatMessageHistory 管理對話記錄  \n",
    "- 掌握 ChatPromptTemplate 與 MessagesPlaceholder 的應用  \n",
    "- 能實作簡單的多輪對話機制 \n",
    "\n",
    "📘 **最終你將具備的能力：**  \n",
    "能夠設計一個具備「對話記憶」的智能 ChatBot，能夠在上下文中延續並理解對話語境。  \n",
    "\n",
    "\n",
    "1. N-Shot Learning 與對話歷史\n",
    "\n",
    "    - 歷史對話可以看成一個 Q&A pair 列表\n",
    "    - 當前模型在推理時，會把「之前的對話內容」作為 prompt 的一部分，再加上使用者最新的輸入，整合後丟進模型。這其實就是一種 few-shot / N-shot 的學習方式：模型從範例中抽取語境來理解「現在該怎麼回答」。\n",
    "\n",
    "2. Stateless vs. Stateful\n",
    "    - Stateless：如果每次請求都完全獨立，沒有任何歷史對話被帶入，那就叫無狀態 (stateless)。\n",
    "    - Stateful：如果系統會保存對話歷史（不論是把歷史傳回模型，還是外部記憶系統存起來），那就是有狀態的 (stateful)。\n",
    "    - 所以是否「能記住」過去，取決於設計，而不是模型本身自帶的能力。\n",
    "\n",
    "3. Tools 的角色\n",
    "    - 讓 ChatBot 強大的是 tools\n",
    "    - 模型本身雖然能生成語言，但 結合外部工具（例如資料庫查詢、計算器、網路搜尋、代碼執行、圖片生成）後，ChatBot 才能真正做到「會推理 + 會行動」，不再只受限於參數內的知識。\n",
    "\n",
    "    - 可以理解成：模型是「大腦」，Tools 是「手腳」。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22673222-780c-4356-a6d1-92c69b28d6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(url=\"https://python.langchain.com/v0.1/assets/images/chat_use_case-eb8a4883931d726e9f23628a0d22e315.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e0c8a7-fc37-4778-9dcc-91fd68d2db66",
   "metadata": {},
   "source": [
    "先學怎麼調動工具: 模型就像是一個訓練有素的阿斯塔特，工具就像是動力甲，噴射背包，爆彈槍，和鏈鋸劍。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b382daa-6e0c-4426-a04c-054de90c4c01",
   "metadata": {},
   "source": [
    "## 工具綁定與工具呼叫 (Tools & ToolMessage)\n",
    "\n",
    "🎯 **本章學完你將能學會什麼：**\n",
    "\n",
    "- 理解 Tool 與 LLM 的互動方式  \n",
    "- 學會使用 LangChain 的 @tool 與 StructuredTool 綁定外部函式  \n",
    "- 掌握 ToolMessage 的設計與呼叫邏輯  \n",
    "- 能整合多個工具（如計算、查詢、WebSearch）讓模型具備「行動能力」  \n",
    "\n",
    "📘 **最終你將具備的能力：**  \n",
    "能夠打造能「思考 + 行動」的 ChatBot，結合多種工具完成自動化任務。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5c8762-d00d-4796-baf2-426e7374c096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../../\")\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "from src.initialization import credential_init\n",
    "\n",
    "credential_init()\n",
    "\n",
    "# Define a calculator tool\n",
    "@tool\n",
    "def add_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"Adds two numbers together.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "# # Create the LLM\n",
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=6,\n",
    "    disable_streaming=False\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "# Bind the tool to the model\n",
    "llm_with_tools = llm.bind_tools([add_numbers])\n",
    "\n",
    "# Run\n",
    "resp = llm_with_tools.invoke(\"What is 42 + 58?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0983637-cd11-40d2-b763-708542e0d68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158acc75-89e6-4be7-9f6c-96bf6424715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_call = resp.tool_calls[0]\n",
    "print(tool_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1ab833-fe5e-4694-8b51-c47a6b6463ad",
   "metadata": {},
   "source": [
    "根據tool_call計算結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a0d967-880b-4d12-8803-b622d31cee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = eval(tool_call['name'])(tool_call['args'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8185469-c58c-45c0-b144-2fca0a1b7fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecee1c5-c35e-41dc-b72c-7677c8cfc5fe",
   "metadata": {},
   "source": [
    "建立ToolMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d55e641-6d2f-403e-ae95-e15a2e4acc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "tool_msg = ToolMessage(\n",
    "    content=str(result),          # usually a string or simple text\n",
    "    tool_call_id=tool_call['id']    # must match the AIMessage tool_call id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e8b8ec-d462-4376-a882-ce39e90b681d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3407c3f1-03a3-4f76-a1ac-ed7c45f59d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbfb73f-d34d-41a8-bcef-786fb5bbc54a",
   "metadata": {},
   "source": [
    "最後，綁定AIMessage和ToolMessage，在進行一次invoke得到結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131c78f3-e5e9-405d-87db-936384fb4426",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools.invoke([resp, tool_msg])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8003fc29-36fc-414e-acba-45535dd60f36",
   "metadata": {},
   "source": [
    "## OpenAI WebSearch API 與網路搜尋應用\n",
    "\n",
    "🎯 **本章學完你將能學會什麼：**\n",
    "\n",
    "- 學會調用 OpenAI WebSearch 工具取得即時資料  \n",
    "- 理解 WebSearch API 的 action、annotations、sources 參數  \n",
    "- 掌握如何在回應中引用來源並分析結果  \n",
    "- 了解 WebSearch 的優缺點與應用策略  \n",
    "\n",
    "📘 **最終你將具備的能力：**  \n",
    "能夠設計可即時檢索、分析並生成報告的 AI 助手，並根據情境選擇最佳資訊來源。  \n",
    "\n",
    "\n",
    "### 基本使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23060df1-9851-4229-a650-fce282385040",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089aebe8-6418-4abf-a89c-e900940d8ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "         model=\"gpt-4o-mini\",\n",
    "         tools=[{\"type\": \"web_search\"}],\n",
    "         input=\"幫我查詢HunterXHunter 最新的進度\"\n",
    "    )\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f00f3a5-4619-4c3d-a844-cb5f1f5897b7",
   "metadata": {},
   "source": [
    "1. A web_search_call output item with the ID of the search call, along with the action taken in web_search_call.action. The action is one of:\n",
    "    - ***search***, which represents a web search. It will usually (but not always) includes the search query and domains which were searched. Search actions incur a tool call cost (see pricing).\n",
    "    - ***open_page***, which represents a page being opened. Supported in reasoning models.\n",
    "    - ***find_in_page***, which represents searching within a page. Supported in reasoning models.\n",
    "2. A message output item containing:\n",
    "    - The text result in message.content[0].text\n",
    "    - Annotations message.content[0].annotations for the cited URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecab106-6a33-4ff4-abde-1ea6f6a21c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f9d764-cf3b-4449-8977-457a73f56904",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.output[1].annotationscontent[0]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db368f73-866a-4031-906e-b13c3ce0cfdb",
   "metadata": {},
   "source": [
    "websearch 結果:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67476408-b45f-47c4-97cc-c45c2b8d5cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ef0e7e-6b52-4c92-b6e9-519fe54d081c",
   "metadata": {},
   "source": [
    "#### Source 參數\n",
    "若要查看在網路搜尋過程中擷取的所有網址，可使用 sources 欄位。\n",
    "與只顯示最相關參考資料的「行內引用（inline citations）」不同，sources 會回傳模型在生成回應時所參考的完整網址清單。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02360c51-807b-4372-8e1e-7842b3438a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "         model=\"gpt-4o-mini\",\n",
    "         tools=[{\"type\": \"web_search\"}],\n",
    "         include=[\"web_search_call.action.sources\"],\n",
    "         input=\"幫我查詢HunterXHunter 最新的進度\"\n",
    "    )\n",
    "\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fe427b-c397-46d3-bf8f-fb3047d0eeca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response.output[0].action.sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d180b0-80e9-48a8-8d94-c377f135df99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e638b0e-7756-445d-8163-a37fb19ec43a",
   "metadata": {},
   "source": [
    "#### User location\n",
    "To refine search results based on geography, you can specify an approximate user location using country, city, region, and/or timezone.\n",
    "\n",
    "- The city and region fields are free text strings, like Minneapolis and Minnesota respectively.\n",
    "- The country field is a two-letter ISO country code, like US. (ISO 3166-1 alpha-2)\n",
    "- The timezone field is an IANA timezone like America/Chicago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6bb1a5-df6e-47a8-b005-713b38711fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "         model=\"gpt-4o-mini\",\n",
    "         tools=[{\"type\": \"web_search\",\n",
    "                 \"user_location\": {\n",
    "                    \"type\": \"approximate\",\n",
    "                    \"country\": \"US\",\n",
    "                     },\n",
    "                 \"search_context_size\": \"medium\"\n",
    "                }],\n",
    "         include=[\"web_search_call.action.sources\"],\n",
    "         input=\"幫我查詢HunterXHunter 最新的進度\"\n",
    "    )\n",
    "                \n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a34144-3180-46d6-a33f-13f5420553b4",
   "metadata": {},
   "source": [
    "其他的argument:\n",
    "\n",
    "- Domain filtering (gpt-5 and o-series models only)\n",
    "- reasoning (gpt-5 and o-series models only)\n",
    "    - effort:\n",
    "        - ***minimal***\n",
    "        - ***low***\n",
    "        - ***medium***\n",
    "        - ***high***\n",
    "    -  summary:\n",
    "        - ***auto***\n",
    "        - ***concise***\n",
    "        - ***detailed***  \n",
    "- tool_choice\n",
    "    - ***none*** means the model will not call any tool and instead generates a message.\n",
    "    - ***auto*** means the model can pick between generating a message or calling one or more tools.\n",
    "    - ***required*** means the model must call one or more tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3edf387-37c8-462d-a420-5db82572d39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "  model=\"gpt-5\",\n",
    "  reasoning={\"effort\": \"low\"},\n",
    "  tools=[\n",
    "      {\n",
    "          \"type\": \"web_search\",\n",
    "          \"filters\": {\n",
    "              \"allowed_domains\": [\n",
    "                  \"pubmed.ncbi.nlm.nih.gov\",\n",
    "                  \"clinicaltrials.gov\",\n",
    "                  \"www.who.int\",\n",
    "                  \"www.cdc.gov\",\n",
    "                  \"www.fda.gov\",\n",
    "              ]\n",
    "          },\n",
    "      }\n",
    "  ],\n",
    "  tool_choice=\"auto\",\n",
    "  include=[\"web_search_call.action.sources\"],\n",
    "  input=\"Please perform a web search on how semaglutide is used in the treatment of diabetes.\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc03788-f1b4-4a2b-8cb4-6e7165f50c5d",
   "metadata": {},
   "source": [
    "### 建立websearch工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0893d4df-9aea-4051-8f4e-e3df0858ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def websearh_tool(query: str) -> str:\n",
    "    \"\"\"Use this tool to find the latest information or information you are not sure\"\"\"\n",
    "\n",
    "    response = client.responses.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    tools=[\n",
    "                        {\"type\": \"web_search\",}\n",
    "                    ],\n",
    "                    tool_choice=\"auto\",\n",
    "                    input=query)\n",
    "    \n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ce1dcc-df81-4a08-b5c8-bb7197618730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bind the tool to the model\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=6,\n",
    "    disable_streaming=False\n",
    "    # other params...\n",
    ")\n",
    "llm_with_tools = llm.bind_tools([websearh_tool])\n",
    "\n",
    "# Run\n",
    "resp = llm_with_tools.invoke(\"台灣2024總統大選結果\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99771f07-3b83-45f6-94ce-d9d1a244f195",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91347678-4c9c-4190-a486-0c44cbc122d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_call = resp.tool_calls[0]\n",
    "result = eval(tool_call['name'])(tool_call['args'])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b096b9-1fa3-474c-b41d-b7a7a61b38e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_function(tool_call):\n",
    "    \n",
    "    return eval(tool_call['name'])(tool_call['args'])\n",
    "\n",
    "\n",
    "def follow_up_answer(aimessage):\n",
    "\n",
    "    tool_call = aimessage.tool_calls[0]\n",
    "    \n",
    "    result = call_function(tool_call)\n",
    "\n",
    "    tool_msg = ToolMessage(\n",
    "        content=str(result),          # usually a string or simple text\n",
    "        tool_call_id=tool_call['id']    # must match the AIMessage tool_call id\n",
    "    )\n",
    "    \n",
    "    follow_up = llm_with_tools.invoke([aimessage, tool_msg])\n",
    "\n",
    "    return follow_up\n",
    "\n",
    "follow_up_answer(aimessage=resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f7fe22-cd1d-470a-927b-31725616d95c",
   "metadata": {},
   "source": [
    "## ChatBot 本體\n",
    "\n",
    "### LLM 沒有記憶性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5f885c-cb85-477f-90e8-147e701e6857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=6,\n",
    "    disable_streaming=False\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "message = HumanMessage(\n",
    "            content=\"Translate this sentence from English to Chinese (繁體中文): I love programming.\"\n",
    "        )\n",
    "\n",
    "model.invoke(\n",
    "    [message]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95345038-9a83-465c-a85a-9b2d5f1b6476",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke([HumanMessage(content=\"What did you just say?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71ee93d-2841-4cc0-b417-3fa5554ba1b3",
   "metadata": {},
   "source": [
    "### 外部記憶\n",
    "\n",
    "如何將外部記憶加入?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed4e110-9be7-4a67-be86-31b62451baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=\"Translate this sentence from English to  Chinese (繁體中文): I love programming.\"\n",
    "        ),\n",
    "        AIMessage(content=\"我愛程式設計.\"),\n",
    "        HumanMessage(content=\"What did you just say?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce0fbac-247d-4230-849c-b5475b1a4c00",
   "metadata": {},
   "source": [
    "透過 MessagePlaceholder接收外部記憶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dea7e35-f9b4-4d78-8a8e-e85225c5fd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate\n",
    "\n",
    "system_prompt = PromptTemplate(template=(\"You are a helpful assistant. Answer all questions to the best of your \"\n",
    "                                         \"ability.\"))\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_message,\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e88d592-3e03-4a5b-81b2-e792154f94ec",
   "metadata": {},
   "source": [
    "### 建立邏輯鍊條"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd2ed39-46d6-460e-b78e-5b0c4d06fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_ = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e2a159-a35a-47db-ac82-43c8d9f85887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "pipeline_.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "            content=\"Translate this sentence from English to Chinese (繁體中文): I love programming.\"\n",
    "            ),\n",
    "            AIMessage(content=\"我愛程式設計.\"),\n",
    "            HumanMessage(content=\"What did you just say?\"),\n",
    "            ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e0029f-228a-4ff5-afb0-91bf92f00d34",
   "metadata": {},
   "source": [
    "## 將對話記錄存入ChatMessageHistory裡\n",
    "\n",
    "### 導入並創建 ChatMessageHistory。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f65a6f-3d57-4cf9-8e8f-34623e7965e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "demo_chat_history = ChatMessageHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e08da2-cb06-41db-ab6a-69532596b405",
   "metadata": {},
   "source": [
    "### 添加用戶和 AI 消息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eaf68e-486a-44cb-8107-c81b2b11e578",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_chat_history.add_user_message(\"Translate this sentence from English to Chinese (繁體中文): I love programming.\")\n",
    "\n",
    "demo_chat_history.add_ai_message(\"我愛程式設計.\")\n",
    "\n",
    "demo_chat_history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cb4014-b66b-40dc-8acc-3e052426ce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_chat_history.add_user_message(\n",
    "    \"What did you just say?\"\n",
    ")\n",
    "\n",
    "response = pipeline_.invoke({\"messages\": demo_chat_history.messages})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dd8248-d495-4798-8d35-7c36862720f4",
   "metadata": {},
   "source": [
    "### 最小範例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc02e090-7b0a-45a4-b377-2e37048340df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chat_history = ChatMessageHistory()\n",
    "pipeline_ = prompt|model|StrOutputParser()\n",
    "\n",
    "while True:\n",
    "    question = input(\"What do you want to ask: \")\n",
    "    if question == \"QUIT\":\n",
    "        break\n",
    "    chat_history.add_user_message(question)\n",
    "    response = pipeline_.invoke({\"messages\": chat_history.messages})\n",
    "\n",
    "    print(response)\n",
    "    chat_history.add_ai_message(response)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec25caf-71ec-4064-923c-ff0c5afd4e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.chdir(\"../../../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789cefac-0701-40ee-bbf9-d92c5259117a",
   "metadata": {},
   "source": [
    "## ChatBot + 檢索系統整合\n",
    "\n",
    "🎯 **本章學完你將能學會什麼：**\n",
    "\n",
    "- 理解如何結合 FAISS 向量資料庫與 ChatBot  \n",
    "- 學會建立可檢索文本的 RAG（Retrieval-Augmented Generation）工作流  \n",
    "- 掌握 StructuredTool 與 Retriever 工具的實際應用  \n",
    "- 能讓 ChatBot 根據知識庫回答特定問題（如唐詩檢索）  \n",
    "\n",
    "📘 **最終你將具備的能力：**  \n",
    "能夠構建一個具備「外部記憶與知識檢索」能力的智慧 ChatBot，結合語意搜尋與生成。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb7abfb-9d66-4ca8-96c7-2358b21d009e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_core.tools import tool\n",
    "from langchain.tools import StructuredTool\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "from src.io.path_definition import get_project_dir\n",
    "from src.initialization import credential_init\n",
    "\n",
    "\n",
    "credential_init()\n",
    "\n",
    "# 引入唐詩向量數據庫\n",
    "filename = os.path.join(get_project_dir(), \"tutorial\", \"LLM+Langchain\", \"Week-2\", \"poem_faiss_index\")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "vectorstore = FAISS.load_local(\n",
    "    filename, embeddings, allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(seearch_type='similarity').configurable_fields(\\\n",
    "        search_kwargs=ConfigurableField(id=\"search_kwargs\")\n",
    "    )\n",
    "\n",
    "\n",
    "class PoemRetrieverArgs(BaseModel):\n",
    "    query: str = Field(description=\"The keyword or phrase to search for Tang poems. 用來搜尋唐詩的關鍵字或是句子\")\n",
    "    k: int = Field(1, description=\"The number of poems to retrieve.\")\n",
    "\n",
    "\n",
    "def _poem_retriever(query: str, k: int):\n",
    "    output = retriever.invoke(query, config={\"configurable\": {\"search_kwargs\": {\"k\": k}}})\n",
    "    return output\n",
    "\n",
    "\n",
    "# 使用 StructuredTool建立工具\n",
    "# 並且通過args_schema來告知輸入的格式\n",
    "\n",
    "poem_retriever = StructuredTool.from_function(\n",
    "    func=_poem_retriever,\n",
    "    args_schema=PoemRetrieverArgs,\n",
    "    description=\"使用這個工具來搜尋唐詩; Use this tool to search for Tang poems.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cdf253-b43a-45e2-8f92-2667ac5480ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "system_prompt = PromptTemplate(template=dedent(\"\"\"You are a helpful assistant. \n",
    "Answer all questions to the best of your ability.\n",
    "\"\"\"))\n",
    "\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_message,\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "#                    model_name=\"gpt-4o-2024-05-13\", temperature=0)\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=6,\n",
    "    disable_streaming=False\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "model_with_tools = model.bind_tools([poem_retriever])\n",
    "\n",
    "chatbot_pipeline = prompt | model_with_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6d758f-2e4a-40b0-9775-ff872fae19db",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "question = \"幫我找3首關於對於人生感嘆的唐詩\"\n",
    "\n",
    "chat_history.add_user_message(question)\n",
    "\n",
    "output = chatbot_pipeline.invoke({\"messages\": chat_history.messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d0c399-c1ce-45ed-a54c-42bc6ff3e3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d032970-2c15-408f-9931-f54b1301a62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "def call_function(tool_call):\n",
    "    \n",
    "    return eval(tool_call['name'])(**tool_call['args'])\n",
    "\n",
    "\n",
    "def follow_up_answer(aimessage):\n",
    "\n",
    "    tool_call = aimessage.tool_calls[0]\n",
    "    \n",
    "    result = call_function(tool_call)\n",
    "\n",
    "    tool_msg = ToolMessage(\n",
    "        content=str(result),          # usually a string or simple text\n",
    "        tool_call_id=tool_call['id']    # must match the AIMessage tool_call id\n",
    "    )\n",
    "    \n",
    "    follow_up = model_with_tools.invoke([aimessage, tool_msg])\n",
    "\n",
    "    return follow_up\n",
    "\n",
    "output = follow_up_answer(aimessage=output)\n",
    "\n",
    "print(output)\n",
    "# follow_up_answer(human_message=question, ai_message=output.content, additional_kwargs=output.additional_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e077d123-6b59-46c8-8ab3-36c0ea9f7408",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "while True:\n",
    "    question = input(\"What do you want to ask: \")\n",
    "    if question == \"QUIT\":\n",
    "        break\n",
    "    chat_history.add_user_message(question)\n",
    "    output = chatbot_pipeline.invoke({\"messages\": chat_history.messages})\n",
    "\n",
    "    if output.tool_calls != []:\n",
    "        response = follow_up_answer(output).content\n",
    "    else:\n",
    "        response = output.content\n",
    "\n",
    "    print(\"***********************\")\n",
    "    print(response)\n",
    "    print(\"***********************\")\n",
    "    \n",
    "    chat_history.add_ai_message(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3142dd21-ccf2-4bd4-8714-d3995bb5d713",
   "metadata": {},
   "source": [
    "### 使用代碼解決數學問題工具\n",
    "\n",
    "- OpenAI API: https://platform.openai.com/docs/guides/tools-code-interpreter\n",
    "- 自己玩玩看。但我們也可以自己手搓寫代碼的工具\n",
    "\n",
    "1. 代碼產生的邏輯鍊條"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52de6f44-3c4a-4c8f-827e-561d0c141049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../../\")\n",
    "\n",
    "from src.initialization import credential_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186123ce-d45c-4bf7-bd03-315cbf3eac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from textwrap import dedent\n",
    "\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain.tools import StructuredTool\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from src.initialization import credential_init\n",
    "\n",
    "credential_init()\n",
    "\n",
    "\n",
    "def build_standard_chat_prompt_template(kwargs):\n",
    "\n",
    "    messages = []\n",
    " \n",
    "    if 'system' in kwargs:\n",
    "        content = kwargs.get('system')\n",
    "        prompt = PromptTemplate(**content)\n",
    "        message = SystemMessagePromptTemplate(prompt=prompt)\n",
    "        messages.append(message)  \n",
    "\n",
    "    if 'human' in kwargs:\n",
    "        content = kwargs.get('human')\n",
    "        prompt = PromptTemplate(**content)\n",
    "        message = HumanMessagePromptTemplate(prompt=prompt)\n",
    "        messages.append(message)\n",
    "        \n",
    "    chat_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "    \n",
    "    return chat_prompt\n",
    "\n",
    "\n",
    "@chain\n",
    "def code_execution(code):\n",
    "    \n",
    "    match = re.findall(r\"python\\n(.*?)\\n```\", code, re.DOTALL)\n",
    "    python_code = match[0]\n",
    "    \n",
    "    lines = python_code.strip()#.split('\\n')\n",
    "    # *stmts, last_line = lines\n",
    "\n",
    "    local_vars = {}\n",
    "    exec(lines, local_vars)\n",
    "\n",
    "    return local_vars\n",
    "\n",
    "\n",
    "def call_function(tool_call):\n",
    "    \n",
    "    return eval(tool_call['name'])(**tool_call['args'])\n",
    "\n",
    "\n",
    "def follow_up_answer(aimessage):\n",
    "\n",
    "    tool_call = aimessage.tool_calls[0]\n",
    "    \n",
    "    result = call_function(tool_call)\n",
    "\n",
    "    tool_msg = ToolMessage(\n",
    "        content=str(result),          # usually a string or simple text\n",
    "        tool_call_id=tool_call['id']    # must match the AIMessage tool_call id\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        follow_up = model_with_tools.invoke([aimessage, tool_msg])\n",
    "    except:\n",
    "        raise ValueError(f\"aimessage={aimessage}\\ntool_msg={tool_msg}\")\n",
    "    \n",
    "    return follow_up\n",
    "\n",
    "\n",
    "system_template = (\n",
    "    \"You are a highly skilled Python developer. Your task is to generate Python code strictly based on the user's instructions.\\n\"\n",
    "    \"Leverage statistical and mathematical libraries such as `statsmodels`, `scipy`, and `numpy` where appropriate to solve the problem.\\n\"\n",
    "    \"Your response must contain only the Python code — no explanations, comments, or additional text.\\n\\n\"\n",
    ")\n",
    "\n",
    "human_template = dedent(\"\"\"{query}\\n\\n\n",
    "                            Always copy the final answer to a variable `answer`\n",
    "                            Code:\n",
    "                        \"\"\")\n",
    "\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"query\"]}}\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# model = ChatGoogleGenerativeAI(\n",
    "#     model=\"gemini-2.0-flash\",\n",
    "#     temperature=0,\n",
    "#     max_tokens=None,\n",
    "#     timeout=None,\n",
    "#     max_retries=6,\n",
    "#     disable_streaming=False\n",
    "#     # other params...\n",
    "# )\n",
    "\n",
    "\n",
    "chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "code_generation = chat_prompt_template|model|StrOutputParser()\n",
    "\n",
    "code_pipeline = code_generation|code_execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd20ef9-1d19-424e-95a7-aec635a6273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeArgs(BaseModel):\n",
    "    query: str = Field(description=\"User request; 用戶需求\")\n",
    "\n",
    "\n",
    "def _calculator(query: str,):\n",
    "    output = code_pipeline.invoke(query)\n",
    "    return output\n",
    "\n",
    "\n",
    "mathematic_tool = StructuredTool.from_function(\n",
    "    func=_calculator,\n",
    "    args_schema=CodeArgs,\n",
    "    description=\"Use this tool to solve mathematic related problem; 使用這個工具解決數學問題\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1c23eb-d494-49a2-b913-d23cdd4d8336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "model_with_tools = model.bind_tools([mathematic_tool])\n",
    "\n",
    "system_prompt = PromptTemplate(template=dedent(\"\"\"You are a helpful assistant. \n",
    "Answer all questions to the best of your ability.\n",
    "\"\"\"))\n",
    "\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_message,\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chatbot_pipeline = prompt | model_with_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd7e55f-75ab-4323-bcdd-a13246308eab",
   "metadata": {},
   "source": [
    "### 挑戰: 北一女段考考題\n",
    "\n",
    "https://drive.google.com/file/d/1csHdgvc5WtbJZ4n39eozogVVPIkPWABf/view\n",
    "\n",
    "- Q1: 以下敘述是否正確: 滿足方程式 x^2 + y^2 + 2x −10y + 30 = 0 之點(x, y)的圖形是一個圓\n",
    "- Q2: 以下敘述是否正確: 過三點 A( 1, − 3), B( 2, 6 ), C( 4, 24 )的圓恰有一個\n",
    "- Q3: 以下敘述是否正確: 直線 3x −4y + 7 = 0 與圓 (x − 2)^2 + (y + 3)^2 = 5 恰有一交點\n",
    "- Q4: 以下敘述是否正確: 圓(x − 2)^2 + (y + 3)^2 = 5 上恰有二點與直線 3x −4y −13= 0 的距離等於 2\n",
    "- Q5: 以下敘述是否正確: P(a, b) 為 圓 (x − 2)^2 + ( y + 3)^2 = 4 上的點,則使 (a^2 + b^2)^0.5 為整數的點共有 8 個"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacfcc2c-95be-4e57-89cd-ce5303e89230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "chat_history = ChatMessageHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a28d83-46e6-4efc-b743-d0bf3a4eeffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    question = input(\"Your question: \")\n",
    "    if question == 'quit':\n",
    "        break\n",
    "    chat_history.add_user_message(question)\n",
    "    output = chatbot_pipeline.invoke({\"messages\": chat_history.messages})\n",
    "    \n",
    "    try:\n",
    "        if output.tool_calls != []:\n",
    "            print(\"Call tool\")\n",
    "            final_answer = follow_up_answer(aimessage=output)\n",
    "        else:\n",
    "            print(\"No Tool\")\n",
    "            final_answer = output\n",
    "        print(f\"AI: {final_answer.content}\")\n",
    "        chat_history.add_ai_message(final_answer.content)\n",
    "    except KeyError:\n",
    "        print(f\"AI: {output.content}\")\n",
    "        chat_history.add_ai_message(output.content)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02f81e8-0696-42b4-93a8-6c04db9f52f1",
   "metadata": {},
   "source": [
    "### 有辦法加入一些基本的機械學習來進行分析嗎?\n",
    "\n",
    "我還不知道，應該是會蠻有趣的\n",
    "\n",
    "到這裡你應該可以認識到，寫ChatBot本體並不困難，但一個ChatBot好不好用是由他所綑綁的工具決定。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fe397f-7d0c-43d7-bef8-e14510ffb8e0",
   "metadata": {},
   "source": [
    "# Websearch 策略與資料來源設計\n",
    "\n",
    "🎯 **本章學完你將能學會什麼：**\n",
    "\n",
    "- 理解 Websearch 的優勢、限制與常見風險  \n",
    "- 學會依據應用場景選擇正確的資訊來源（Wikipedia、Fandom、API）  \n",
    "- 掌握資料品質控制與來源過濾策略  \n",
    "- 掌握利用 LLM 動態生成 Pydantic Schema 的技巧  \n",
    "- 能建立一個可從 Wikipedia / Fandom 抽取結構化資料的自動化 Pipeline  \n",
    "\n",
    "📘 **最終你將具備的能力：**  \n",
    "- 具備以 LLM 為核心的網路資訊擷取與分析能力，能建立高品質、多來源的 AI 研究與內容生成系統。\n",
    "- 能夠設計結合爬蟲、LLM 代碼生成與結構化資料擷取的智慧資料分析系統。   \n",
    "\n",
    "## Websearch 優缺點與應用策略\n",
    "\n",
    "### 優點\n",
    "- **多元化來源**：涵蓋範圍廣，能提供多角度資訊。\n",
    "- **即時性**：能快速取得公開網頁上的最新內容。\n",
    "- **靈活性**：適合需要「多來源比對」的問題。\n",
    "\n",
    "### 缺點\n",
    "- **碎片化**：資訊分散、格式不一，難以直接系統化使用。\n",
    "- **品質參差不齊**：來源可靠度不同，可能存在錯誤或過時資訊。\n",
    "- **限制與風險**：部分 API 或搜尋過程可能因政策、安全或授權而阻擋特定內容。\n",
    "\n",
    "---\n",
    "\n",
    "### 何時適合使用 Websearch\n",
    "- 需要 **多角度觀點**（如新聞、論壇、社群資訊）。\n",
    "- **開放探索**，對來源精確度要求不高。\n",
    "- 無法透過單一可靠資料庫滿足需求時。\n",
    "\n",
    "---\n",
    "\n",
    "### 何時更適合使用特定來源\n",
    "- **專門領域**：如 Wikipedia、Fandom Wiki（例如遊戲、小說、Warhammer 40k）。\n",
    "- **結構化資料需求**：來源有規則的網址與內容組織，便於程式化檢索。\n",
    "- **高可信度需求**：減少處理過多雜訊。\n",
    "\n",
    "---\n",
    "\n",
    "### API 使用注意事項\n",
    "- **安全審查阻擋**：可能因涉及「不允許或敏感內容」而無法獲取公開資料。\n",
    "- **授權與限制**：包含付費牆、Rate Limit、隱私規範等。\n",
    "- **備援角色**：Websearch 可作為補充方案，但不一定是萬能解決方式。\n",
    "\n",
    "---\n",
    "\n",
    "### 總結\n",
    "Websearch 提供了 **廣泛而多元的資訊**，但也帶來 **碎片化與品質問題**。  \n",
    "若需求明確、可依靠結構化且可信的來源（如 Wikipedia、Fandom），應優先選用。  \n",
    "若需要多角度、開放探索或無特定資料庫可依賴時，Websearch 才能發揮最大價值。\n",
    "\n",
    "\n",
    "## 返無 歸一\n",
    "\n",
    "- 假設你確定在某個來源肯定有資訊的時候，取得該來源的網址\n",
    "- 使用第一周和第三周的技巧爬取網址的內容\n",
    "- 透過LLM提取你要的訊息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a50883-eb4e-4411-a1df-d45f4348f0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from src.initialization import credential_init\n",
    "\n",
    "\n",
    "def build_standard_chat_prompt_template(kwargs):\n",
    "\n",
    "    messages = []\n",
    " \n",
    "    if 'system' in kwargs:\n",
    "        content = kwargs.get('system')\n",
    "        prompt = PromptTemplate(**content)\n",
    "        message = SystemMessagePromptTemplate(prompt=prompt)\n",
    "        messages.append(message)  \n",
    "\n",
    "    if 'human' in kwargs:\n",
    "        content = kwargs.get('human')\n",
    "        prompt = PromptTemplate(**content)\n",
    "        message = HumanMessagePromptTemplate(prompt=prompt)\n",
    "        messages.append(message)\n",
    "        \n",
    "    chat_prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "    \n",
    "    return chat_prompt_template\n",
    "\n",
    "credential_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020d4e82-2653-4ca3-a9ab-f266479ba176",
   "metadata": {},
   "source": [
    "### 抽取Wikipedia的內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864440a6-581a-44ed-a5bb-c65d1f4c1913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "query = \"獵人中的念能力系統\"\n",
    "\n",
    "# Wikipedia語言選擇\n",
    "URL = \"https://ja.wikipedia.org/w/api.php\"\n",
    "\n",
    "\n",
    "PARAMS = {\n",
    "    \"action\": \"parse\",\n",
    "    # Wikipedia 頁面的 關鍵字\n",
    "    \"page\": \"HUNTER×HUNTER\",\n",
    "    \"prop\": \"text\",\n",
    "    \"format\": \"json\"\n",
    "}\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"AI Tutorial Bot/1.0 (mengchiehling@gmail.com)\"\n",
    "}\n",
    "\n",
    "response = session.get(url=URL, params=PARAMS, headers=HEADERS)\n",
    "\n",
    "data = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c785982-fc65-41e9-8fde-09286016f9a8",
   "metadata": {},
   "source": [
    "使用 bs4 處理數據 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b4f0ab-ce4f-4d0f-b03a-5f8ca35d8b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_content = data['parse']['text'][\"*\"]\n",
    "\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# 移除 style 和 script\n",
    "for tag in soup([\"style\", \"script\"]):\n",
    "    tag.decompose()\n",
    "\n",
    "# 提取文字\n",
    "text_content = soup.get_text(separator=\"\\n\")\n",
    "\n",
    "# 清理空白與空行\n",
    "cleaned_text = \"\\n\".join(\n",
    "    line.strip() for line in text_content.splitlines() if line.strip()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c483ad-bbe3-4097-8479-a5f69fe48537",
   "metadata": {},
   "source": [
    "慢慢寫Pydantic物件是可以實現的目標的，但是在應用中我們希望有更好的自動化: 根據使用者的需求自動生成物件\n",
    "\n",
    "我們嘗試結合代碼生成來輔助完成目標"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e7fb5c-1a59-43f5-ba17-c9fa49166e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_example = dedent(\"\"\"\n",
    "    # Example 1: Name extraction using Pydantic and LangChain\n",
    "\n",
    "    from pydantic import BaseModel, Field\n",
    "    from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "    class NameExtractor(BaseModel):\n",
    "        name: str = Field(description=\"The extracted name from the input text\")\n",
    "\n",
    "    output_parser = PydanticOutputParser(pydantic_object=NameExtractor)\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "    ---\n",
    "    # Example 2: Multiple product information extraction using Pydantic and Langchain\n",
    "\n",
    "    from typing import List\n",
    "    \n",
    "    class Product(BaseModel):\n",
    "        name: str = Field(description=\"Product\")\n",
    "        brand: str = Field(description=\"The brand name\")\n",
    "        country_code: str = Field(description=\"ISO 3166-1 alpha-2 of the country of the brand\")\n",
    "\n",
    "    class ProductOutput(BaseModel):\n",
    "        products: List[Product] = Field(description=\"A list of products\")\n",
    "\n",
    "    output_parser = PydanticOutputParser(pydantic_object=NameExtractor)\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "    \n",
    "\"\"\")\n",
    "\n",
    "\n",
    "system_template = dedent(f\"\"\"\n",
    "    You are an expert Python developer specializing in large language models and the LangChain framework.\n",
    "    Your objective is to generate **only valid, executable Python code** that solves the user's request.\n",
    "\n",
    "    Requirements:\n",
    "    - Use Pydantic models when defining structured outputs.\n",
    "    - Ensure imports are correct and minimal.\n",
    "    - Follow PEP 8 formatting standards.\n",
    "    - Do not include any explanations, markdown, comments, or extra text outside the code block.\n",
    "    - You have have the output_parser and the format_instruction of the Pydantic models.\n",
    "\n",
    "    Example structure:\n",
    "    {code_example}\n",
    "\"\"\")\n",
    "\n",
    "human_template = dedent(\"\"\"\n",
    "                        {query}\n",
    "                        Code:\n",
    "                        \"\"\")\n",
    "\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"query\"]}}\n",
    "\n",
    "chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "code_generation = chat_prompt_template|model|StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8449aef9-cf5c-43cb-9a28-6cb8a93bfcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_code = code_generation.invoke({\"query\": query})\n",
    "print(generated_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa98e812-a0be-4a70-82f5-9ac142b66a85",
   "metadata": {},
   "source": [
    "### 代碼執行工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3962439f-2aca-4a27-bf91-05aa01db7581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "@chain\n",
    "def code_execution(code):\n",
    "    \n",
    "    match = re.findall(r\"python\\n(.*?)\\n```\", code, re.DOTALL)\n",
    "    python_code = match[0]\n",
    "    \n",
    "    lines = python_code.strip()#.split('\\n')\n",
    "    # *stmts, last_line = lines\n",
    "\n",
    "    local_vars = {}\n",
    "    exec(lines, local_vars)\n",
    "\n",
    "    return local_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf658777-da30-4c71-be1a-85db64f88b45",
   "metadata": {},
   "source": [
    "執行產生的代碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcddf59-2aab-418f-80b7-ce3d585eda94",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_vars = code_execution.invoke(generated_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba66780-462e-4840-aab5-891b6f30987b",
   "metadata": {},
   "source": [
    "產生需要的pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b0c809-d6c9-4b6e-acbb-cb384380fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_answer_pipeline(output_parser, format_instructions):\n",
    "\n",
    "    human_template = dedent(\"\"\"{query}\n",
    "                           \n",
    "                           context:\n",
    "                           {context}\n",
    "                           output format instruction = {format_instruction} \n",
    "                        \"\"\")\n",
    "\n",
    "\n",
    "    input_ = {\"system\": {\"template\": \"You are a helpful AI assistant.\"},\n",
    "              \"human\": {\"template\": human_template,\n",
    "                        \"input_variable\": [\"query\"],\n",
    "                        \"partial_variables\": {'format_instruction': format_instructions}\n",
    "                        }}\n",
    "    \n",
    "    chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "    \n",
    "    model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    \n",
    "    answer_pipeline = chat_prompt_template|model|output_parser\n",
    "\n",
    "    return answer_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729a689f-cd1c-41b7-8492-9fabc48b4eb9",
   "metadata": {},
   "source": [
    "執行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58735891-8732-4fed-9fae-befa8a36e000",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_pipeline = build_answer_pipeline(output_parser=local_vars['output_parser'], format_instructions=local_vars['format_instructions'])\n",
    "\n",
    "output = answer_pipeline.invoke({\"query\": query, \"context\": cleaned_text})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ebdaf7-e0cb-4ea4-83bb-3a78a850c0c8",
   "metadata": {},
   "source": [
    "### Fan Wiki\n",
    "\n",
    "萬機之神歐姆尼賽亞的化身\n",
    "\n",
    "https://warhammer40k.fandom.com/wiki/Titan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c618afff-8453-43bf-97b3-d485adc4d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def parsing_process(url):\n",
    "    \"\"\"\n",
    "    Fetches and extracts text content from a given URL.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL of the web page to fetch and parse.\n",
    "\n",
    "    Returns:\n",
    "    str: Cleaned text content extracted from the web page.\n",
    "\n",
    "    Raises:\n",
    "    requests.exceptions.RequestException: If an error occurs while fetching the URL.\n",
    "\n",
    "    Notes:\n",
    "    - This function sends a GET request to the specified URL.\n",
    "    - It uses BeautifulSoup to parse the HTML content of the response.\n",
    "    - Any <style> tags in the HTML are removed to extract only textual content.\n",
    "    - The extracted text is cleaned by removing extra whitespace and empty lines.\n",
    "    \"\"\"\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Get the content of the response\n",
    "    html_content = response.text\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # 移除 style 和 script\n",
    "    for tag in soup([\"style\", \"script\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # Extract and print only the text content\n",
    "    text_content = soup.get_text(separator='\\n')\n",
    "\n",
    "    # Clean up the text (optional)\n",
    "    cleaned_text = '\\n'.join(line.strip() for line in text_content.splitlines() if line.strip())\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "url = \"https://warhammer40k.fandom.com/wiki/Titan\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeab876-e8f1-46be-b55e-0c078b632b9e",
   "metadata": {},
   "source": [
    "提取網頁內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328ef7c0-9650-476e-abf4-fa97941e3304",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = parsing_process(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de949f37-eac9-4934-b5d3-8658de5a6c90",
   "metadata": {},
   "source": [
    "#### 基本問題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2f9843-1759-4919-8297-2b83eb556cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"幫我找出所有忠誠派泰坦級別\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282b3066-9f2e-4a6a-a83b-6c69ab69cd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_code = code_generation.invoke({\"query\": query})\n",
    "\n",
    "local_vars = code_execution.invoke(generated_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb5a1f6-3a28-4510-8b96-c2d4556ebc7c",
   "metadata": {},
   "source": [
    "建立新的pipeline並且執行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de8f1a4-a199-4f83-bb92-c4ce19974751",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_pipeline = build_answer_pipeline(output_parser=local_vars['output_parser'], format_instructions=local_vars['format_instructions'])\n",
    "\n",
    "output = answer_pipeline.invoke({\"query\": query, \"context\": cleaned_text})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843802cc-f500-4615-aeb8-e0bfd746ab0e",
   "metadata": {},
   "source": [
    "#### 試試看更具有挑戰的問題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aa7a2e-a2fc-459a-b1ea-92e115d583fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"幫我找出所有陣營的所有泰坦級別\"\n",
    "\n",
    "generated_code = code_generation.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54b98bd-7994-40dc-8a26-0ffb439f4a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_vars = code_execution.invoke(generated_code)\n",
    "\n",
    "answer_pipeline = build_answer_pipeline(output_parser=local_vars['output_parser'], format_instructions=local_vars['format_instructions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a9078d-1acd-4da8-9cb3-3e51acbf6036",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = answer_pipeline.invoke({\"query\": query, \"context\": cleaned_text})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a58350-7baf-45c7-9beb-bcd4c15f8a2c",
   "metadata": {},
   "source": [
    "## 加入Callback 進行追蹤ChatBot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
