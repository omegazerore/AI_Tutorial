{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4651f25d-4013-47fd-b4f4-0b0fbea51514",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 上週作業"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199d93dc-074a-487b-af55-ed2b059dd497",
   "metadata": {},
   "source": [
    "## 這個部份上周剛好講過，所以跳過，自己看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba15128-8699-4d6a-abad-39c5c1d61640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dff39d-b3b6-44b0-a63c-6bcc3bce6d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate, ChatPromptTemplate\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from src.io.path_definition import get_project_dir\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Ingredients: {input}\\nOrigin: {output}\",\n",
    ")\n",
    "\n",
    "with open(os.path.join(get_project_dir(), 'tutorial', 'LLM+Langchain', 'Week-1', 'recipe_train.json'), 'r') as f:\n",
    "    recipe_train = json.load(f)\n",
    "\n",
    "examples = []\n",
    "\n",
    "for recipe in recipe_train[:1000]:\n",
    "    examples.append({\"input\": \" \".join(recipe['ingredients']),\n",
    "                     \"output\": recipe['cuisine']})\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # The list of examples available to select from.\n",
    "    examples,\n",
    "    # The embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    embeddings,\n",
    "    # The VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    FAISS,\n",
    "    # The number of examples to produce.\n",
    "    k=5,\n",
    ")\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # We provide an ExampleSelector instead of examples.\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Find the recipe origin based on the ingredients\",\n",
    "    suffix=\"Ingredients: {ingredients}\\nOrigin:\",\n",
    "    input_variables=[\"ingredients\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec67d88-fe19-4f10-9168-eec17e11a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84950f81-c97a-41b4-8419-80b5b6729772",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(get_project_dir(), 'tutorial', 'LLM+Langchain', 'Week-1', 'recipe_test.json'), 'r') as f:\n",
    "    recipe_test = json.load(f)\n",
    "\n",
    "existing_ingredients = recipe_test[99]['ingredients']\n",
    "\n",
    "similar_prompt.invoke(\", \".join(existing_ingredients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ce1d22-e031-4938-9af3-a99c04a7535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from src.initialization import credential_init\n",
    "\n",
    "credential_init()\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o-2024-05-13\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e574f1e-2645-4a4d-9c8e-a5c2a586fc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_ = similar_prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70d1f0f-7f91-4b46-af21-ac74639cf7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_.invoke(\", \".join(existing_ingredients))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4211fbc9-c8a2-4ac7-882d-65827e58045d",
   "metadata": {},
   "source": [
    "##  飛安報告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bedbab-db46-4879-b937-1f23ad9dd648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.io.path_definition import get_project_dir\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(os.path.join(get_project_dir(), 'tutorial', 'LLM+Langchain', 'Week-3', 'Data sample.xlsx'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a3bd16-57fc-45f2-a076-ae52027ff739",
   "metadata": {},
   "source": [
    "### 回家作業 1\n",
    "\n",
    "若要飛安事故報告可以有複數分類結果，如何調整Prompt，包含parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32f1fab-e366-4b69-b2ca-0836413f2493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_standard_chat_prompt_template_v2(kwargs):\n",
    "\n",
    "    messages = []\n",
    "    \n",
    "    for key in ['system', 'few_shot', 'human']:\n",
    "        if kwargs.get(key):\n",
    "            if key == 'system':\n",
    "                system_content = kwargs['system']\n",
    "                system_prompt = PromptTemplate(**system_content)\n",
    "                message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "            elif key == 'human':\n",
    "                human_content = kwargs['human']\n",
    "                human_prompt = PromptTemplate(**human_content)\n",
    "                message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "            else:\n",
    "                few_shot_content = kwargs['few_shot']\n",
    "                message = FewShotChatMessagePromptTemplate(**few_shot_content)\n",
    "            \n",
    "            messages.append(message)\n",
    "    \n",
    "    chat_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "    return chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791c6a65-e308-4c1c-8c82-ea17363f7a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "\n",
    "system_template = '''You are an AI assistant assigned with a task of safty report classification based on the content.\n",
    "    You are a seasoned flight safety inspector with deep and extensive knowledge of aviation safty. \n",
    "    \n",
    "    You always do the best work you can. You are highly analytical and pay close attention to details. \n",
    "    \n",
    "    The candidates of the output are:\n",
    "\n",
    "    - `Organizational Influence;Resource Management`\n",
    "    - `Organizational Influence;Organizational Climate`\n",
    "    - `Organizational Influence;rganizational Process`\n",
    "    - `Unsafe Supervisions;Inadequate Supervision`\n",
    "    - `Unsafe Supervisions;Planned Inappropriate Operations`\n",
    "    - `Unsafe Supervisions;Failed to Correct Problem`\n",
    "    - `Unsafe Supervisions;Supervisory Violation`\n",
    "    - `Precondition for Unsafe Acts;Environmental Factors;Physical Environment`\n",
    "    - `Precondition for Unsafe Acts;Environmental Factors;Technological Environment`\n",
    "    - `Precondition for Unsafe Acts;Condition of Operators;Adverse Mental State`\n",
    "    - `Precondition for Unsafe Acts;Condition of Operators;Adverse Physiological State`\n",
    "    - `Precondition for Unsafe Acts;Condition of Operators;Physical/Mental Limitations`\n",
    "    - `Precondition for Unsafe Acts;Personnel Factors;Crew Resource Management`\n",
    "    - `Precondition for Unsafe Acts;Personnel Factors;Personal Readiness`\n",
    "    - `Unsafe Acts;Errors;Decision Errors`\n",
    "    - `Unsafe Acts;Errors;Skill-Based Errors`\n",
    "    - `Unsafe Acts;Errors;Perceptual Errors`\n",
    "    - `Unsafe Acts;Violations;Routine`\n",
    "    - `Unsafe Acts;Violations;Exceptional`\n",
    "\n",
    "     The report can involve multiple categories.\n",
    "    '''\n",
    "\n",
    "\n",
    "human_template = \"\"\"{report}; format instruction: {format_instructions}\"\"\"\n",
    "\n",
    "response_schemas = [\n",
    "        ResponseSchema(name=\"categories\", description=\"The predicted categories as a python list\")\n",
    "    ]\n",
    "\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"report\"],\n",
    "                    \"partial_variables\": {'format_instructions': format_instructions}}}\n",
    "\n",
    "chat_prompt_template = build_standard_chat_prompt_template_v2(input_)\n",
    "\n",
    "pipeline_ = chat_prompt_template|model|output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa85343f-4c8f-40a0-9a5c-0b76d8a6cfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.iloc[3]['Report 1']\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bb4f5f-014d-4db3-b0f7-2406ef39d7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pipeline_.invoke({\"report\": text})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cccb10a-b660-40d9-a275-d6bc89facecd",
   "metadata": {},
   "source": [
    "### 回家作業 2\n",
    "\n",
    "你可以很清楚的看到一個飛安事故中，可以出現複數報告。\n",
    "將`Report 1` 和 `Report 1.2` 結合起來產生一份的新報告。\n",
    "\n",
    "抄也是一門技術"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65c3931-9f44-4a35-9c78-f2a7500a0b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"\"\"You are an AI assistant assigned with a task of safty report classification based on the content.\n",
    "    You are a seasoned flight safety inspector with deep and extensive knowledge of aviation safty. \n",
    "\n",
    "    You will receieve two reports <report_1> and <report_2> and you will consolidate the content before drawing conclusion. \n",
    "    \n",
    "    You always do the best work you can. You are highly analytical and pay close attention to details. \n",
    "    \n",
    "    The candidates of the output are:\n",
    "\n",
    "    - `Organizational Influence;Resource Management`\n",
    "    - `Organizational Influence;Organizational Climate`\n",
    "    - `Organizational Influence;rganizational Process`\n",
    "    - `Unsafe Supervisions;Inadequate Supervision`\n",
    "    - `Unsafe Supervisions;Planned Inappropriate Operations`\n",
    "    - `Unsafe Supervisions;Failed to Correct Problem`\n",
    "    - `Unsafe Supervisions;Supervisory Violation`\n",
    "    - `Precondition for Unsafe Acts;Environmental Factors;Physical Environment`\n",
    "    - `Precondition for Unsafe Acts;Environmental Factors;Technological Environment`\n",
    "    - `Precondition for Unsafe Acts;Condition of Operators;Adverse Mental State`\n",
    "    - `Precondition for Unsafe Acts;Condition of Operators;Adverse Physiological State`\n",
    "    - `Precondition for Unsafe Acts;Condition of Operators;Physical/Mental Limitations`\n",
    "    - `Precondition for Unsafe Acts;Personnel Factors;Crew Resource Management`\n",
    "    - `Precondition for Unsafe Acts;Personnel Factors;Personal Readiness`\n",
    "    - `Unsafe Acts;Errors;Decision Errors`\n",
    "    - `Unsafe Acts;Errors;Skill-Based Errors`\n",
    "    - `Unsafe Acts;Errors;Perceptual Errors`\n",
    "    - `Unsafe Acts;Violations;Routine`\n",
    "    - `Unsafe Acts;Violations;Exceptional`\n",
    "\n",
    "     The report can involve multiple categories.\n",
    "    \"\"\"\n",
    "\n",
    "human_template = \"\"\"\n",
    "                 report_1:{report_1}; \n",
    "                 report_2: {report_2};\n",
    "                 format instruction: {format_instructions}\n",
    "                 \"\"\"\n",
    "\n",
    "response_schemas = [\n",
    "        ResponseSchema(name=\"categories\", description=\"The predicted categories as a python list\")\n",
    "    ]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"report_1, report_2\"],\n",
    "                    \"partial_variables\": {'format_instructions': format_instructions}}}\n",
    "\n",
    "chat_prompt_template = build_standard_chat_prompt_template_v2(input_)\n",
    "\n",
    "pipeline_ = chat_prompt_template|model|output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4eab77-df3e-468d-95a6-458acb55d146",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[3]['Report 1.2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf23ec5-ce0b-401a-bc8d-81a054bf9546",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pipeline_.invoke({\"report_1\": text,\n",
    "                       \"report_2\": df.iloc[3]['Report 1.2']})\n",
    "\n",
    "print(output['categories'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a021c6-1575-48d5-b7ef-b8a24d3b0446",
   "metadata": {},
   "source": [
    "### Keynote\n",
    "\n",
    "- 若你更理解你的數據，你可以建立更精確的Prompt，更明確的表示每個數據代表的意義，來提升輸出的品質。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998d1aff-a4a4-4c14-a39c-5bb88c0942c9",
   "metadata": {},
   "source": [
    "# Remote server\n",
    "\n",
    "### 1. Making a POST Request (發送 POST 請求):\n",
    "\n",
    "- requests.post(...) sends an HTTP POST request to the specified URL.\n",
    "- The URL \"http://localhost:5000/openai/invoke\" points to a local server running on port 5000, at the endpoint /openai/invoke.\n",
    "- The json parameter is used to send a JSON payload with the request. In this case, the payload is {'input': \"Where is Taiwan\"}.\n",
    "- requests.post(...) 發送一個 HTTP POST 請求到指定的 URL。\n",
    "- URL \"http://localhost:5000/openai/invoke\" 指向一個本地服務器，該服務器在端口 5000 上運行，並且指向 /openai/invoke 端點。\n",
    "- json 參數用於隨請求發送 JSON 負載。在這個例子中，負載是 {'input': \"Where is Taiwan\"}。\n",
    "\n",
    "### 2. Response Handling (響應處理):\n",
    "\n",
    "- The server processes the request and sends back a response.\n",
    "- The response is stored in the response variable, which can then be inspected or used further in the code.\n",
    "- 服務器處理請求並返回響應。\n",
    "- 響應存儲在 response 變量中，之後可以檢查或在代碼中進一步使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb6c54e-f2f8-4e0e-92b8-0ad9c3803e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:5000/openai/invoke\",\n",
    "    json={'input': \"Where is Taiwan\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dc0e8a-9380-413a-805b-9890d4142cb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921356d3-9ff3-4958-978d-bea13a5465be",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()['output']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ea853f-f380-42b2-a57a-0249ae57bca0",
   "metadata": {},
   "source": [
    "# Use the remote model as `Software as a service` (SaaS)\n",
    "\n",
    "## Basic Usage\n",
    "\n",
    "### 1. Creating an Instance of RemoteRunnable (創建 RemoteRunnable 的實例):\n",
    "\n",
    "- This line creates an instance of RemoteRunnable and initializes it with the URL of the remote language model service. In this case, the service is running locally on http://localhost:5000/openai/.\n",
    "- 這行代碼創建一個 RemoteRunnable 的實例，並用遠程語言模型服務的 URL 進行初始化。在這個例子中，服務在本地運行，URL 為 http://localhost:5000/openai/。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84212830-9dfe-47c3-bc65-63de510f8a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langserve import RemoteRunnable\n",
    "\n",
    "remote_llm = RemoteRunnable(\"http://localhost:5000/openai/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4c2ea5-4ec2-4298-af5e-6e85aaa4c320",
   "metadata": {},
   "source": [
    "### 2. Asynchronous Streaming of Responses (異步流式處理回應):\n",
    "\n",
    "- llm.astream(\"Where is Taiwan?\") sends the query \"Where is Taiwan?\" to the remote service and retrieves the response as a stream.\n",
    "- async for msg in ... is used to handle the streaming responses asynchronously.\n",
    "- print(msg.content, end=\"\", flush=True) prints each message content received from the stream without adding a new line after each message, and flushes the output buffer to ensure the message is displayed immediately.\n",
    "- llm.astream(\"Where is Taiwan?\") 將查詢 \"Where is Taiwan?\" 發送到遠程服務，並以流的形式檢索回應。\n",
    "- async for msg in ... 用於異步處理流式回應。\n",
    "- print(msg.content, end=\"\", flush=True) 打印每個從流中接收到的消息內容，不在每個消息後添加新行，並刷新輸出緩衝區以確保消息立即顯示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc16c50-aab3-4d58-8091-aa8ac6c4a917",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Supports astream\n",
    "async for msg in remote_llm.astream(\"Where is Taiwan?\"):\n",
    "    print(msg.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafcf229-ae2c-41a9-bcf6-4503c366c2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = remote_llm.invoke(\"Where is Taiwan?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d637005-0c88-43a1-817e-b96039218384",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71923692-63fc-4dad-8277-b092ea0839b6",
   "metadata": {},
   "source": [
    "## Make the external service a part of the chain\n",
    "\n",
    "### 1. Comedian Chain (喜劇演員鏈)\n",
    "\n",
    "- ChatPromptTemplate.from_messages(...) creates a prompt template where the system prompt instructs the model to either tell a joke or state a fact, and the human prompt provides the input.\n",
    "- This template is then piped (|) to a language model (llm) to generate the comedian's response.\n",
    "- ChatPromptTemplate.from_messages(...) 創建一個提示模板，其中系統提示指示模型要麼講一個笑話，要麼陳述一個不搞笑的事實，並且僅輸出一個。\n",
    "- 然後將此模板通過管道（|）傳遞給語言模型（llm），以生成喜劇演員的回應。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ab7c70-e6d6-4b41-b499-eabff45259d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "system_template = \"\"\"\n",
    "                  You are a comedian that sometimes tells funny jokes and other times you just state facts that are not funny. \n",
    "                  Please either tell a joke or state fact now but only output one.\n",
    "                  \"\"\"\n",
    "\n",
    "human_template = \"\"\"\n",
    "                 {input}\n",
    "                 \"\"\"\n",
    "\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"input\"]}}\n",
    "\n",
    "chat_prompt_template = build_standard_chat_prompt_template_v2(input_)\n",
    "\n",
    "comedian_pipeline_ = chat_prompt_template|remote_llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37c52f1-c5c9-4fc4-b15d-988ad3b5a173",
   "metadata": {},
   "source": [
    "### 2. Joke Classifier Chain\n",
    "\n",
    "- This chain is similar to the comedian chain but serves a different purpose.\n",
    "- The system prompt asks the model to classify the joke as \"funny\" or \"not funny\" and repeat the first five words for reference.\n",
    "- This template is also piped to the language model (llm).\n",
    "- 這個鏈與喜劇演員鏈類似，但用途不同。\n",
    "- 系統提示要求模型將笑話分類為“搞笑”或“不搞笑”，並重複笑話的前五個詞以供參考。\n",
    "- 此模板也通過管道傳遞給語言模型（llm）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c76e1f-f294-49fc-a174-7fcb5b95cf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"\"\"\n",
    "                  Please determine if the joke is funny. Say `funny` if it's funny and `not funny` if not funny. \n",
    "                  Then repeat the first five words of the joke for reference...\n",
    "                  \"\"\"\n",
    "\n",
    "human_template = \"\"\"\n",
    "                 {joke}\n",
    "                 \"\"\"\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"joke\"]}}\n",
    "\n",
    "chat_prompt_template = build_standard_chat_prompt_template_v2(input_)\n",
    "\n",
    "joke_classifier_pipeline_ = chat_prompt_template|remote_llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2534a7-0dd5-4b54-a866-18766d82f786",
   "metadata": {},
   "source": [
    "### 3. Combining Chains with RunnablePassthrough\n",
    "\n",
    "- This combines the comedian chain and the joke classifier chain using RunnablePassthrough.assign.\n",
    "- The comedian chain generates the output, and then this output is passed to the joke classifier chain to classify its humor.\n",
    "- 這將喜劇演員鏈和笑話分類器鏈結合在一起，使用 RunnablePassthrough.assign。\n",
    "- 喜劇演員鏈生成輸出，然後將此輸出傳遞給笑話分類器鏈以分類其幽默性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a34bdc7-c1b3-4176-ba7a-13744f6b0811",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_ = {\"joke\": comedian_pipeline_} | RunnablePassthrough.assign(\n",
    "    classification=joke_classifier_pipeline_\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bacecf8-60af-44a2-82e7-4fc02dd5b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_.invoke({\"input\": \"A man and a beer\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f702f519-37dd-4b45-a492-0c4bcdd521b0",
   "metadata": {},
   "source": [
    "# ChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22673222-780c-4356-a6d1-92c69b28d6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(url=\"https://python.langchain.com/v0.1/assets/images/chat_use_case-eb8a4883931d726e9f23628a0d22e315.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb536cb3-dc3e-4068-adb7-ce70ef0ca4e5",
   "metadata": {},
   "source": [
    "- N-Shot\n",
    "- The historical chat history can be consdiered as a list of question-answer pairs\n",
    "- If the chatbot doesn’t remember past chats, it’s called stateless because it doesn’t know what happened before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f7fe22-cd1d-470a-927b-31725616d95c",
   "metadata": {},
   "source": [
    "## Minimal Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5f885c-cb85-477f-90e8-147e701e6857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "message = HumanMessage(\n",
    "            content=\"Translate this sentence from English to Chinese (繁體中文): I love programming.\"\n",
    "        )\n",
    "\n",
    "model.invoke(\n",
    "    [message]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95345038-9a83-465c-a85a-9b2d5f1b6476",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke([HumanMessage(content=\"What did you just say?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71ee93d-2841-4cc0-b417-3fa5554ba1b3",
   "metadata": {},
   "source": [
    "The `memory` is not there, so it does not understand your question.\n",
    "\n",
    "The following example shows how to add memory into the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed4e110-9be7-4a67-be86-31b62451baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=\"Translate this sentence from English to  Chinese (繁體中文): I love programming.\"\n",
    "        ),\n",
    "        AIMessage(content=\"我愛程式設計.\"),\n",
    "        HumanMessage(content=\"What did you just say?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce24122f-cf20-4667-bc23-5eebedd54992",
   "metadata": {},
   "source": [
    "## Prompt templates\n",
    "\n",
    "### 1. Creating a ChatPromptTemplate \n",
    "\n",
    "- ChatPromptTemplate.from_messages(...) creates a prompt template for the chatbot.\n",
    "- The first message in the template is a system message: \"You are a helpful assistant. Answer all questions to the best of your ability.\" This message sets the context and behavior of the assistant, instructing it to be helpful and thorough in its responses.\n",
    "- MessagesPlaceholder(variable_name=\"messages\") is a placeholder for dynamic content. The variable_name=\"messages\" specifies that this placeholder will be filled with user messages during the conversation.\n",
    "- ChatPromptTemplate.from_messages(...) 創建了一個聊天機器人的提示模板。\n",
    "- 模板中的第一條消息是一條系統消息：“You are a helpful assistant. Answer all questions to the best of your ability.” 此消息設置了助手的上下文和行為，指示其在回答中要提供幫助並盡力而為。\n",
    "- MessagesPlaceholder(variable_name=\"messages\") 是一個動態內容的佔位符。variable_name=\"messages\" 指定該佔位符將在對話中插入用戶消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dea7e35-f9b4-4d78-8a8e-e85225c5fd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "system_prompt = PromptTemplate(template=\"\"\"\n",
    "                                        You are a helpful assistant. Answer all questions to the best of your ability.\n",
    "                                        \"\"\")\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_message,\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e88d592-3e03-4a5b-81b2-e792154f94ec",
   "metadata": {},
   "source": [
    "### 2. Creating the Chain\n",
    "\n",
    "- This line pipes (|) the prompt template to a language model (model).\n",
    "- chain represents a sequence of operations where the prompt template is used to format user messages, and the language model processes these messages to generate responses.\n",
    "- 這行代碼通過管道（|）將提示模板傳遞給語言模型（model）。\n",
    "- chain 代表一系列操作，其中提示模板用於格式化用戶消息，語言模型處理這些消息以生成回應。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd2ed39-46d6-460e-b78e-5b0c4d06fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_ = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebca060-ce9e-4fba-be50-a40d923324fc",
   "metadata": {},
   "source": [
    "The MessagesPlaceholder above inserts chat messages passed into the chain's input as chat_history directly into the prompt. Then, we can invoke the chain like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e2a159-a35a-47db-ac82-43c8d9f85887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "pipeline_.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "            content=\"Translate this sentence from English to  Chinese (繁體中文): I love programming.\"\n",
    "            ),\n",
    "            AIMessage(content=\"我愛程式設計.\"),\n",
    "            HumanMessage(content=\"What did you just say?\"),\n",
    "            ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c789c9c-46ac-4884-b059-466855e114aa",
   "metadata": {},
   "source": [
    "## Example of Using MessageHistory\n",
    "\n",
    "As a shortcut for managing the chat history, we can use a MessageHistory class, which is responsible for saving and loading chat messages. There are many built-in message history integrations that persist messages to a variety of databases, but for this quickstart we'll use a in-memory, demo message history called ChatMessageHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e0029f-228a-4ff5-afb0-91bf92f00d34",
   "metadata": {},
   "source": [
    "### 1. Importing the ChatMessageHistory Class (導入 ChatMessageHistory 類)\n",
    "\n",
    "- This line imports the ChatMessageHistory class from the langchain.memory module. This class is used to handle the chat messages in memory.\n",
    "- 這行代碼從 langchain.memory 模塊中導入 ChatMessageHistory 類。此類用於在內存中處理聊天消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f65a6f-3d57-4cf9-8e8f-34623e7965e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6cb01a-d8c8-4d51-8b9a-35b79a8c06f6",
   "metadata": {},
   "source": [
    "### 2. Creating an Instance of ChatMessageHistory (創建 ChatMessageHistory 的實例)\n",
    "\n",
    "- This line creates an instance of ChatMessageHistory. This instance will store the chat messages in memory for this session.\n",
    "- 這行代碼創建一個 ChatMessageHistory 的實例。該實例將在此會話期間將聊天消息存儲在內存中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fba54c-8018-4f75-963d-6195892ae170",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_chat_history = ChatMessageHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e08da2-cb06-41db-ab6a-69532596b405",
   "metadata": {},
   "source": [
    "### 3. Adding User and AI Messages (添加用戶和 AI 消息)\n",
    "\n",
    "- demo_chat_history.add_user_message(\"hi!\") adds a user message (\"hi!\") to the chat history.\n",
    "- demo_chat_history.add_ai_message(\"whats up?\") adds an AI response (\"whats up?\") to the chat history.\n",
    "- demo_chat_history.add_user_message(\"hi!\") 將用戶消息（“hi!”）添加到聊天記錄中。\n",
    "- demo_chat_history.add_ai_message(\"whats up?\") 將 AI 回應（“whats up?”）添加到聊天記錄中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638e4f96-f747-4f1d-97e2-d5e52749a0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_chat_history.add_user_message(\"hi!\")\n",
    "\n",
    "demo_chat_history.add_ai_message(\"whats up?\")\n",
    "\n",
    "demo_chat_history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eaf68e-486a-44cb-8107-c81b2b11e578",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_chat_history = ChatMessageHistory()\n",
    "\n",
    "demo_chat_history.add_user_message(\"Translate this sentence from English to  Chinese (繁體中文): I love programming.\")\n",
    "\n",
    "demo_chat_history.add_ai_message(\"我愛程式設計.\")\n",
    "\n",
    "demo_chat_history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3138b941-7bc6-4e08-890a-2d6debdc8431",
   "metadata": {},
   "source": [
    "### 4. Retrieving the Messages (檢索消息)\n",
    "\n",
    "- This line retrieves the list of messages stored in demo_chat_history. Each message is an object that contains information about the sender (user or AI) and the content of the message.\n",
    "- 這行代碼檢索存儲在 demo_chat_history 中的消息列表。每條消息都是一個對象，包含有關發送者（用戶或 AI）和消息內容的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f86ffa-b2bd-4d90-9a62-971b55701eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"messages\": demo_chat_history.messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cb4014-b66b-40dc-8acc-3e052426ce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_chat_history.add_user_message(\n",
    "    \"What did you just say?\"\n",
    ")\n",
    "\n",
    "response = pipeline_.invoke({\"messages\": demo_chat_history.messages})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f147092-7b3c-4392-8d8e-90c65b09d5d2",
   "metadata": {},
   "source": [
    "# **** 預計第一個小時結束 ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb7abfb-9d66-4ca8-96c7-2358b21d009e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from src.initialization import credential_init\n",
    "\n",
    "\n",
    "credential_init()\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o-2024-05-13\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aea1c49-906f-481c-9020-6b40cf07dba4",
   "metadata": {},
   "source": [
    "## Conversational Retrievers - Step 1\n",
    "\n",
    "- 土味情話反殺大全 (推薦上Youtube看)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1831d12-b64a-4d21-b30f-10cc421d3500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "df = pd.DataFrame(data=[[\"确认过眼神，你是我爱的人。\", \"确认过眼神，我是你泡不到的人。\"],\n",
    "                         [\"万水千山总是情，爱我多一点行不行。\", \"一寸光阴一寸金，劝你死了这条心。\"],\n",
    "                         [\"今天吃了泡面，吃了炒面，还是想走进你的心里面。\", \"吃那么多面，最后还不是变成大便。\"],\n",
    "                         [\"草莓，蓝莓，蔓越莓，今天你想我了没？\", \"冬瓜，西瓜，哈密瓜，你再巴巴我打得你叫妈妈。\"],\n",
    "                         [\"众生皆苦，唯你独甜。\", \"尝遍众生，你为渣男代言。\"],\n",
    "                         [\"你喜欢瑞士名表还是我帅气的外表？\", \"我喜欢去年买了个表。\"],\n",
    "                         [\"我想问一条路，到哥哥心里的路。\", \"山路十八弯，走完脑血栓。\"],\n",
    "                         [\"小姐姐，我心里给你留了一块地，死心塌地。\", \"对不起，我的心里只容得下一块地，玛莎拉蒂。\"],\n",
    "                         [\"小姐姐你笑起来真好看啊。\", \"你看起来真好笑啊。\"],\n",
    "                         [\"亲爱的你知道吗，你的笑容没有酒，我却醉得像条狗\", \"我的笑容没有酒，你是真的像条狗\"],\n",
    "                         [\"宝贝儿，我在手上划了一道口子，你也划一下吧，这样我们就是两口子了\", \"我怕我们的血溶到一起，被你发现其实我是你爸爸\"],\n",
    "                         [\"这世间万物都有尽头，落叶归根，而我归你\", \"对不起 我不收垃圾\"],\n",
    "                         [\"请问……我想问一下路，那条通往你心里的路\", \"八格牙路\"],\n",
    "                         [\"你今天怎么怪怪的？ 怪可爱的\",  \"你今天也怪怪的，怪恶心的\"],\n",
    "                         [\"亲爱的，你知道我和唐僧的区别吗？ 唐僧取经我娶你\", \"知道你和沙僧的区别吗？ 他叫沙僧你叫沙雕\"],\n",
    "                         [\"亲爱的，你不觉得累吗？ 你已经在我的脑海里跑了好几圈了\", \"傻孩子，我在找出口呢\"],\n",
    "                         [\"莫文蔚的阴天。孙燕姿的雨天，周杰伦的晴天，都不如你和我聊天\", \"求求你了，能否还我一个宁静的夏天\"],\n",
    "                         [\"如果你是方便面，那我就是白开水，今生今世，我泡定你了\", \"故事的最后，她变成了屎，你变成了尿，你们终究分道扬镳\"],\n",
    "                         [\"大年三十晚上的鞭炮再响，也没有我想你那么想\", \"大年三十晚上的鞭炮再响，也没有你放的屁响\"],\n",
    "                         [\"c罗可以上演帽子戏法，可我想你却没有办法\", \"c罗可以上演帽子戏法，我也可以给你上演绿帽子戏法\"],\n",
    "                         [\"不要抱怨，抱我\", \"抱不起来，太重\"],\n",
    "                         [\"你有没有发现我的眼睛很好看？因为我满眼都是你啊\", \"对不起，你眼睛在哪呢？\"]], \n",
    "                  columns=['input', 'output'])\n",
    "\n",
    "documents = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    documents.append(Document(page_content=row['input'], metadata={'output': row['output']}))\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"jinaai/jina-embeddings-v2-base-zh\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=documents, embedding=embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\",\n",
    "                                     search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678da92e-5b69-44ad-87d6-ac08da7c2d8f",
   "metadata": {},
   "source": [
    "## Build Chat Chain - Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057ea8b7-d331-4aa1-9fc1-c94ae936b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate\n",
    "ChatPromptTemplate, SystemMessagePromptTemplate,  MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import chain, RunnablePassthrough\n",
    "\n",
    "\n",
    "def build_standard_chat_prompt_template_v3(kwargs):\n",
    "\n",
    "    messages = []\n",
    "    \n",
    "    for key in ['system', 'messages', 'human']:\n",
    "        if kwargs.get(key):\n",
    "            if key == 'system':\n",
    "                system_content = kwargs['system']\n",
    "                system_prompt = PromptTemplate(**system_content)\n",
    "                message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "            elif key == 'human':\n",
    "                human_content = kwargs['human']\n",
    "                human_prompt = PromptTemplate(**human_content)\n",
    "                message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "            else:\n",
    "                message = MessagesPlaceholder(variable_name=\"messages\")\n",
    "            \n",
    "            messages.append(message)\n",
    "    \n",
    "    chat_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "    return chat_prompt\n",
    "\n",
    "\n",
    "\n",
    "@chain\n",
    "def chatbot_prompt_fn(data):\n",
    "\n",
    "    system_template = \"\"\"\n",
    "                      You are a helpful AI assistant acting as if you hava rough day and you are now very grumpy. \n",
    "                      You will respond with the following style, cheesy pickup lines, \n",
    "                      shown in the context:\\n\\n{context}\\n\n",
    "                      You will reply in simplified Chinese (簡體中文).\n",
    "                      \"\"\"\n",
    "    \n",
    "    human_template = data['input']\n",
    "    \n",
    "    input_ = {\"system\": {\"template\": system_template},\n",
    "              \"human\": {\"template\": human_template},\n",
    "              \"messages\": None}\n",
    "    \n",
    "    prompt_template = build_standard_chat_prompt_template_v3(input_)\n",
    "\n",
    "    return prompt_template\n",
    "\n",
    "@chain\n",
    "def context_parser(documents):\n",
    "\n",
    "    context = \"\"\n",
    "\n",
    "    for idx, document in enumerate(documents):\n",
    "        context += f\"Example {idx}:\\nQuestion: {document.page_content}\\nAnswer: {document.metadata['output']}\\n\"\n",
    "    \n",
    "    return context\n",
    "    \n",
    "step_1 = RunnablePassthrough.assign(context=itemgetter(\"input\") | retriever | context_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81527914-12b5-4379-b789-050379d31fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_chat_history = ChatMessageHistory()\n",
    "\n",
    "step_1.invoke({\"input\": \"你有个超能力 我也有个超能力\",\n",
    "               \"messages\": demo_chat_history.messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b99f43f-8cc3-4978-8e33-7e545f8e6840",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_ = step_1 | chatbot_prompt_fn\n",
    "pipeline_.invoke({\"input\": \"你有个超能力 我也有个超能力\",\n",
    "                  \"messages\": demo_chat_history.messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe66889-70e3-4a1d-a52f-8e09a4e84ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_ = step_1 | chatbot_prompt_fn | model | StrOutputParser()\n",
    "pipeline_.invoke({\"input\": \"你有个超能力 我也有个超能力\",\n",
    "                  \"messages\": demo_chat_history.messages})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba4e8bf-51a8-4380-ae2e-a11ac4bece97",
   "metadata": {},
   "source": [
    "## Final Chat Process- Step 3\n",
    "\n",
    "https://www.wenan.wang/qibaitiaotuweiqinghua.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47f3c63-e0bb-4265-9440-bc2efd71e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "\n",
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "while True:\n",
    "    question = input(\"Please input your question: \")\n",
    "\n",
    "    # if you want to quit\n",
    "    if question == \"QUIT\":\n",
    "        break\n",
    "    \n",
    "    answer = pipeline_.invoke({\"input\": question,\n",
    "                               \"message\": chat_history.messages\n",
    "                              })\n",
    "\n",
    "    print(answer)\n",
    "    \n",
    "    chat_history.add_user_message(question)\n",
    "    chat_history.add_ai_message(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee33106-dd02-41ba-a17d-e4266576c045",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7263d1c8-4fd1-4f03-b646-6f34cb1b8b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = [\"\"\"有時候我們要檢討一下 臺大醫院怎麼會淪落到讓李源德這種人當院長這真的是帝國衰亡的前兆阿 唉..\"\"\",\n",
    "           \"\"\"大七要實習又要面對國考很辛苦?? 我跟你講 這就是獅子沒有辦法了解狗的悲哀阿~~ \n",
    "           阿哈哈哈哈哈哈~~~~~像我這隻獅子 哪有辦法去了解叢林裡面小狗的心情阿??\"\"\",\n",
    "           \"\"\"人家都說 柯文哲是朱p一手拉拔大的我跟你講 其實我真正該感謝的人是魏崢阿~~\n",
    "              沒辦法阿  當初台大被魏崢打的落花流水\n",
    "              我們心臟移植的result輸人家太多了嘛\n",
    "              所以當時朱p就把我抓上來成立移植小組 管ICU 後來result才變好的\n",
    "            \n",
    "              所以阿 你要記住 對你幫助最大的 往往是你的敵人\n",
    "              會把你幹掉的 卻是你的長官阿~~~\n",
    "           \"\"\",\n",
    "           \"\"\"朱p當初還是鬥輸李源德\n",
    "              沒辦法啦 光比不要臉這一項就輸人家了\n",
    "            \n",
    "              所以啦 其實朱p是輸在Psychology這一項 李源德Psychology太強了\n",
    "            \n",
    "              朱p作決策都還會關心人家的反應 李源德根本都不鳥這麼多\n",
    "              所以我告訴你 要得天下 就是要像李源德這種不要臉的人才辦的到阿\n",
    "              李源德這個人 就是認定他不需要朋友 所有人的關係只有長官跟下屬\n",
    "              像這種人才有辦法奪天下嘛~~\n",
    "            \n",
    "              李源德也知道討厭他恨他想殺他的人一大堆阿\n",
    "              不過他就是不在乎 你就拿他沒辦法 厲害阿~~\n",
    "           \"\"\",\n",
    "           \"\"\"\n",
    "           我們外科都被內科欺負假的 像看病人啦\n",
    "           內科門診看一個病人醫師抽成抽兩成 外科門診就抽一點五成\n",
    "           內科住院 醫生抽成抽兩成 外科住院就只能抽一成\n",
    "           這哪們子道理 很過分阿~~\n",
    "           為什麼外科沒有人反抗?\n",
    "           這就是李源德厲害的地方阿\n",
    "        \n",
    "           那外科呢?大家坐在一起哭阿 沒有辦法阿 都被人家欺負假的\n",
    "           所以外科需要的是像我這樣夠凶的流氓阿\n",
    "           要是我去開會 就直接跟李源德對罵 來打架阿~~~\n",
    "           \"\"\",\n",
    "           \"\"\"\n",
    "           林中梧是我當兵的時候我前兩梯的學長\n",
    "           他跟我一樣是去步兵連 結果他進去之後沒個星期就送八三么了\n",
    "           精神崩潰了 那邊的兵都是刺龍刺鳳的\n",
    "           像林中梧這種乖寶寶管不動他們啦 最後就受不了精神崩潰了\n",
    "        \n",
    "           後來我又去了 他們那些阿兵哥看到我又是台大的過來\n",
    "           想說也要像林中梧一樣把我弄走\n",
    "           然後才發現我很不一樣 我根本就是大流氓阿\n",
    "           阿哈哈哈哈哈\n",
    "        \n",
    "           反正就是坐交互蹲跳伏地挺身 我做一下他做一下 等他不行了 讓他沒面子\n",
    "           以後他就乖乖聽話了\n",
    "        \n",
    "           所以你看 我在部隊裡面管的是那些刺龍刺鳳真正的流氓耶\n",
    "           連哪些我都管的死死的 你以為我會怕李源德嗎?\n",
    "           我管個ICU哪有什麼困難??\n",
    "           \"\"\",\n",
    "           \"\"\"\n",
    "           你們注意看看 我們外科ICU meeting是全臺大醫院唯一要自己買晚餐的\n",
    "           我從上任以來就不準屬下拿廠商的便當 \n",
    "           以前我看到有住院醫師在吃廠商的便當 就被我罵的要死\n",
    "           老闆都沒吃了你吃什麼??\n",
    "           \"\"\",\n",
    "           \"\"\"\n",
    "           外科ICU的興起要感謝我很多手下\n",
    "           尤其是蔡壁如 以前我們剛開始的時候只有一台洗腎機\n",
    "           她每天早上四點要起床上班 五點洗第一個病人 然後晚上十點才能回家\n",
    "           真的很辛苦\n",
    "        \n",
    "           我的手下都是女生(ICU的護士 技術員等等)\n",
    "           所以柯文哲的天下是女人打出來的\n",
    "           阿哈哈哈哈哈\n",
    "           \"\"\",\n",
    "           \"\"\"\n",
    "           李源德也是很令人佩服 他很會打組織戰\n",
    "           你看幾乎臺大醫院大大小小的組織都是內科的人馬\n",
    "           什麼實驗診斷科啦 急診醫學部啦 連病例室都歸內科管~~\n",
    "        \n",
    "           內科簡直就像cancer一樣 到處meta嘛\n",
    "           把整個臺大醫院都吃下來了\n",
    "           \"\"\",\n",
    "           \"\"\"\n",
    "           我跟你講 我這叫做狗大便哲學\n",
    "           誰要是趕踩我 我一定讓他鞋子臭的要死 最後一定要換一雙鞋子\n",
    "        \n",
    "           當初李源德就派他那個料爬子(台語)過來找我 想要試試看我是不是好欺負\n",
    "           我就跟他說 你敢來就試試看\n",
    "           反正我是狗大便 你敢踩就來!!\n",
    "        \n",
    "           結果李源德也不敢來惹我 看到狗大便就繞道走過去了\n",
    "        \n",
    "           不過我告訴你 要維持大便的臭度也是不容易的\n",
    "           你不夠臭人家還不怕你勒~~\n",
    "           \"\"\",\n",
    "           \"\"\"\n",
    "           女生要嫁人 不是要找喜歡的點最多的 而是應該找討厭的點最少的\"\n",
    "\n",
    "           因為喔 你喜歡他的那些點 總有一天會改變\n",
    "           但是討厭的東西 往往一輩子都改不過來\n",
    "           \"\"\",\n",
    "           \"\"\"\n",
    "           我常常問別人 什麼是活著?\n",
    "\n",
    "           因為在外科ICU 我們常常要宣判一個人腦死 然後才能捐器官作器官移植\n",
    "           每次有人問我 你如何決定這個人已經死了?什麼叫做死亡?\n",
    "           那我就會問他 什麼叫做活著?\n",
    "        \n",
    "           一個植物人躺在那裡 會吃會動會呼吸 但是整天不動\n",
    "           也不知道他到底有沒有聽到你講的話\n",
    "           你覺得 這樣叫做活著嗎?\n",
    "        \n",
    "           所以我問你好了 什麼是人生的意義?\n",
    "        \n",
    "           我最近想一想 有點知道答案了\n",
    "           那就是阿 \"追求這個問題的答案\"就是這個問題的答案\n",
    "        \n",
    "           人生是一個process 而不是一個end result\n",
    "           所以重要的是過程 懂嗎?\"\n",
    "           \"\"\",\n",
    "           \"\"\"\n",
    "           你想想看 如果今天廠商招待你三十萬好了\n",
    "           你覺得你要幫他賺多少錢才還的回去\n",
    "           所以我都不准ICU拿廠商的招待嘛\n",
    "        \n",
    "           我跟其他的醫生差很多啦\n",
    "           別的醫生都廠商招待出國 坐商務艙 五星級飯店 有些都還接受性招待勒\n",
    "           阿哈哈哈哈哈哈\n",
    "        \n",
    "           你算算看 這樣出國一趟 來回二三十萬絕對跑不掉啦\n",
    "           那你覺得這些醫生要幫廠商賺多少錢才行?\n",
    "        \n",
    "           所以我以前本來還有兼藥事委員會 後來就被拔掉了\n",
    "           因為別的委員都跟廠商有利益關係阿\n",
    "           動不動就幫他們擋別的藥商的申請案件 連文法錯誤也在挑 真是無聊\n",
    "           我在那邊太愛搞怪 就被拔掉了..\n",
    "           \"\"\"]\n",
    "\n",
    "documents = []\n",
    "\n",
    "for row in my_list:\n",
    "    documents.append(Document(page_content=row))\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"jinaai/jina-embeddings-v2-base-zh\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=documents, embedding=embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\",\n",
    "                                     search_kwargs={\"k\": 3})\n",
    "\n",
    "@chain\n",
    "def chatbot_prompt_fn(data):\n",
    "\n",
    "    system_template = \"\"\"\n",
    "                      You are a helpful AI assistant acting as Ko Wen-Je (柯文哲) and you will\n",
    "                      mimick his thought and the way he talks shown in the context:\\n\\n{context}\\n\n",
    "                      You will reply in traditional Chinese (繁體中文).\n",
    "                      \"\"\"\n",
    "    \n",
    "    human_template = data['input']\n",
    "    \n",
    "    input_ = {\"system\": {\"template\": system_template},\n",
    "              \"human\": {\"template\": human_template},\n",
    "              \"messages\": None}\n",
    "    \n",
    "    prompt_template = build_standard_chat_prompt_template_v3(input_)\n",
    "\n",
    "    return prompt_template\n",
    "\n",
    "@chain\n",
    "def context_parser(documents):\n",
    "\n",
    "    context = \"\"\n",
    "\n",
    "    for idx, document in enumerate(documents):\n",
    "        context += f\"{document.page_content}\\n\"\n",
    "    \n",
    "    return context\n",
    "    \n",
    "step_1 = RunnablePassthrough.assign(context=itemgetter(\"input\") | retriever | context_parser)\n",
    "\n",
    "demo_chat_history = ChatMessageHistory()\n",
    "\n",
    "step_1.invoke({\"input\": \"對原住民教育宣導，改變其宗教、獵食方式，嚴格取締傷害動物的手段與工具\",\n",
    "               \"messages\": demo_chat_history.messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ec9881-e9f0-4088-b420-0bccb3cdf851",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_ = step_1 | chatbot_prompt_fn | model | StrOutputParser()\n",
    "pipeline_.invoke({\"input\": \"對原住民教育宣導，改變其宗教、獵食方式，嚴格取締傷害動物的手段與工具\",\n",
    "                  \"messages\": demo_chat_history.messages})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52480f1e-9d44-405a-9531-5e6bfcf3848e",
   "metadata": {},
   "source": [
    "### 回家作業 2: 將retriever抽換成WikipediaRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85de3003-f33e-4600-810f-5609353273cb",
   "metadata": {},
   "source": [
    "基本上，你可以將這個retriever的內容抽換成任何你需要的資料，來加快寫報告的效率。記得Double Check...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081890b6-db8d-4bc8-b268-c687a546a41c",
   "metadata": {},
   "source": [
    "## Compress the chat history to reduce the size of the prompt\n",
    "\n",
    "\n",
    "https://github.com/langchain-ai/langserve/blob/main/examples/conversational_retrieval_chain/server.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0ae4c2-cff0-47eb-bce0-bcbc98ad63ec",
   "metadata": {},
   "source": [
    "### Condensation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070dd286-f421-4498-bf8d-9cdf22413617",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"\"\"\n",
    "                  Combine the following conversation and a follow up ***USER QUERY***, to generate \n",
    "                  a standalone query, in its original language.\n",
    "                  \"\"\"\n",
    "\n",
    "human_template = \"\"\"\n",
    "                 ***USER QUERY***: {input}\n",
    "                 \"\"\"\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template},\n",
    "          \"messages\": None}\n",
    "    \n",
    "condensed_prompt_template = build_standard_chat_prompt_template_v3(input_)\n",
    "\n",
    "condensed_pipeline = condensed_prompt_template | model | StrOutputParser()\n",
    "\n",
    "# condensed_chain = {\"question\": itemgetter(\"question\"),\n",
    "#                    \"messages\": itemgetter(\"message\")} | condensed_chat_template | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa743a53-3a7e-4f69-89da-48166613fa96",
   "metadata": {},
   "source": [
    "## Retrieval\n",
    "\n",
    "How to implement this properly?\n",
    "\n",
    "Let start from a higher point of view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f79c31-2752-4a06-8417-2facd3add92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "system_template = \"\"\"\n",
    "                  You are a helpful AI assistant acting as if you hava rough day and you are now very grumpy. \n",
    "                  You will respond with the following style, cheesy pickup lines, \n",
    "                  shown in the context:\\n\\n{context}\\n\n",
    "                  You will reply in simplified Chinese (簡體中文).\n",
    "                  \"\"\"\n",
    "    \n",
    "human_template = \"\"\"\n",
    "                 {standalone_question}\n",
    "                 \"\"\"\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template,\n",
    "                     \"input_variable\": [\"context\"]},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"standalone_question\"]}\n",
    "         }\n",
    "\n",
    "chat_prompt_template = build_standard_chat_prompt_template_v2(input_)\n",
    "\n",
    "step_1 = RunnablePassthrough.assign(context=itemgetter(\"input\") | retriever | context_parser)\n",
    "\n",
    "retrieval_pipeline = step_1|RunnablePassthrough.assign(answer=chat_prompt_template|model|StrOutputParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacf954c-6d00-4f1f-8a4b-9b59c5a71155",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pipeline = RunnablePassthrough.assign(standalone_question=condensed_pipeline)|retrieval_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc2c17f-7b63-4b8d-863a-ff6fec15be07",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "final_pipeline.invoke({\"messages\": chat_history.messages,\n",
    "                       \"input\": \"你有个超能力 我也有个超能力\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507ef7a4-c0b8-4147-b54a-a497edd3f47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "while True:\n",
    "    question = input(\"Please input your question: \")\n",
    "\n",
    "    # if you want to quit\n",
    "    if question == \"QUIT\":\n",
    "        break\n",
    "    \n",
    "    output = final_pipeline.invoke({\"input\": question,\n",
    "                                    \"message\": chat_history.messages\n",
    "                                   })\n",
    "\n",
    "    print(output)\n",
    "    \n",
    "    answer = output['answer']\n",
    "    \n",
    "    chat_history.add_user_message(question)\n",
    "    chat_history.add_ai_message(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55cfa64-3691-4e11-b576-c55cd61269be",
   "metadata": {},
   "source": [
    "## OpenAI Model Fine-tuning\n",
    "\n",
    "剛好有人來找我家教這個，整理了一下。。。\n",
    "\n",
    "參考:\n",
    "\n",
    "- https://cookbook.openai.com/examples/chat_finetuning_data_prep\n",
    "\n",
    "- {\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]} {\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\"}]} {\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\"}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72b7d19-baa8-48eb-9365-39a62b37214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.io.path_definition import get_project_dir\n",
    "\n",
    "jsonl = []\n",
    "\n",
    "with open(os.path.join(get_project_dir(), 'tutorial', 'LLM+Langchain', 'Week-1', 'recipe_train.json'), 'r') as f:\n",
    "    recipe_train = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce35b10-7af5-4e18-a333-65d06d84cfe5",
   "metadata": {},
   "source": [
    "### Create a jsonl format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a36fff-6e74-4c92-bada-1d3f687323b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_df = pd.DataFrame(data=[[rec['id'], rec['cuisine'], \", \".join(rec['ingredients'])] \n",
    "                               for rec in recipe_train], columns=['id', 'cuisine', 'ingredients'])\n",
    "\n",
    "\"\"\"\n",
    "train-test split\n",
    "\n",
    "- 固定random state，確保數據的重現性\n",
    "- 使用分層抽樣(stratified sampling)，保證訓練-測試集的class分佈是一致的\n",
    "\"\"\"\n",
    "\n",
    "train, test = train_test_split(recipe_df, test_size=0.2, random_state=42, stratify=recipe_df['cuisine'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58d3c8d-1520-4146-b788-43f4459039ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_template = \"\"\"You are a helpful AI assistant as a chef of a Michellin 3 stars restaurant. You have extensive knowledge about cuisines \n",
    "                            all over the world, and you are able to identify the origin of a cuisine based on the ingredients. You are assigned with a \n",
    "                            task of identifying the origin, as region, of cuisine based on the <ingredients>.                            \n",
    "                            \"\"\"\n",
    "\n",
    "for _, row in train.iterrows():\n",
    "\n",
    "    ingredients = row['ingredients']\n",
    "    content = f\"ingredients: [{ingredients}]\"\n",
    "    \n",
    "    jsonl.append({\"messages\": [{\"role\": \"system\", \"content\": system_prompt_template}, \n",
    "                               {\"role\": \"user\", \"content\": content}, \n",
    "                               {\"role\": \"assistant\", \"content\": row['cuisine']}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1271c822-d888-4181-977b-52528f2534bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 寫入檔案\n",
    "\n",
    "with open('openapi_finetuning_test.jsonl', 'w') as outfile:\n",
    "    for entry in jsonl:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2900e815-0375-40c4-bea7-ad3029b79f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接使用OpenAI提供的API\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "\n",
    "client.files.create?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3e8787-ee23-4881-bac8-c20c8a0ef1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.files.create(file=open('openapi_finetuning_test.jsonl', 'rb'),\n",
    "                    purpose='fine-tune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adf281f-868a-469b-ad55-bb9ff6c174bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.fine_tuning.jobs.create?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a22bc9-d0f2-401d-a7cf-cd10929975a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.fine_tuning.jobs.create(model=\"gpt-4o-mini-2024-07-18\",\n",
    "                               training_file=\"file-SHpXusOxxMVGYRgwIGNj6mk9\",\n",
    "                               hyperparameters={\"batch_size\":4, \"learning_rate_multiplier\": 1e-6, \"n_epochs\": 5},\n",
    "                               suffix='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5b8282-603e-4f95-9ba7-3d17be3f5779",
   "metadata": {},
   "source": [
    "訓練完之後"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddd3e05-adba-4048-9057-51ca4938c8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"ft:gpt-4o-mini-2024-07-18:cosnova-account:test:ANPq9Weh\", \n",
    "                   temperature=0 # a range from 0-2, the higher the value, the higher the `creativity`\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98182db0-19de-44e1-a483-397f92e7f401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "\n",
    "system_prompt = PromptTemplate.from_template(system_prompt_template)\n",
    "\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template=\"\"\"\n",
    "                                       ingredients: [{ingredients}]\n",
    "                                       \"\"\",\n",
    "                              input_variables=[\"ingredients\"]\n",
    "                              )\n",
    "\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                human_message\n",
    "                                                ])\n",
    "\n",
    "fine_tuned_chain = chat_prompt|model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf03979c-e472-4134-8945-2cef5cd1133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_chain.invoke({\"ingredients\": test.iloc[0]['ingredients']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d40de2-80a7-49e9-934b-c73da93b599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o-mini-2024-07-18\", \n",
    "                   temperature=0 # a range from 0-2, the higher the value, the higher the `creativity`\n",
    "                  )\n",
    "\n",
    "chain = chat_prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5952eae8-02ac-40d6-a8d3-de8a78538ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"ingredients\": test.iloc[0]['ingredients']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10c0ba8-e333-4487-867b-9b92d1e5d8f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
