{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4651f25d-4013-47fd-b4f4-0b0fbea51514",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 上週作業"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199d93dc-074a-487b-af55-ed2b059dd497",
   "metadata": {},
   "source": [
    "## 這個部份上周剛好講過，所以跳過，自己看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba15128-8699-4d6a-abad-39c5c1d61640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dff39d-b3b6-44b0-a63c-6bcc3bce6d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate, ChatPromptTemplate\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from src.io.path_definition import get_project_dir\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Ingredients: {input}\\nOrigin: {output}\",\n",
    ")\n",
    "\n",
    "with open(os.path.join(get_project_dir(), 'tutorial', 'LLM+Langchain', 'Week-1', 'recipe_train.json'), 'r') as f:\n",
    "    recipe_train = json.load(f)\n",
    "\n",
    "examples = []\n",
    "\n",
    "for recipe in recipe_train[:1000]:\n",
    "    examples.append({\"input\": \" \".join(recipe['ingredients']),\n",
    "                     \"output\": recipe['cuisine']})\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # The list of examples available to select from.\n",
    "    examples,\n",
    "    # The embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    embeddings,\n",
    "    # The VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    FAISS,\n",
    "    # The number of examples to produce.\n",
    "    k=5,\n",
    ")\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # We provide an ExampleSelector instead of examples.\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Find the recipe origin based on the ingredients\",\n",
    "    suffix=\"Ingredients: {ingredients}\\nOrigin:\",\n",
    "    input_variables=[\"ingredients\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec67d88-fe19-4f10-9168-eec17e11a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84950f81-c97a-41b4-8419-80b5b6729772",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(get_project_dir(), 'tutorial', 'LLM+Langchain', 'Week-1', 'recipe_test.json'), 'r') as f:\n",
    "    recipe_test = json.load(f)\n",
    "\n",
    "existing_ingredients = recipe_test[99]['ingredients']\n",
    "\n",
    "similar_prompt.invoke(\", \".join(existing_ingredients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ce1d22-e031-4938-9af3-a99c04a7535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from src.initialization import credential_init\n",
    "\n",
    "credential_init()\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o-2024-05-13\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e574f1e-2645-4a4d-9c8e-a5c2a586fc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_ = similar_prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70d1f0f-7f91-4b46-af21-ac74639cf7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_.invoke(\", \".join(existing_ingredients))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4211fbc9-c8a2-4ac7-882d-65827e58045d",
   "metadata": {},
   "source": [
    "##  飛安報告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bedbab-db46-4879-b937-1f23ad9dd648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.io.path_definition import get_project_dir\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(os.path.join(get_project_dir(), 'tutorial', 'LLM+Langchain', 'Week-3', 'Data sample.xlsx'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a3bd16-57fc-45f2-a076-ae52027ff739",
   "metadata": {},
   "source": [
    "### 回家作業 1\n",
    "\n",
    "若要飛安事故報告可以有複數分類結果，如何調整Prompt，包含parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32f1fab-e366-4b69-b2ca-0836413f2493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_standard_chat_prompt_template_v2(kwargs):\n",
    "\n",
    "    messages = []\n",
    "    \n",
    "    for key in ['system', 'few_shot', 'human']:\n",
    "        if kwargs.get(key):\n",
    "            if key == 'system':\n",
    "                system_content = kwargs['system']\n",
    "                system_prompt = PromptTemplate(**system_content)\n",
    "                message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "            elif key == 'human':\n",
    "                human_content = kwargs['human']\n",
    "                human_prompt = PromptTemplate(**human_content)\n",
    "                message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "            else:\n",
    "                few_shot_content = kwargs['few_shot']\n",
    "                message = FewShotChatMessagePromptTemplate(**few_shot_content)\n",
    "            \n",
    "            messages.append(message)\n",
    "    \n",
    "    chat_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "    return chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791c6a65-e308-4c1c-8c82-ea17363f7a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "\n",
    "system_template = '''You are an AI assistant assigned with a task of safty report classification based on the content.\n",
    "    You are a seasoned flight safety inspector with deep and extensive knowledge of aviation safty. \n",
    "    \n",
    "    You always do the best work you can. You are highly analytical and pay close attention to details. \n",
    "    \n",
    "    The candidates of the output are:\n",
    "\n",
    "    - `Organizational Influence;Resource Management`\n",
    "    - `Organizational Influence;Organizational Climate`\n",
    "    - `Organizational Influence;rganizational Process`\n",
    "    - `Unsafe Supervisions;Inadequate Supervision`\n",
    "    - `Unsafe Supervisions;Planned Inappropriate Operations`\n",
    "    - `Unsafe Supervisions;Failed to Correct Problem`\n",
    "    - `Unsafe Supervisions;Supervisory Violation`\n",
    "    - `Precondition for Unsafe Acts;Environmental Factors;Physical Environment`\n",
    "    - `Precondition for Unsafe Acts;Environmental Factors;Technological Environment`\n",
    "    - `Precondition for Unsafe Acts;Condition of Operators;Adverse Mental State`\n",
    "    - `Precondition for Unsafe Acts;Condition of Operators;Adverse Physiological State`\n",
    "    - `Precondition for Unsafe Acts;Condition of Operators;Physical/Mental Limitations`\n",
    "    - `Precondition for Unsafe Acts;Personnel Factors;Crew Resource Management`\n",
    "    - `Precondition for Unsafe Acts;Personnel Factors;Personal Readiness`\n",
    "    - `Unsafe Acts;Errors;Decision Errors`\n",
    "    - `Unsafe Acts;Errors;Skill-Based Errors`\n",
    "    - `Unsafe Acts;Errors;Perceptual Errors`\n",
    "    - `Unsafe Acts;Violations;Routine`\n",
    "    - `Unsafe Acts;Violations;Exceptional`\n",
    "\n",
    "     The report can involve multiple categories.\n",
    "    '''\n",
    "\n",
    "\n",
    "human_template = \"\"\"{report}; format instruction: {format_instructions}\"\"\"\n",
    "\n",
    "response_schemas = [\n",
    "        ResponseSchema(name=\"categories\", description=\"The predicted categories as a python list\")\n",
    "    ]\n",
    "\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"report\"],\n",
    "                    \"partial_variables\": {'format_instructions': format_instructions}}}\n",
    "\n",
    "chat_prompt_template = build_standard_chat_prompt_template_v2(input_)\n",
    "\n",
    "pipeline_ = chat_prompt_template|model|output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa85343f-4c8f-40a0-9a5c-0b76d8a6cfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.iloc[3]['Report 1']\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bb4f5f-014d-4db3-b0f7-2406ef39d7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pipeline_.invoke({\"report\": text})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cccb10a-b660-40d9-a275-d6bc89facecd",
   "metadata": {},
   "source": [
    "### 回家作業 2\n",
    "\n",
    "你可以很清楚的看到一個飛安事故中，可以出現複數報告。\n",
    "將`Report 1` 和 `Report 1.2` 結合起來產生一份的新報告。\n",
    "\n",
    "抄也是一門技術"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65c3931-9f44-4a35-9c78-f2a7500a0b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"\"\"You are an AI assistant assigned with a task of safty report classification based on the content.\n",
    "    You are a seasoned flight safety inspector with deep and extensive knowledge of aviation safty. \n",
    "\n",
    "    You will receieve two reports <report_1> and <report_2> and you will consolidate the content before drawing conclusion. \n",
    "    \n",
    "    You always do the best work you can. You are highly analytical and pay close attention to details. \n",
    "    \n",
    "    The candidates of the output are:\n",
    "\n",
    "    - `Organizational Influence;Resource Management`\n",
    "    - `Organizational Influence;Organizational Climate`\n",
    "    - `Organizational Influence;rganizational Process`\n",
    "    - `Unsafe Supervisions;Inadequate Supervision`\n",
    "    - `Unsafe Supervisions;Planned Inappropriate Operations`\n",
    "    - `Unsafe Supervisions;Failed to Correct Problem`\n",
    "    - `Unsafe Supervisions;Supervisory Violation`\n",
    "    - `Precondition for Unsafe Acts;Environmental Factors;Physical Environment`\n",
    "    - `Precondition for Unsafe Acts;Environmental Factors;Technological Environment`\n",
    "    - `Precondition for Unsafe Acts;Condition of Operators;Adverse Mental State`\n",
    "    - `Precondition for Unsafe Acts;Condition of Operators;Adverse Physiological State`\n",
    "    - `Precondition for Unsafe Acts;Condition of Operators;Physical/Mental Limitations`\n",
    "    - `Precondition for Unsafe Acts;Personnel Factors;Crew Resource Management`\n",
    "    - `Precondition for Unsafe Acts;Personnel Factors;Personal Readiness`\n",
    "    - `Unsafe Acts;Errors;Decision Errors`\n",
    "    - `Unsafe Acts;Errors;Skill-Based Errors`\n",
    "    - `Unsafe Acts;Errors;Perceptual Errors`\n",
    "    - `Unsafe Acts;Violations;Routine`\n",
    "    - `Unsafe Acts;Violations;Exceptional`\n",
    "\n",
    "     The report can involve multiple categories.\n",
    "    \"\"\"\n",
    "\n",
    "human_template = \"\"\"\n",
    "                 report_1:{report_1}; \n",
    "                 report_2: {report_2};\n",
    "                 format instruction: {format_instructions}\n",
    "                 \"\"\"\n",
    "\n",
    "response_schemas = [\n",
    "        ResponseSchema(name=\"categories\", description=\"The predicted categories as a python list\")\n",
    "    ]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"report_1, report_2\"],\n",
    "                    \"partial_variables\": {'format_instructions': format_instructions}}}\n",
    "\n",
    "chat_prompt_template = build_standard_chat_prompt_template_v2(input_)\n",
    "\n",
    "pipeline_ = chat_prompt_template|model|output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4eab77-df3e-468d-95a6-458acb55d146",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[3]['Report 1.2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf23ec5-ce0b-401a-bc8d-81a054bf9546",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pipeline_.invoke({\"report_1\": text,\n",
    "                       \"report_2\": df.iloc[3]['Report 1.2']})\n",
    "\n",
    "print(output['categories'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a021c6-1575-48d5-b7ef-b8a24d3b0446",
   "metadata": {},
   "source": [
    "### Keynote\n",
    "\n",
    "- 若你更理解你的數據，你可以建立更精確的Prompt，更明確的表示每個數據代表的意義，來提升輸出的品質。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998d1aff-a4a4-4c14-a39c-5bb88c0942c9",
   "metadata": {},
   "source": [
    "# Remote server\n",
    "\n",
    "### 1. Making a POST Request (發送 POST 請求):\n",
    "\n",
    "- requests.post(...) sends an HTTP POST request to the specified URL.\n",
    "- The URL \"http://localhost:5000/openai/invoke\" points to a local server running on port 5000, at the endpoint /openai/invoke.\n",
    "- The json parameter is used to send a JSON payload with the request. In this case, the payload is {'input': \"Where is Taiwan\"}.\n",
    "- requests.post(...) 發送一個 HTTP POST 請求到指定的 URL。\n",
    "- URL \"http://localhost:5000/openai/invoke\" 指向一個本地服務器，該服務器在端口 5000 上運行，並且指向 /openai/invoke 端點。\n",
    "- json 參數用於隨請求發送 JSON 負載。在這個例子中，負載是 {'input': \"Where is Taiwan\"}。\n",
    "\n",
    "### 2. Response Handling (響應處理):\n",
    "\n",
    "- The server processes the request and sends back a response.\n",
    "- The response is stored in the response variable, which can then be inspected or used further in the code.\n",
    "- 服務器處理請求並返回響應。\n",
    "- 響應存儲在 response 變量中，之後可以檢查或在代碼中進一步使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb6c54e-f2f8-4e0e-92b8-0ad9c3803e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:5000/openai/invoke\",\n",
    "    json={'input': \"Where is Taiwan\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dc0e8a-9380-413a-805b-9890d4142cb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921356d3-9ff3-4958-978d-bea13a5465be",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()['output']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ea853f-f380-42b2-a57a-0249ae57bca0",
   "metadata": {},
   "source": [
    "# Use the remote model as `Software as a service` (SaaS)\n",
    "\n",
    "## Basic Usage\n",
    "\n",
    "### 1. Creating an Instance of RemoteRunnable (創建 RemoteRunnable 的實例):\n",
    "\n",
    "- This line creates an instance of RemoteRunnable and initializes it with the URL of the remote language model service. In this case, the service is running locally on http://localhost:5000/openai/.\n",
    "- 這行代碼創建一個 RemoteRunnable 的實例，並用遠程語言模型服務的 URL 進行初始化。在這個例子中，服務在本地運行，URL 為 http://localhost:5000/openai/。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84212830-9dfe-47c3-bc65-63de510f8a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langserve import RemoteRunnable\n",
    "\n",
    "llm = RemoteRunnable(\"http://localhost:5000/openai/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4c2ea5-4ec2-4298-af5e-6e85aaa4c320",
   "metadata": {},
   "source": [
    "### 2. Asynchronous Streaming of Responses (異步流式處理回應):\n",
    "\n",
    "- llm.astream(\"Where is Taiwan?\") sends the query \"Where is Taiwan?\" to the remote service and retrieves the response as a stream.\n",
    "- async for msg in ... is used to handle the streaming responses asynchronously.\n",
    "- print(msg.content, end=\"\", flush=True) prints each message content received from the stream without adding a new line after each message, and flushes the output buffer to ensure the message is displayed immediately.\n",
    "- llm.astream(\"Where is Taiwan?\") 將查詢 \"Where is Taiwan?\" 發送到遠程服務，並以流的形式檢索回應。\n",
    "- async for msg in ... 用於異步處理流式回應。\n",
    "- print(msg.content, end=\"\", flush=True) 打印每個從流中接收到的消息內容，不在每個消息後添加新行，並刷新輸出緩衝區以確保消息立即顯示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc16c50-aab3-4d58-8091-aa8ac6c4a917",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Supports astream\n",
    "async for msg in llm.astream(\"Where is Taiwan?\"):\n",
    "    print(msg.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafcf229-ae2c-41a9-bcf6-4503c366c2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm.invoke(\"Where is Taiwan?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d637005-0c88-43a1-817e-b96039218384",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71923692-63fc-4dad-8277-b092ea0839b6",
   "metadata": {},
   "source": [
    "## Make the external service a part of the chain\n",
    "\n",
    "### 1. Comedian Chain (喜劇演員鏈)\n",
    "\n",
    "- ChatPromptTemplate.from_messages(...) creates a prompt template where the system prompt instructs the model to either tell a joke or state a fact, and the human prompt provides the input.\n",
    "- This template is then piped (|) to a language model (llm) to generate the comedian's response.\n",
    "- ChatPromptTemplate.from_messages(...) 創建一個提示模板，其中系統提示指示模型要麼講一個笑話，要麼陳述一個不搞笑的事實，並且僅輸出一個。\n",
    "- 然後將此模板通過管道（|）傳遞給語言模型（llm），以生成喜劇演員的回應。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ab7c70-e6d6-4b41-b499-eabff45259d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "system_template = \"\"\"\n",
    "                  You are a comedian that sometimes tells funny jokes and other times you just state facts that are not funny. \n",
    "                  Please either tell a joke or state fact now but only output one.\n",
    "                  \"\"\"\n",
    "\n",
    "human_template = \"\"\"\n",
    "                 {input}\n",
    "                 \"\"\"\n",
    "\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"input\"]}}\n",
    "\n",
    "chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "comedian_pipeline_ = chat_prompt_template|llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37c52f1-c5c9-4fc4-b15d-988ad3b5a173",
   "metadata": {},
   "source": [
    "### 2. Joke Classifier Chain\n",
    "\n",
    "- This chain is similar to the comedian chain but serves a different purpose.\n",
    "- The system prompt asks the model to classify the joke as \"funny\" or \"not funny\" and repeat the first five words for reference.\n",
    "- This template is also piped to the language model (llm).\n",
    "- 這個鏈與喜劇演員鏈類似，但用途不同。\n",
    "- 系統提示要求模型將笑話分類為“搞笑”或“不搞笑”，並重複笑話的前五個詞以供參考。\n",
    "- 此模板也通過管道傳遞給語言模型（llm）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c76e1f-f294-49fc-a174-7fcb5b95cf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"\"\"\n",
    "                  Please determine if the joke is funny. Say `funny` if it's funny and `not funny` if not funny. \n",
    "                  Then repeat the first five words of the joke for reference...\n",
    "                  \"\"\"\n",
    "\n",
    "human_template = \"\"\"\n",
    "                 {input}\n",
    "                 \"\"\"\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"input\"]}}\n",
    "\n",
    "chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "joke_classifier_pipeline_ = chat_prompt_template|llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2534a7-0dd5-4b54-a866-18766d82f786",
   "metadata": {},
   "source": [
    "### 3. Combining Chains with RunnablePassthrough\n",
    "\n",
    "- This combines the comedian chain and the joke classifier chain using RunnablePassthrough.assign.\n",
    "- The comedian chain generates the output, and then this output is passed to the joke classifier chain to classify its humor.\n",
    "- 這將喜劇演員鏈和笑話分類器鏈結合在一起，使用 RunnablePassthrough.assign。\n",
    "- 喜劇演員鏈生成輸出，然後將此輸出傳遞給笑話分類器鏈以分類其幽默性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a34bdc7-c1b3-4176-ba7a-13744f6b0811",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_ = {\"joke\": comedian_chain} | RunnablePassthrough.assign(\n",
    "    classification=joke_classifier_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bacecf8-60af-44a2-82e7-4fc02dd5b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_.invoke({\"input\": \"A man and a beer\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f702f519-37dd-4b45-a492-0c4bcdd521b0",
   "metadata": {},
   "source": [
    "# ChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22673222-780c-4356-a6d1-92c69b28d6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(url=\"https://python.langchain.com/v0.1/assets/images/chat_use_case-eb8a4883931d726e9f23628a0d22e315.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb536cb3-dc3e-4068-adb7-ce70ef0ca4e5",
   "metadata": {},
   "source": [
    "- N-Shot\n",
    "- The historical chat history can be consdiered as a list of question-answer pairs\n",
    "- If the chatbot doesn’t remember past chats, it’s called stateless because it doesn’t know what happened before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f7fe22-cd1d-470a-927b-31725616d95c",
   "metadata": {},
   "source": [
    "## Minimal Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5f885c-cb85-477f-90e8-147e701e6857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "message = HumanMessage(\n",
    "            content=\"Translate this sentence from English to Chinese (繁體中文): I love programming.\"\n",
    "        )\n",
    "\n",
    "model.invoke(\n",
    "    [message]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95345038-9a83-465c-a85a-9b2d5f1b6476",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke([HumanMessage(content=\"What did you just say?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71ee93d-2841-4cc0-b417-3fa5554ba1b3",
   "metadata": {},
   "source": [
    "The `memory` is not there, so it does not understand your question.\n",
    "\n",
    "The following example shows how to add memory into the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed4e110-9be7-4a67-be86-31b62451baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=\"Translate this sentence from English to  Chinese (繁體中文): I love programming.\"\n",
    "        ),\n",
    "        AIMessage(content=\"我愛程式設計.\"),\n",
    "        HumanMessage(content=\"What did you just say?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce24122f-cf20-4667-bc23-5eebedd54992",
   "metadata": {},
   "source": [
    "## Prompt templates\n",
    "\n",
    "### 1. Creating a ChatPromptTemplate \n",
    "\n",
    "- ChatPromptTemplate.from_messages(...) creates a prompt template for the chatbot.\n",
    "- The first message in the template is a system message: \"You are a helpful assistant. Answer all questions to the best of your ability.\" This message sets the context and behavior of the assistant, instructing it to be helpful and thorough in its responses.\n",
    "- MessagesPlaceholder(variable_name=\"messages\") is a placeholder for dynamic content. The variable_name=\"messages\" specifies that this placeholder will be filled with user messages during the conversation.\n",
    "- ChatPromptTemplate.from_messages(...) 創建了一個聊天機器人的提示模板。\n",
    "- 模板中的第一條消息是一條系統消息：“You are a helpful assistant. Answer all questions to the best of your ability.” 此消息設置了助手的上下文和行為，指示其在回答中要提供幫助並盡力而為。\n",
    "- MessagesPlaceholder(variable_name=\"messages\") 是一個動態內容的佔位符。variable_name=\"messages\" 指定該佔位符將在對話中插入用戶消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dea7e35-f9b4-4d78-8a8e-e85225c5fd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "system_prompt = PromptTemplate(template=\"\"\"\n",
    "                                        You are a helpful assistant. Answer all questions to the best of your ability.\n",
    "                                        \"\"\")\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_message,\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e88d592-3e03-4a5b-81b2-e792154f94ec",
   "metadata": {},
   "source": [
    "### 2. Creating the Chain\n",
    "\n",
    "- This line pipes (|) the prompt template to a language model (model).\n",
    "- chain represents a sequence of operations where the prompt template is used to format user messages, and the language model processes these messages to generate responses.\n",
    "- 這行代碼通過管道（|）將提示模板傳遞給語言模型（model）。\n",
    "- chain 代表一系列操作，其中提示模板用於格式化用戶消息，語言模型處理這些消息以生成回應。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd2ed39-46d6-460e-b78e-5b0c4d06fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_ = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebca060-ce9e-4fba-be50-a40d923324fc",
   "metadata": {},
   "source": [
    "The MessagesPlaceholder above inserts chat messages passed into the chain's input as chat_history directly into the prompt. Then, we can invoke the chain like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e2a159-a35a-47db-ac82-43c8d9f85887",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "            content=\"Translate this sentence from English to  Chinese (繁體中文): I love programming.\"\n",
    "            ),\n",
    "            AIMessage(content=\"我愛程式設計.\"),\n",
    "            HumanMessage(content=\"What did you just say?\"),\n",
    "            ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c789c9c-46ac-4884-b059-466855e114aa",
   "metadata": {},
   "source": [
    "## Example of Using MessageHistory\n",
    "\n",
    "As a shortcut for managing the chat history, we can use a MessageHistory class, which is responsible for saving and loading chat messages. There are many built-in message history integrations that persist messages to a variety of databases, but for this quickstart we'll use a in-memory, demo message history called ChatMessageHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e0029f-228a-4ff5-afb0-91bf92f00d34",
   "metadata": {},
   "source": [
    "### 1. Importing the ChatMessageHistory Class (導入 ChatMessageHistory 類)\n",
    "\n",
    "- This line imports the ChatMessageHistory class from the langchain.memory module. This class is used to handle the chat messages in memory.\n",
    "- 這行代碼從 langchain.memory 模塊中導入 ChatMessageHistory 類。此類用於在內存中處理聊天消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f65a6f-3d57-4cf9-8e8f-34623e7965e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6cb01a-d8c8-4d51-8b9a-35b79a8c06f6",
   "metadata": {},
   "source": [
    "### 2. Creating an Instance of ChatMessageHistory (創建 ChatMessageHistory 的實例)\n",
    "\n",
    "- This line creates an instance of ChatMessageHistory. This instance will store the chat messages in memory for this session.\n",
    "- 這行代碼創建一個 ChatMessageHistory 的實例。該實例將在此會話期間將聊天消息存儲在內存中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fba54c-8018-4f75-963d-6195892ae170",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_chat_history = ChatMessageHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e08da2-cb06-41db-ab6a-69532596b405",
   "metadata": {},
   "source": [
    "### 3. Adding User and AI Messages (添加用戶和 AI 消息)\n",
    "\n",
    "- demo_chat_history.add_user_message(\"hi!\") adds a user message (\"hi!\") to the chat history.\n",
    "- demo_chat_history.add_ai_message(\"whats up?\") adds an AI response (\"whats up?\") to the chat history.\n",
    "- demo_chat_history.add_user_message(\"hi!\") 將用戶消息（“hi!”）添加到聊天記錄中。\n",
    "- demo_chat_history.add_ai_message(\"whats up?\") 將 AI 回應（“whats up?”）添加到聊天記錄中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638e4f96-f747-4f1d-97e2-d5e52749a0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_chat_history.add_user_message(\"hi!\")\n",
    "\n",
    "demo_chat_history.add_ai_message(\"whats up?\")\n",
    "\n",
    "demo_chat_history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eaf68e-486a-44cb-8107-c81b2b11e578",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_chat_history = ChatMessageHistory()\n",
    "\n",
    "demo_chat_history.add_user_message(\"Translate this sentence from English to  Chinese (繁體中文): I love programming.\")\n",
    "\n",
    "demo_chat_history.add_ai_message(\"我愛程式設計.\")\n",
    "\n",
    "demo_chat_history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3138b941-7bc6-4e08-890a-2d6debdc8431",
   "metadata": {},
   "source": [
    "### 4. Retrieving the Messages (檢索消息)\n",
    "\n",
    "- This line retrieves the list of messages stored in demo_chat_history. Each message is an object that contains information about the sender (user or AI) and the content of the message.\n",
    "- 這行代碼檢索存儲在 demo_chat_history 中的消息列表。每條消息都是一個對象，包含有關發送者（用戶或 AI）和消息內容的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f86ffa-b2bd-4d90-9a62-971b55701eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"messages\": demo_chat_history.messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cb4014-b66b-40dc-8acc-3e052426ce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_chat_history.add_user_message(\n",
    "    \"What did you just say?\"\n",
    ")\n",
    "\n",
    "response = pipeline_.invoke({\"messages\": demo_chat_history.messages})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f147092-7b3c-4392-8d8e-90c65b09d5d2",
   "metadata": {},
   "source": [
    "# **** 預計第一個小時結束 ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb7abfb-9d66-4ca8-96c7-2358b21d009e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from src.initialization import credential_init\n",
    "\n",
    "\n",
    "credential_init()\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o-2024-05-13\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aea1c49-906f-481c-9020-6b40cf07dba4",
   "metadata": {},
   "source": [
    "## Conversational Retrievers - Step 1\n",
    "\n",
    "- 土味情話反殺大全 (推薦上Youtube看)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1831d12-b64a-4d21-b30f-10cc421d3500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "df = pd.DataFrame(data=[[\"确认过眼神，你是我爱的人。\", \"确认过眼神，我是你泡不到的人。\"],\n",
    "                         [\"万水千山总是情，爱我多一点行不行。\", \"一寸光阴一寸金，劝你死了这条心。\"],\n",
    "                         [\"今天吃了泡面，吃了炒面，还是想走进你的心里面。\", \"吃那么多面，最后还不是变成大便。\"],\n",
    "                         [\"草莓，蓝莓，蔓越莓，今天你想我了没？\", \"冬瓜，西瓜，哈密瓜，你再巴巴我打得你叫妈妈。\"],\n",
    "                         [\"众生皆苦，唯你独甜。\", \"尝遍众生，你为渣男代言。\"],\n",
    "                         [\"你喜欢瑞士名表还是我帅气的外表？\", \"我喜欢去年买了个表。\"],\n",
    "                         [\"我想问一条路，到哥哥心里的路。\", \"山路十八弯，走完脑血栓。\"],\n",
    "                         [\"小姐姐，我心里给你留了一块地，死心塌地。\", \"对不起，我的心里只容得下一块地，玛莎拉蒂。\"],\n",
    "                         [\"小姐姐你笑起来真好看啊。\", \"你看起来真好笑啊。\"],\n",
    "                         [\"亲爱的你知道吗，你的笑容没有酒，我却醉得像条狗\", \"我的笑容没有酒，你是真的像条狗\"],\n",
    "                         [\"宝贝儿，我在手上划了一道口子，你也划一下吧，这样我们就是两口子了\", \"我怕我们的血溶到一起，被你发现其实我是你爸爸\"],\n",
    "                         [\"这世间万物都有尽头，落叶归根，而我归你\", \"对不起 我不收垃圾\"],\n",
    "                         [\"请问……我想问一下路，那条通往你心里的路\", \"八格牙路\"],\n",
    "                         [\"你今天怎么怪怪的？ 怪可爱的\",  \"你今天也怪怪的，怪恶心的\"],\n",
    "                         [\"亲爱的，你知道我和唐僧的区别吗？ 唐僧取经我娶你\", \"知道你和沙僧的区别吗？ 他叫沙僧你叫沙雕\"],\n",
    "                         [\"亲爱的，你不觉得累吗？ 你已经在我的脑海里跑了好几圈了\", \"傻孩子，我在找出口呢\"],\n",
    "                         [\"莫文蔚的阴天。孙燕姿的雨天，周杰伦的晴天，都不如你和我聊天\", \"求求你了，能否还我一个宁静的夏天\"],\n",
    "                         [\"如果你是方便面，那我就是白开水，今生今世，我泡定你了\", \"故事的最后，她变成了屎，你变成了尿，你们终究分道扬镳\"],\n",
    "                         [\"大年三十晚上的鞭炮再响，也没有我想你那么想\", \"大年三十晚上的鞭炮再响，也没有你放的屁响\"],\n",
    "                         [\"c罗可以上演帽子戏法，可我想你却没有办法\", \"c罗可以上演帽子戏法，我也可以给你上演绿帽子戏法\"],\n",
    "                         [\"不要抱怨，抱我\", \"抱不起来，太重\"],\n",
    "                         [\"你有没有发现我的眼睛很好看？因为我满眼都是你啊\", \"对不起，你眼睛在哪呢？\"]], \n",
    "                  columns=['input', 'output'])\n",
    "\n",
    "documents = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    documents.append(Document(page_content=row['input'], metadata={'output': row['output']}))\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=documents, embedding=embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\",\n",
    "                                     search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678da92e-5b69-44ad-87d6-ac08da7c2d8f",
   "metadata": {},
   "source": [
    "## Build Chat Chain - Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057ea8b7-d331-4aa1-9fc1-c94ae936b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate\n",
    "ChatPromptTemplate, SystemMessagePromptTemplate,  MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import chain, RunnablePassthrough\n",
    "\n",
    "\n",
    "def build_standard_chat_prompt_template_v3(kwargs):\n",
    "\n",
    "    messages = []\n",
    "    \n",
    "    for key in ['system', 'messages', 'human']:\n",
    "        if kwargs.get(key):\n",
    "            if key == 'system':\n",
    "                system_content = kwargs['system']\n",
    "                system_prompt = PromptTemplate(**system_content)\n",
    "                message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "            elif key == 'human':\n",
    "                human_content = kwargs['human']\n",
    "                human_prompt = PromptTemplate(**human_content)\n",
    "                message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "            else:\n",
    "                message = MessagesPlaceholder(variable_name=\"messages\")\n",
    "            \n",
    "            messages.append(message)\n",
    "    \n",
    "    chat_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "    return chat_prompt\n",
    "\n",
    "\n",
    "\n",
    "@chain\n",
    "def chatbot_prompt_fn(data):\n",
    "\n",
    "    system_template = \"\"\"\n",
    "                      You are a helpful AI assistant acting as if you hava rough day and you are now very grumpy. \n",
    "                      You will respond with the following style, cheesy pickup lines, \n",
    "                      shown in the context:\\n\\n{context}\\n\n",
    "                      You will reply in simplified Chinese (簡體中文).\n",
    "                      \"\"\"\n",
    "    \n",
    "    human_template = data['input']\n",
    "    \n",
    "    input_ = {\"system\": {\"template\": system_template},\n",
    "              \"human\": {\"template\": human_template},\n",
    "              \"messages\": None}\n",
    "    \n",
    "    prompt_template = build_standard_chat_prompt_template_v3(input_)\n",
    "\n",
    "    return prompt_template\n",
    "\n",
    "@chain\n",
    "def context_parser(documents):\n",
    "\n",
    "    context = \"\"\n",
    "\n",
    "    for idx, document in enumerate(documents):\n",
    "        context += f\"Example {idx}:\\nQuestion: {document.page_content}\\nAnswer: {document.metadata['output']}\\n\"\n",
    "    \n",
    "    return context\n",
    "    \n",
    "step_1 = RunnablePassthrough.assign(context=itemgetter(\"input\") | retriever | context_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81527914-12b5-4379-b789-050379d31fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_chat_history = ChatMessageHistory()\n",
    "\n",
    "step_1.invoke({\"input\": \"你有个超能力 我也有个超能力\",\n",
    "               \"messages\": demo_chat_history.messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b99f43f-8cc3-4978-8e33-7e545f8e6840",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_ = step_1 | chatbot_prompt_fn\n",
    "pipeline_.invoke({\"input\": \"你有个超能力 我也有个超能力\",\n",
    "                  \"messages\": demo_chat_history.messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe66889-70e3-4a1d-a52f-8e09a4e84ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_ = step_1 | chatbot_prompt_fn | model | StrOutputParser()\n",
    "pipeline_.invoke({\"input\": \"你有个超能力 我也有个超能力\",\n",
    "                  \"messages\": demo_chat_history.messages})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba4e8bf-51a8-4380-ae2e-a11ac4bece97",
   "metadata": {},
   "source": [
    "## Test - Step 3\n",
    "\n",
    "https://www.wenan.wang/qibaitiaotuweiqinghua.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47f3c63-e0bb-4265-9440-bc2efd71e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "\n",
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "while True:\n",
    "    question = input(\"Please input your question: \")\n",
    "\n",
    "    # if you want to quit\n",
    "    if question == \"QUIT\":\n",
    "        break\n",
    "    \n",
    "    answer = pipeline_.invoke({\"input\": question,\n",
    "                               \"message\": chat_history.messages\n",
    "                              })\n",
    "\n",
    "    print(answer)\n",
    "    \n",
    "    chat_history.add_user_message(question)\n",
    "    chat_history.add_ai_message(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee33106-dd02-41ba-a17d-e4266576c045",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52480f1e-9d44-405a-9531-5e6bfcf3848e",
   "metadata": {},
   "source": [
    "### 回家作業 2: 將retriever抽換成WikipediaRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85de3003-f33e-4600-810f-5609353273cb",
   "metadata": {},
   "source": [
    "基本上，你可以將這個retriever的內容抽換成任何你需要的資料，來加快寫報告的效率。記得Double Check...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081890b6-db8d-4bc8-b268-c687a546a41c",
   "metadata": {},
   "source": [
    "## Compress the chat history to reduce the size of the prompt\n",
    "\n",
    "\n",
    "https://github.com/langchain-ai/langserve/blob/main/examples/conversational_retrieval_chain/server.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0ae4c2-cff0-47eb-bce0-bcbc98ad63ec",
   "metadata": {},
   "source": [
    "### Condensation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070dd286-f421-4498-bf8d-9cdf22413617",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"\"\"\n",
    "                  Combine the following conversation and a follow up ***USER QUERY***, to generate \n",
    "                  a standalone query, in its original language.\n",
    "                  \"\"\"\n",
    "\n",
    "human_template = \"\"\"\n",
    "                 ***USER QUERY***: {input}\n",
    "                 \"\"\"\n",
    "input_ = {\"system\": {\"template\": system_template},\n",
    "          \"human\": {\"template\": human_template},\n",
    "          \"messages\": None}\n",
    "    \n",
    "condensed_prompt_template = build_standard_chat_prompt_template_v3(input_)\n",
    "\n",
    "condensed_pipeline = condensed_prompt_template | model | StrOutputParser()\n",
    "\n",
    "# condensed_chain = {\"question\": itemgetter(\"question\"),\n",
    "#                    \"messages\": itemgetter(\"message\")} | condensed_chat_template | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa743a53-3a7e-4f69-89da-48166613fa96",
   "metadata": {},
   "source": [
    "## Retrieval\n",
    "\n",
    "How to implement this properly?\n",
    "\n",
    "Let start from a higher point of view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f79c31-2752-4a06-8417-2facd3add92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "system_template = \"\"\"\n",
    "                  You are a helpful AI assistant acting as if you hava rough day and you are now very grumpy. \n",
    "                  You will respond with the following style, cheesy pickup lines, \n",
    "                  shown in the context:\\n\\n{context}\\n\n",
    "                  You will reply in simplified Chinese (簡體中文).\n",
    "                  \"\"\"\n",
    "    \n",
    "human_template = \"\"\"\n",
    "                 {standalone_question}\n",
    "                 \"\"\"\n",
    "\n",
    "input_ = {\"system\": {\"template\": system_template,\n",
    "                     \"input_variable\": [\"context\"]},\n",
    "          \"human\": {\"template\": human_template,\n",
    "                    \"input_variable\": [\"standalone_question\"]}\n",
    "         }\n",
    "\n",
    "chat_prompt_template = build_standard_chat_prompt_template_v2(input_)\n",
    "\n",
    "step_1 = RunnablePassthrough.assign(context=itemgetter(\"input\") | retriever | context_parser)\n",
    "\n",
    "retrieval_pipeline = step_1|RunnablePassthrough.assign(answer=chat_prompt_template|model|StrOutputParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacf954c-6d00-4f1f-8a4b-9b59c5a71155",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pipeline = RunnablePassthrough.assign(standalone_question=condensed_pipeline)|retrieval_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc2c17f-7b63-4b8d-863a-ff6fec15be07",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "final_pipeline.invoke({\"messages\": chat_history.messages,\n",
    "                       \"input\": \"你有个超能力 我也有个超能力\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507ef7a4-c0b8-4147-b54a-a497edd3f47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "while True:\n",
    "    question = input(\"Please input your question: \")\n",
    "\n",
    "    # if you want to quit\n",
    "    if question == \"QUIT\":\n",
    "        break\n",
    "    \n",
    "    output = final_pipeline.invoke({\"input\": question,\n",
    "                                    \"message\": chat_history.messages\n",
    "                                   })\n",
    "\n",
    "    print(output)\n",
    "    \n",
    "    answer = output['answer']\n",
    "    \n",
    "    chat_history.add_user_message(question)\n",
    "    chat_history.add_ai_message(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55cfa64-3691-4e11-b576-c55cd61269be",
   "metadata": {},
   "source": [
    "## OpenAI Model Fine-tuning\n",
    "\n",
    "剛好有人來找我家教這個，整理了一下。。。\n",
    "\n",
    "參考:\n",
    "\n",
    "- https://cookbook.openai.com/examples/chat_finetuning_data_prep\n",
    "\n",
    "- {\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]} {\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\"}]} {\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\"}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72b7d19-baa8-48eb-9365-39a62b37214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.io.path_definition import get_project_dir\n",
    "\n",
    "jsonl = []\n",
    "\n",
    "with open(os.path.join(get_project_dir(), 'tutorial', 'LLM+Langchain', 'Week-1', 'recipe_train.json'), 'r') as f:\n",
    "    recipe_train = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce35b10-7af5-4e18-a333-65d06d84cfe5",
   "metadata": {},
   "source": [
    "### Create a jsonl format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a36fff-6e74-4c92-bada-1d3f687323b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_df = pd.DataFrame(data=[[rec['id'], rec['cuisine'], \", \".join(rec['ingredients'])] \n",
    "                               for rec in recipe_train], columns=['id', 'cuisine', 'ingredients'])\n",
    "\n",
    "\"\"\"\n",
    "train-test split\n",
    "\n",
    "- 固定random state，確保數據的重現性\n",
    "- 使用分層抽樣(stratified sampling)，保證訓練-測試集的class分佈是一致的\n",
    "\"\"\"\n",
    "\n",
    "train, test = train_test_split(recipe_df, test_size=0.2, random_state=42, stratify=recipe_df['cuisine'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58d3c8d-1520-4146-b788-43f4459039ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_template = \"\"\"You are a helpful AI assistant as a chef of a Michellin 3 stars restaurant. You have extensive knowledge about cuisines \n",
    "                            all over the world, and you are able to identify the origin of a cuisine based on the ingredients. You are assigned with a \n",
    "                            task of identifying the origin, as region, of cuisine based on the <ingredients>.                            \n",
    "                            \"\"\"\n",
    "\n",
    "for _, row in train.iterrows():\n",
    "\n",
    "    ingredients = row['ingredients']\n",
    "    content = f\"ingredients: [{ingredients}]\"\n",
    "    \n",
    "    jsonl.append({\"messages\": [{\"role\": \"system\", \"content\": system_prompt_template}, \n",
    "                               {\"role\": \"user\", \"content\": content}, \n",
    "                               {\"role\": \"assistant\", \"content\": row['cuisine']}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1271c822-d888-4181-977b-52528f2534bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 寫入檔案\n",
    "\n",
    "with open('openapi_finetuning_test.jsonl', 'w') as outfile:\n",
    "    for entry in jsonl:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2900e815-0375-40c4-bea7-ad3029b79f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接使用OpenAI提供的API\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "\n",
    "client.files.create?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3e8787-ee23-4881-bac8-c20c8a0ef1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.files.create(file=open('openapi_finetuning_test.jsonl', 'rb'),\n",
    "                    purpose='fine-tune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adf281f-868a-469b-ad55-bb9ff6c174bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.fine_tuning.jobs.create?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a22bc9-d0f2-401d-a7cf-cd10929975a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.fine_tuning.jobs.create(model=\"gpt-4o-mini-2024-07-18\",\n",
    "                               training_file=\"file-SHpXusOxxMVGYRgwIGNj6mk9\",\n",
    "                               hyperparameters={\"batch_size\":4, \"learning_rate_multiplier\": 1e-6, \"n_epochs\": 5},\n",
    "                               suffix='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5b8282-603e-4f95-9ba7-3d17be3f5779",
   "metadata": {},
   "source": [
    "訓練完之後"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddd3e05-adba-4048-9057-51ca4938c8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"ft:gpt-4o-mini-2024-07-18:cosnova-account:test:ANPq9Weh\", \n",
    "                   temperature=0 # a range from 0-2, the higher the value, the higher the `creativity`\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98182db0-19de-44e1-a483-397f92e7f401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "\n",
    "system_prompt = PromptTemplate.from_template(system_prompt_template)\n",
    "\n",
    "system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "\n",
    "human_prompt = PromptTemplate(template=\"\"\"\n",
    "                                       ingredients: [{ingredients}]\n",
    "                                       \"\"\",\n",
    "                              input_variables=[\"ingredients\"]\n",
    "                              )\n",
    "\n",
    "human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                human_message\n",
    "                                                ])\n",
    "\n",
    "fine_tuned_chain = chat_prompt|model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf03979c-e472-4134-8945-2cef5cd1133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_chain.invoke({\"ingredients\": test.iloc[0]['ingredients']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d40de2-80a7-49e9-934b-c73da93b599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o-mini-2024-07-18\", \n",
    "                   temperature=0 # a range from 0-2, the higher the value, the higher the `creativity`\n",
    "                  )\n",
    "\n",
    "chain = chat_prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5952eae8-02ac-40d6-a8d3-de8a78538ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"ingredients\": test.iloc[0]['ingredients']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10c0ba8-e333-4487-867b-9b92d1e5d8f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
