{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fabc653-5ca9-484b-a864-0b6a363ac275",
   "metadata": {},
   "source": [
    "# 📖 Agent 實作範例：自動化有聲書生成\n",
    "\n",
    "本 Notebook 展示如何透過 Agent 串接 **故事 → 圖像 → 語音** 的自動化流程。  \n",
    "每個步驟都會將輸出保存為檔案，避免重複 Token 消耗，並方便後續流程使用。\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 整體流程\n",
    "1. 故事內容生成  \n",
    "2. 圖片內容生成  \n",
    "3. 語音內容生成  \n",
    "\n",
    "---\n",
    "\n",
    "## 1️⃣ 故事內容生成\n",
    "**輸入**  \n",
    "- 草稿文字  \n",
    "- 既有內容 (`.txt` 檔案)  \n",
    "\n",
    "**處理流程**  \n",
    "1. 將文字送入 Langserve 服務  \n",
    "2. 接收生成的故事段落  \n",
    "\n",
    "**輸出**  \n",
    "- 將結果存為 `.txt` 檔  \n",
    "- 避免 Agent 一直傳遞整份故事，降低 Token 消耗  \n",
    "\n",
    "---\n",
    "\n",
    "## 2️⃣ 圖片內容生成\n",
    "**輸入**  \n",
    "- 最新生成的故事文字  \n",
    "- 既有圖片 (`.png`)  \n",
    "\n",
    "**處理流程**  \n",
    "1. 將圖片編碼為 base64  \n",
    "2. 文字與圖片送入 Langserve 服務  \n",
    "3. 接收回傳的圖片（base64 格式）  \n",
    "\n",
    "**輸出**  \n",
    "- 將 base64 解碼為二進位資料，存成 `.png` 檔  \n",
    "\n",
    "---\n",
    "\n",
    "## 3️⃣ 語音內容生成\n",
    "**輸入**  \n",
    "- 最新生成的故事文字  \n",
    "\n",
    "**處理流程**  \n",
    "1. 將文字送入 Langserve 服務  \n",
    "2. 接收回傳的語音（base64 格式）  \n",
    "\n",
    "**輸出**  \n",
    "- 將 base64 解碼為二進位資料，存成 `.mp3` 或 `.wav` 檔  \n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 流程圖\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[故事草稿/舊內容 .txt] --> B[送入 Langserve 生成故事]\n",
    "    B --> C[故事內容 .txt]\n",
    "    C --> D[送入 Langserve 生成圖片 (base64)]\n",
    "    D --> E[解碼並存為 .png]\n",
    "    C --> F[送入 Langserve 生成語音 (base64)]\n",
    "    F --> G[解碼並存為 .mp3 / .wav]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28811758-cd03-42fc-a88f-1be3988b70fb",
   "metadata": {},
   "source": [
    "## LangServe 服務測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c95db07-6153-4ec9-863c-61a0a49a8288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "os.chdir(\"../../../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314b6765-7764-46ec-bc16-dfa8b5ed4c86",
   "metadata": {},
   "source": [
    "測試故事生成服務"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d168449-4550-49b8-acd2-21b4d3009efa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    \"http://localhost:8080/story_telling/invoke\",\n",
    "    json={\"input\":{'scratch': \"Create a chapter of a baby owl capturing a rodent in the night as his dinner\",\n",
    "                   'context': \"\"}\n",
    "         }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2882f0e-1aff-4e8f-b7f1-1c101683aa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "story_json = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ff1e2-79b3-4b5c-acc7-71fd70025553",
   "metadata": {},
   "outputs": [],
   "source": [
    "story_json['output']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3654b3-4cf9-4587-a4cd-b023b6e11613",
   "metadata": {},
   "source": [
    "測試影像生成服務"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174143dd-8f26-486d-9d7e-f22db96d5e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "image_generation_module = importlib.import_module(\"tutorial.LLM+Langchain.Week-8.logic.image_generation\")\n",
    "image_create_pipeline = image_generation_module.image_create_pipeline(image_generation_module.system_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f923fa-d406-482b-855b-b0e869afeada",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_image = requests.post(\n",
    "    \"http://localhost:8080/image_generation/invoke\",\n",
    "    json={\"input\": {'story': story_json['output'],\n",
    "                    \"image_io\": []}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1011d1-ebd2-47b3-9e7e-a1845ac0fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_image.json()['output']['nl_prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4f4f2a-2262-40cc-985a-86d4ecfd3f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "# Decode to bytes\n",
    "image_bytes = base64.b64decode(response_image.json()['output']['image_base64'])\n",
    "\n",
    "with open(\"tutorial/LLM+Langchain/Week-8/story_2_image.png\", \"wb\") as fh:\n",
    "    fh.write(image_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30494746-8671-4ece-81d8-83037a8a1075",
   "metadata": {},
   "source": [
    "測試生成後續後續的故事"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592e7b42-5213-41be-9ee8-f0bc0d069453",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_next_tory = requests.post(\n",
    "    \"http://localhost:8080/story_telling/invoke\",\n",
    "    json={\"input\":{'scratch': \"Create the next chapter following the context\",\n",
    "                   'context': story_json['output']}\n",
    "         }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe8ed59-f76c-4069-913a-8e6d482856ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_chapter = response_next_tory.json()['output']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57512583-d7ba-4593-9a9a-3eb67d71849b",
   "metadata": {},
   "source": [
    "根據故事和上一張圖片，產生出下一張圖片\n",
    "\n",
    "透過requests送出base64 string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951b9a93-38e3-46d5-a052-3cc958095de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    \"http://localhost:8080/image_generation/invoke\",\n",
    "    json={\"input\": {'story': next_chapter + f\"\\nPrevious image description:\\n\\n{response_image.json()['output']['nl_prompt']}\",\n",
    "                    # 'image_io': [response_image.json()['output']['image_base64']]\n",
    "                    'image_io': []\n",
    "                   }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0003108d-c97c-4c40-8db9-39594eae610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()['output']['nl_prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769f6c0b-6904-4e79-aa5c-05dffb0fac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "image_bytes = base64.b64decode(response.json()['output']['image_base64'])\n",
    "\n",
    "with open(\"tutorial/LLM+Langchain/Week-8/story_3_image.png\", \"wb\") as fh:\n",
    "    fh.write(image_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35819f5-9fa1-4dea-ad1e-1f04a98303bb",
   "metadata": {},
   "source": [
    "測試語音生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425c3044-274f-4b64-a310-aeb1a37780da",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    \"http://localhost:8080/audio_generation/invoke\",\n",
    "    json={\"input\": {'input': \"How are you doing?\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41234ade-3023-4889-91e1-a9c0230a8452",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_bytes = base64.b64decode(response.json()['output'])\n",
    "\n",
    "with open(\"tutorial/LLM+Langchain/Week-8/test_sample.mp3\", \"wb\") as f:\n",
    "    f.write(audio_bytes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62454159-038d-4e37-999b-ff81159fb7de",
   "metadata": {},
   "source": [
    "## 生成工具模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2768e64a-de75-4196-b7b5-640b189e1669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "from typing import Optional, Any, List, Tuple, Callable\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.tools import BaseTool\n",
    "from langchain_core.runnables import Runnable\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic import FilePath\n",
    "\n",
    "from src.initialization import credential_init\n",
    "\n",
    "\n",
    "class ToolTemplate(BaseTool):\n",
    "\n",
    "    \"\"\"\n",
    "    ToolTemplateHTTP: 一個專門用來呼叫 Langserve REST API 的 Agent Tool\n",
    "\n",
    "    - 使用 PydanticOutputParser 保證輸入格式正確\n",
    "    - 支援多欄位 input/output 處理器\n",
    "    - 對 API 呼叫加上錯誤處理\n",
    "    \"\"\"\n",
    "    \n",
    "    runnable: str = Field(..., description='The Langserve endpoint')\n",
    "    name: str\n",
    "    input_parser: PydanticOutputParser\n",
    "    description: str\n",
    "    input_data_processors: Optional[List[Tuple[str, Callable[[Any], Any]]]] = None\n",
    "    output_data_processors: Optional[List[Tuple[Optional[str], Callable[[Any], Any]]]] = None\n",
    "    \n",
    "    @classmethod\n",
    "    def create(cls, runnable: str, name: str, description_template: str,\n",
    "               input_parser: PydanticOutputParser, input_data_processors: Optional=None,\n",
    "               output_data_processors: Optional=None):\n",
    "\n",
    "        \"\"\"建立 Tool 實例，會自動把輸入格式需求加入 description\"\"\"\n",
    "        \n",
    "        input_format_instruction = input_parser.get_format_instructions()\n",
    "        \n",
    "        description = description_template.format(\n",
    "            input_format_instruction=input_format_instruction\n",
    "        )\n",
    "        \n",
    "        return cls(runnable=runnable, name=name, description=description,\n",
    "                   input_parser=input_parser, input_data_processors=input_data_processors,\n",
    "                   output_data_processors=output_data_processors)\n",
    "    \n",
    "    def _run(self, query: str):\n",
    "\n",
    "        \"\"\"執行 Tool，同步版本\"\"\"\n",
    "        \n",
    "        # 1. 驗證 & parse 輸入\n",
    "        try:\n",
    "            input_ = self.input_parser.parse(query)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to parse input with parser: {e}, query={query}\")\n",
    "        \n",
    "        runnable_inputs = input_.model_dump()\n",
    "\n",
    "        # 2. input processors（前處理）\n",
    "        if self.input_data_processors:\n",
    "            for field, fn in self.input_data_processors:\n",
    "                if field in runnable_inputs:\n",
    "                    runnable_inputs[field] = fn(runnable_inputs[field])\n",
    "            \n",
    "        # 3. 呼叫 Langserve REST API\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                str(self.runnable),\n",
    "                json={\"input\": runnable_inputs},\n",
    "                timeout=60,\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Langserve call failed: {e}, inputs={runnable_inputs}\")\n",
    "\n",
    "        if \"output\" not in result:\n",
    "            raise RuntimeError(f\"Invalid response format from Langserve: {result}\")\n",
    "        \n",
    "        output = result['output']\n",
    "\n",
    "        # 4. update the state varaibles:\n",
    "        for key in session_state.keys():\n",
    "            if key in output:\n",
    "                session_state[key] = output[key]\n",
    "        \n",
    "        # 5. output processors（後處理）\n",
    "        if self.output_data_processors:\n",
    "            for field, fn in self.output_data_processors:\n",
    "                if not field:\n",
    "                    fn(output, runnable_inputs['filename'])\n",
    "                else:\n",
    "                    fn(output[field], runnable_inputs['filename'])\n",
    "                    \n",
    "        # 預設回傳「檔名」如果有 filename，否則回傳輸出的字串\n",
    "        return runnable_inputs.get(\"filename\", output)\n",
    "\n",
    "    def _arun(self, query: str):\n",
    "\n",
    "        raise NotImplementedError(\"This tool does not support async\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7a7685-7318-4349-89a4-159e2879d88e",
   "metadata": {},
   "source": [
    "### State Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd71fca-8a1e-4842-acc9-572921972b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_state = {}\n",
    "\n",
    "session_state['nl_prompt'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e253ad6e-6831-4399-be92-59af97e4c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class StoryInput(BaseModel):\n",
    "    scratch: str = Field(description=dedent(\"\"\"\\\n",
    "                                            The draft, notes, or rough idea for the current page of the story.\n",
    "                                           （故事當前頁面的草稿、筆記或初步構想\n",
    "                                            \"\"\"))\n",
    "    context: List[FilePath] = Field(default_factory=list, description=dedent(\"\"\"\\\n",
    "                                                              A list of previously generated .txt files that contain story content.  \n",
    "                                                              Used to maintain narrative consistency and continuity across images.  \n",
    "                                                              先前生成的 .txt 檔案清單，其中包含故事內容。  \n",
    "                                                              用於保持影像生成過程中的敘事一致性與連貫性。\n",
    "                                                              \"\"\"))\n",
    "    filename: str = Field(..., description=dedent(\"\"\"\\\n",
    "                                             The file path where the generated story text will be saved.\n",
    "                                            （生成的故事文本將被儲存的檔案路徑）\n",
    "                                             \"\"\"))\n",
    "\n",
    "\n",
    "def export_to_txt(text, filename: Path):\n",
    "\n",
    "    dir_ = Path(filename).parent\n",
    "\n",
    "    if not os.path.isdir(dir_):\n",
    "        os.makedirs(dir_)\n",
    "    \n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text)\n",
    "\n",
    "\n",
    "def read_from_txt(filename) -> str:\n",
    "\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "def read_from_list_of_text(filenames) -> str:\n",
    "\n",
    "    return \"\\n\\n\".join([read_from_txt(f) for f in filenames])\n",
    "\n",
    "\n",
    "story_input_data_processors = [(\"context\", read_from_list_of_text)]\n",
    "\n",
    "story_output_data_processors = [(None, export_to_txt)]\n",
    "\n",
    "story_telling_tool = ToolTemplate.create(\n",
    "    runnable=\"http://localhost:8080/story_telling/invoke\",\n",
    "    name=\"Story generation tool\",\n",
    "    description_template=dedent(\"\"\"\\\n",
    "                                This tool is designed to generate a story one page at a time.\n",
    "                                Provide a draft or idea for the current page (`scratch`), along with \n",
    "                                the preceding story context stored as .txt files (`context`), \n",
    "                                and specify where the generated text should be saved (`filename`).\n",
    "                                    \n",
    "                                Input format: {input_format_instruction}\n",
    "                                \"\"\"\n",
    "                                ),\n",
    "    input_parser=PydanticOutputParser(pydantic_object=StoryInput),\n",
    "    output_data_processors = story_output_data_processors,\n",
    "    input_data_processors = story_input_data_processors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eacf5e2-200b-4a00-8cbd-202ee93ef350",
   "metadata": {},
   "source": [
    "### 影像生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc7ff16-8827-4c9d-a68c-35681e0e071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageInput(BaseModel):\n",
    "    story: str = Field(description=dedent(\"\"\"\\\n",
    "                                          The narrative or context used to generate the image prompt.  \n",
    "                                          故事情節或上下文，用於生成影像提示。\n",
    "                                          \"\"\")\n",
    "                      )\n",
    "    # FilePath ensures the input path exists and is a valid file\n",
    "    image_io: List[str] = Field([], description=dedent(\"\"\"\\\n",
    "                                                   Path to the previously generated image.  \n",
    "                                                   Used in img2img generation to maintain visual and texture consistency.  \n",
    "                                                   先前生成影像的路徑。  \n",
    "                                                   在 img2img 生成中用於保持視覺與材質的一致性。\n",
    "                                                   \"\"\")\n",
    "                                    )\n",
    "    # Path allows flexibility: file may not exist yet but must be a valid path object.\n",
    "    filename: str = Field(..., description=dedent(\"\"\"\\\n",
    "                                                  Destination file path where the generated image will be saved.  \n",
    "                                                  生成影像的儲存檔案路徑。\n",
    "                                                  \"\"\")\n",
    "                          )\n",
    "\n",
    "\n",
    "def image_to_base64(image_path):\n",
    "    \n",
    "    with Image.open(image_path) as image:\n",
    "        \n",
    "        # Save the Image to a Buffer\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=\"JPEG\")\n",
    "        \n",
    "        # Encode the Image to Base64\n",
    "        image_str = base64.b64encode(buffered.getvalue())\n",
    "    \n",
    "    return image_str.decode('utf-8')\n",
    "\n",
    "\n",
    "def image_to_base64_from_list(filenames) -> List[Optional[str]]:\n",
    "\n",
    "    # return [image_to_base64(f) for f in filenames]\n",
    "    return []\n",
    "\n",
    "\n",
    "def export_to_image(content, filename):\n",
    "    \n",
    "    dir_ = Path(filename).parent\n",
    "\n",
    "    if not os.path.isdir(dir_):\n",
    "        os.makedirs(dir_)\n",
    "    \n",
    "    image_bytes = base64.b64decode(content)\n",
    "    \n",
    "    with open(filename, \"wb\") as fh:\n",
    "        fh.write(image_bytes)\n",
    "\n",
    "\n",
    "def story_adaptation(story: str):\n",
    "\n",
    "    nl_prompt = session_state['nl_prompt']\n",
    "    \n",
    "    if nl_prompt:\n",
    "        story += f\"\\nPrevious image description:\\n\\n{nl_prompt}\"\n",
    "\n",
    "    print(f\"******\\n{story}\\n*******\")\n",
    "    \n",
    "    return story\n",
    "    \n",
    "\n",
    "\n",
    "image_output_data_processors = [('image_base64', export_to_image)]\n",
    "\n",
    "image_input_data_processors = [(\"story\", story_adaptation),\n",
    "                                (\"image_io\", image_to_base64_from_list)]\n",
    "\n",
    "image_tool = ToolTemplate.create(\n",
    "    runnable=\"http://localhost:8080/image_generation/invoke\",\n",
    "    name=\"Image generation tool\",\n",
    "    description_template=dedent(\"\"\"\\\n",
    "                                This tool is designed to generate an image to the correspoinding narrative.\n",
    "                                Provide a narrative (`story`), along with \n",
    "                                the preceding images stored as .png files (`image_io`), \n",
    "                                and specify where the generated text should be saved (`filename`).\n",
    "                                    \n",
    "                                Input format: {input_format_instruction}\n",
    "                                \"\"\"\n",
    "                                ),\n",
    "    input_parser=PydanticOutputParser(pydantic_object=ImageInput),\n",
    "    output_data_processors = image_output_data_processors,\n",
    "    input_data_processors = image_input_data_processors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eace21c-9917-4f4c-8a3e-7afc8765cc49",
   "metadata": {},
   "source": [
    "### 語音生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0fa546-d99b-4ae3-920e-0bf4963a2bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioInput(BaseModel):\n",
    "    input: str = Field(description=dedent(\"\"\"\\\n",
    "                                          The narrative or context used to generate the audio content with text to sound (TTS).  \n",
    "                                          故事情節或上下文，用於TTS文字轉語音。\n",
    "                                          \"\"\")\n",
    "                                    )\n",
    "    # Path allows flexibility: file may not exist yet but must be a valid path object.\n",
    "    filename: str = Field(..., description=dedent(\"\"\"\\\n",
    "                                                  Destination mp3 file path where the generated audio will be saved.  \n",
    "                                                  mp3語音檔的儲存檔案路徑。\n",
    "                                                  \"\"\")\n",
    "                         )\n",
    "\n",
    "def export_to_audio(content, filename):\n",
    "\n",
    "    dir_ = Path(filename).parent\n",
    "\n",
    "    if not os.path.isdir(dir_):\n",
    "        os.makedirs(dir_)\n",
    "    \n",
    "    audio_bytes = base64.b64decode(content)\n",
    "    \n",
    "    with open(filename, \"wb\") as fh:\n",
    "        fh.write(audio_bytes)\n",
    "\n",
    "\n",
    "audio_output_data_processors = [(None, export_to_audio)]\n",
    "\n",
    "audio_tool = ToolTemplate.create(\n",
    "    runnable=\"http://localhost:8080/audio_generation/invoke\",\n",
    "    name=\"Audio generation tool\",\n",
    "    description_template=dedent(\"\"\"\\\n",
    "                                This tool is designed to generate an .mp3 file to a corresponding narrative.\n",
    "                                Provide a narrative (`story`), along with \n",
    "                                and specify where the generated audio should be saved (`filename`).\n",
    "                                    \n",
    "                                Input format: {input_format_instruction}\n",
    "                                \"\"\"\n",
    "                                ),\n",
    "    input_parser = PydanticOutputParser(pydantic_object=AudioInput),\n",
    "    output_data_processors = audio_output_data_processors,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8281faeb-b553-47c7-9b03-b7f9b6c671e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "\n",
    "from src.agent.react_zero_shot import prompt_template as zero_shot_prompt_template\n",
    "\n",
    "prompt = PromptTemplate.from_template(zero_shot_prompt_template)\n",
    "\n",
    "tools = [story_telling_tool, image_tool, audio_tool]\n",
    "\n",
    "credential_init()\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o\", temperature=0, \n",
    "                  )\n",
    "\n",
    "zero_shot_agent = create_react_agent(\n",
    "    llm=model,\n",
    "    tools=tools,\n",
    "    prompt=prompt,\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(agent=zero_shot_agent, tools=tools, verbose=True, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60f298a-18b7-4b23-96a4-7757b5f1aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = dedent(\"\"\"\n",
    "Create a chapter of a baby owl capturing a rodent in the night as his dinner.\n",
    "After having the final answer, please create a corresponding image and a corresponding mp3 file.\n",
    "The saved image (.png), text (.txt), and audio (.mp3) should have same name in the folder `tutorial/LLM+Langchain/Week-8/story_test`\n",
    "\"\"\")\n",
    "\n",
    "agent_executor.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25bad6b-5459-440d-bf6f-4b4258abf2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7613df9-de89-4546-b738-323a7cc3a9c9",
   "metadata": {},
   "source": [
    "成功的生成了一頁的內容，Agent可以幫我們生成整個故事嗎?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89f39ff-aeb8-433f-b18b-e20b622e5739",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "         I want to create an 4 pages story for a child. He likes snow owl.\n",
    "         For each page, please create a corresponding image and record the story as an mp3.\n",
    "         After having the final answer, please create a corresponding image and record the story as an mp3. \n",
    "         The saved image and mp3 should have same name, following the structure of \n",
    "         <Page - idx>, with idx as a number starting from 1, in the folder `tutorial/LLM+Langchain/Week-8/story_automation`\n",
    "         \"\"\"\n",
    "\n",
    "agent_executor.invoke({\"input\": prompt})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3683842a-53ee-420d-9423-637ce9cd723a",
   "metadata": {},
   "source": [
    "## 🎧 互動式有聲書內容生成\n",
    "\n",
    "- 不一定需要完整的 **Agent 架構**，因為流程的每一步（故事 → 圖像 → 語音）都已經明確定義，能由使用者主動觸發。  \n",
    "- 可以直接基於 **聊天機器人** 的互動形式進行，每次輸入使用者的需求或指令後，系統依照指定步驟生成對應內容。  \n",
    "- 使用者可以在故事生成過程中即時調整方向，例如指定角色、情節走向或語氣，提升 **客製化體驗**。  \n",
    "- 這種互動方式非常適合 **語言學習** 場景：  \n",
    "  - 學習者能一邊閱讀故事、一邊聽有聲輸出  \n",
    "  - 可即時修改故事情節，產生更貼近學習需求的內容  \n",
    "  - 搭配圖片與語音，提升沉浸式學習效果  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adda8172-d8ae-4bf0-80be-14810309324a",
   "metadata": {},
   "source": [
    "# Langgraph\n",
    "\n",
    "在現代 LLM 應用開發中，**LangChain** 和 **LangGraph** 是兩個常被提及的工具，它們皆旨在簡化與大型語言模型的整合流程，但各自的設計理念和使用場景略有不同。\n",
    "\n",
    "## LangChain vs. LangGraph 比較\n",
    "\n",
    "| 特點 | LangChain | LangGraph |\n",
    "|------|-----------|-----------|\n",
    "| 架構風格 | 函式導向（Function-based） | 圖形導向（Graph-based） |\n",
    "| 核心用途 | 組合不同的工具與鏈（Chains）來處理自然語言任務 | 定義狀態轉換與流程控制的狀態機 |\n",
    "| 適合場景 | 線性流程、多工具串接、Agent任務 | 有狀態的工作流程、多步驟決策、動態流程 |\n",
    "| 控制流程能力 | 較弱，需透過程式邏輯控制 | 較強，內建狀態管理與動態流程切換 |\n",
    "| 易學程度 | 相對容易上手 | 初期需要理解狀態機模型 |\n",
    "| 通用性 | 通用性強、模組多 | 更適合複雜與長期任務管理 |\n",
    "\n",
    "## 為什麼選擇 LangGraph？\n",
    "\n",
    "LangGraph 建立在 LangChain 的基礎上，但提供了更明確的流程控制能力。透過「有向圖」的方式定義節點（Node）與狀態轉移（Edge），開發者可以更容易地設計出具備**記憶狀態**、**可回溯性**、**動態流程切換**的 LLM 應用。\n",
    "\n",
    "簡而言之，**LangChain** 更像是設計工具組合的「積木箱」，而 **LangGraph** 則是設計流程和邏輯的「流程編輯器」。\n",
    "\n",
    "---\n",
    "\n",
    "接下來，我們將透過實作一個 **計劃-行動代理人（Plan-Action Agent）** 的例子，深入探索 LangGraph 的設計理念與強大功能。\n",
    "\n",
    "\n",
    "## 什麼是計劃-行動代理人？\n",
    "\n",
    "計劃-行動代理人（Plan-Action Agent）是智能體的一種，它依賴於計劃生成和行動執行來達成目標。簡單來說，這種代理人會首先根據當前情況建立一個行動計劃，然後根據計劃逐步執行各項任務，直到達成預定的目標。\n",
    "\n",
    "計劃-行動代理人的工作流程通常包含以下幾個步驟：\n",
    "1. **觀察環境**：代理人會先收集當前的環境狀態信息。\n",
    "2. **計劃生成**：根據當前狀況，代理人會生成一個包含一系列行動的計劃。\n",
    "3. **行動執行**：代理人按照計劃中的步驟逐一執行，並在必要時調整計劃。\n",
    "4. **反饋與調整**：根據執行結果和環境變化，代理人會更新計劃，確保最終目標的達成。\n",
    "\n",
    "## Langgraph 中的計劃-行動代理人\n",
    "\n",
    "Langgraph 是一個強大的框架，它允許我們定義和實現多種形式的智能代理系統。在 Langgraph 中，計劃-行動代理人通常會利用圖論和計劃生成算法來選擇最佳的行動方案。通過簡單的接口和模塊化設計，Langgraph 使得構建和訓練這類代理人變得更加高效且可擴展。\n",
    "\n",
    "在接下來的教程中，我們將展示如何使用 Langgraph 構建和訓練一個計劃-行動代理人，並通過一些實際範例來說明其應用場景。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1856cc-79de-4822-b617-42d6c1b16df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../../\")\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "from src.initialization import credential_init\n",
    "\n",
    "credential_init()\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o-mini\", \n",
    "                   temperature=0\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931bc721-634e-410f-95f2-6efe391d2b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, List, Tuple, Union, Optional\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# The State\n",
    "\n",
    "class PlanExecute(TypedDict):\n",
    "    input: str\n",
    "    plan: List[str]\n",
    "    past_steps: List[Tuple]\n",
    "    response: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a9d27-84a2-4237-b99b-883458533bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Literal, Union\n",
    "from textwrap import dedent\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"\n",
    "    Plan to follow in the future\n",
    "    \"\"\"\n",
    "    type: Literal['plan'] = 'plan'\n",
    "    steps: List[str] = Field(description=\"different steps to follow, should be in sorted order\")\n",
    "\n",
    "\n",
    "planner_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\n",
    "        dedent(\"\"\"\\\n",
    "        For the given objective, come up with a simple step by step plan.\n",
    "        This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps.\n",
    "        The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "        \"\"\")),\n",
    "        (\"placeholder\", \"{messages}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_planner = model.with_structured_output(Plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899b7f45-cc73-419d-9758-3d7c062029e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = planner_prompt|model_planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92635c74-5dfa-48ed-a421-66da1310d7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "planner.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\"user\", \"What is the hometown of the current Australia open winner\")\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db72109-c583-4f59-8db8-44118c619f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Response(BaseModel):\n",
    "#     \"\"\"Response to user.\"\"\"\n",
    "#     type: Literal[\"response\"] = \"response\"\n",
    "#     response: str = Field(descirption=\"The answer to the objective\")\n",
    "\n",
    "\n",
    "# class Act(BaseModel):\n",
    "#     \"\"\"Action to Perform.\"\"\"\n",
    "\n",
    "#     response = Optional[str] = Field(None, description=\"Direct response to the user.\")\n",
    "#     steps = Optional[List[str]] = Field(None, description=\"Plan steps to follow, in sorted order\")\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"Response to user.\"\"\"\n",
    "    type: Literal['response'] = 'response'\n",
    "    response: str\n",
    "\n",
    "\n",
    "class Act(BaseModel):\n",
    "    \"\"\"Action to perform.\"\"\"\n",
    "\n",
    "    action: Union[Response, Plan] = Field(\n",
    "        description=dedent(\"\"\"\\\n",
    "        Action to perform. If you want to respond to user, use `response`.\n",
    "        If you need to further use tools to get the answer, use `plan`.\n",
    "        \"\"\"\n",
    "    )\n",
    "    )\n",
    "\n",
    "replanner_prompt = ChatPromptTemplate.from_template(\n",
    "    dedet(\"\"\"For the given objective, come up with a simple step by step plan. \\\n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "\n",
    "Your objective was this:\n",
    "{input}\n",
    "\n",
    "Your original plan was this:\n",
    "{plan}\n",
    "\n",
    "You have currently done the follow steps:\n",
    "{past_steps}\n",
    "\n",
    "Update your plan accordingly. If no more steps are needed and you can return to the user, then respond with that. Otherwise, fill out the plan. Only add steps to the plan that still NEED to be done. Do not return previously done steps as part of the plan.\"\"\"\n",
    "))\n",
    "\n",
    "model_act = model.with_structured_output(Act)\n",
    "\n",
    "replanner = replanner_prompt | model_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484482cd-3357-4f15-9c23-0225528edcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "replanner.invoke({\"plan\": \"Identify the current Australian Open winner for the most recent tournament.\",\n",
    "                  \"past_steps\": [],\n",
    "                  \"input\": \"What is the hometown of the current Australia open winner\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaecd86e-f890-45ac-9648-c182d650ba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eddea48-2237-45cc-afb6-b6a825df4cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_parser = PydanticOutputParser(pydantic_object=Inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6bd8da-c262-4da6-b566-6a62adb0e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f86669-8576-407b-8bb7-4030e8087ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from openai import OpenAI\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "class Inputs(BaseModel):\n",
    "    query: str = Field(description=\"User query\")\n",
    "\n",
    "\n",
    "class SearchTool(BaseTool):\n",
    "\n",
    "    input_parser: PydanticOutputParser = PydanticOutputParser(pydantic_object=Inputs)\n",
    "    input_format_instructions: str = input_parser.get_format_instructions()\n",
    "    \n",
    "    name:str = \"websearch tool\"\n",
    "    description_template: str = dedent(\"\"\"\n",
    "    Currently it is 2025.    \n",
    "    Use this tool to collect information from the internet, when you are not sure you know the answer.\n",
    "    Action Input format instructions: {input_format_instructions}\n",
    "    \"\"\")\n",
    "\n",
    "    description: str = description_template.format(input_format_instructions=input_format_instructions)\n",
    "    \n",
    "    def _run(self, query):\n",
    "        \n",
    "        input_ = self.input_parser.parse(query)\n",
    "        \n",
    "        query = input_.query\n",
    "        \n",
    "        messages = [{\"role\": \"user\",\n",
    "                     \"content\": query}]\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model='gpt-4o-mini-search-preview',\n",
    "            web_search_options={\"search_context_size\": 'medium',\n",
    "                                \"user_location\": {\n",
    "                                        \"type\": \"approximate\",\n",
    "                                        \"approximate\": {\n",
    "                                            \"country\": \"TW\",\n",
    "                                        }\n",
    "                                    },\n",
    "                                },\n",
    "            messages=messages\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    async def _arun(self, query: str):\n",
    "        \"\"\"Asynchronous version of the run method.\"\"\"\n",
    "        input_ = self.input_parser.parse(query)\n",
    "        query = input_.query\n",
    "        \n",
    "        messages = [{\"role\": \"user\", \"content\": query}]\n",
    "        \n",
    "        # Run the synchronous API call in an asynchronous way using asyncio.to_thread\n",
    "        response = await asyncio.to_thread(\n",
    "            client.chat.completions.create,\n",
    "            model='gpt-4o-mini-search-preview',\n",
    "            web_search_options={\n",
    "                \"search_context_size\": 'medium',\n",
    "                \"user_location\": {\n",
    "                    \"type\": \"approximate\",\n",
    "                    \"approximate\": {\n",
    "                        \"country\": \"TW\",\n",
    "                    }\n",
    "                },\n",
    "            },\n",
    "            messages=messages\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc969039-04eb-4b23-8b06-a149ad764a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_template = dedent(\"\"\"\n",
    "Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "\n",
    "Thought: you should always think about what to do\n",
    "\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "\n",
    "Action Input: the input to the action\n",
    "\n",
    "Observation: the result of the action\n",
    "\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "\n",
    "Thought: I now know the final answer\n",
    "\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Previous conversation history:\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Thought:{agent_scratchpad}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "agent_prompt_template = PromptTemplate.from_template(agent_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d394e5ab-4ec7-412b-bd27-3eb73c3623e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langgraph.graph import END\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "# from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "tools = [SearchTool()]\n",
    "agent_model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "\n",
    "# Choose the LLM that will drive the agent\n",
    "zero_shot_agent = create_react_agent(llm=agent_model, tools=tools, prompt=agent_prompt_template)\n",
    "agent_executor = AgentExecutor(agent=zero_shot_agent, tools=tools, verbose=True)\n",
    "\n",
    "async def execute_step(state: PlanExecute):\n",
    "    plan = state[\"plan\"]\n",
    "    plan_str = \"\\n\".join(f\"{i + 1}. {step}\" for i, step in enumerate(plan))\n",
    "    task = plan[0]\n",
    "    task_formatted = f\"\"\"For the following plan:\n",
    "{plan_str}\\n\\nYou are tasked with executing step {1}, {task}.\"\"\"\n",
    "    \n",
    "    agent_response = await agent_executor.ainvoke(\n",
    "        {\"input\": task_formatted}\n",
    "    )\n",
    "\n",
    "    current_step = [(task, agent_response['output'])]\n",
    "    \n",
    "    if \"past_steps\" not in state:\n",
    "        return {\"past_steps\": current_step}\n",
    "    else:\n",
    "        return {\"past_steps\": state[\"past_steps\"] + current_step}\n",
    "\n",
    "\n",
    "async def plan_step(state: PlanExecute):\n",
    "    plan = await planner.ainvoke({\"messages\": [(\"user\", state[\"input\"])]})\n",
    "    return {\"plan\": plan.steps}\n",
    "\n",
    "\n",
    "async def replan_step(state: PlanExecute):\n",
    "    output = await replanner.ainvoke(state)\n",
    "    print(\"*************************\")\n",
    "    print(output)\n",
    "    print(\"*************************\")\n",
    "    if isinstance(output.action, Response):\n",
    "        return {\"response\": output.action.response}\n",
    "    else:\n",
    "        if output.action.steps:\n",
    "            return {\"plan\": output.action.steps}\n",
    "        else:\n",
    "            return {\"response\": state['past_steps'][-1][1]}\n",
    "\n",
    "\n",
    "def should_end(state: PlanExecute):\n",
    "    if \"response\" in state and state[\"response\"]:\n",
    "        return END\n",
    "    else:\n",
    "        return \"agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ac6db2-e3ce-499a-94fe-77bf5b9eede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "workflow = StateGraph(PlanExecute)\n",
    "\n",
    "# Add the plan node\n",
    "workflow.add_node(\"planner\", plan_step)\n",
    "\n",
    "# Add the execution step\n",
    "workflow.add_node(\"agent\", execute_step)\n",
    "\n",
    "# Add a replan node\n",
    "workflow.add_node(\"replan\", replan_step)\n",
    "\n",
    "workflow.add_edge(START, \"planner\")\n",
    "\n",
    "# From plan we go to agent\n",
    "workflow.add_edge(\"planner\", \"agent\")\n",
    "\n",
    "# From agent, we replan\n",
    "workflow.add_edge(\"agent\", \"replan\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"replan\",\n",
    "    should_end,\n",
    "    [\"agent\", END]\n",
    ")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20752f84-b2b2-4e21-a791-9daaa2d6af34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(app.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79a1255-a90c-43d7-a2f8-7330fe15d71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"recursion_limit\": 50}\n",
    "inputs = {\"input\": \"what is the hometown of the mens 2023 Australia open winner?\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22f0040-bcf8-463c-9e92-b32da959e533",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for event in app.astream(inputs, config=config):\n",
    "    for k, v in event.items():\n",
    "        if k != \"__end__\":\n",
    "            print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a29ef6-8697-47bf-a461-aae3989d56c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7f25ad-9e27-41d1-bdbb-f5fbcc4d6e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
