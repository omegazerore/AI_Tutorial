{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fabc653-5ca9-484b-a864-0b6a363ac275",
   "metadata": {},
   "source": [
    "# ğŸ“– Agent å¯¦ä½œç¯„ä¾‹ï¼šè‡ªå‹•åŒ–æœ‰è²æ›¸ç”Ÿæˆ\n",
    "\n",
    "æœ¬ Notebook å±•ç¤ºå¦‚ä½•é€é Agent ä¸²æ¥ **æ•…äº‹ â†’ åœ–åƒ â†’ èªéŸ³** çš„è‡ªå‹•åŒ–æµç¨‹ã€‚  \n",
    "æ¯å€‹æ­¥é©Ÿéƒ½æœƒå°‡è¼¸å‡ºä¿å­˜ç‚ºæª”æ¡ˆï¼Œé¿å…é‡è¤‡ Token æ¶ˆè€—ï¼Œä¸¦æ–¹ä¾¿å¾ŒçºŒæµç¨‹ä½¿ç”¨ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ æ•´é«”æµç¨‹\n",
    "1. æ•…äº‹å…§å®¹ç”Ÿæˆ  \n",
    "2. åœ–ç‰‡å…§å®¹ç”Ÿæˆ  \n",
    "3. èªéŸ³å…§å®¹ç”Ÿæˆ  \n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£ æ•…äº‹å…§å®¹ç”Ÿæˆ\n",
    "**è¼¸å…¥**  \n",
    "- è‰ç¨¿æ–‡å­—  \n",
    "- æ—¢æœ‰å…§å®¹ (`.txt` æª”æ¡ˆ)  \n",
    "\n",
    "**è™•ç†æµç¨‹**  \n",
    "1. å°‡æ–‡å­—é€å…¥ Langserve æœå‹™  \n",
    "2. æ¥æ”¶ç”Ÿæˆçš„æ•…äº‹æ®µè½  \n",
    "\n",
    "**è¼¸å‡º**  \n",
    "- å°‡çµæœå­˜ç‚º `.txt` æª”  \n",
    "- é¿å… Agent ä¸€ç›´å‚³éæ•´ä»½æ•…äº‹ï¼Œé™ä½ Token æ¶ˆè€—  \n",
    "\n",
    "---\n",
    "\n",
    "## 2ï¸âƒ£ åœ–ç‰‡å…§å®¹ç”Ÿæˆ\n",
    "**è¼¸å…¥**  \n",
    "- æœ€æ–°ç”Ÿæˆçš„æ•…äº‹æ–‡å­—  \n",
    "- æ—¢æœ‰åœ–ç‰‡ (`.png`)  \n",
    "\n",
    "**è™•ç†æµç¨‹**  \n",
    "1. å°‡åœ–ç‰‡ç·¨ç¢¼ç‚º base64  \n",
    "2. æ–‡å­—èˆ‡åœ–ç‰‡é€å…¥ Langserve æœå‹™  \n",
    "3. æ¥æ”¶å›å‚³çš„åœ–ç‰‡ï¼ˆbase64 æ ¼å¼ï¼‰  \n",
    "\n",
    "**è¼¸å‡º**  \n",
    "- å°‡ base64 è§£ç¢¼ç‚ºäºŒé€²ä½è³‡æ–™ï¼Œå­˜æˆ `.png` æª”  \n",
    "\n",
    "---\n",
    "\n",
    "## 3ï¸âƒ£ èªéŸ³å…§å®¹ç”Ÿæˆ\n",
    "**è¼¸å…¥**  \n",
    "- æœ€æ–°ç”Ÿæˆçš„æ•…äº‹æ–‡å­—  \n",
    "\n",
    "**è™•ç†æµç¨‹**  \n",
    "1. å°‡æ–‡å­—é€å…¥ Langserve æœå‹™  \n",
    "2. æ¥æ”¶å›å‚³çš„èªéŸ³ï¼ˆbase64 æ ¼å¼ï¼‰  \n",
    "\n",
    "**è¼¸å‡º**  \n",
    "- å°‡ base64 è§£ç¢¼ç‚ºäºŒé€²ä½è³‡æ–™ï¼Œå­˜æˆ `.mp3` æˆ– `.wav` æª”  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”„ æµç¨‹åœ–\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[æ•…äº‹è‰ç¨¿/èˆŠå…§å®¹ .txt] --> B[é€å…¥ Langserve ç”Ÿæˆæ•…äº‹]\n",
    "    B --> C[æ•…äº‹å…§å®¹ .txt]\n",
    "    C --> D[é€å…¥ Langserve ç”Ÿæˆåœ–ç‰‡ (base64)]\n",
    "    D --> E[è§£ç¢¼ä¸¦å­˜ç‚º .png]\n",
    "    C --> F[é€å…¥ Langserve ç”ŸæˆèªéŸ³ (base64)]\n",
    "    F --> G[è§£ç¢¼ä¸¦å­˜ç‚º .mp3 / .wav]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28811758-cd03-42fc-a88f-1be3988b70fb",
   "metadata": {},
   "source": [
    "## LangServe æœå‹™æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c95db07-6153-4ec9-863c-61a0a49a8288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "os.chdir(\"../../../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314b6765-7764-46ec-bc16-dfa8b5ed4c86",
   "metadata": {},
   "source": [
    "æ¸¬è©¦æ•…äº‹ç”Ÿæˆæœå‹™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d168449-4550-49b8-acd2-21b4d3009efa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    \"http://localhost:8080/story_telling/invoke\",\n",
    "    json={\"input\":{'scratch': \"Create a chapter of a baby owl capturing a rodent in the night as his dinner\",\n",
    "                   'context': \"\"}\n",
    "         }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2882f0e-1aff-4e8f-b7f1-1c101683aa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "story_json = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ff1e2-79b3-4b5c-acc7-71fd70025553",
   "metadata": {},
   "outputs": [],
   "source": [
    "story_json['output']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3654b3-4cf9-4587-a4cd-b023b6e11613",
   "metadata": {},
   "source": [
    "æ¸¬è©¦å½±åƒç”Ÿæˆæœå‹™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174143dd-8f26-486d-9d7e-f22db96d5e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "image_generation_module = importlib.import_module(\"tutorial.LLM+Langchain.Week-8.logic.image_generation\")\n",
    "image_create_pipeline = image_generation_module.image_create_pipeline(image_generation_module.system_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f923fa-d406-482b-855b-b0e869afeada",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_image = requests.post(\n",
    "    \"http://localhost:8080/image_generation/invoke\",\n",
    "    json={\"input\": {'story': story_json['output'],\n",
    "                    \"image_io\": []}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1011d1-ebd2-47b3-9e7e-a1845ac0fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_image.json()['output']['nl_prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4f4f2a-2262-40cc-985a-86d4ecfd3f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "# Decode to bytes\n",
    "image_bytes = base64.b64decode(response_image.json()['output']['image_base64'])\n",
    "\n",
    "with open(\"tutorial/LLM+Langchain/Week-8/story_2_image.png\", \"wb\") as fh:\n",
    "    fh.write(image_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30494746-8671-4ece-81d8-83037a8a1075",
   "metadata": {},
   "source": [
    "æ¸¬è©¦ç”Ÿæˆå¾ŒçºŒå¾ŒçºŒçš„æ•…äº‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592e7b42-5213-41be-9ee8-f0bc0d069453",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_next_tory = requests.post(\n",
    "    \"http://localhost:8080/story_telling/invoke\",\n",
    "    json={\"input\":{'scratch': \"Create the next chapter following the context\",\n",
    "                   'context': story_json['output']}\n",
    "         }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe8ed59-f76c-4069-913a-8e6d482856ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_chapter = response_next_tory.json()['output']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57512583-d7ba-4593-9a9a-3eb67d71849b",
   "metadata": {},
   "source": [
    "æ ¹æ“šæ•…äº‹å’Œä¸Šä¸€å¼µåœ–ç‰‡ï¼Œç”¢ç”Ÿå‡ºä¸‹ä¸€å¼µåœ–ç‰‡\n",
    "\n",
    "é€érequestsé€å‡ºbase64 string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951b9a93-38e3-46d5-a052-3cc958095de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    \"http://localhost:8080/image_generation/invoke\",\n",
    "    json={\"input\": {'story': next_chapter + f\"\\nPrevious image description:\\n\\n{response_image.json()['output']['nl_prompt']}\",\n",
    "                    # 'image_io': [response_image.json()['output']['image_base64']]\n",
    "                    'image_io': []\n",
    "                   }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0003108d-c97c-4c40-8db9-39594eae610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()['output']['nl_prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769f6c0b-6904-4e79-aa5c-05dffb0fac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "image_bytes = base64.b64decode(response.json()['output']['image_base64'])\n",
    "\n",
    "with open(\"tutorial/LLM+Langchain/Week-8/story_3_image.png\", \"wb\") as fh:\n",
    "    fh.write(image_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35819f5-9fa1-4dea-ad1e-1f04a98303bb",
   "metadata": {},
   "source": [
    "æ¸¬è©¦èªéŸ³ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425c3044-274f-4b64-a310-aeb1a37780da",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    \"http://localhost:8080/audio_generation/invoke\",\n",
    "    json={\"input\": {'input': \"How are you doing?\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41234ade-3023-4889-91e1-a9c0230a8452",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_bytes = base64.b64decode(response.json()['output'])\n",
    "\n",
    "with open(\"tutorial/LLM+Langchain/Week-8/test_sample.mp3\", \"wb\") as f:\n",
    "    f.write(audio_bytes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62454159-038d-4e37-999b-ff81159fb7de",
   "metadata": {},
   "source": [
    "## ç”Ÿæˆå·¥å…·æ¨¡æ¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2768e64a-de75-4196-b7b5-640b189e1669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "from typing import Optional, Any, List, Tuple, Callable\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.tools import BaseTool\n",
    "from langchain_core.runnables import Runnable\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic import FilePath\n",
    "\n",
    "from src.initialization import credential_init\n",
    "\n",
    "\n",
    "class ToolTemplate(BaseTool):\n",
    "\n",
    "    \"\"\"\n",
    "    ToolTemplateHTTP: ä¸€å€‹å°ˆé–€ç”¨ä¾†å‘¼å« Langserve REST API çš„ Agent Tool\n",
    "\n",
    "    - ä½¿ç”¨ PydanticOutputParser ä¿è­‰è¼¸å…¥æ ¼å¼æ­£ç¢º\n",
    "    - æ”¯æ´å¤šæ¬„ä½ input/output è™•ç†å™¨\n",
    "    - å° API å‘¼å«åŠ ä¸ŠéŒ¯èª¤è™•ç†\n",
    "    \"\"\"\n",
    "    \n",
    "    runnable: str = Field(..., description='The Langserve endpoint')\n",
    "    name: str\n",
    "    input_parser: PydanticOutputParser\n",
    "    description: str\n",
    "    input_data_processors: Optional[List[Tuple[str, Callable[[Any], Any]]]] = None\n",
    "    output_data_processors: Optional[List[Tuple[Optional[str], Callable[[Any], Any]]]] = None\n",
    "    \n",
    "    @classmethod\n",
    "    def create(cls, runnable: str, name: str, description_template: str,\n",
    "               input_parser: PydanticOutputParser, input_data_processors: Optional=None,\n",
    "               output_data_processors: Optional=None):\n",
    "\n",
    "        \"\"\"å»ºç«‹ Tool å¯¦ä¾‹ï¼Œæœƒè‡ªå‹•æŠŠè¼¸å…¥æ ¼å¼éœ€æ±‚åŠ å…¥ description\"\"\"\n",
    "        \n",
    "        input_format_instruction = input_parser.get_format_instructions()\n",
    "        \n",
    "        description = description_template.format(\n",
    "            input_format_instruction=input_format_instruction\n",
    "        )\n",
    "        \n",
    "        return cls(runnable=runnable, name=name, description=description,\n",
    "                   input_parser=input_parser, input_data_processors=input_data_processors,\n",
    "                   output_data_processors=output_data_processors)\n",
    "    \n",
    "    def _run(self, query: str):\n",
    "\n",
    "        \"\"\"åŸ·è¡Œ Toolï¼ŒåŒæ­¥ç‰ˆæœ¬\"\"\"\n",
    "        \n",
    "        # 1. é©—è­‰ & parse è¼¸å…¥\n",
    "        try:\n",
    "            input_ = self.input_parser.parse(query)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to parse input with parser: {e}, query={query}\")\n",
    "        \n",
    "        runnable_inputs = input_.model_dump()\n",
    "\n",
    "        # 2. input processorsï¼ˆå‰è™•ç†ï¼‰\n",
    "        if self.input_data_processors:\n",
    "            for field, fn in self.input_data_processors:\n",
    "                if field in runnable_inputs:\n",
    "                    runnable_inputs[field] = fn(runnable_inputs[field])\n",
    "            \n",
    "        # 3. å‘¼å« Langserve REST API\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                str(self.runnable),\n",
    "                json={\"input\": runnable_inputs},\n",
    "                timeout=60,\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Langserve call failed: {e}, inputs={runnable_inputs}\")\n",
    "\n",
    "        if \"output\" not in result:\n",
    "            raise RuntimeError(f\"Invalid response format from Langserve: {result}\")\n",
    "        \n",
    "        output = result['output']\n",
    "\n",
    "        # 4. update the state varaibles:\n",
    "        for key in session_state.keys():\n",
    "            if key in output:\n",
    "                session_state[key] = output[key]\n",
    "        \n",
    "        # 5. output processorsï¼ˆå¾Œè™•ç†ï¼‰\n",
    "        if self.output_data_processors:\n",
    "            for field, fn in self.output_data_processors:\n",
    "                if not field:\n",
    "                    fn(output, runnable_inputs['filename'])\n",
    "                else:\n",
    "                    fn(output[field], runnable_inputs['filename'])\n",
    "                    \n",
    "        # é è¨­å›å‚³ã€Œæª”åã€å¦‚æœæœ‰ filenameï¼Œå¦å‰‡å›å‚³è¼¸å‡ºçš„å­—ä¸²\n",
    "        return runnable_inputs.get(\"filename\", output)\n",
    "\n",
    "    def _arun(self, query: str):\n",
    "\n",
    "        raise NotImplementedError(\"This tool does not support async\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7a7685-7318-4349-89a4-159e2879d88e",
   "metadata": {},
   "source": [
    "### State Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd71fca-8a1e-4842-acc9-572921972b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_state = {}\n",
    "\n",
    "session_state['nl_prompt'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e253ad6e-6831-4399-be92-59af97e4c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class StoryInput(BaseModel):\n",
    "    scratch: str = Field(description=dedent(\"\"\"\\\n",
    "                                            The draft, notes, or rough idea for the current page of the story.\n",
    "                                           ï¼ˆæ•…äº‹ç•¶å‰é é¢çš„è‰ç¨¿ã€ç­†è¨˜æˆ–åˆæ­¥æ§‹æƒ³\n",
    "                                            \"\"\"))\n",
    "    context: List[FilePath] = Field(default_factory=list, description=dedent(\"\"\"\\\n",
    "                                                              A list of previously generated .txt files that contain story content.  \n",
    "                                                              Used to maintain narrative consistency and continuity across images.  \n",
    "                                                              å…ˆå‰ç”Ÿæˆçš„ .txt æª”æ¡ˆæ¸…å–®ï¼Œå…¶ä¸­åŒ…å«æ•…äº‹å…§å®¹ã€‚  \n",
    "                                                              ç”¨æ–¼ä¿æŒå½±åƒç”Ÿæˆéç¨‹ä¸­çš„æ•˜äº‹ä¸€è‡´æ€§èˆ‡é€£è²«æ€§ã€‚\n",
    "                                                              \"\"\"))\n",
    "    filename: str = Field(..., description=dedent(\"\"\"\\\n",
    "                                             The file path where the generated story text will be saved.\n",
    "                                            ï¼ˆç”Ÿæˆçš„æ•…äº‹æ–‡æœ¬å°‡è¢«å„²å­˜çš„æª”æ¡ˆè·¯å¾‘ï¼‰\n",
    "                                             \"\"\"))\n",
    "\n",
    "\n",
    "def export_to_txt(text, filename: Path):\n",
    "\n",
    "    dir_ = Path(filename).parent\n",
    "\n",
    "    if not os.path.isdir(dir_):\n",
    "        os.makedirs(dir_)\n",
    "    \n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text)\n",
    "\n",
    "\n",
    "def read_from_txt(filename) -> str:\n",
    "\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "def read_from_list_of_text(filenames) -> str:\n",
    "\n",
    "    return \"\\n\\n\".join([read_from_txt(f) for f in filenames])\n",
    "\n",
    "\n",
    "story_input_data_processors = [(\"context\", read_from_list_of_text)]\n",
    "\n",
    "story_output_data_processors = [(None, export_to_txt)]\n",
    "\n",
    "story_telling_tool = ToolTemplate.create(\n",
    "    runnable=\"http://localhost:8080/story_telling/invoke\",\n",
    "    name=\"Story generation tool\",\n",
    "    description_template=dedent(\"\"\"\\\n",
    "                                This tool is designed to generate a story one page at a time.\n",
    "                                Provide a draft or idea for the current page (`scratch`), along with \n",
    "                                the preceding story context stored as .txt files (`context`), \n",
    "                                and specify where the generated text should be saved (`filename`).\n",
    "                                    \n",
    "                                Input format: {input_format_instruction}\n",
    "                                \"\"\"\n",
    "                                ),\n",
    "    input_parser=PydanticOutputParser(pydantic_object=StoryInput),\n",
    "    output_data_processors = story_output_data_processors,\n",
    "    input_data_processors = story_input_data_processors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eacf5e2-200b-4a00-8cbd-202ee93ef350",
   "metadata": {},
   "source": [
    "### å½±åƒç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc7ff16-8827-4c9d-a68c-35681e0e071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageInput(BaseModel):\n",
    "    story: str = Field(description=dedent(\"\"\"\\\n",
    "                                          The narrative or context used to generate the image prompt.  \n",
    "                                          æ•…äº‹æƒ…ç¯€æˆ–ä¸Šä¸‹æ–‡ï¼Œç”¨æ–¼ç”Ÿæˆå½±åƒæç¤ºã€‚\n",
    "                                          \"\"\")\n",
    "                      )\n",
    "    # FilePath ensures the input path exists and is a valid file\n",
    "    image_io: List[str] = Field([], description=dedent(\"\"\"\\\n",
    "                                                   Path to the previously generated image.  \n",
    "                                                   Used in img2img generation to maintain visual and texture consistency.  \n",
    "                                                   å…ˆå‰ç”Ÿæˆå½±åƒçš„è·¯å¾‘ã€‚  \n",
    "                                                   åœ¨ img2img ç”Ÿæˆä¸­ç”¨æ–¼ä¿æŒè¦–è¦ºèˆ‡æè³ªçš„ä¸€è‡´æ€§ã€‚\n",
    "                                                   \"\"\")\n",
    "                                    )\n",
    "    # Path allows flexibility: file may not exist yet but must be a valid path object.\n",
    "    filename: str = Field(..., description=dedent(\"\"\"\\\n",
    "                                                  Destination file path where the generated image will be saved.  \n",
    "                                                  ç”Ÿæˆå½±åƒçš„å„²å­˜æª”æ¡ˆè·¯å¾‘ã€‚\n",
    "                                                  \"\"\")\n",
    "                          )\n",
    "\n",
    "\n",
    "def image_to_base64(image_path):\n",
    "    \n",
    "    with Image.open(image_path) as image:\n",
    "        \n",
    "        # Save the Image to a Buffer\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=\"JPEG\")\n",
    "        \n",
    "        # Encode the Image to Base64\n",
    "        image_str = base64.b64encode(buffered.getvalue())\n",
    "    \n",
    "    return image_str.decode('utf-8')\n",
    "\n",
    "\n",
    "def image_to_base64_from_list(filenames) -> List[Optional[str]]:\n",
    "\n",
    "    # return [image_to_base64(f) for f in filenames]\n",
    "    return []\n",
    "\n",
    "\n",
    "def export_to_image(content, filename):\n",
    "    \n",
    "    dir_ = Path(filename).parent\n",
    "\n",
    "    if not os.path.isdir(dir_):\n",
    "        os.makedirs(dir_)\n",
    "    \n",
    "    image_bytes = base64.b64decode(content)\n",
    "    \n",
    "    with open(filename, \"wb\") as fh:\n",
    "        fh.write(image_bytes)\n",
    "\n",
    "\n",
    "def story_adaptation(story: str):\n",
    "\n",
    "    nl_prompt = session_state['nl_prompt']\n",
    "    \n",
    "    if nl_prompt:\n",
    "        story += f\"\\nPrevious image description:\\n\\n{nl_prompt}\"\n",
    "\n",
    "    print(f\"******\\n{story}\\n*******\")\n",
    "    \n",
    "    return story\n",
    "    \n",
    "\n",
    "\n",
    "image_output_data_processors = [('image_base64', export_to_image)]\n",
    "\n",
    "image_input_data_processors = [(\"story\", story_adaptation),\n",
    "                                (\"image_io\", image_to_base64_from_list)]\n",
    "\n",
    "image_tool = ToolTemplate.create(\n",
    "    runnable=\"http://localhost:8080/image_generation/invoke\",\n",
    "    name=\"Image generation tool\",\n",
    "    description_template=dedent(\"\"\"\\\n",
    "                                This tool is designed to generate an image to the correspoinding narrative.\n",
    "                                Provide a narrative (`story`), along with \n",
    "                                the preceding images stored as .png files (`image_io`), \n",
    "                                and specify where the generated text should be saved (`filename`).\n",
    "                                    \n",
    "                                Input format: {input_format_instruction}\n",
    "                                \"\"\"\n",
    "                                ),\n",
    "    input_parser=PydanticOutputParser(pydantic_object=ImageInput),\n",
    "    output_data_processors = image_output_data_processors,\n",
    "    input_data_processors = image_input_data_processors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eace21c-9917-4f4c-8a3e-7afc8765cc49",
   "metadata": {},
   "source": [
    "### èªéŸ³ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0fa546-d99b-4ae3-920e-0bf4963a2bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioInput(BaseModel):\n",
    "    input: str = Field(description=dedent(\"\"\"\\\n",
    "                                          The narrative or context used to generate the audio content with text to sound (TTS).  \n",
    "                                          æ•…äº‹æƒ…ç¯€æˆ–ä¸Šä¸‹æ–‡ï¼Œç”¨æ–¼TTSæ–‡å­—è½‰èªéŸ³ã€‚\n",
    "                                          \"\"\")\n",
    "                                    )\n",
    "    # Path allows flexibility: file may not exist yet but must be a valid path object.\n",
    "    filename: str = Field(..., description=dedent(\"\"\"\\\n",
    "                                                  Destination mp3 file path where the generated audio will be saved.  \n",
    "                                                  mp3èªéŸ³æª”çš„å„²å­˜æª”æ¡ˆè·¯å¾‘ã€‚\n",
    "                                                  \"\"\")\n",
    "                         )\n",
    "\n",
    "def export_to_audio(content, filename):\n",
    "\n",
    "    dir_ = Path(filename).parent\n",
    "\n",
    "    if not os.path.isdir(dir_):\n",
    "        os.makedirs(dir_)\n",
    "    \n",
    "    audio_bytes = base64.b64decode(content)\n",
    "    \n",
    "    with open(filename, \"wb\") as fh:\n",
    "        fh.write(audio_bytes)\n",
    "\n",
    "\n",
    "audio_output_data_processors = [(None, export_to_audio)]\n",
    "\n",
    "audio_tool = ToolTemplate.create(\n",
    "    runnable=\"http://localhost:8080/audio_generation/invoke\",\n",
    "    name=\"Audio generation tool\",\n",
    "    description_template=dedent(\"\"\"\\\n",
    "                                This tool is designed to generate an .mp3 file to a corresponding narrative.\n",
    "                                Provide a narrative (`story`), along with \n",
    "                                and specify where the generated audio should be saved (`filename`).\n",
    "                                    \n",
    "                                Input format: {input_format_instruction}\n",
    "                                \"\"\"\n",
    "                                ),\n",
    "    input_parser = PydanticOutputParser(pydantic_object=AudioInput),\n",
    "    output_data_processors = audio_output_data_processors,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8281faeb-b553-47c7-9b03-b7f9b6c671e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "\n",
    "from src.agent.react_zero_shot import prompt_template as zero_shot_prompt_template\n",
    "\n",
    "prompt = PromptTemplate.from_template(zero_shot_prompt_template)\n",
    "\n",
    "tools = [story_telling_tool, image_tool, audio_tool]\n",
    "\n",
    "credential_init()\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o\", temperature=0, \n",
    "                  )\n",
    "\n",
    "zero_shot_agent = create_react_agent(\n",
    "    llm=model,\n",
    "    tools=tools,\n",
    "    prompt=prompt,\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(agent=zero_shot_agent, tools=tools, verbose=True, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60f298a-18b7-4b23-96a4-7757b5f1aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = dedent(\"\"\"\n",
    "Create a chapter of a baby owl capturing a rodent in the night as his dinner.\n",
    "After having the final answer, please create a corresponding image and a corresponding mp3 file.\n",
    "The saved image (.png), text (.txt), and audio (.mp3) should have same name in the folder `tutorial/LLM+Langchain/Week-8/story_test`\n",
    "\"\"\")\n",
    "\n",
    "agent_executor.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25bad6b-5459-440d-bf6f-4b4258abf2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7613df9-de89-4546-b738-323a7cc3a9c9",
   "metadata": {},
   "source": [
    "æˆåŠŸçš„ç”Ÿæˆäº†ä¸€é çš„å…§å®¹ï¼ŒAgentå¯ä»¥å¹«æˆ‘å€‘ç”Ÿæˆæ•´å€‹æ•…äº‹å—?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89f39ff-aeb8-433f-b18b-e20b622e5739",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "         I want to create an 4 pages story for a child. He likes snow owl.\n",
    "         For each page, please create a corresponding image and record the story as an mp3.\n",
    "         After having the final answer, please create a corresponding image and record the story as an mp3. \n",
    "         The saved image and mp3 should have same name, following the structure of \n",
    "         <Page - idx>, with idx as a number starting from 1, in the folder `tutorial/LLM+Langchain/Week-8/story_automation`\n",
    "         \"\"\"\n",
    "\n",
    "agent_executor.invoke({\"input\": prompt})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3683842a-53ee-420d-9423-637ce9cd723a",
   "metadata": {},
   "source": [
    "## ğŸ§ äº’å‹•å¼æœ‰è²æ›¸å…§å®¹ç”Ÿæˆ\n",
    "\n",
    "- ä¸ä¸€å®šéœ€è¦å®Œæ•´çš„ **Agent æ¶æ§‹**ï¼Œå› ç‚ºæµç¨‹çš„æ¯ä¸€æ­¥ï¼ˆæ•…äº‹ â†’ åœ–åƒ â†’ èªéŸ³ï¼‰éƒ½å·²ç¶“æ˜ç¢ºå®šç¾©ï¼Œèƒ½ç”±ä½¿ç”¨è€…ä¸»å‹•è§¸ç™¼ã€‚  \n",
    "- å¯ä»¥ç›´æ¥åŸºæ–¼ **èŠå¤©æ©Ÿå™¨äºº** çš„äº’å‹•å½¢å¼é€²è¡Œï¼Œæ¯æ¬¡è¼¸å…¥ä½¿ç”¨è€…çš„éœ€æ±‚æˆ–æŒ‡ä»¤å¾Œï¼Œç³»çµ±ä¾ç…§æŒ‡å®šæ­¥é©Ÿç”Ÿæˆå°æ‡‰å…§å®¹ã€‚  \n",
    "- ä½¿ç”¨è€…å¯ä»¥åœ¨æ•…äº‹ç”Ÿæˆéç¨‹ä¸­å³æ™‚èª¿æ•´æ–¹å‘ï¼Œä¾‹å¦‚æŒ‡å®šè§’è‰²ã€æƒ…ç¯€èµ°å‘æˆ–èªæ°£ï¼Œæå‡ **å®¢è£½åŒ–é«”é©—**ã€‚  \n",
    "- é€™ç¨®äº’å‹•æ–¹å¼éå¸¸é©åˆ **èªè¨€å­¸ç¿’** å ´æ™¯ï¼š  \n",
    "  - å­¸ç¿’è€…èƒ½ä¸€é‚Šé–±è®€æ•…äº‹ã€ä¸€é‚Šè½æœ‰è²è¼¸å‡º  \n",
    "  - å¯å³æ™‚ä¿®æ”¹æ•…äº‹æƒ…ç¯€ï¼Œç”¢ç”Ÿæ›´è²¼è¿‘å­¸ç¿’éœ€æ±‚çš„å…§å®¹  \n",
    "  - æ­é…åœ–ç‰‡èˆ‡èªéŸ³ï¼Œæå‡æ²‰æµ¸å¼å­¸ç¿’æ•ˆæœ  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adda8172-d8ae-4bf0-80be-14810309324a",
   "metadata": {},
   "source": [
    "# Langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafca2e5-93fc-4a49-92c6-d0cae5664cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "output_response_schemas = [\n",
    "        ResponseSchema(name=\"story\", description=\"the story content in the page\"),\n",
    "        ResponseSchema(name=\"page index\", description=\"The page number of the story\"),\n",
    "    ]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(output_response_schemas)\n",
    "\n",
    "output_format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "           Create a story page {idx}, based on the description: {text}\n",
    "\n",
    "           The answer continues from previous content:\n",
    "           {context}\n",
    "\n",
    "           After having the final answer, please create a corresponding image and record the story as an mp3. \n",
    "           The saved image and mp3 should have same name, following the structure of \n",
    "           <Page - idx>, in the folder `tutorial/LLM+Langchain/Week-8`\n",
    "\n",
    "           The output should have the following format: {output_format_instruction}\n",
    "           \"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(template=template,\n",
    "                                 input_variables=[\"text\", \"context\", \"idx\"],\n",
    "                                 partial_variables={\"output_format_instruction\": output_format_instructions})\n",
    "\n",
    "agent_chain = RunnablePassthrough.assign(input=prompt_template)|agent_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0e5fb0-709c-4a51-a737-d95d949df1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = agent_chain.invoke({\"text\": \"A little cat just woke up in the morning\",\n",
    "                        \"context\": \"The beginning of the story:\\n\",\n",
    "                        \"idx\": str(1)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5c5db4-a1f1-48a9-9316-4734886a4d11",
   "metadata": {},
   "source": [
    "è‹¥æ˜¯ä»¥ä¸‹æ­¥é©Ÿå¤±æ•—ï¼Œå˜—è©¦é‡æ–°ç”Ÿæˆã€‚é€™æ˜¯å¤§èªè¨€æ¨¡å‹ï¼Œæ²’æœ‰ä¿è­‰å¯ä»¥100%ç”¢å‡ºä½ å¸Œæœ›çš„æ ¼å¼ã€‚æˆ‘å€‘åªèƒ½ç›¡å¯èƒ½æé«˜æˆåŠŸè¼¸å‡ºçš„æ©Ÿç‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727746b3-7d2a-44cb-bc2a-65ee873aec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4eec1e-8a56-49f5-9562-9fad0f524536",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser.parse(Q['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d871f0-78aa-408e-b362-c2ec3d58f6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser.parse(Q['output'])['story']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04b14f4-af8c-48f1-a35f-19581300062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser.parse(Q['output'])['page index']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7173de4-9db3-4e49-bf0f-af35f2654cf9",
   "metadata": {},
   "source": [
    "### ç¬¬äºŒé "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c9062a-d980-43e3-a374-aaef6f31bf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_list = [output_parser.parse(Q['output'])['story']]\n",
    "print(context_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96840fa-af12-432a-89a8-79eadb9dcca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_2 = agent_chain.invoke({\"text\": \"Whisker found a dove and wanted to hunt it down!\",\n",
    "                          \"context\": \":\\n\".join(context_list),\n",
    "                          \"idx\": str(2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9eba24-c286-4c95-95df-9d83f439e766",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc45668-0d3a-4582-b6b7-6f8a53c06e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser.parse(Q_2['output'])['story']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd27823-fe18-4e44-998c-06e4629d9f67",
   "metadata": {},
   "source": [
    "### okay, it looks fine, let us see how to make it a interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854b6e69-3036-45d2-9461-b17f667a3fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"å‰æƒ…æè¦\"\n",
    "context_list = []\n",
    "\n",
    "# é é¢èµ·å§‹\n",
    "idx = 1\n",
    "\n",
    "while True:\n",
    "    if len(context_list) == 0:\n",
    "        context = \"The beginning of the story:\\n\"\n",
    "    else:\n",
    "        context = \"\\n\".join(context_list)\n",
    "\n",
    "    text = input(\"è«‹è¼¸å…¥æ•…äº‹å…§å®¹: è‹¥æƒ³è¦çµæŸ è«‹è¼¸å…¥ `QUIT`\")\n",
    "\n",
    "    if text == \"QUIT\":\n",
    "        break\n",
    "    \n",
    "    Q = agent_chain.invoke({\"text\": text,\n",
    "                            \"context\": context,\n",
    "                            \"idx\": str(idx)})\n",
    "\n",
    "    story = output_parser.parse(Q['output'])['story']\n",
    "    \n",
    "    # ä¸‹ä¸€é \n",
    "    idx += 1\n",
    "\n",
    "    context_list.append(output_parser.parse(Q['output'])['story'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931bc721-634e-410f-95f2-6efe391d2b61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
