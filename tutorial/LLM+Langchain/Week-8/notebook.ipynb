{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fabc653-5ca9-484b-a864-0b6a363ac275",
   "metadata": {},
   "source": [
    "# 📖 Agent 實作範例：自動化有聲書生成\n",
    "\n",
    "本 Notebook 展示如何透過 Agent 串接 **故事 → 圖像 → 語音** 的自動化流程。  \n",
    "每個步驟都會將輸出保存為檔案，避免重複 Token 消耗，並方便後續流程使用。\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 整體流程\n",
    "1. 故事內容生成  \n",
    "2. 圖片內容生成  \n",
    "3. 語音內容生成  \n",
    "\n",
    "---\n",
    "\n",
    "## 1️⃣ 故事內容生成\n",
    "**輸入**  \n",
    "- 草稿文字  \n",
    "- 既有內容 (`.txt` 檔案)  \n",
    "\n",
    "**處理流程**  \n",
    "1. 將文字送入 Langserve 服務  \n",
    "2. 接收生成的故事段落  \n",
    "\n",
    "**輸出**  \n",
    "- 將結果存為 `.txt` 檔  \n",
    "- 避免 Agent 一直傳遞整份故事，降低 Token 消耗  \n",
    "\n",
    "---\n",
    "\n",
    "## 2️⃣ 圖片內容生成\n",
    "**輸入**  \n",
    "- 最新生成的故事文字  \n",
    "- 既有圖片 (`.png`)  \n",
    "\n",
    "**處理流程**  \n",
    "1. 將圖片編碼為 base64  \n",
    "2. 文字與圖片送入 Langserve 服務  \n",
    "3. 接收回傳的圖片（base64 格式）  \n",
    "\n",
    "**輸出**  \n",
    "- 將 base64 解碼為二進位資料，存成 `.png` 檔  \n",
    "\n",
    "---\n",
    "\n",
    "## 3️⃣ 語音內容生成\n",
    "**輸入**  \n",
    "- 最新生成的故事文字  \n",
    "\n",
    "**處理流程**  \n",
    "1. 將文字送入 Langserve 服務  \n",
    "2. 接收回傳的語音（base64 格式）  \n",
    "\n",
    "**輸出**  \n",
    "- 將 base64 解碼為二進位資料，存成 `.mp3` 或 `.wav` 檔  \n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 流程圖\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[故事草稿/舊內容 .txt] --> B[送入 Langserve 生成故事]\n",
    "    B --> C[故事內容 .txt]\n",
    "    C --> D[送入 Langserve 生成圖片 (base64)]\n",
    "    D --> E[解碼並存為 .png]\n",
    "    C --> F[送入 Langserve 生成語音 (base64)]\n",
    "    F --> G[解碼並存為 .mp3 / .wav]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28811758-cd03-42fc-a88f-1be3988b70fb",
   "metadata": {},
   "source": [
    "## LangServe 服務測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c95db07-6153-4ec9-863c-61a0a49a8288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "os.chdir(\"../../../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314b6765-7764-46ec-bc16-dfa8b5ed4c86",
   "metadata": {},
   "source": [
    "測試故事生成服務"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d168449-4550-49b8-acd2-21b4d3009efa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    \"http://localhost:8080/story_telling/invoke\",\n",
    "    json={\"input\":{'scratch': \"Create a chapter of a baby owl capturing a rodent in the night as his dinner\",\n",
    "                   'context': \"\"}\n",
    "         }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2882f0e-1aff-4e8f-b7f1-1c101683aa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "story_json = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ff1e2-79b3-4b5c-acc7-71fd70025553",
   "metadata": {},
   "outputs": [],
   "source": [
    "story_json['output']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3654b3-4cf9-4587-a4cd-b023b6e11613",
   "metadata": {},
   "source": [
    "測試影像生成服務"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174143dd-8f26-486d-9d7e-f22db96d5e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "image_generation_module = importlib.import_module(\"tutorial.LLM+Langchain.Week-8.logic.image_generation\")\n",
    "image_create_pipeline = image_generation_module.image_create_pipeline(image_generation_module.system_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f923fa-d406-482b-855b-b0e869afeada",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_image = requests.post(\n",
    "    \"http://localhost:8080/image_generation/invoke\",\n",
    "    json={\"input\": {'story': story_json['output'],\n",
    "                    \"image_io\": []}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1011d1-ebd2-47b3-9e7e-a1845ac0fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_image.json()['output']['nl_prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4f4f2a-2262-40cc-985a-86d4ecfd3f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "# Decode to bytes\n",
    "image_bytes = base64.b64decode(response_image.json()['output']['image_base64'])\n",
    "\n",
    "with open(\"tutorial/LLM+Langchain/Week-8/story_2_image.png\", \"wb\") as fh:\n",
    "    fh.write(image_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30494746-8671-4ece-81d8-83037a8a1075",
   "metadata": {},
   "source": [
    "測試生成後續後續的故事"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592e7b42-5213-41be-9ee8-f0bc0d069453",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_next_tory = requests.post(\n",
    "    \"http://localhost:8080/story_telling/invoke\",\n",
    "    json={\"input\":{'scratch': \"Create the next chapter following the context\",\n",
    "                   'context': story_json['output']}\n",
    "         }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe8ed59-f76c-4069-913a-8e6d482856ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_chapter = response_next_tory.json()['output']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57512583-d7ba-4593-9a9a-3eb67d71849b",
   "metadata": {},
   "source": [
    "根據故事和上一張圖片，產生出下一張圖片\n",
    "\n",
    "透過requests送出base64 string.\n",
    "\n",
    "本來是想要透過img2img的手法控制圖片風格和內容一致性的，但是發現不確定性很高，所以決定從上一張圖片的prompt下手，看看能不能調高一致性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951b9a93-38e3-46d5-a052-3cc958095de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    \"http://localhost:8080/image_generation/invoke\",\n",
    "    json={\"input\": {'story': next_chapter + f\"\\nPrevious image description:\\n\\n{response_image.json()['output']['nl_prompt']}\",\n",
    "                    # 'image_io': [response_image.json()['output']['image_base64']]\n",
    "                    'image_io': []\n",
    "                   }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0003108d-c97c-4c40-8db9-39594eae610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()['output']['nl_prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769f6c0b-6904-4e79-aa5c-05dffb0fac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "image_bytes = base64.b64decode(response.json()['output']['image_base64'])\n",
    "\n",
    "with open(\"tutorial/LLM+Langchain/Week-8/story_3_image.png\", \"wb\") as fh:\n",
    "    fh.write(image_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35819f5-9fa1-4dea-ad1e-1f04a98303bb",
   "metadata": {},
   "source": [
    "測試語音生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425c3044-274f-4b64-a310-aeb1a37780da",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    \"http://localhost:8080/audio_generation/invoke\",\n",
    "    json={\"input\": {'input': \"How are you doing?\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41234ade-3023-4889-91e1-a9c0230a8452",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_bytes = base64.b64decode(response.json()['output'])\n",
    "\n",
    "with open(\"tutorial/LLM+Langchain/Week-8/test_sample.mp3\", \"wb\") as f:\n",
    "    f.write(audio_bytes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62454159-038d-4e37-999b-ff81159fb7de",
   "metadata": {},
   "source": [
    "## 生成工具模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2768e64a-de75-4196-b7b5-640b189e1669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "from typing import Optional, Any, List, Tuple, Callable\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.tools import BaseTool\n",
    "from langchain_core.runnables import Runnable\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic import FilePath\n",
    "\n",
    "from src.initialization import credential_init\n",
    "\n",
    "\n",
    "class ToolTemplate(BaseTool):\n",
    "\n",
    "    \"\"\"\n",
    "    ToolTemplateHTTP: 一個專門用來呼叫 Langserve REST API 的 Agent Tool\n",
    "\n",
    "    - 使用 PydanticOutputParser 保證輸入格式正確\n",
    "    - 支援多欄位 input/output 處理器\n",
    "    - 對 API 呼叫加上錯誤處理\n",
    "    \"\"\"\n",
    "    \n",
    "    runnable: str = Field(..., description='The Langserve endpoint')\n",
    "    name: str\n",
    "    input_parser: PydanticOutputParser\n",
    "    description: str\n",
    "    input_data_processors: Optional[List[Tuple[str, Callable[[Any], Any]]]] = None\n",
    "    output_data_processors: Optional[List[Tuple[Optional[str], Callable[[Any], Any]]]] = None\n",
    "    \n",
    "    @classmethod\n",
    "    def create(cls, runnable: str, name: str, description_template: str,\n",
    "               input_parser: PydanticOutputParser, input_data_processors: Optional=None,\n",
    "               output_data_processors: Optional=None):\n",
    "\n",
    "        \"\"\"建立 Tool 實例，會自動把輸入格式需求加入 description\"\"\"\n",
    "        \n",
    "        input_format_instruction = input_parser.get_format_instructions()\n",
    "        \n",
    "        description = description_template.format(\n",
    "            input_format_instruction=input_format_instruction\n",
    "        )\n",
    "        \n",
    "        return cls(runnable=runnable, name=name, description=description,\n",
    "                   input_parser=input_parser, input_data_processors=input_data_processors,\n",
    "                   output_data_processors=output_data_processors)\n",
    "    \n",
    "    def _run(self, query: str):\n",
    "\n",
    "        \"\"\"執行 Tool，同步版本\"\"\"\n",
    "        \n",
    "        # 1. 驗證 & parse 輸入\n",
    "        try:\n",
    "            input_ = self.input_parser.parse(query)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to parse input with parser: {e}, query={query}\")\n",
    "        \n",
    "        runnable_inputs = input_.model_dump()\n",
    "\n",
    "        # 2. input processors（前處理）\n",
    "        if self.input_data_processors:\n",
    "            for field, fn in self.input_data_processors:\n",
    "                if field in runnable_inputs:\n",
    "                    runnable_inputs[field] = fn(runnable_inputs[field])\n",
    "            \n",
    "        # 3. 呼叫 Langserve REST API\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                str(self.runnable),\n",
    "                json={\"input\": runnable_inputs},\n",
    "                timeout=60,\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Langserve call failed: {e}, inputs={runnable_inputs}\")\n",
    "\n",
    "        if \"output\" not in result:\n",
    "            raise RuntimeError(f\"Invalid response format from Langserve: {result}\")\n",
    "        \n",
    "        output = result['output']\n",
    "\n",
    "        # 4. update the state varaibles:\n",
    "        for key in session_state.keys():\n",
    "            if key in output:\n",
    "                session_state[key] = output[key]\n",
    "        \n",
    "        # 5. output processors（後處理）\n",
    "        if self.output_data_processors:\n",
    "            for field, fn in self.output_data_processors:\n",
    "                if not field:\n",
    "                    fn(output, runnable_inputs['filename'])\n",
    "                else:\n",
    "                    fn(output[field], runnable_inputs['filename'])\n",
    "                    \n",
    "        # 預設回傳「檔名」如果有 filename，否則回傳輸出的字串\n",
    "        return runnable_inputs.get(\"filename\", output)\n",
    "\n",
    "    def _arun(self, query: str):\n",
    "\n",
    "        raise NotImplementedError(\"This tool does not support async\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7a7685-7318-4349-89a4-159e2879d88e",
   "metadata": {},
   "source": [
    "### State Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd71fca-8a1e-4842-acc9-572921972b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_state = {}\n",
    "\n",
    "session_state['nl_prompt'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e253ad6e-6831-4399-be92-59af97e4c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class StoryInput(BaseModel):\n",
    "    scratch: str = Field(description=dedent(\"\"\"\\\n",
    "                                            The draft, notes, or rough idea for the current page of the story.\n",
    "                                           （故事當前頁面的草稿、筆記或初步構想\n",
    "                                            \"\"\"))\n",
    "    context: List[FilePath] = Field(default_factory=list, description=dedent(\"\"\"\\\n",
    "                                                              A list of previously generated .txt files that contain story content.  \n",
    "                                                              Used to maintain narrative consistency and continuity across images.  \n",
    "                                                              先前生成的 .txt 檔案清單，其中包含故事內容。  \n",
    "                                                              用於保持影像生成過程中的敘事一致性與連貫性。\n",
    "                                                              \"\"\"))\n",
    "    filename: str = Field(..., description=dedent(\"\"\"\\\n",
    "                                             The file path where the generated story text will be saved.\n",
    "                                            （生成的故事文本將被儲存的檔案路徑）\n",
    "                                             \"\"\"))\n",
    "\n",
    "\n",
    "def export_to_txt(text, filename: Path):\n",
    "\n",
    "    dir_ = Path(filename).parent\n",
    "\n",
    "    if not os.path.isdir(dir_):\n",
    "        os.makedirs(dir_)\n",
    "    \n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text)\n",
    "\n",
    "\n",
    "def read_from_txt(filename) -> str:\n",
    "\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "def read_from_list_of_text(filenames) -> str:\n",
    "\n",
    "    return \"\\n\\n\".join([read_from_txt(f) for f in filenames])\n",
    "\n",
    "\n",
    "story_input_data_processors = [(\"context\", read_from_list_of_text)]\n",
    "\n",
    "story_output_data_processors = [(None, export_to_txt)]\n",
    "\n",
    "story_telling_tool = ToolTemplate.create(\n",
    "    runnable=\"http://localhost:8080/story_telling/invoke\",\n",
    "    name=\"Story generation tool\",\n",
    "    description_template=dedent(\"\"\"\\\n",
    "                                This tool is designed to generate a story one page at a time.\n",
    "                                Provide a draft or idea for the current page (`scratch`), along with \n",
    "                                the preceding story context stored as .txt files (`context`), \n",
    "                                and specify where the generated text should be saved (`filename`).\n",
    "                                    \n",
    "                                Input format: {input_format_instruction}\n",
    "                                \"\"\"\n",
    "                                ),\n",
    "    input_parser=PydanticOutputParser(pydantic_object=StoryInput),\n",
    "    output_data_processors = story_output_data_processors,\n",
    "    input_data_processors = story_input_data_processors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eacf5e2-200b-4a00-8cbd-202ee93ef350",
   "metadata": {},
   "source": [
    "### 影像生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc7ff16-8827-4c9d-a68c-35681e0e071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageInput(BaseModel):\n",
    "    story: str = Field(description=dedent(\"\"\"\\\n",
    "                                          The narrative or context used to generate the image prompt.  \n",
    "                                          故事情節或上下文，用於生成影像提示。\n",
    "                                          \"\"\")\n",
    "                      )\n",
    "    # FilePath ensures the input path exists and is a valid file\n",
    "    image_io: List[str] = Field([], description=dedent(\"\"\"\\\n",
    "                                                   Path to the previously generated image.  \n",
    "                                                   Used in img2img generation to maintain visual and texture consistency.  \n",
    "                                                   先前生成影像的路徑。  \n",
    "                                                   在 img2img 生成中用於保持視覺與材質的一致性。\n",
    "                                                   \"\"\")\n",
    "                                    )\n",
    "    # Path allows flexibility: file may not exist yet but must be a valid path object.\n",
    "    filename: str = Field(..., description=dedent(\"\"\"\\\n",
    "                                                  Destination file path where the generated image will be saved.  \n",
    "                                                  生成影像的儲存檔案路徑。\n",
    "                                                  \"\"\")\n",
    "                          )\n",
    "\n",
    "\n",
    "def image_to_base64(image_path):\n",
    "    \n",
    "    with Image.open(image_path) as image:\n",
    "        \n",
    "        # Save the Image to a Buffer\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=\"JPEG\")\n",
    "        \n",
    "        # Encode the Image to Base64\n",
    "        image_str = base64.b64encode(buffered.getvalue())\n",
    "    \n",
    "    return image_str.decode('utf-8')\n",
    "\n",
    "\n",
    "def image_to_base64_from_list(filenames) -> List[Optional[str]]:\n",
    "\n",
    "    # return [image_to_base64(f) for f in filenames]\n",
    "    return []\n",
    "\n",
    "\n",
    "def export_to_image(content, filename):\n",
    "    \n",
    "    dir_ = Path(filename).parent\n",
    "\n",
    "    if not os.path.isdir(dir_):\n",
    "        os.makedirs(dir_)\n",
    "    \n",
    "    image_bytes = base64.b64decode(content)\n",
    "    \n",
    "    with open(filename, \"wb\") as fh:\n",
    "        fh.write(image_bytes)\n",
    "\n",
    "\n",
    "def story_adaptation(story: str):\n",
    "\n",
    "    nl_prompt = session_state['nl_prompt']\n",
    "    \n",
    "    if nl_prompt:\n",
    "        story += f\"\\nPrevious image description:\\n\\n{nl_prompt}\"\n",
    "\n",
    "    print(f\"******\\n{story}\\n*******\")\n",
    "    \n",
    "    return story\n",
    "    \n",
    "\n",
    "\n",
    "image_output_data_processors = [('image_base64', export_to_image)]\n",
    "\n",
    "image_input_data_processors = [(\"story\", story_adaptation),\n",
    "                                (\"image_io\", image_to_base64_from_list)]\n",
    "\n",
    "image_tool = ToolTemplate.create(\n",
    "    runnable=\"http://localhost:8080/image_generation/invoke\",\n",
    "    name=\"Image generation tool\",\n",
    "    description_template=dedent(\"\"\"\\\n",
    "                                This tool is designed to generate an image to the correspoinding narrative.\n",
    "                                Provide a narrative (`story`), along with \n",
    "                                the preceding images stored as .png files (`image_io`), \n",
    "                                and specify where the generated text should be saved (`filename`).\n",
    "                                    \n",
    "                                Input format: {input_format_instruction}\n",
    "                                \"\"\"\n",
    "                                ),\n",
    "    input_parser=PydanticOutputParser(pydantic_object=ImageInput),\n",
    "    output_data_processors = image_output_data_processors,\n",
    "    input_data_processors = image_input_data_processors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eace21c-9917-4f4c-8a3e-7afc8765cc49",
   "metadata": {},
   "source": [
    "### 語音生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0fa546-d99b-4ae3-920e-0bf4963a2bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioInput(BaseModel):\n",
    "    input: str = Field(description=dedent(\"\"\"\\\n",
    "                                          The narrative or context used to generate the audio content with text to sound (TTS).  \n",
    "                                          故事情節或上下文，用於TTS文字轉語音。\n",
    "                                          \"\"\")\n",
    "                                    )\n",
    "    # Path allows flexibility: file may not exist yet but must be a valid path object.\n",
    "    filename: str = Field(..., description=dedent(\"\"\"\\\n",
    "                                                  Destination mp3 file path where the generated audio will be saved.  \n",
    "                                                  mp3語音檔的儲存檔案路徑。\n",
    "                                                  \"\"\")\n",
    "                         )\n",
    "\n",
    "def export_to_audio(content, filename):\n",
    "\n",
    "    dir_ = Path(filename).parent\n",
    "\n",
    "    if not os.path.isdir(dir_):\n",
    "        os.makedirs(dir_)\n",
    "    \n",
    "    audio_bytes = base64.b64decode(content)\n",
    "    \n",
    "    with open(filename, \"wb\") as fh:\n",
    "        fh.write(audio_bytes)\n",
    "\n",
    "\n",
    "audio_output_data_processors = [(None, export_to_audio)]\n",
    "\n",
    "audio_tool = ToolTemplate.create(\n",
    "    runnable=\"http://localhost:8080/audio_generation/invoke\",\n",
    "    name=\"Audio generation tool\",\n",
    "    description_template=dedent(\"\"\"\\\n",
    "                                This tool is designed to generate an .mp3 file to a corresponding narrative.\n",
    "                                Provide a narrative (`story`), along with \n",
    "                                and specify where the generated audio should be saved (`filename`).\n",
    "                                    \n",
    "                                Input format: {input_format_instruction}\n",
    "                                \"\"\"\n",
    "                                ),\n",
    "    input_parser = PydanticOutputParser(pydantic_object=AudioInput),\n",
    "    output_data_processors = audio_output_data_processors,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8281faeb-b553-47c7-9b03-b7f9b6c671e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "\n",
    "from src.agent.react_zero_shot import prompt_template as zero_shot_prompt_template\n",
    "\n",
    "prompt = PromptTemplate.from_template(zero_shot_prompt_template)\n",
    "\n",
    "tools = [story_telling_tool, image_tool, audio_tool]\n",
    "\n",
    "credential_init()\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o\", temperature=0, \n",
    "                  )\n",
    "\n",
    "zero_shot_agent = create_react_agent(\n",
    "    llm=model,\n",
    "    tools=tools,\n",
    "    prompt=prompt,\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(agent=zero_shot_agent, tools=tools, verbose=True, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60f298a-18b7-4b23-96a4-7757b5f1aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = dedent(\"\"\"\n",
    "Create a chapter of a baby owl capturing a rodent in the night as his dinner.\n",
    "After having the final answer, please create a corresponding image and a corresponding mp3 file.\n",
    "The saved image (.png), text (.txt), and audio (.mp3) should have same name in the folder `tutorial/LLM+Langchain/Week-8/story_test`\n",
    "\"\"\")\n",
    "\n",
    "agent_executor.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25bad6b-5459-440d-bf6f-4b4258abf2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7613df9-de89-4546-b738-323a7cc3a9c9",
   "metadata": {},
   "source": [
    "成功的生成了一頁的內容，Agent可以幫我們生成整個故事嗎?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89f39ff-aeb8-433f-b18b-e20b622e5739",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "         I want to create an 4 pages story for a child. He likes snow owl.\n",
    "         For each page, please create a corresponding image and record the story as an mp3.\n",
    "         After having the final answer, please create a corresponding image and record the story as an mp3. \n",
    "         The saved image and mp3 should have same name, following the structure of \n",
    "         <Page - idx>, with idx as a number starting from 1, in the folder `tutorial/LLM+Langchain/Week-8/story_automation`\n",
    "         \"\"\"\n",
    "\n",
    "agent_executor.invoke({\"input\": prompt})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3683842a-53ee-420d-9423-637ce9cd723a",
   "metadata": {},
   "source": [
    "## 🎧 互動式有聲書內容生成\n",
    "\n",
    "- 不一定需要完整的 **Agent 架構**，因為流程的每一步（故事 → 圖像 → 語音）都已經明確定義，能由使用者主動觸發。  \n",
    "- 可以直接基於 **聊天機器人** 的互動形式進行，每次輸入使用者的需求或指令後，系統依照指定步驟生成對應內容。  \n",
    "- 使用者可以在故事生成過程中即時調整方向，例如指定角色、情節走向或語氣，提升 **客製化體驗**。  \n",
    "- 這種互動方式非常適合 **語言學習** 場景：  \n",
    "  - 學習者能一邊閱讀故事、一邊聽有聲輸出  \n",
    "  - 可即時修改故事情節，產生更貼近學習需求的內容  \n",
    "  - 搭配圖片與語音，提升沉浸式學習效果  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adda8172-d8ae-4bf0-80be-14810309324a",
   "metadata": {},
   "source": [
    "# 🧩 LangGraph 框架與計劃-行動代理人簡介\n",
    "\n",
    "## 一、LangGraph 是什麼？\n",
    "\n",
    "**LangGraph** 是一個專門為構建 **多智能體系統（Multi-Agent Systems）** 而設計的框架，提供了結構化的方式來設計、協調與管理多個智能體之間的互動。  \n",
    "它建立在 **LangChain** 的概念之上，並引入了「圖（Graph）」的思維模型：  \n",
    "每個智能體（Agent）被視為圖中的一個節點（Node），節點之間的邊（Edge）代表任務流或資訊交換的關係。\n",
    "\n",
    "### 🌟 LangGraph 的核心特點\n",
    "\n",
    "- 🔗 **節點導向設計**：每個節點可代表一個智能體、工具或決策模組。  \n",
    "- 🔄 **靈活的任務流程**：支援有條件的任務轉移（例如根據上下文或任務狀態動態選擇不同智能體）。  \n",
    "- 🧠 **模型互補性**：可同時使用多種大型語言模型（LLMs）或不同的外部工具。  \n",
    "- 📊 **可視化工作流**：開發者可以直觀地觀察智能體之間的互動與任務流向。  \n",
    "- 🧩 **有狀態的任務控制**：支援任務回溯、重試與流程中斷恢復等功能。  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 二、LangChain vs. LangGraph\n",
    "\n",
    "在現代 LLM 應用開發中，**LangChain** 和 **LangGraph** 是兩個常被提及的框架。  \n",
    "它們皆旨在簡化與大型語言模型的整合流程，但在設計理念與使用場景上有所不同。\n",
    "\n",
    "| 特點 | LangChain | LangGraph |\n",
    "|------|-----------|-----------|\n",
    "| 架構風格 | 函式導向（Function-based） | 圖形導向（Graph-based） |\n",
    "| 核心用途 | 組合不同的工具與鏈（Chains）以處理任務 | 定義狀態轉換與流程控制的狀態機 |\n",
    "| 適合場景 | 線性流程、多工具串接、單一任務代理人 | 有狀態的工作流程、多步驟決策、動態流程切換 |\n",
    "| 控制流程能力 | 較弱，需透過程式邏輯控制 | 較強，內建狀態管理與動態節點切換 |\n",
    "| 易學程度 | 相對容易上手 | 初期需理解狀態機與圖結構概念 |\n",
    "| 通用性 | 模組多、通用性強 | 更適合複雜與長期任務管理 |\n",
    "\n",
    "> 💡 **簡而言之**：  \n",
    "> - **LangChain** 像是一個「積木箱」，提供各種模組供你自由組合。  \n",
    "> - **LangGraph** 則像是一個「流程編輯器」，讓你清楚定義任務節點與流程邏輯。\n",
    "\n",
    "---\n",
    "\n",
    "## 三、為什麼選擇 LangGraph？\n",
    "\n",
    "LangGraph 建立在 LangChain 的基礎上，但提供了更明確的 **流程控制能力**。  \n",
    "透過「有向圖（Directed Graph）」定義節點（Node）與狀態轉移（Edge），開發者可以輕鬆設計出具備以下特性的 LLM 應用：\n",
    "\n",
    "- ✅ **記憶狀態（Stateful）**：能保存上下文與任務進度。  \n",
    "- 🔁 **可回溯（Reversible）**：支援任務失敗時的重試與回溯。  \n",
    "- ⚡ **動態流程（Dynamic Flow）**：根據結果或上下文動態切換下一步節點。  \n",
    "- 🧩 **多智能體協作（Multi-Agent Coordination）**：支援多個智能體協同工作。  \n",
    "\n",
    "這使得 LangGraph 特別適合需要長期規劃、決策、以及多階段任務的應用場景，例如：\n",
    "- AI 助理工作流程管理  \n",
    "- 自動化決策系統  \n",
    "- 複雜任務規劃與執行（如科研助理、專案管理代理人）\n",
    "\n",
    "---\n",
    "\n",
    "## 四、計劃-行動代理人（Plan-Action Agent）\n",
    "\n",
    "### 🔍 什麼是計劃-行動代理人？\n",
    "\n",
    "計劃-行動代理人是一種基於「**計劃（Plan）** → **行動（Action）** → **反饋（Feedback）**」迭代循環的智能體。  \n",
    "它會根據環境狀況生成計劃，並逐步執行與調整，直到達成目標。\n",
    "\n",
    "典型的工作流程如下：\n",
    "\n",
    "1. **觀察環境**：收集當前環境狀態或上下文。  \n",
    "2. **計劃生成**：根據觀察結果生成一系列行動計劃。  \n",
    "3. **行動執行**：逐步執行每個計劃步驟。  \n",
    "4. **反饋與調整**：根據執行結果與環境變化調整計劃，確保最終目標的達成。\n",
    "\n",
    "---\n",
    "\n",
    "## 五、在 LangGraph 中實現計劃-行動代理人\n",
    "\n",
    "LangGraph 為構建這類代理人提供了理想的結構。  \n",
    "在 LangGraph 中，我們可以透過「節點」定義不同的智能體角色（如計劃生成器、執行者、監督者），並以「邊」定義他們之間的互動流程。\n",
    "\n",
    "例如：\n",
    "\n",
    "[觀察節點] → [計劃生成節點] → [行動執行節點] → [反饋節點]\n",
    "\n",
    "↘———————————————↗（根據反饋動態回到計劃生成）\n",
    "\n",
    "\n",
    "這樣的設計允許代理人在任務執行中進行：\n",
    "- 任務重試（retry）  \n",
    "- 動態決策（dynamic routing）  \n",
    "- 自適應策略調整（adaptive planning）  \n",
    "\n",
    "---\n",
    "\n",
    "## 六、接下來的內容\n",
    "\n",
    "接下來，我們將透過實作一個 **計劃-行動代理人（Plan-Action Agent）** 的範例，  \n",
    "展示如何使用 LangGraph 來：\n",
    "\n",
    "- 定義多個智能體節點  \n",
    "- 設計任務流程與條件轉移  \n",
    "- 管理狀態與任務回溯  \n",
    "\n",
    "透過這個實作，你將能理解 LangGraph 在 **多智能體協作**、**動態決策** 與 **有狀態任務控制** 方面的強大能力。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1856cc-79de-4822-b617-42d6c1b16df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../../\")\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "from src.initialization import credential_init\n",
    "\n",
    "credential_init()\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o-mini\", \n",
    "                   temperature=0\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931bc721-634e-410f-95f2-6efe391d2b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, List, Tuple, Union, Optional\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# The State\n",
    "\n",
    "class PlanExecute(TypedDict):\n",
    "    input: str\n",
    "    plan: List[str]\n",
    "    past_steps: List[Tuple]\n",
    "    response: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a9d27-84a2-4237-b99b-883458533bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Literal, Union\n",
    "from textwrap import dedent\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"\n",
    "    Plan to follow in the future\n",
    "    \"\"\"\n",
    "    type: Literal['plan'] = 'plan'\n",
    "    steps: List[str] = Field(description=\"different steps to follow, should be in sorted order\")\n",
    "\n",
    "\n",
    "planner_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\n",
    "        dedent(\"\"\"\\\n",
    "        For the given objective, come up with a simple step by step plan.\n",
    "        This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps.\n",
    "        The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "        \"\"\")),\n",
    "        (\"placeholder\", \"{messages}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_planner = model.with_structured_output(Plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899b7f45-cc73-419d-9758-3d7c062029e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = planner_prompt|model_planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92635c74-5dfa-48ed-a421-66da1310d7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "planner.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\"user\", \"What is the hometown of the current Australia open winner\")\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db72109-c583-4f59-8db8-44118c619f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response(BaseModel):\n",
    "    \"\"\"Response to user.\"\"\"\n",
    "    type: Literal['response'] = 'response'\n",
    "    response: str\n",
    "\n",
    "\n",
    "class Act(BaseModel):\n",
    "    \"\"\"Action to perform.\"\"\"\n",
    "\n",
    "    action: Union[Response, Plan] = Field(\n",
    "        description=dedent(\"\"\"\\\n",
    "        Action to perform. If you want to respond to user, use `response`.\n",
    "        If you need to further use tools to get the answer, use `plan`.\n",
    "        \"\"\"\n",
    "    )\n",
    "    )\n",
    "\n",
    "replanner_prompt = ChatPromptTemplate.from_template(\n",
    "    dedent(\"\"\"For the given objective, come up with a simple step by step plan. \\\n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "\n",
    "Your objective was this:\n",
    "{input}\n",
    "\n",
    "Your original plan was this:\n",
    "{plan}\n",
    "\n",
    "You have currently done the follow steps:\n",
    "{past_steps}\n",
    "\n",
    "Update your plan accordingly. If no more steps are needed and you can return to the user, then respond with that. Otherwise, fill out the plan. Only add steps to the plan that still NEED to be done. Do not return previously done steps as part of the plan.\"\"\"\n",
    "))\n",
    "\n",
    "model_act = model.with_structured_output(Act)\n",
    "\n",
    "replanner = replanner_prompt | model_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484482cd-3357-4f15-9c23-0225528edcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "replanner.invoke({\"plan\": \"Identify the current Australian Open winner for the most recent tournament.\",\n",
    "                  \"past_steps\": [],\n",
    "                  \"input\": \"What is the hometown of the current Australia open winner\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaecd86e-f890-45ac-9648-c182d650ba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f86669-8576-407b-8bb7-4030e8087ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from openai import OpenAI\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "class Inputs(BaseModel):\n",
    "    query: str = Field(description=\"User query\")\n",
    "\n",
    "\n",
    "class SearchTool(BaseTool):\n",
    "\n",
    "    input_output_parser: PydanticOutputParser = PydanticOutputParser(pydantic_object=Inputs)\n",
    "    input_format_instructions: str = input_output_parser.get_format_instructions()\n",
    "    \n",
    "    name:str = \"websearch tool\"\n",
    "    description_template:str = dedent(\"\"\"\n",
    "    Currently it is 2025.    \n",
    "    Use this tool to collect information from the internet, when you are not sure you know the answer.\n",
    "    The input contains the user's question `query` and the ISO 3166-1 alpha-2 `country_code` inferred from the user's language.\n",
    "    input format instructions: {input_format_instructions}\n",
    "    \"\"\")\n",
    "\n",
    "    description: str = description_template.format(input_format_instructions=input_format_instructions)\n",
    "\n",
    "    def _build_messages_and_opts(self, query: str):\n",
    "        \"\"\"Shared logic for sync + async\"\"\"\n",
    "        \n",
    "        input_ = self.input_output_parser.parse(query)\n",
    "        query = input_.query\n",
    "        country_code = input_.country_code\n",
    "\n",
    "        tool = {\"type\": \"web_search\",\n",
    "                         \"user_location\":{\n",
    "                             \"type\": \"approximate\",\n",
    "                             \"country\": country_code,\n",
    "                         },\n",
    "                        \"search_context_size\": \"medium\"\n",
    "                        }\n",
    "        return query, tool\n",
    "        \n",
    "    def _run(self, query):\n",
    "        \n",
    "        query, tool = self._build_messages_and_opts(query)\n",
    "\n",
    "        response = client.responses.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    tools=[tool],\n",
    "                    tool_choice=\"auto\",\n",
    "                    input=query)\n",
    "\n",
    "        \n",
    "        return response.output_text\n",
    "    \n",
    "    async def _arun(self, query: str):\n",
    "        \n",
    "        query, tool = self._build_messages_and_opts(query)\n",
    "\n",
    "        response = await async_client.responses.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            tools=[tool],\n",
    "            tool_choice=\"auto\",\n",
    "            input=query\n",
    "        )\n",
    "\n",
    "        return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc969039-04eb-4b23-8b06-a149ad764a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_template = dedent(\"\"\"\n",
    "Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "When you need to use a tool to get knowledge, the vectorstore tools have a high priority than websearch tool.\n",
    "\n",
    "To use a tool, you MUST strictly follow this format (case-sensitive, exact words)::\n",
    "\n",
    "```\n",
    "\n",
    "Thought: Do I need to use a tool? Yes\n",
    "\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "\n",
    "Action Input: the input to the action\n",
    "\n",
    "Observation: [the result of the action]\n",
    "\n",
    "```\n",
    "\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "\n",
    "\n",
    "When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n",
    "\n",
    "```\n",
    "\n",
    "Thought: Do I need to use a tool? No\n",
    "\n",
    "Final Answer: [your response here]. The final response should be in Traditional Chinese (繁體中文).\n",
    "\n",
    "```\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Thought:{agent_scratchpad}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "agent_prompt_template = PromptTemplate.from_template(agent_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d394e5ab-4ec7-412b-bd27-3eb73c3623e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langgraph.graph import END\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "# from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "tools = [SearchTool()]\n",
    "agent_model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Choose the LLM that will drive the agent\n",
    "zero_shot_agent = create_react_agent(llm=agent_model, tools=tools, prompt=agent_prompt_template)\n",
    "agent_executor = AgentExecutor(agent=zero_shot_agent, tools=tools, verbose=True, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e7f912-1194-4f19-9520-def70d4aa36c",
   "metadata": {},
   "source": [
    "### Define the functionalities of the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916a8892-d9a6-429f-9f39-4023457db696",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def execute_step(state: PlanExecute):\n",
    "    \"\"\"\n",
    "    此方法負責執行目前計劃（plan）中的第一個步驟。\n",
    "    在 LangGraph 節點中，它通常作為「任務執行節點」，負責根據計劃指令呼叫代理模型（agent）來執行具體任務，並將執行結果記錄到狀態（state）中。\n",
    "    \n",
    "    主要邏輯：\n",
    "    \n",
    "    從 state 讀取目前的計劃 (plan)。\n",
    "    \n",
    "    以編號方式將整個計劃格式化成可讀字串。\n",
    "    \n",
    "    取出計劃中的第一個步驟作為要執行的任務。\n",
    "    \n",
    "    呼叫 agent_executor.ainvoke()，讓智能代理執行該任務。\n",
    "    \n",
    "    將執行結果與任務名稱以 tuple 形式存入 past_steps。\n",
    "    \n",
    "    若 state 中尚無過去紀錄，建立新的 past_steps；否則將新結果附加到現有紀錄中。\n",
    "    \n",
    "    參數：\n",
    "    \n",
    "    state (PlanExecute)：包含計劃、過往步驟與當前輸入的執行狀態。\n",
    "    \n",
    "    回傳：\n",
    "    \n",
    "    dict：包含更新後的 past_steps，即執行任務的歷史紀錄。\n",
    "    \n",
    "    應用情境：\n",
    "    此節點可用於多步驟任務執行流程，例如：\n",
    "    \n",
    "    根據計劃逐步執行動作（如資料蒐集、分析、報告撰寫等）。\n",
    "    \n",
    "    在多代理系統中由特定 agent 執行分配任務。\n",
    "    \"\"\"\n",
    "    plan = state[\"plan\"]\n",
    "    plan_str = \"\\n\".join(f\"{i + 1}. {step}\" for i, step in enumerate(plan))\n",
    "    task = plan[0]\n",
    "    task_formatted = f\"\"\"For the following plan:{plan_str}\\n\\nYou are tasked with executing step {1}, {task}.\"\"\"\n",
    "    \n",
    "    agent_response = await agent_executor.ainvoke(\n",
    "        {\"input\": task_formatted}\n",
    "    )\n",
    "\n",
    "    current_step = [(task, agent_response['output'])]\n",
    "    \n",
    "    if \"past_steps\" not in state:\n",
    "        return {\"past_steps\": current_step}\n",
    "    else:\n",
    "        return {\"past_steps\": state[\"past_steps\"] + current_step}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b1399c-c278-4e57-92eb-a628e8a31e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def plan_step(state: PlanExecute):\n",
    "    \"\"\"\n",
    "    此方法負責根據使用者輸入建立完整的計劃（plan）。\n",
    "    在 LangGraph 節點中，它通常作為「計劃生成節點」，讓規劃器（planner）根據使用者指令產生一系列可執行步驟。\n",
    "    \n",
    "    主要邏輯：\n",
    "    \n",
    "    呼叫 planner.ainvoke()，將使用者輸入的訊息傳入規劃模型。\n",
    "    \n",
    "    接收規劃器輸出的步驟（plan.steps）。\n",
    "    \n",
    "    將這些步驟存入 state，以便後續節點執行。\n",
    "    \n",
    "    參數：\n",
    "    \n",
    "    state (PlanExecute)：包含使用者輸入的執行狀態（state[\"input\"] 為用戶的主要指令）。\n",
    "    \n",
    "    回傳：\n",
    "    \n",
    "    dict：包含一個鍵值對 { \"plan\": plan.steps }，即模型生成的任務步驟清單。\n",
    "    \n",
    "    應用情境：\n",
    "    此節點可用於任務導向系統的開頭階段，例如：\n",
    "    \n",
    "    將「撰寫市場報告」分解為「資料蒐集 → 整理 → 分析 → 撰寫」。\n",
    "    \n",
    "    在多階段推理或自動化任務中生成具體工作計劃。\n",
    "    \"\"\"\n",
    "    \n",
    "    plan = await planner.ainvoke({\"messages\": [(\"user\", state[\"input\"])]})\n",
    "    return {\"plan\": plan.steps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597949a2-3dd0-473d-b33b-9e0556bad85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def replan_step(state: PlanExecute):\n",
    "\n",
    "    \"\"\"\n",
    "    此方法負責在任務執行過程中進行動態重新規劃（Replanning）。\n",
    "    當代理執行中遇到問題、條件改變或4任務未完成時，該節點可呼叫「再規劃器」（replanner）生成新的計劃或直接輸出最終回應。\n",
    "    \n",
    "    主要邏輯：\n",
    "    \n",
    "    呼叫 replanner.ainvoke()，將當前執行狀態作為輸入。\n",
    "    \n",
    "    若返回結果包含 Response 類型的動作，表示可直接輸出最終回覆。\n",
    "    \n",
    "    若結果為新的計劃步驟，則更新 plan。\n",
    "    \n",
    "    若無新步驟可執行，則使用最後一次執行的結果作為回應。\n",
    "    \n",
    "    參數：\n",
    "    \n",
    "    state (PlanExecute)：包含目前的執行狀態、歷史步驟與已知計劃。\n",
    "    \n",
    "    回傳：\n",
    "    \n",
    "    dict：包含新的 plan 或最終 response。\n",
    "    \n",
    "    應用情境：\n",
    "    此節點用於：\n",
    "    \n",
    "    執行任務時根據失敗步驟或環境變化調整計劃。\n",
    "    \n",
    "    讓智能體具備「自我修正」能力。\n",
    "    \n",
    "    在長程任務中根據中間輸出重新安排後續步驟。\n",
    "    \"\"\"\n",
    "    \n",
    "    output = await replanner.ainvoke(state)\n",
    "    if isinstance(output.action, Response):\n",
    "        return {\"response\": output.action.response}\n",
    "    else:\n",
    "        if output.action.steps:\n",
    "            return {\"plan\": output.action.steps}\n",
    "        else:\n",
    "            return {\"response\": state['past_steps'][-1][1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891decb3-0846-4687-bc28-73a849dbcce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_end(state: PlanExecute):\n",
    "    \"\"\"\n",
    "    此方法負責決定工作流程是否結束。\n",
    "    在 LangGraph 節點中，它通常是條件分支（conditional edge）的判斷邏輯，用來檢查任務是否完成或是否應該回到代理繼續執行。\n",
    "    \n",
    "    主要邏輯：\n",
    "    \n",
    "    若 state 中存在 response 且其值非空，代表任務已完成 → 回傳 END。\n",
    "    \n",
    "    否則，回傳 \"agent\"，表示仍需由代理節點繼續執行。\n",
    "    \n",
    "    參數：\n",
    "    \n",
    "    state (PlanExecute)：目前的執行狀態。\n",
    "    \n",
    "    回傳：\n",
    "    \n",
    "    END（任務完成，結束節點）或 \"agent\"（繼續執行下一階段）。\n",
    "    \n",
    "    應用情境：\n",
    "    此節點常用於：\n",
    "    \n",
    "    流程控制（決定任務是否結束）。\n",
    "    \n",
    "    多節點 LangGraph 流程中設置終止條件。\n",
    "    \n",
    "    確保系統在任務完成後適時停止，不進行多餘操作。\n",
    "    \"\"\"\n",
    "    \n",
    "    if \"response\" in state and state[\"response\"]:\n",
    "        return END\n",
    "    else:\n",
    "        return \"agent\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1e7ad6-5d36-4928-b96a-0a572f9b8b55",
   "metadata": {},
   "source": [
    "LangGraph 是宣告式（declarative）的，而非純粹動態（purely dynamic）的。\n",
    "這表示當你在建立工作流程圖（workflow graph）時（也就是在執行之前），LangGraph 需要事先知道所有可能的分支——\n",
    "即使你的條件函式（should_end）會在執行階段才決定實際要走哪一個分支。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ac6db2-e3ce-499a-94fe-77bf5b9eede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "workflow = StateGraph(PlanExecute)\n",
    "\n",
    "# Add the plan node\n",
    "workflow.add_node(\"planner\", plan_step)\n",
    "\n",
    "# Add the execution step\n",
    "workflow.add_node(\"agent\", execute_step)\n",
    "\n",
    "# Add a replan node\n",
    "workflow.add_node(\"replan\", replan_step)\n",
    "\n",
    "workflow.add_edge(START, \"planner\")\n",
    "\n",
    "# From plan we go to agent\n",
    "workflow.add_edge(\"planner\", \"agent\")\n",
    "\n",
    "# From agent, we replan\n",
    "workflow.add_edge(\"agent\", \"replan\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"replan\",\n",
    "    should_end,\n",
    "    [\"agent\", END]\n",
    ")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20752f84-b2b2-4e21-a791-9daaa2d6af34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(app.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79a1255-a90c-43d7-a2f8-7330fe15d71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"recursion_limit\": 50}\n",
    "inputs = {\"input\": \"what is the hometown of the mens 2023 Australia open winner?\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22f0040-bcf8-463c-9e92-b32da959e533",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for event in app.astream(inputs, config=config):\n",
    "    for k, v in event.items():\n",
    "        if k != \"__end__\":\n",
    "            print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a29ef6-8697-47bf-a461-aae3989d56c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"recursion_limit\": 50}\n",
    "inputs = {\"input\": \"富邦悍將的啦啦隊三本柱是誰?\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7f25ad-9e27-41d1-bdbb-f5fbcc4d6e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for event in app.astream(inputs, config=config):\n",
    "    for k, v in event.items():\n",
    "        if k != \"__end__\":\n",
    "            print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c91039-b819-4532-ac19-413968f9a645",
   "metadata": {},
   "source": [
    "## 💬 評論：LangGraph 任務執行觀察4與優化建議\n",
    "\n",
    "這次的最終答案是正確的——**「富邦悍將啦啦隊的三本柱為李珠珢、李雅英、南珉貞」**，結果符合事實。  \n",
    "不過整體推理與行動過程顯得有些「笨拙」，存在明顯的優化空間。\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 問題觀察\n",
    "1. **重複與低效的查詢步驟**  \n",
    "   多次執行類似的 `websearch`，查詢關鍵字差異不大，導致系統重複獲取相近內容，浪費資源與時間。  \n",
    "\n",
    "2. **上下文理解不足**  \n",
    "   當已經在前一步確定「三本柱」指的是富邦悍將的韓籍啦啦隊成員時，後續步驟仍然重新查詢「三位成員的名字」，  \n",
    "   顯示模型沒有有效利用先前的狀態（`past_steps`）。  \n",
    "\n",
    "3. **行動鏈（Agent Chain）過長且缺乏判斷中止條件**  \n",
    "   系統在應該可以結束的階段仍繼續嘗試執行查詢，最終多次出現 `\"Agent stopped due to iteration limit or time limit\"`。  \n",
    "\n",
    "4. **格式錯誤頻繁**  \n",
    "   多次出現 `Invalid Format: Missing 'Action:' after 'Thought:'`，  \n",
    "   顯示代理在產生指令時的格式化邏輯需要加強。  \n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ 改進建議\n",
    "1. **強化上下文記憶使用（State Management）**  \n",
    "   - 在 `execute_step` 或 `replan_step` 階段中，明確讓 agent 檢查 `past_steps`，避免重複查詢相同問題。  \n",
    "\n",
    "2. **加入動態停止條件（Dynamic Stop Condition）**  \n",
    "   - 若 `agent_response` 已包含高置信度的具體名稱（例如三個人名），可直接將狀態切換為 `END`，無需繼續循環。  \n",
    "\n",
    "3. **優化搜尋策略**  \n",
    "   - 將多步查詢整合為單一明確查詢，例如：  \n",
    "     「富邦悍將 啦啦隊 三本柱 2025 成員名單」，以減少噪音。  \n",
    "\n",
    "4. **修正輸出格式生成邏輯**  \n",
    "   - 在 LLM Prompt Template 中，使用結構化格式（如 JSON 或 YAML）明確定義 `Thought / Action / Observation` 區塊，減少格式錯誤。  \n",
    "\n",
    "5. **可考慮加入 Re-Planning 判斷強化**  \n",
    "   - 當系統偵測到重複查詢或內容無變化時，可自動觸發 `replan_step`，切換策略或直接輸出結果。  \n",
    "\n",
    "---\n",
    "\n",
    "### ✅ 總結\n",
    "整體而言，系統最終得出了正確答案，說明任務設計邏輯有效；  \n",
    "但在過程中表現出缺乏上下文記憶、重複查詢與格式錯誤等問題。  \n",
    "\n",
    "若能加強狀態判斷、結果置信度評估與查詢整合，將能顯著提升效率與智能程度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eaa988-b1ca-4a6e-9f98-d84660b4a4ee",
   "metadata": {},
   "source": [
    "### Reserved / Special Parts of `config`\n",
    "\n",
    "Below are the reserved or commonly used keys in the `config` dictionary when working with **LangGraph**:\n",
    "\n",
    "| Key | Purpose | Example |\n",
    "|------|----------|----------|\n",
    "| `\"configurable\"` | ✅ **Main section for user-defined and framework config.** Used by checkpointers, node settings, etc. | `{\"configurable\": {\"thread_id\": \"abc123\"}}` |\n",
    "| `\"recursion_limit\"` | **Limits graph recursion depth.** Prevents infinite loops or too-deep graph calls. | `{\"recursion_limit\": 10}` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8107d5-f8f3-4d87-8ba2-ddc6f7c6700a",
   "metadata": {},
   "source": [
    "# LangGraph Memory\n",
    "\n",
    "## configurable\n",
    "\n",
    "- 取得工作流的最後一個state\n",
    "- 根據當前的state繼續執行\n",
    "\n",
    "Dummy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8db5eeb4-0d36-4a74-a116-8b6c103c8093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from operator import add\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "class State(TypedDict):\n",
    "    foo: str\n",
    "    bar: Annotated[list[str], add]\n",
    "\n",
    "def node_a(state: State):\n",
    "    return {\"foo\": \"a\", \"bar\": [\"a\"]}\n",
    "\n",
    "def node_b(state: State):\n",
    "    return {\"foo\": \"b\", \"bar\": [\"b\"]}\n",
    "\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"node_a\", node_a)\n",
    "workflow.add_node(\"node_b\", node_b)\n",
    "workflow.add_edge(START, \"node_a\")\n",
    "workflow.add_edge(\"node_a\", \"node_b\")\n",
    "workflow.add_edge(\"node_b\", END)\n",
    "\n",
    "\"\"\"\n",
    "Checkpoints:\n",
    "\n",
    "The state of a thread at a particular point in time is called a checkpoint. \n",
    "Checkpoint is a snapshot of the graph state saved at each superstep and is represented by StateSnapshot object.\n",
    "\"\"\"\n",
    "\n",
    "# 要取得state的內容，你必須要同時有 InMemeorySaver 和 configurable\n",
    "checkpointer = InMemorySaver()\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "graph = workflow.compile(checkpointer=checkpointer)\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9d3de89-b134-4577-a0e9-b062514a5e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'foo': 'b', 'bar': ['a', 'b']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"foo\": \"\"}, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e44fd29-d2bd-44da-8238-c23ec1d3ca8a",
   "metadata": {},
   "source": [
    "## Get state\n",
    "\n",
    "當與已儲存的圖形狀態互動時，您必須指定一個執行緒識別碼（thread identifier）。您可以透過呼叫 graph.get_state(config) 來查看圖形的最新狀態。此方法會回傳一個 StateSnapshot 物件，該物件對應於在 config 中提供的執行緒 ID 所關聯的最新檢查點（checkpoint），或者，如果提供了檢查點 ID，則會回傳與該執行緒相關的檢查點狀態。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "801dda91-4fbc-4da1-b311-dfa94e4492ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StateSnapshot(values={'foo': 'b', 'bar': ['a', 'b']}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b3e73-2779-65d2-8002-85fcb600bf9f'}}, metadata={'source': 'loop', 'step': 2, 'parents': {}}, created_at='2025-10-28T10:16:41.400059+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b3e73-2776-6e88-8001-7bdc4ca4f9ca'}}, tasks=(), interrupts=())\n"
     ]
    }
   ],
   "source": [
    "state = graph.get_state(config)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246a7d63-3861-46e2-80a8-762132f6e48b",
   "metadata": {},
   "source": [
    "## Get state history\n",
    "\n",
    "您可以透過呼叫 graph.get_state_history(config) 來取得指定執行緒的完整圖形執行歷史。此方法會回傳一個與 config 中提供的執行緒 ID 相關聯的 StateSnapshot 物件清單。值得注意的是，這些檢查點（checkpoints）會依時間順序排列，最新的檢查點／StateSnapshot 會位於清單的最前端。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44ec5c35-e499-480e-8acc-a63ac0b3e927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StateSnapshot(values={'foo': 'b', 'bar': ['a', 'b']}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b3e73-2779-65d2-8002-85fcb600bf9f'}}, metadata={'source': 'loop', 'step': 2, 'parents': {}}, created_at='2025-10-28T10:16:41.400059+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b3e73-2776-6e88-8001-7bdc4ca4f9ca'}}, tasks=(), interrupts=()),\n",
       " StateSnapshot(values={'foo': 'a', 'bar': ['a']}, next=('node_b',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b3e73-2776-6e88-8001-7bdc4ca4f9ca'}}, metadata={'source': 'loop', 'step': 1, 'parents': {}}, created_at='2025-10-28T10:16:41.399053+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b3e73-2776-6e87-8000-b612ffec4f4a'}}, tasks=(PregelTask(id='30122c1c-bce9-9bd8-c69d-e727b4e353c5', name='node_b', path=('__pregel_pull', 'node_b'), error=None, interrupts=(), state=None, result={'foo': 'b', 'bar': ['b']}),), interrupts=()),\n",
       " StateSnapshot(values={'foo': '', 'bar': []}, next=('node_a',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b3e73-2776-6e87-8000-b612ffec4f4a'}}, metadata={'source': 'loop', 'step': 0, 'parents': {}}, created_at='2025-10-28T10:16:41.399053+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b3e73-2774-66fb-bfff-23a0352c90ed'}}, tasks=(PregelTask(id='f4fe2c79-4446-3244-2d47-5d04b51744ce', name='node_a', path=('__pregel_pull', 'node_a'), error=None, interrupts=(), state=None, result={'foo': 'a', 'bar': ['a']}),), interrupts=()),\n",
       " StateSnapshot(values={'bar': []}, next=('__start__',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b3e73-2774-66fb-bfff-23a0352c90ed'}}, metadata={'source': 'input', 'step': -1, 'parents': {}}, created_at='2025-10-28T10:16:41.398041+00:00', parent_config=None, tasks=(PregelTask(id='7af8e514-861f-263b-fbee-04cf6a67d5ab', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'foo': ''}),), interrupts=())]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "list(graph.get_state_history(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b419ad-c79b-453c-b69f-8f985eb2ca72",
   "metadata": {},
   "source": [
    "## Get State: specific to checkpoint_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a72db02-8e54-4f19-b987-651a01ec7981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StateSnapshot(values={}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_id': '1f0b3d75-e6a5-60d5-8001-d924b38e6b13'}}, metadata=None, created_at=None, parent_config=None, tasks=(), interrupts=())\n"
     ]
    }
   ],
   "source": [
    "state = graph.get_state({\"configurable\": {\"thread_id\": \"1\",\n",
    "                                          \"checkpoint_id\": '1f0b3d75-e6a5-60d5-8001-d924b38e6b13'}})\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8fb2b1-df7d-46d2-9a89-397ea0d6f7a2",
   "metadata": {},
   "source": [
    "## Replay\n",
    "\n",
    "也可以重播先前的圖形執行過程。若我們在呼叫圖形時同時指定 thread_id 和 checkpoint_id，系統將會重新執行對應於該 checkpoint_id 檢查點之前的所有步驟，並僅執行該檢查點之後的步驟。\n",
    "\n",
    "- thread_id 是執行緒的識別碼。\n",
    "- checkpoint_id 是指向該執行緒中某個特定檢查點的識別碼。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e21ee627-250b-43a5-a3df-be5fb7bf26d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph.invoke(None, config={\"configurable\": {\"thread_id\": \"1\",\n",
    "#                                             \"checkpoint_id\": '1f0b3d75-e6a5-60d5-8001-d924b38e6b13'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58ebaa7-a916-47e9-a994-df0dec39a239",
   "metadata": {},
   "source": [
    "重要的是，LangGraph 具備辨識特定步驟是否已被執行過的能力。若某個步驟已經執行過，LangGraph 會在圖形中重播該步驟，而不會重新執行它——但這僅適用於在所提供的 checkpoint_id 之前的步驟。至於 checkpoint_id 之後的所有步驟，則都會被重新執行（即產生一個新的分支），即使這些步驟先前已經執行過。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4635ac1a-2245-4182-aa83-4500876a5937",
   "metadata": {},
   "source": [
    "## Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "944518be-cb53-4375-b705-ce214f1e6a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'foo': 'b', 'bar': ['a', 'b', 'a', 'b']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"foo\": \"c\"}, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fff561c-83b9-4c62-8a5b-d3ab867cf2c5",
   "metadata": {},
   "source": [
    "## InMemoryStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0c610fe-c69a-4df8-a376-6dd5e5f78df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "in_memory_store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2ae107-d000-4c6a-b8b6-69ce01d9fd4e",
   "metadata": {},
   "source": [
    "記憶（Memories）是以一個元組（tuple）作為命名空間（namespace）來區分的，在此特定範例中為 (<user_id>, \"memories\")。命名空間的長度可以是任意的，並且可以代表任何內容，不一定必須與使用者相關。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e652249-f717-4fe5-936c-98377a8e9e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = \"1\"\n",
    "namespace_for_memory = (user_id, \"memories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f481f61-527c-4711-a331-7702f9574d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "memory_id = str(uuid.uuid4())\n",
    "\n",
    "memory = {\"food_preference\" : \"I like pizza\"}\n",
    "\n",
    "# 在特定用戶，特定主題之下，將特定內容用 memory_id 標記並且儲存\n",
    "in_memory_store.put(namespace_for_memory, memory_id, memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd2a052-2594-4529-b79d-ff95a9c3652f",
   "metadata": {},
   "source": [
    "### 檢索\n",
    "\n",
    "根據用戶ID和主題來搜尋相關紀錄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "749ac1e2-bbf5-4d53-a4b0-ce3e71251d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Item(namespace=['1', 'memories'], key='d7f2fe39-10e0-437e-8eca-1ee515ae61fb', value={'food_preference': 'I like pizza'}, created_at='2025-10-28T10:17:15.486303+00:00', updated_at='2025-10-28T10:17:15.486303+00:00', score=None)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_memory_store.search(namespace_for_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1771e17a-0af0-4446-b5fa-9033b5a2a425",
   "metadata": {},
   "source": [
    ">- value：此記憶的內容（本身為一個字典）\n",
    ">- key：此命名空間中該記憶的唯一鍵值\n",
    ">- namespace：由字串組成的清單，表示此記憶類型所屬的命名空間\n",
    ">- created_at：此記憶的建立時間戳記\n",
    ">- updated_at：此記憶的更新時間戳記"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd95bf1-40f8-4b1e-ab4b-5a0b46d7838e",
   "metadata": {},
   "source": [
    "加入語意搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "087dc1c9-0fc1-4d56-9a5b-2a7afdacd760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import init_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c74a52b-1ca9-4722-9d7e-292c81426205",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded = init_embeddings(\"huggingface:BAAI/bge-m3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90dd50a8-31f1-4588-8431-a8c2009bf069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeded.embed_query(\"Hello World\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971b8b40-e907-44a7-b040-7ea8853f0592",
   "metadata": {},
   "source": [
    "The main configuration options:\n",
    "\n",
    ">- embed：嵌入（embedding）提供者（例如 \"openai:text-embedding-3-small\"），或是自訂函式的路徑（doc）。可用的 provider:model 組合取決於 LangChain 的支援。\n",
    ">- dims：所選嵌入模型的維度大小（例如 OpenAI 的 text-embedding-3-small 為 1536 維）。\n",
    ">- fields：要建立索引的欄位清單。可使用 [\"$\"] 來索引整份文件，或指定 JSON 路徑，例如 [\"text\", \"summary\", \"messages[-1]\"]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "423e2856-e034-4524-8664-1d3730c03da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = InMemoryStore(\n",
    "    index={\n",
    "        \"embed\": embeded,  # Embedding provider\n",
    "        \"dims\": 1024,                              # Embedding dimensions\n",
    "        \"fields\": [\"food_preference\"]         # Fields to embed\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "78a8b28b-cfd9-4c20-a32e-9f4d7c11fd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "store.put(namespace_for_memory, memory_id, memory)\n",
    "\n",
    "store.put(\n",
    "    namespace_for_memory,\n",
    "    str(uuid.uuid4()),\n",
    "    {\n",
    "        \"food_preference\": \"I love Italian cuisine\",\n",
    "        \"context\": \"Discussing dinner plans\"\n",
    "    },\n",
    "    index=[\"food_preference\"]  # Only embed \"food_preferences\" field\n",
    ")\n",
    "\n",
    "store.put(\n",
    "    namespace_for_memory,\n",
    "    str(uuid.uuid4()),\n",
    "    {\"system_info\": \"Last updated: 2024-01-01\"},\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d4c4ac2c-fa2f-437e-a41d-a0683392f82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Item(namespace=['1', 'memories'], key='d7f2fe39-10e0-437e-8eca-1ee515ae61fb', value={'food_preference': 'I like pizza'}, created_at='2025-10-28T10:48:53.729832+00:00', updated_at='2025-10-28T10:48:53.729832+00:00', score=0.6639824248624141),\n",
       " Item(namespace=['1', 'memories'], key='2b3639aa-4a3d-46db-afbf-6502bc9af114', value={'food_preference': 'I love Italian cuisine', 'context': 'Discussing dinner plans'}, created_at='2025-10-28T10:48:53.867637+00:00', updated_at='2025-10-28T10:48:53.867637+00:00', score=0.6459898931646416),\n",
       " Item(namespace=['1', 'memories'], key='42fecaf3-147c-41cd-9671-95acbf02928c', value={'system_info': 'Last updated: 2024-01-01'}, created_at='2025-10-28T10:48:53.867637+00:00', updated_at='2025-10-28T10:48:53.867637+00:00', score=None)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store.search(\n",
    "    namespace_for_memory,\n",
    "    query=\"What does the user like to eat?\",\n",
    "    limit=5  # Return top 3 matches\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ee3748-e91a-4ef7-911a-05c172f61b6a",
   "metadata": {},
   "source": [
    "### Search with Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4e12ad88-bad8-47e4-bcdd-ee4cb9956641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Item(namespace=['1', 'memories'], key='9ac5c177-be61-4ae7-a6f9-5d932fade177', value={'food_preference': 'I love Italian cuisine', 'context': 'Discussing dinner plans'}, created_at='2025-10-28T10:38:02.187671+00:00', updated_at='2025-10-28T10:38:02.187671+00:00', score=None)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store.search(namespace_for_memory, filter={\"context\": \"Discussing dinner plans\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c22f284-f5b8-4478-a437-300124864cf4",
   "metadata": {},
   "source": [
    "在Workflow中調用store\n",
    "\n",
    "We can access the in_memory_store and the user_id in any node by passing store: BaseStore and config: RunnableConfig as node arguments. Here's how we might use semantic search in a node to find relevant memories\n",
    "\n",
    "照著官方文件做，會報錯。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "64caa2e5-cb43-44b1-8692-ec7042a44658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x268cc903280>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langchain_core.stores import BaseStore\n",
    "\n",
    "class State(TypedDict):\n",
    "    foo: str\n",
    "    bar: Annotated[list[str], add]\n",
    "\n",
    "def node_a(state: State, config: RunnableConfig):\n",
    "\n",
    "    print(config[\"configurable\"][\"user_id\"])\n",
    "\n",
    "    print(\"*****\")\n",
    "    print(config['configurable']['__pregel_runtime'])\n",
    "    print(\"*****\")\n",
    "\n",
    "    store_ = config['configurable']['__pregel_runtime'].store\n",
    "    \n",
    "    print(store_.search(namespace_for_memory, filter={\"context\": \"Discussing dinner plans\"}))\n",
    "    \n",
    "    return {\"foo\": \"a\", \"bar\": [\"a\"]}\n",
    "\n",
    "def node_b(state: State, config: RunnableConfig):\n",
    "    return {\"foo\": \"b\", \"bar\": [\"b\"]}\n",
    "\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"node_a\", node_a)\n",
    "workflow.add_node(\"node_b\", node_b)\n",
    "workflow.add_edge(START, \"node_a\")\n",
    "workflow.add_edge(\"node_a\", \"node_b\")\n",
    "workflow.add_edge(\"node_b\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1cc2df89-cb73-4780-b1d0-bf8e8d885300",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = InMemorySaver()\n",
    "config = {\"configurable\": {\"thread_id\": \"100\",\n",
    "                           \"user_id\": 1}}\n",
    "\n",
    "graph = workflow.compile(checkpointer=checkpointer, store=store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ad925b18-5977-4c3d-917a-63fa6c827296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "*****\n",
      "Runtime(context=None, store=<langgraph.store.memory.InMemoryStore object at 0x00000268CBE93AB0>, stream_writer=<function Pregel.stream.<locals>.stream_writer at 0x00000268CC904160>, previous=None)\n",
      "*****\n",
      "[Item(namespace=['1', 'memories'], key='2b3639aa-4a3d-46db-afbf-6502bc9af114', value={'food_preference': 'I love Italian cuisine', 'context': 'Discussing dinner plans'}, created_at='2025-10-28T10:48:53.867637+00:00', updated_at='2025-10-28T10:48:53.867637+00:00', score=None)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'foo': 'b', 'bar': ['a', 'b']}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"foo\": \"\"},  config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8205aa57-d0f3-4a75-88c6-40d9f7887ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = graph.nodes['node_a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb4c937-9299-46d9-96b3-411d39d4feae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
