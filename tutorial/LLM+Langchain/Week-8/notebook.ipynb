{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fabc653-5ca9-484b-a864-0b6a363ac275",
   "metadata": {},
   "source": [
    "# ğŸ“– Agent å¯¦ä½œç¯„ä¾‹ï¼šè‡ªå‹•åŒ–æœ‰è²æ›¸ç”Ÿæˆ\n",
    "\n",
    "æœ¬ Notebook å±•ç¤ºå¦‚ä½•é€é Agent ä¸²æ¥ **æ•…äº‹ â†’ åœ–åƒ â†’ èªéŸ³** çš„è‡ªå‹•åŒ–æµç¨‹ã€‚  \n",
    "æ¯å€‹æ­¥é©Ÿéƒ½æœƒå°‡è¼¸å‡ºä¿å­˜ç‚ºæª”æ¡ˆï¼Œé¿å…é‡è¤‡ Token æ¶ˆè€—ï¼Œä¸¦æ–¹ä¾¿å¾ŒçºŒæµç¨‹ä½¿ç”¨ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ æ•´é«”æµç¨‹\n",
    "1. æ•…äº‹å…§å®¹ç”Ÿæˆ  \n",
    "2. åœ–ç‰‡å…§å®¹ç”Ÿæˆ  \n",
    "3. èªéŸ³å…§å®¹ç”Ÿæˆ  \n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£ æ•…äº‹å…§å®¹ç”Ÿæˆ\n",
    "**è¼¸å…¥**  \n",
    "- è‰ç¨¿æ–‡å­—  \n",
    "- æ—¢æœ‰å…§å®¹ (`.txt` æª”æ¡ˆ)  \n",
    "\n",
    "**è™•ç†æµç¨‹**  \n",
    "1. å°‡æ–‡å­—é€å…¥ Langserve æœå‹™  \n",
    "2. æ¥æ”¶ç”Ÿæˆçš„æ•…äº‹æ®µè½  \n",
    "\n",
    "**è¼¸å‡º**  \n",
    "- å°‡çµæœå­˜ç‚º `.txt` æª”  \n",
    "- é¿å… Agent ä¸€ç›´å‚³éæ•´ä»½æ•…äº‹ï¼Œé™ä½ Token æ¶ˆè€—  \n",
    "\n",
    "---\n",
    "\n",
    "## 2ï¸âƒ£ åœ–ç‰‡å…§å®¹ç”Ÿæˆ\n",
    "**è¼¸å…¥**  \n",
    "- æœ€æ–°ç”Ÿæˆçš„æ•…äº‹æ–‡å­—  \n",
    "- æ—¢æœ‰åœ–ç‰‡ (`.png`)  \n",
    "\n",
    "**è™•ç†æµç¨‹**  \n",
    "1. å°‡åœ–ç‰‡ç·¨ç¢¼ç‚º base64  \n",
    "2. æ–‡å­—èˆ‡åœ–ç‰‡é€å…¥ Langserve æœå‹™  \n",
    "3. æ¥æ”¶å›å‚³çš„åœ–ç‰‡ï¼ˆbase64 æ ¼å¼ï¼‰  \n",
    "\n",
    "**è¼¸å‡º**  \n",
    "- å°‡ base64 è§£ç¢¼ç‚ºäºŒé€²ä½è³‡æ–™ï¼Œå­˜æˆ `.png` æª”  \n",
    "\n",
    "---\n",
    "\n",
    "## 3ï¸âƒ£ èªéŸ³å…§å®¹ç”Ÿæˆ\n",
    "**è¼¸å…¥**  \n",
    "- æœ€æ–°ç”Ÿæˆçš„æ•…äº‹æ–‡å­—  \n",
    "\n",
    "**è™•ç†æµç¨‹**  \n",
    "1. å°‡æ–‡å­—é€å…¥ Langserve æœå‹™  \n",
    "2. æ¥æ”¶å›å‚³çš„èªéŸ³ï¼ˆbase64 æ ¼å¼ï¼‰  \n",
    "\n",
    "**è¼¸å‡º**  \n",
    "- å°‡ base64 è§£ç¢¼ç‚ºäºŒé€²ä½è³‡æ–™ï¼Œå­˜æˆ `.mp3` æˆ– `.wav` æª”  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”„ æµç¨‹åœ–\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[æ•…äº‹è‰ç¨¿/èˆŠå…§å®¹ .txt] --> B[é€å…¥ Langserve ç”Ÿæˆæ•…äº‹]\n",
    "    B --> C[æ•…äº‹å…§å®¹ .txt]\n",
    "    C --> D[é€å…¥ Langserve ç”Ÿæˆåœ–ç‰‡ (base64)]\n",
    "    D --> E[è§£ç¢¼ä¸¦å­˜ç‚º .png]\n",
    "    C --> F[é€å…¥ Langserve ç”ŸæˆèªéŸ³ (base64)]\n",
    "    F --> G[è§£ç¢¼ä¸¦å­˜ç‚º .mp3 / .wav]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28811758-cd03-42fc-a88f-1be3988b70fb",
   "metadata": {},
   "source": [
    "## LangServe æœå‹™æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c95db07-6153-4ec9-863c-61a0a49a8288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "os.chdir(\"../../../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314b6765-7764-46ec-bc16-dfa8b5ed4c86",
   "metadata": {},
   "source": [
    "æ¸¬è©¦æ•…äº‹ç”Ÿæˆæœå‹™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d168449-4550-49b8-acd2-21b4d3009efa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    \"http://localhost:8080/story_telling/invoke\",\n",
    "    json={\"input\":{'scratch': \"Create a chapter of a baby owl capturing a rodent in the night as his dinner\",\n",
    "                   'context': \"\"}\n",
    "         }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2882f0e-1aff-4e8f-b7f1-1c101683aa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "story_json = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ff1e2-79b3-4b5c-acc7-71fd70025553",
   "metadata": {},
   "outputs": [],
   "source": [
    "story_json['output']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3654b3-4cf9-4587-a4cd-b023b6e11613",
   "metadata": {},
   "source": [
    "æ¸¬è©¦å½±åƒç”Ÿæˆæœå‹™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174143dd-8f26-486d-9d7e-f22db96d5e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "image_generation_module = importlib.import_module(\"tutorial.LLM+Langchain.Week-8.logic.image_generation\")\n",
    "image_create_pipeline = image_generation_module.image_create_pipeline(image_generation_module.system_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f923fa-d406-482b-855b-b0e869afeada",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_image = requests.post(\n",
    "    \"http://localhost:8080/image_generation/invoke\",\n",
    "    json={\"input\": {'story': story_json['output'],\n",
    "                    \"image_io\": []}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1011d1-ebd2-47b3-9e7e-a1845ac0fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_image.json()['output']['nl_prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4f4f2a-2262-40cc-985a-86d4ecfd3f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "# Decode to bytes\n",
    "image_bytes = base64.b64decode(response_image.json()['output']['image_base64'])\n",
    "\n",
    "with open(\"tutorial/LLM+Langchain/Week-8/story_2_image.png\", \"wb\") as fh:\n",
    "    fh.write(image_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30494746-8671-4ece-81d8-83037a8a1075",
   "metadata": {},
   "source": [
    "æ¸¬è©¦ç”Ÿæˆå¾ŒçºŒå¾ŒçºŒçš„æ•…äº‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592e7b42-5213-41be-9ee8-f0bc0d069453",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_next_tory = requests.post(\n",
    "    \"http://localhost:8080/story_telling/invoke\",\n",
    "    json={\"input\":{'scratch': \"Create the next chapter following the context\",\n",
    "                   'context': story_json['output']}\n",
    "         }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe8ed59-f76c-4069-913a-8e6d482856ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_chapter = response_next_tory.json()['output']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57512583-d7ba-4593-9a9a-3eb67d71849b",
   "metadata": {},
   "source": [
    "æ ¹æ“šæ•…äº‹å’Œä¸Šä¸€å¼µåœ–ç‰‡ï¼Œç”¢ç”Ÿå‡ºä¸‹ä¸€å¼µåœ–ç‰‡\n",
    "\n",
    "é€érequestsé€å‡ºbase64 string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951b9a93-38e3-46d5-a052-3cc958095de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    \"http://localhost:8080/image_generation/invoke\",\n",
    "    json={\"input\": {'story': next_chapter + f\"\\nPrevious image description:\\n\\n{response_image.json()['output']['nl_prompt']}\",\n",
    "                    # 'image_io': [response_image.json()['output']['image_base64']]\n",
    "                    'image_io': []\n",
    "                   }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0003108d-c97c-4c40-8db9-39594eae610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()['output']['nl_prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769f6c0b-6904-4e79-aa5c-05dffb0fac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "image_bytes = base64.b64decode(response.json()['output']['image_base64'])\n",
    "\n",
    "with open(\"tutorial/LLM+Langchain/Week-8/story_3_image.png\", \"wb\") as fh:\n",
    "    fh.write(image_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35819f5-9fa1-4dea-ad1e-1f04a98303bb",
   "metadata": {},
   "source": [
    "æ¸¬è©¦èªéŸ³ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425c3044-274f-4b64-a310-aeb1a37780da",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    \"http://localhost:8080/audio_generation/invoke\",\n",
    "    json={\"input\": {'input': \"How are you doing?\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41234ade-3023-4889-91e1-a9c0230a8452",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_bytes = base64.b64decode(response.json()['output'])\n",
    "\n",
    "with open(\"tutorial/LLM+Langchain/Week-8/test_sample.mp3\", \"wb\") as f:\n",
    "    f.write(audio_bytes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62454159-038d-4e37-999b-ff81159fb7de",
   "metadata": {},
   "source": [
    "## ç”Ÿæˆå·¥å…·æ¨¡æ¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2768e64a-de75-4196-b7b5-640b189e1669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "from typing import Optional, Any, List, Tuple, Callable\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.tools import BaseTool\n",
    "from langchain_core.runnables import Runnable\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic import FilePath\n",
    "\n",
    "from src.initialization import credential_init\n",
    "\n",
    "\n",
    "class ToolTemplate(BaseTool):\n",
    "\n",
    "    \"\"\"\n",
    "    ToolTemplateHTTP: ä¸€å€‹å°ˆé–€ç”¨ä¾†å‘¼å« Langserve REST API çš„ Agent Tool\n",
    "\n",
    "    - ä½¿ç”¨ PydanticOutputParser ä¿è­‰è¼¸å…¥æ ¼å¼æ­£ç¢º\n",
    "    - æ”¯æ´å¤šæ¬„ä½ input/output è™•ç†å™¨\n",
    "    - å° API å‘¼å«åŠ ä¸ŠéŒ¯èª¤è™•ç†\n",
    "    \"\"\"\n",
    "    \n",
    "    runnable: str = Field(..., description='The Langserve endpoint')\n",
    "    name: str\n",
    "    input_parser: PydanticOutputParser\n",
    "    description: str\n",
    "    input_data_processors: Optional[List[Tuple[str, Callable[[Any], Any]]]] = None\n",
    "    output_data_processors: Optional[List[Tuple[Optional[str], Callable[[Any], Any]]]] = None\n",
    "    \n",
    "    @classmethod\n",
    "    def create(cls, runnable: str, name: str, description_template: str,\n",
    "               input_parser: PydanticOutputParser, input_data_processors: Optional=None,\n",
    "               output_data_processors: Optional=None):\n",
    "\n",
    "        \"\"\"å»ºç«‹ Tool å¯¦ä¾‹ï¼Œæœƒè‡ªå‹•æŠŠè¼¸å…¥æ ¼å¼éœ€æ±‚åŠ å…¥ description\"\"\"\n",
    "        \n",
    "        input_format_instruction = input_parser.get_format_instructions()\n",
    "        \n",
    "        description = description_template.format(\n",
    "            input_format_instruction=input_format_instruction\n",
    "        )\n",
    "        \n",
    "        return cls(runnable=runnable, name=name, description=description,\n",
    "                   input_parser=input_parser, input_data_processors=input_data_processors,\n",
    "                   output_data_processors=output_data_processors)\n",
    "    \n",
    "    def _run(self, query: str):\n",
    "\n",
    "        \"\"\"åŸ·è¡Œ Toolï¼ŒåŒæ­¥ç‰ˆæœ¬\"\"\"\n",
    "        \n",
    "        # 1. é©—è­‰ & parse è¼¸å…¥\n",
    "        try:\n",
    "            input_ = self.input_parser.parse(query)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to parse input with parser: {e}, query={query}\")\n",
    "        \n",
    "        runnable_inputs = input_.model_dump()\n",
    "\n",
    "        # 2. input processorsï¼ˆå‰è™•ç†ï¼‰\n",
    "        if self.input_data_processors:\n",
    "            for field, fn in self.input_data_processors:\n",
    "                if field in runnable_inputs:\n",
    "                    runnable_inputs[field] = fn(runnable_inputs[field])\n",
    "            \n",
    "        # 3. å‘¼å« Langserve REST API\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                str(self.runnable),\n",
    "                json={\"input\": runnable_inputs},\n",
    "                timeout=60,\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Langserve call failed: {e}, inputs={runnable_inputs}\")\n",
    "\n",
    "        if \"output\" not in result:\n",
    "            raise RuntimeError(f\"Invalid response format from Langserve: {result}\")\n",
    "        \n",
    "        output = result['output']\n",
    "\n",
    "        # 4. update the state varaibles:\n",
    "        for key in session_state.keys():\n",
    "            if key in output:\n",
    "                session_state[key] = output[key]\n",
    "        \n",
    "        # 5. output processorsï¼ˆå¾Œè™•ç†ï¼‰\n",
    "        if self.output_data_processors:\n",
    "            for field, fn in self.output_data_processors:\n",
    "                if not field:\n",
    "                    fn(output, runnable_inputs['filename'])\n",
    "                else:\n",
    "                    fn(output[field], runnable_inputs['filename'])\n",
    "                    \n",
    "        # é è¨­å›å‚³ã€Œæª”åã€å¦‚æœæœ‰ filenameï¼Œå¦å‰‡å›å‚³è¼¸å‡ºçš„å­—ä¸²\n",
    "        return runnable_inputs.get(\"filename\", output)\n",
    "\n",
    "    def _arun(self, query: str):\n",
    "\n",
    "        raise NotImplementedError(\"This tool does not support async\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7a7685-7318-4349-89a4-159e2879d88e",
   "metadata": {},
   "source": [
    "### State Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd71fca-8a1e-4842-acc9-572921972b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_state = {}\n",
    "\n",
    "session_state['nl_prompt'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e253ad6e-6831-4399-be92-59af97e4c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class StoryInput(BaseModel):\n",
    "    scratch: str = Field(description=dedent(\"\"\"\\\n",
    "                                            The draft, notes, or rough idea for the current page of the story.\n",
    "                                           ï¼ˆæ•…äº‹ç•¶å‰é é¢çš„è‰ç¨¿ã€ç­†è¨˜æˆ–åˆæ­¥æ§‹æƒ³\n",
    "                                            \"\"\"))\n",
    "    context: List[FilePath] = Field(default_factory=list, description=dedent(\"\"\"\\\n",
    "                                                              A list of previously generated .txt files that contain story content.  \n",
    "                                                              Used to maintain narrative consistency and continuity across images.  \n",
    "                                                              å…ˆå‰ç”Ÿæˆçš„ .txt æª”æ¡ˆæ¸…å–®ï¼Œå…¶ä¸­åŒ…å«æ•…äº‹å…§å®¹ã€‚  \n",
    "                                                              ç”¨æ–¼ä¿æŒå½±åƒç”Ÿæˆéç¨‹ä¸­çš„æ•˜äº‹ä¸€è‡´æ€§èˆ‡é€£è²«æ€§ã€‚\n",
    "                                                              \"\"\"))\n",
    "    filename: str = Field(..., description=dedent(\"\"\"\\\n",
    "                                             The file path where the generated story text will be saved.\n",
    "                                            ï¼ˆç”Ÿæˆçš„æ•…äº‹æ–‡æœ¬å°‡è¢«å„²å­˜çš„æª”æ¡ˆè·¯å¾‘ï¼‰\n",
    "                                             \"\"\"))\n",
    "\n",
    "\n",
    "def export_to_txt(text, filename: Path):\n",
    "\n",
    "    dir_ = Path(filename).parent\n",
    "\n",
    "    if not os.path.isdir(dir_):\n",
    "        os.makedirs(dir_)\n",
    "    \n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text)\n",
    "\n",
    "\n",
    "def read_from_txt(filename) -> str:\n",
    "\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "def read_from_list_of_text(filenames) -> str:\n",
    "\n",
    "    return \"\\n\\n\".join([read_from_txt(f) for f in filenames])\n",
    "\n",
    "\n",
    "story_input_data_processors = [(\"context\", read_from_list_of_text)]\n",
    "\n",
    "story_output_data_processors = [(None, export_to_txt)]\n",
    "\n",
    "story_telling_tool = ToolTemplate.create(\n",
    "    runnable=\"http://localhost:8080/story_telling/invoke\",\n",
    "    name=\"Story generation tool\",\n",
    "    description_template=dedent(\"\"\"\\\n",
    "                                This tool is designed to generate a story one page at a time.\n",
    "                                Provide a draft or idea for the current page (`scratch`), along with \n",
    "                                the preceding story context stored as .txt files (`context`), \n",
    "                                and specify where the generated text should be saved (`filename`).\n",
    "                                    \n",
    "                                Input format: {input_format_instruction}\n",
    "                                \"\"\"\n",
    "                                ),\n",
    "    input_parser=PydanticOutputParser(pydantic_object=StoryInput),\n",
    "    output_data_processors = story_output_data_processors,\n",
    "    input_data_processors = story_input_data_processors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eacf5e2-200b-4a00-8cbd-202ee93ef350",
   "metadata": {},
   "source": [
    "### å½±åƒç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc7ff16-8827-4c9d-a68c-35681e0e071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageInput(BaseModel):\n",
    "    story: str = Field(description=dedent(\"\"\"\\\n",
    "                                          The narrative or context used to generate the image prompt.  \n",
    "                                          æ•…äº‹æƒ…ç¯€æˆ–ä¸Šä¸‹æ–‡ï¼Œç”¨æ–¼ç”Ÿæˆå½±åƒæç¤ºã€‚\n",
    "                                          \"\"\")\n",
    "                      )\n",
    "    # FilePath ensures the input path exists and is a valid file\n",
    "    image_io: List[str] = Field([], description=dedent(\"\"\"\\\n",
    "                                                   Path to the previously generated image.  \n",
    "                                                   Used in img2img generation to maintain visual and texture consistency.  \n",
    "                                                   å…ˆå‰ç”Ÿæˆå½±åƒçš„è·¯å¾‘ã€‚  \n",
    "                                                   åœ¨ img2img ç”Ÿæˆä¸­ç”¨æ–¼ä¿æŒè¦–è¦ºèˆ‡æè³ªçš„ä¸€è‡´æ€§ã€‚\n",
    "                                                   \"\"\")\n",
    "                                    )\n",
    "    # Path allows flexibility: file may not exist yet but must be a valid path object.\n",
    "    filename: str = Field(..., description=dedent(\"\"\"\\\n",
    "                                                  Destination file path where the generated image will be saved.  \n",
    "                                                  ç”Ÿæˆå½±åƒçš„å„²å­˜æª”æ¡ˆè·¯å¾‘ã€‚\n",
    "                                                  \"\"\")\n",
    "                          )\n",
    "\n",
    "\n",
    "def image_to_base64(image_path):\n",
    "    \n",
    "    with Image.open(image_path) as image:\n",
    "        \n",
    "        # Save the Image to a Buffer\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=\"JPEG\")\n",
    "        \n",
    "        # Encode the Image to Base64\n",
    "        image_str = base64.b64encode(buffered.getvalue())\n",
    "    \n",
    "    return image_str.decode('utf-8')\n",
    "\n",
    "\n",
    "def image_to_base64_from_list(filenames) -> List[Optional[str]]:\n",
    "\n",
    "    # return [image_to_base64(f) for f in filenames]\n",
    "    return []\n",
    "\n",
    "\n",
    "def export_to_image(content, filename):\n",
    "    \n",
    "    dir_ = Path(filename).parent\n",
    "\n",
    "    if not os.path.isdir(dir_):\n",
    "        os.makedirs(dir_)\n",
    "    \n",
    "    image_bytes = base64.b64decode(content)\n",
    "    \n",
    "    with open(filename, \"wb\") as fh:\n",
    "        fh.write(image_bytes)\n",
    "\n",
    "\n",
    "def story_adaptation(story: str):\n",
    "\n",
    "    nl_prompt = session_state['nl_prompt']\n",
    "    \n",
    "    if nl_prompt:\n",
    "        story += f\"\\nPrevious image description:\\n\\n{nl_prompt}\"\n",
    "\n",
    "    print(f\"******\\n{story}\\n*******\")\n",
    "    \n",
    "    return story\n",
    "    \n",
    "\n",
    "\n",
    "image_output_data_processors = [('image_base64', export_to_image)]\n",
    "\n",
    "image_input_data_processors = [(\"story\", story_adaptation),\n",
    "                                (\"image_io\", image_to_base64_from_list)]\n",
    "\n",
    "image_tool = ToolTemplate.create(\n",
    "    runnable=\"http://localhost:8080/image_generation/invoke\",\n",
    "    name=\"Image generation tool\",\n",
    "    description_template=dedent(\"\"\"\\\n",
    "                                This tool is designed to generate an image to the correspoinding narrative.\n",
    "                                Provide a narrative (`story`), along with \n",
    "                                the preceding images stored as .png files (`image_io`), \n",
    "                                and specify where the generated text should be saved (`filename`).\n",
    "                                    \n",
    "                                Input format: {input_format_instruction}\n",
    "                                \"\"\"\n",
    "                                ),\n",
    "    input_parser=PydanticOutputParser(pydantic_object=ImageInput),\n",
    "    output_data_processors = image_output_data_processors,\n",
    "    input_data_processors = image_input_data_processors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eace21c-9917-4f4c-8a3e-7afc8765cc49",
   "metadata": {},
   "source": [
    "### èªéŸ³ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0fa546-d99b-4ae3-920e-0bf4963a2bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioInput(BaseModel):\n",
    "    input: str = Field(description=dedent(\"\"\"\\\n",
    "                                          The narrative or context used to generate the audio content with text to sound (TTS).  \n",
    "                                          æ•…äº‹æƒ…ç¯€æˆ–ä¸Šä¸‹æ–‡ï¼Œç”¨æ–¼TTSæ–‡å­—è½‰èªéŸ³ã€‚\n",
    "                                          \"\"\")\n",
    "                                    )\n",
    "    # Path allows flexibility: file may not exist yet but must be a valid path object.\n",
    "    filename: str = Field(..., description=dedent(\"\"\"\\\n",
    "                                                  Destination mp3 file path where the generated audio will be saved.  \n",
    "                                                  mp3èªéŸ³æª”çš„å„²å­˜æª”æ¡ˆè·¯å¾‘ã€‚\n",
    "                                                  \"\"\")\n",
    "                         )\n",
    "\n",
    "def export_to_audio(content, filename):\n",
    "\n",
    "    dir_ = Path(filename).parent\n",
    "\n",
    "    if not os.path.isdir(dir_):\n",
    "        os.makedirs(dir_)\n",
    "    \n",
    "    audio_bytes = base64.b64decode(content)\n",
    "    \n",
    "    with open(filename, \"wb\") as fh:\n",
    "        fh.write(audio_bytes)\n",
    "\n",
    "\n",
    "audio_output_data_processors = [(None, export_to_audio)]\n",
    "\n",
    "audio_tool = ToolTemplate.create(\n",
    "    runnable=\"http://localhost:8080/audio_generation/invoke\",\n",
    "    name=\"Audio generation tool\",\n",
    "    description_template=dedent(\"\"\"\\\n",
    "                                This tool is designed to generate an .mp3 file to a corresponding narrative.\n",
    "                                Provide a narrative (`story`), along with \n",
    "                                and specify where the generated audio should be saved (`filename`).\n",
    "                                    \n",
    "                                Input format: {input_format_instruction}\n",
    "                                \"\"\"\n",
    "                                ),\n",
    "    input_parser = PydanticOutputParser(pydantic_object=AudioInput),\n",
    "    output_data_processors = audio_output_data_processors,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8281faeb-b553-47c7-9b03-b7f9b6c671e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "\n",
    "from src.agent.react_zero_shot import prompt_template as zero_shot_prompt_template\n",
    "\n",
    "prompt = PromptTemplate.from_template(zero_shot_prompt_template)\n",
    "\n",
    "tools = [story_telling_tool, image_tool, audio_tool]\n",
    "\n",
    "credential_init()\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o\", temperature=0, \n",
    "                  )\n",
    "\n",
    "zero_shot_agent = create_react_agent(\n",
    "    llm=model,\n",
    "    tools=tools,\n",
    "    prompt=prompt,\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(agent=zero_shot_agent, tools=tools, verbose=True, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60f298a-18b7-4b23-96a4-7757b5f1aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = dedent(\"\"\"\n",
    "Create a chapter of a baby owl capturing a rodent in the night as his dinner.\n",
    "After having the final answer, please create a corresponding image and a corresponding mp3 file.\n",
    "The saved image (.png), text (.txt), and audio (.mp3) should have same name in the folder `tutorial/LLM+Langchain/Week-8/story_test`\n",
    "\"\"\")\n",
    "\n",
    "agent_executor.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25bad6b-5459-440d-bf6f-4b4258abf2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7613df9-de89-4546-b738-323a7cc3a9c9",
   "metadata": {},
   "source": [
    "æˆåŠŸçš„ç”Ÿæˆäº†ä¸€é çš„å…§å®¹ï¼ŒAgentå¯ä»¥å¹«æˆ‘å€‘ç”Ÿæˆæ•´å€‹æ•…äº‹å—?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89f39ff-aeb8-433f-b18b-e20b622e5739",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "         I want to create an 4 pages story for a child. He likes snow owl.\n",
    "         For each page, please create a corresponding image and record the story as an mp3.\n",
    "         After having the final answer, please create a corresponding image and record the story as an mp3. \n",
    "         The saved image and mp3 should have same name, following the structure of \n",
    "         <Page - idx>, with idx as a number starting from 1, in the folder `tutorial/LLM+Langchain/Week-8/story_automation`\n",
    "         \"\"\"\n",
    "\n",
    "agent_executor.invoke({\"input\": prompt})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3683842a-53ee-420d-9423-637ce9cd723a",
   "metadata": {},
   "source": [
    "## ğŸ§ äº’å‹•å¼æœ‰è²æ›¸å…§å®¹ç”Ÿæˆ\n",
    "\n",
    "- ä¸ä¸€å®šéœ€è¦å®Œæ•´çš„ **Agent æ¶æ§‹**ï¼Œå› ç‚ºæµç¨‹çš„æ¯ä¸€æ­¥ï¼ˆæ•…äº‹ â†’ åœ–åƒ â†’ èªéŸ³ï¼‰éƒ½å·²ç¶“æ˜ç¢ºå®šç¾©ï¼Œèƒ½ç”±ä½¿ç”¨è€…ä¸»å‹•è§¸ç™¼ã€‚  \n",
    "- å¯ä»¥ç›´æ¥åŸºæ–¼ **èŠå¤©æ©Ÿå™¨äºº** çš„äº’å‹•å½¢å¼é€²è¡Œï¼Œæ¯æ¬¡è¼¸å…¥ä½¿ç”¨è€…çš„éœ€æ±‚æˆ–æŒ‡ä»¤å¾Œï¼Œç³»çµ±ä¾ç…§æŒ‡å®šæ­¥é©Ÿç”Ÿæˆå°æ‡‰å…§å®¹ã€‚  \n",
    "- ä½¿ç”¨è€…å¯ä»¥åœ¨æ•…äº‹ç”Ÿæˆéç¨‹ä¸­å³æ™‚èª¿æ•´æ–¹å‘ï¼Œä¾‹å¦‚æŒ‡å®šè§’è‰²ã€æƒ…ç¯€èµ°å‘æˆ–èªæ°£ï¼Œæå‡ **å®¢è£½åŒ–é«”é©—**ã€‚  \n",
    "- é€™ç¨®äº’å‹•æ–¹å¼éå¸¸é©åˆ **èªè¨€å­¸ç¿’** å ´æ™¯ï¼š  \n",
    "  - å­¸ç¿’è€…èƒ½ä¸€é‚Šé–±è®€æ•…äº‹ã€ä¸€é‚Šè½æœ‰è²è¼¸å‡º  \n",
    "  - å¯å³æ™‚ä¿®æ”¹æ•…äº‹æƒ…ç¯€ï¼Œç”¢ç”Ÿæ›´è²¼è¿‘å­¸ç¿’éœ€æ±‚çš„å…§å®¹  \n",
    "  - æ­é…åœ–ç‰‡èˆ‡èªéŸ³ï¼Œæå‡æ²‰æµ¸å¼å­¸ç¿’æ•ˆæœ  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adda8172-d8ae-4bf0-80be-14810309324a",
   "metadata": {},
   "source": [
    "# Langgraph\n",
    "\n",
    "åœ¨ç¾ä»£ LLM æ‡‰ç”¨é–‹ç™¼ä¸­ï¼Œ**LangChain** å’Œ **LangGraph** æ˜¯å…©å€‹å¸¸è¢«æåŠçš„å·¥å…·ï¼Œå®ƒå€‘çš†æ—¨åœ¨ç°¡åŒ–èˆ‡å¤§å‹èªè¨€æ¨¡å‹çš„æ•´åˆæµç¨‹ï¼Œä½†å„è‡ªçš„è¨­è¨ˆç†å¿µå’Œä½¿ç”¨å ´æ™¯ç•¥æœ‰ä¸åŒã€‚\n",
    "\n",
    "## LangChain vs. LangGraph æ¯”è¼ƒ\n",
    "\n",
    "| ç‰¹é» | LangChain | LangGraph |\n",
    "|------|-----------|-----------|\n",
    "| æ¶æ§‹é¢¨æ ¼ | å‡½å¼å°å‘ï¼ˆFunction-basedï¼‰ | åœ–å½¢å°å‘ï¼ˆGraph-basedï¼‰ |\n",
    "| æ ¸å¿ƒç”¨é€” | çµ„åˆä¸åŒçš„å·¥å…·èˆ‡éˆï¼ˆChainsï¼‰ä¾†è™•ç†è‡ªç„¶èªè¨€ä»»å‹™ | å®šç¾©ç‹€æ…‹è½‰æ›èˆ‡æµç¨‹æ§åˆ¶çš„ç‹€æ…‹æ©Ÿ |\n",
    "| é©åˆå ´æ™¯ | ç·šæ€§æµç¨‹ã€å¤šå·¥å…·ä¸²æ¥ã€Agentä»»å‹™ | æœ‰ç‹€æ…‹çš„å·¥ä½œæµç¨‹ã€å¤šæ­¥é©Ÿæ±ºç­–ã€å‹•æ…‹æµç¨‹ |\n",
    "| æ§åˆ¶æµç¨‹èƒ½åŠ› | è¼ƒå¼±ï¼Œéœ€é€éç¨‹å¼é‚è¼¯æ§åˆ¶ | è¼ƒå¼·ï¼Œå…§å»ºç‹€æ…‹ç®¡ç†èˆ‡å‹•æ…‹æµç¨‹åˆ‡æ› |\n",
    "| æ˜“å­¸ç¨‹åº¦ | ç›¸å°å®¹æ˜“ä¸Šæ‰‹ | åˆæœŸéœ€è¦ç†è§£ç‹€æ…‹æ©Ÿæ¨¡å‹ |\n",
    "| é€šç”¨æ€§ | é€šç”¨æ€§å¼·ã€æ¨¡çµ„å¤š | æ›´é©åˆè¤‡é›œèˆ‡é•·æœŸä»»å‹™ç®¡ç† |\n",
    "\n",
    "## ç‚ºä»€éº¼é¸æ“‡ LangGraphï¼Ÿ\n",
    "\n",
    "LangGraph å»ºç«‹åœ¨ LangChain çš„åŸºç¤ä¸Šï¼Œä½†æä¾›äº†æ›´æ˜ç¢ºçš„æµç¨‹æ§åˆ¶èƒ½åŠ›ã€‚é€éã€Œæœ‰å‘åœ–ã€çš„æ–¹å¼å®šç¾©ç¯€é»ï¼ˆNodeï¼‰èˆ‡ç‹€æ…‹è½‰ç§»ï¼ˆEdgeï¼‰ï¼Œé–‹ç™¼è€…å¯ä»¥æ›´å®¹æ˜“åœ°è¨­è¨ˆå‡ºå…·å‚™**è¨˜æ†¶ç‹€æ…‹**ã€**å¯å›æº¯æ€§**ã€**å‹•æ…‹æµç¨‹åˆ‡æ›**çš„ LLM æ‡‰ç”¨ã€‚\n",
    "\n",
    "ç°¡è€Œè¨€ä¹‹ï¼Œ**LangChain** æ›´åƒæ˜¯è¨­è¨ˆå·¥å…·çµ„åˆçš„ã€Œç©æœ¨ç®±ã€ï¼Œè€Œ **LangGraph** å‰‡æ˜¯è¨­è¨ˆæµç¨‹å’Œé‚è¼¯çš„ã€Œæµç¨‹ç·¨è¼¯å™¨ã€ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "æ¥ä¸‹ä¾†ï¼Œæˆ‘å€‘å°‡é€éå¯¦ä½œä¸€å€‹ **è¨ˆåŠƒ-è¡Œå‹•ä»£ç†äººï¼ˆPlan-Action Agentï¼‰** çš„ä¾‹å­ï¼Œæ·±å…¥æ¢ç´¢ LangGraph çš„è¨­è¨ˆç†å¿µèˆ‡å¼·å¤§åŠŸèƒ½ã€‚\n",
    "\n",
    "\n",
    "## ä»€éº¼æ˜¯è¨ˆåŠƒ-è¡Œå‹•ä»£ç†äººï¼Ÿ\n",
    "\n",
    "è¨ˆåŠƒ-è¡Œå‹•ä»£ç†äººï¼ˆPlan-Action Agentï¼‰æ˜¯æ™ºèƒ½é«”çš„ä¸€ç¨®ï¼Œå®ƒä¾è³´æ–¼è¨ˆåŠƒç”Ÿæˆå’Œè¡Œå‹•åŸ·è¡Œä¾†é”æˆç›®æ¨™ã€‚ç°¡å–®ä¾†èªªï¼Œé€™ç¨®ä»£ç†äººæœƒé¦–å…ˆæ ¹æ“šç•¶å‰æƒ…æ³å»ºç«‹ä¸€å€‹è¡Œå‹•è¨ˆåŠƒï¼Œç„¶å¾Œæ ¹æ“šè¨ˆåŠƒé€æ­¥åŸ·è¡Œå„é …ä»»å‹™ï¼Œç›´åˆ°é”æˆé å®šçš„ç›®æ¨™ã€‚\n",
    "\n",
    "è¨ˆåŠƒ-è¡Œå‹•ä»£ç†äººçš„å·¥ä½œæµç¨‹é€šå¸¸åŒ…å«ä»¥ä¸‹å¹¾å€‹æ­¥é©Ÿï¼š\n",
    "1. **è§€å¯Ÿç’°å¢ƒ**ï¼šä»£ç†äººæœƒå…ˆæ”¶é›†ç•¶å‰çš„ç’°å¢ƒç‹€æ…‹ä¿¡æ¯ã€‚\n",
    "2. **è¨ˆåŠƒç”Ÿæˆ**ï¼šæ ¹æ“šç•¶å‰ç‹€æ³ï¼Œä»£ç†äººæœƒç”Ÿæˆä¸€å€‹åŒ…å«ä¸€ç³»åˆ—è¡Œå‹•çš„è¨ˆåŠƒã€‚\n",
    "3. **è¡Œå‹•åŸ·è¡Œ**ï¼šä»£ç†äººæŒ‰ç…§è¨ˆåŠƒä¸­çš„æ­¥é©Ÿé€ä¸€åŸ·è¡Œï¼Œä¸¦åœ¨å¿…è¦æ™‚èª¿æ•´è¨ˆåŠƒã€‚\n",
    "4. **åé¥‹èˆ‡èª¿æ•´**ï¼šæ ¹æ“šåŸ·è¡Œçµæœå’Œç’°å¢ƒè®ŠåŒ–ï¼Œä»£ç†äººæœƒæ›´æ–°è¨ˆåŠƒï¼Œç¢ºä¿æœ€çµ‚ç›®æ¨™çš„é”æˆã€‚\n",
    "\n",
    "## Langgraph ä¸­çš„è¨ˆåŠƒ-è¡Œå‹•ä»£ç†äºº\n",
    "\n",
    "Langgraph æ˜¯ä¸€å€‹å¼·å¤§çš„æ¡†æ¶ï¼Œå®ƒå…è¨±æˆ‘å€‘å®šç¾©å’Œå¯¦ç¾å¤šç¨®å½¢å¼çš„æ™ºèƒ½ä»£ç†ç³»çµ±ã€‚åœ¨ Langgraph ä¸­ï¼Œè¨ˆåŠƒ-è¡Œå‹•ä»£ç†äººé€šå¸¸æœƒåˆ©ç”¨åœ–è«–å’Œè¨ˆåŠƒç”Ÿæˆç®—æ³•ä¾†é¸æ“‡æœ€ä½³çš„è¡Œå‹•æ–¹æ¡ˆã€‚é€šéç°¡å–®çš„æ¥å£å’Œæ¨¡å¡ŠåŒ–è¨­è¨ˆï¼ŒLanggraph ä½¿å¾—æ§‹å»ºå’Œè¨“ç·´é€™é¡ä»£ç†äººè®Šå¾—æ›´åŠ é«˜æ•ˆä¸”å¯æ“´å±•ã€‚\n",
    "\n",
    "åœ¨æ¥ä¸‹ä¾†çš„æ•™ç¨‹ä¸­ï¼Œæˆ‘å€‘å°‡å±•ç¤ºå¦‚ä½•ä½¿ç”¨ Langgraph æ§‹å»ºå’Œè¨“ç·´ä¸€å€‹è¨ˆåŠƒ-è¡Œå‹•ä»£ç†äººï¼Œä¸¦é€šéä¸€äº›å¯¦éš›ç¯„ä¾‹ä¾†èªªæ˜å…¶æ‡‰ç”¨å ´æ™¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1856cc-79de-4822-b617-42d6c1b16df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../../\")\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "from src.initialization import credential_init\n",
    "\n",
    "credential_init()\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o-mini\", \n",
    "                   temperature=0\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931bc721-634e-410f-95f2-6efe391d2b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, List, Tuple, Union, Optional\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# The State\n",
    "\n",
    "class PlanExecute(TypedDict):\n",
    "    input: str\n",
    "    plan: List[str]\n",
    "    past_steps: List[Tuple]\n",
    "    response: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a9d27-84a2-4237-b99b-883458533bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Literal, Union\n",
    "from textwrap import dedent\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"\n",
    "    Plan to follow in the future\n",
    "    \"\"\"\n",
    "    type: Literal['plan'] = 'plan'\n",
    "    steps: List[str] = Field(description=\"different steps to follow, should be in sorted order\")\n",
    "\n",
    "\n",
    "planner_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\n",
    "        dedent(\"\"\"\\\n",
    "        For the given objective, come up with a simple step by step plan.\n",
    "        This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps.\n",
    "        The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "        \"\"\")),\n",
    "        (\"placeholder\", \"{messages}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_planner = model.with_structured_output(Plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899b7f45-cc73-419d-9758-3d7c062029e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = planner_prompt|model_planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92635c74-5dfa-48ed-a421-66da1310d7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "planner.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\"user\", \"What is the hometown of the current Australia open winner\")\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db72109-c583-4f59-8db8-44118c619f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Response(BaseModel):\n",
    "#     \"\"\"Response to user.\"\"\"\n",
    "#     type: Literal[\"response\"] = \"response\"\n",
    "#     response: str = Field(descirption=\"The answer to the objective\")\n",
    "\n",
    "\n",
    "# class Act(BaseModel):\n",
    "#     \"\"\"Action to Perform.\"\"\"\n",
    "\n",
    "#     response = Optional[str] = Field(None, description=\"Direct response to the user.\")\n",
    "#     steps = Optional[List[str]] = Field(None, description=\"Plan steps to follow, in sorted order\")\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"Response to user.\"\"\"\n",
    "    type: Literal['response'] = 'response'\n",
    "    response: str\n",
    "\n",
    "\n",
    "class Act(BaseModel):\n",
    "    \"\"\"Action to perform.\"\"\"\n",
    "\n",
    "    action: Union[Response, Plan] = Field(\n",
    "        description=dedent(\"\"\"\\\n",
    "        Action to perform. If you want to respond to user, use `response`.\n",
    "        If you need to further use tools to get the answer, use `plan`.\n",
    "        \"\"\"\n",
    "    )\n",
    "    )\n",
    "\n",
    "replanner_prompt = ChatPromptTemplate.from_template(\n",
    "    dedet(\"\"\"For the given objective, come up with a simple step by step plan. \\\n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "\n",
    "Your objective was this:\n",
    "{input}\n",
    "\n",
    "Your original plan was this:\n",
    "{plan}\n",
    "\n",
    "You have currently done the follow steps:\n",
    "{past_steps}\n",
    "\n",
    "Update your plan accordingly. If no more steps are needed and you can return to the user, then respond with that. Otherwise, fill out the plan. Only add steps to the plan that still NEED to be done. Do not return previously done steps as part of the plan.\"\"\"\n",
    "))\n",
    "\n",
    "model_act = model.with_structured_output(Act)\n",
    "\n",
    "replanner = replanner_prompt | model_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484482cd-3357-4f15-9c23-0225528edcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "replanner.invoke({\"plan\": \"Identify the current Australian Open winner for the most recent tournament.\",\n",
    "                  \"past_steps\": [],\n",
    "                  \"input\": \"What is the hometown of the current Australia open winner\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaecd86e-f890-45ac-9648-c182d650ba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eddea48-2237-45cc-afb6-b6a825df4cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_parser = PydanticOutputParser(pydantic_object=Inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6bd8da-c262-4da6-b566-6a62adb0e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f86669-8576-407b-8bb7-4030e8087ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from openai import OpenAI\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "class Inputs(BaseModel):\n",
    "    query: str = Field(description=\"User query\")\n",
    "\n",
    "\n",
    "class SearchTool(BaseTool):\n",
    "\n",
    "    input_parser: PydanticOutputParser = PydanticOutputParser(pydantic_object=Inputs)\n",
    "    input_format_instructions: str = input_parser.get_format_instructions()\n",
    "    \n",
    "    name:str = \"websearch tool\"\n",
    "    description_template: str = dedent(\"\"\"\n",
    "    Currently it is 2025.    \n",
    "    Use this tool to collect information from the internet, when you are not sure you know the answer.\n",
    "    Action Input format instructions: {input_format_instructions}\n",
    "    \"\"\")\n",
    "\n",
    "    description: str = description_template.format(input_format_instructions=input_format_instructions)\n",
    "    \n",
    "    def _run(self, query):\n",
    "        \n",
    "        input_ = self.input_parser.parse(query)\n",
    "        \n",
    "        query = input_.query\n",
    "        \n",
    "        messages = [{\"role\": \"user\",\n",
    "                     \"content\": query}]\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model='gpt-4o-mini-search-preview',\n",
    "            web_search_options={\"search_context_size\": 'medium',\n",
    "                                \"user_location\": {\n",
    "                                        \"type\": \"approximate\",\n",
    "                                        \"approximate\": {\n",
    "                                            \"country\": \"TW\",\n",
    "                                        }\n",
    "                                    },\n",
    "                                },\n",
    "            messages=messages\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    async def _arun(self, query: str):\n",
    "        \"\"\"Asynchronous version of the run method.\"\"\"\n",
    "        input_ = self.input_parser.parse(query)\n",
    "        query = input_.query\n",
    "        \n",
    "        messages = [{\"role\": \"user\", \"content\": query}]\n",
    "        \n",
    "        # Run the synchronous API call in an asynchronous way using asyncio.to_thread\n",
    "        response = await asyncio.to_thread(\n",
    "            client.chat.completions.create,\n",
    "            model='gpt-4o-mini-search-preview',\n",
    "            web_search_options={\n",
    "                \"search_context_size\": 'medium',\n",
    "                \"user_location\": {\n",
    "                    \"type\": \"approximate\",\n",
    "                    \"approximate\": {\n",
    "                        \"country\": \"TW\",\n",
    "                    }\n",
    "                },\n",
    "            },\n",
    "            messages=messages\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc969039-04eb-4b23-8b06-a149ad764a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_template = dedent(\"\"\"\n",
    "Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "\n",
    "Thought: you should always think about what to do\n",
    "\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "\n",
    "Action Input: the input to the action\n",
    "\n",
    "Observation: the result of the action\n",
    "\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "\n",
    "Thought: I now know the final answer\n",
    "\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Previous conversation history:\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Thought:{agent_scratchpad}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "agent_prompt_template = PromptTemplate.from_template(agent_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d394e5ab-4ec7-412b-bd27-3eb73c3623e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langgraph.graph import END\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "# from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "tools = [SearchTool()]\n",
    "agent_model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "\n",
    "# Choose the LLM that will drive the agent\n",
    "zero_shot_agent = create_react_agent(llm=agent_model, tools=tools, prompt=agent_prompt_template)\n",
    "agent_executor = AgentExecutor(agent=zero_shot_agent, tools=tools, verbose=True)\n",
    "\n",
    "async def execute_step(state: PlanExecute):\n",
    "    plan = state[\"plan\"]\n",
    "    plan_str = \"\\n\".join(f\"{i + 1}. {step}\" for i, step in enumerate(plan))\n",
    "    task = plan[0]\n",
    "    task_formatted = f\"\"\"For the following plan:\n",
    "{plan_str}\\n\\nYou are tasked with executing step {1}, {task}.\"\"\"\n",
    "    \n",
    "    agent_response = await agent_executor.ainvoke(\n",
    "        {\"input\": task_formatted}\n",
    "    )\n",
    "\n",
    "    current_step = [(task, agent_response['output'])]\n",
    "    \n",
    "    if \"past_steps\" not in state:\n",
    "        return {\"past_steps\": current_step}\n",
    "    else:\n",
    "        return {\"past_steps\": state[\"past_steps\"] + current_step}\n",
    "\n",
    "\n",
    "async def plan_step(state: PlanExecute):\n",
    "    plan = await planner.ainvoke({\"messages\": [(\"user\", state[\"input\"])]})\n",
    "    return {\"plan\": plan.steps}\n",
    "\n",
    "\n",
    "async def replan_step(state: PlanExecute):\n",
    "    output = await replanner.ainvoke(state)\n",
    "    print(\"*************************\")\n",
    "    print(output)\n",
    "    print(\"*************************\")\n",
    "    if isinstance(output.action, Response):\n",
    "        return {\"response\": output.action.response}\n",
    "    else:\n",
    "        if output.action.steps:\n",
    "            return {\"plan\": output.action.steps}\n",
    "        else:\n",
    "            return {\"response\": state['past_steps'][-1][1]}\n",
    "\n",
    "\n",
    "def should_end(state: PlanExecute):\n",
    "    if \"response\" in state and state[\"response\"]:\n",
    "        return END\n",
    "    else:\n",
    "        return \"agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ac6db2-e3ce-499a-94fe-77bf5b9eede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "workflow = StateGraph(PlanExecute)\n",
    "\n",
    "# Add the plan node\n",
    "workflow.add_node(\"planner\", plan_step)\n",
    "\n",
    "# Add the execution step\n",
    "workflow.add_node(\"agent\", execute_step)\n",
    "\n",
    "# Add a replan node\n",
    "workflow.add_node(\"replan\", replan_step)\n",
    "\n",
    "workflow.add_edge(START, \"planner\")\n",
    "\n",
    "# From plan we go to agent\n",
    "workflow.add_edge(\"planner\", \"agent\")\n",
    "\n",
    "# From agent, we replan\n",
    "workflow.add_edge(\"agent\", \"replan\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"replan\",\n",
    "    should_end,\n",
    "    [\"agent\", END]\n",
    ")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20752f84-b2b2-4e21-a791-9daaa2d6af34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(app.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79a1255-a90c-43d7-a2f8-7330fe15d71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"recursion_limit\": 50}\n",
    "inputs = {\"input\": \"what is the hometown of the mens 2023 Australia open winner?\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22f0040-bcf8-463c-9e92-b32da959e533",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for event in app.astream(inputs, config=config):\n",
    "    for k, v in event.items():\n",
    "        if k != \"__end__\":\n",
    "            print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a29ef6-8697-47bf-a461-aae3989d56c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7f25ad-9e27-41d1-bdbb-f5fbcc4d6e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
