{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fabc653-5ca9-484b-a864-0b6a363ac275",
   "metadata": {},
   "source": [
    "# 📖 Agent 實作範例：自動化有聲書生成\n",
    "\n",
    "本 Notebook 展示如何透過 Agent 串接 **故事 → 圖像 → 語音** 的自動化流程。  \n",
    "每個步驟都會將輸出保存為檔案，避免重複 Token 消耗，並方便後續流程使用。\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 整體流程\n",
    "1. 故事內容生成  \n",
    "2. 圖片內容生成  \n",
    "3. 語音內容生成  \n",
    "\n",
    "---\n",
    "\n",
    "## 1️⃣ 故事內容生成\n",
    "**輸入**  \n",
    "- 草稿文字  \n",
    "- 既有內容 (`.txt` 檔案)  \n",
    "\n",
    "**處理流程**  \n",
    "1. 將文字送入 Langserve 服務  \n",
    "2. 接收生成的故事段落  \n",
    "\n",
    "**輸出**  \n",
    "- 將結果存為 `.txt` 檔  \n",
    "- 避免 Agent 一直傳遞整份故事，降低 Token 消耗  \n",
    "\n",
    "---\n",
    "\n",
    "## 2️⃣ 圖片內容生成\n",
    "**輸入**  \n",
    "- 最新生成的故事文字  \n",
    "- 既有圖片 (`.png`)  \n",
    "\n",
    "**處理流程**  \n",
    "1. 將圖片編碼為 base64  \n",
    "2. 文字與圖片送入 Langserve 服務  \n",
    "3. 接收回傳的圖片（base64 格式）  \n",
    "\n",
    "**輸出**  \n",
    "- 將 base64 解碼為二進位資料，存成 `.png` 檔  \n",
    "\n",
    "---\n",
    "\n",
    "## 3️⃣ 語音內容生成\n",
    "**輸入**  \n",
    "- 最新生成的故事文字  \n",
    "\n",
    "**處理流程**  \n",
    "1. 將文字送入 Langserve 服務  \n",
    "2. 接收回傳的語音（base64 格式）  \n",
    "\n",
    "**輸出**  \n",
    "- 將 base64 解碼為二進位資料，存成 `.mp3` 或 `.wav` 檔  \n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 流程圖\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[故事草稿/舊內容 .txt] --> B[送入 Langserve 生成故事]\n",
    "    B --> C[故事內容 .txt]\n",
    "    C --> D[送入 Langserve 生成圖片 (base64)]\n",
    "    D --> E[解碼並存為 .png]\n",
    "    C --> F[送入 Langserve 生成語音 (base64)]\n",
    "    F --> G[解碼並存為 .mp3 / .wav]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28811758-cd03-42fc-a88f-1be3988b70fb",
   "metadata": {},
   "source": [
    "## LangServe 服務測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c95db07-6153-4ec9-863c-61a0a49a8288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "os.chdir(\"../../../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314b6765-7764-46ec-bc16-dfa8b5ed4c86",
   "metadata": {},
   "source": [
    "測試故事生成服務"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d168449-4550-49b8-acd2-21b4d3009efa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    \"http://localhost:8080/story_telling/invoke\",\n",
    "    json={\"input\":{'scratch': \"Create a chapter of a baby owl capturing a rodent in the night as his dinner\",\n",
    "                   'context': \"\"}\n",
    "         }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2882f0e-1aff-4e8f-b7f1-1c101683aa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "story_json = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ff1e2-79b3-4b5c-acc7-71fd70025553",
   "metadata": {},
   "outputs": [],
   "source": [
    "story_json['output']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3654b3-4cf9-4587-a4cd-b023b6e11613",
   "metadata": {},
   "source": [
    "測試影像生成服務"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174143dd-8f26-486d-9d7e-f22db96d5e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "image_generation_module = importlib.import_module(\"tutorial.LLM+Langchain.Week-8.logic.image_generation\")\n",
    "image_create_pipeline = image_generation_module.image_create_pipeline(image_generation_module.system_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f923fa-d406-482b-855b-b0e869afeada",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_image = requests.post(\n",
    "    \"http://localhost:8080/image_generation/invoke\",\n",
    "    json={\"input\": {'story': story_json['output'],\n",
    "                    \"image_io\": []}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1011d1-ebd2-47b3-9e7e-a1845ac0fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_image.json()['output']['nl_prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4f4f2a-2262-40cc-985a-86d4ecfd3f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "# Decode to bytes\n",
    "image_bytes = base64.b64decode(response_image.json()['output']['image_base64'])\n",
    "\n",
    "with open(\"tutorial/LLM+Langchain/Week-8/story_2_image.png\", \"wb\") as fh:\n",
    "    fh.write(image_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30494746-8671-4ece-81d8-83037a8a1075",
   "metadata": {},
   "source": [
    "測試生成後續後續的故事"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592e7b42-5213-41be-9ee8-f0bc0d069453",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_next_tory = requests.post(\n",
    "    \"http://localhost:8080/story_telling/invoke\",\n",
    "    json={\"input\":{'scratch': \"Create the next chapter following the context\",\n",
    "                   'context': story_json['output']}\n",
    "         }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe8ed59-f76c-4069-913a-8e6d482856ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_chapter = response_next_tory.json()['output']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57512583-d7ba-4593-9a9a-3eb67d71849b",
   "metadata": {},
   "source": [
    "根據故事和上一張圖片，產生出下一張圖片\n",
    "\n",
    "透過requests送出base64 string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951b9a93-38e3-46d5-a052-3cc958095de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    \"http://localhost:8080/image_generation/invoke\",\n",
    "    json={\"input\": {'story': next_chapter + f\"\\nPrevious image description:\\n\\n{response_image.json()['output']['nl_prompt']}\",\n",
    "                    # 'image_io': [response_image.json()['output']['image_base64']]\n",
    "                    'image_io': []\n",
    "                   }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0003108d-c97c-4c40-8db9-39594eae610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()['output']['nl_prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769f6c0b-6904-4e79-aa5c-05dffb0fac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "image_bytes = base64.b64decode(response.json()['output']['image_base64'])\n",
    "\n",
    "with open(\"tutorial/LLM+Langchain/Week-8/story_3_image.png\", \"wb\") as fh:\n",
    "    fh.write(image_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35819f5-9fa1-4dea-ad1e-1f04a98303bb",
   "metadata": {},
   "source": [
    "測試語音生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425c3044-274f-4b64-a310-aeb1a37780da",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    \"http://localhost:8080/audio_generation/invoke\",\n",
    "    json={\"input\": {'input': \"How are you doing?\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41234ade-3023-4889-91e1-a9c0230a8452",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_bytes = base64.b64decode(response.json()['output'])\n",
    "\n",
    "with open(\"tutorial/LLM+Langchain/Week-8/test_sample.mp3\", \"wb\") as f:\n",
    "    f.write(audio_bytes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62454159-038d-4e37-999b-ff81159fb7de",
   "metadata": {},
   "source": [
    "## 生成工具模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2768e64a-de75-4196-b7b5-640b189e1669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "from typing import Optional, Any, List, Tuple, Callable\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.tools import BaseTool\n",
    "from langchain_core.runnables import Runnable\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic import FilePath\n",
    "\n",
    "from src.initialization import credential_init\n",
    "\n",
    "\n",
    "class ToolTemplate(BaseTool):\n",
    "\n",
    "    \"\"\"\n",
    "    ToolTemplateHTTP: 一個專門用來呼叫 Langserve REST API 的 Agent Tool\n",
    "\n",
    "    - 使用 PydanticOutputParser 保證輸入格式正確\n",
    "    - 支援多欄位 input/output 處理器\n",
    "    - 對 API 呼叫加上錯誤處理\n",
    "    \"\"\"\n",
    "    \n",
    "    runnable: str = Field(..., description='The Langserve endpoint')\n",
    "    name: str\n",
    "    input_parser: PydanticOutputParser\n",
    "    description: str\n",
    "    input_data_processors: Optional[List[Tuple[str, Callable[[Any], Any]]]] = None\n",
    "    output_data_processors: Optional[List[Tuple[Optional[str], Callable[[Any], Any]]]] = None\n",
    "    \n",
    "    @classmethod\n",
    "    def create(cls, runnable: str, name: str, description_template: str,\n",
    "               input_parser: PydanticOutputParser, input_data_processors: Optional=None,\n",
    "               output_data_processors: Optional=None):\n",
    "\n",
    "        \"\"\"建立 Tool 實例，會自動把輸入格式需求加入 description\"\"\"\n",
    "        \n",
    "        input_format_instruction = input_parser.get_format_instructions()\n",
    "        \n",
    "        description = description_template.format(\n",
    "            input_format_instruction=input_format_instruction\n",
    "        )\n",
    "        \n",
    "        return cls(runnable=runnable, name=name, description=description,\n",
    "                   input_parser=input_parser, input_data_processors=input_data_processors,\n",
    "                   output_data_processors=output_data_processors)\n",
    "    \n",
    "    def _run(self, query: str):\n",
    "\n",
    "        \"\"\"執行 Tool，同步版本\"\"\"\n",
    "        \n",
    "        # 1. 驗證 & parse 輸入\n",
    "        try:\n",
    "            input_ = self.input_parser.parse(query)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to parse input with parser: {e}, query={query}\")\n",
    "        \n",
    "        runnable_inputs = input_.model_dump()\n",
    "\n",
    "        # 2. input processors（前處理）\n",
    "        if self.input_data_processors:\n",
    "            for field, fn in self.input_data_processors:\n",
    "                if field in runnable_inputs:\n",
    "                    runnable_inputs[field] = fn(runnable_inputs[field])\n",
    "            \n",
    "        # 3. 呼叫 Langserve REST API\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                str(self.runnable),\n",
    "                json={\"input\": runnable_inputs},\n",
    "                timeout=60,\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Langserve call failed: {e}, inputs={runnable_inputs}\")\n",
    "\n",
    "        if \"output\" not in result:\n",
    "            raise RuntimeError(f\"Invalid response format from Langserve: {result}\")\n",
    "        \n",
    "        output = result['output']\n",
    "\n",
    "        # 4. update the state varaibles:\n",
    "        for key in session_state.keys():\n",
    "            if key in output:\n",
    "                session_state[key] = output[key]\n",
    "        \n",
    "        # 5. output processors（後處理）\n",
    "        if self.output_data_processors:\n",
    "            for field, fn in self.output_data_processors:\n",
    "                if not field:\n",
    "                    fn(output, runnable_inputs['filename'])\n",
    "                else:\n",
    "                    fn(output[field], runnable_inputs['filename'])\n",
    "                    \n",
    "        # 預設回傳「檔名」如果有 filename，否則回傳輸出的字串\n",
    "        return runnable_inputs.get(\"filename\", output)\n",
    "\n",
    "    def _arun(self, query: str):\n",
    "\n",
    "        raise NotImplementedError(\"This tool does not support async\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7a7685-7318-4349-89a4-159e2879d88e",
   "metadata": {},
   "source": [
    "### State Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd71fca-8a1e-4842-acc9-572921972b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_state = {}\n",
    "\n",
    "session_state['nl_prompt'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e253ad6e-6831-4399-be92-59af97e4c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class StoryInput(BaseModel):\n",
    "    scratch: str = Field(description=dedent(\"\"\"\\\n",
    "                                            The draft, notes, or rough idea for the current page of the story.\n",
    "                                           （故事當前頁面的草稿、筆記或初步構想\n",
    "                                            \"\"\"))\n",
    "    context: List[FilePath] = Field(default_factory=list, description=dedent(\"\"\"\\\n",
    "                                                              A list of previously generated .txt files that contain story content.  \n",
    "                                                              Used to maintain narrative consistency and continuity across images.  \n",
    "                                                              先前生成的 .txt 檔案清單，其中包含故事內容。  \n",
    "                                                              用於保持影像生成過程中的敘事一致性與連貫性。\n",
    "                                                              \"\"\"))\n",
    "    filename: str = Field(..., description=dedent(\"\"\"\\\n",
    "                                             The file path where the generated story text will be saved.\n",
    "                                            （生成的故事文本將被儲存的檔案路徑）\n",
    "                                             \"\"\"))\n",
    "\n",
    "\n",
    "def export_to_txt(text, filename: Path):\n",
    "\n",
    "    dir_ = Path(filename).parent\n",
    "\n",
    "    if not os.path.isdir(dir_):\n",
    "        os.makedirs(dir_)\n",
    "    \n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text)\n",
    "\n",
    "\n",
    "def read_from_txt(filename) -> str:\n",
    "\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "def read_from_list_of_text(filenames) -> str:\n",
    "\n",
    "    return \"\\n\\n\".join([read_from_txt(f) for f in filenames])\n",
    "\n",
    "\n",
    "story_input_data_processors = [(\"context\", read_from_list_of_text)]\n",
    "\n",
    "story_output_data_processors = [(None, export_to_txt)]\n",
    "\n",
    "story_telling_tool = ToolTemplate.create(\n",
    "    runnable=\"http://localhost:8080/story_telling/invoke\",\n",
    "    name=\"Story generation tool\",\n",
    "    description_template=dedent(\"\"\"\\\n",
    "                                This tool is designed to generate a story one page at a time.\n",
    "                                Provide a draft or idea for the current page (`scratch`), along with \n",
    "                                the preceding story context stored as .txt files (`context`), \n",
    "                                and specify where the generated text should be saved (`filename`).\n",
    "                                    \n",
    "                                Input format: {input_format_instruction}\n",
    "                                \"\"\"\n",
    "                                ),\n",
    "    input_parser=PydanticOutputParser(pydantic_object=StoryInput),\n",
    "    output_data_processors = story_output_data_processors,\n",
    "    input_data_processors = story_input_data_processors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eacf5e2-200b-4a00-8cbd-202ee93ef350",
   "metadata": {},
   "source": [
    "### 影像生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc7ff16-8827-4c9d-a68c-35681e0e071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageInput(BaseModel):\n",
    "    story: str = Field(description=dedent(\"\"\"\\\n",
    "                                          The narrative or context used to generate the image prompt.  \n",
    "                                          故事情節或上下文，用於生成影像提示。\n",
    "                                          \"\"\")\n",
    "                      )\n",
    "    # FilePath ensures the input path exists and is a valid file\n",
    "    image_io: List[str] = Field([], description=dedent(\"\"\"\\\n",
    "                                                   Path to the previously generated image.  \n",
    "                                                   Used in img2img generation to maintain visual and texture consistency.  \n",
    "                                                   先前生成影像的路徑。  \n",
    "                                                   在 img2img 生成中用於保持視覺與材質的一致性。\n",
    "                                                   \"\"\")\n",
    "                                    )\n",
    "    # Path allows flexibility: file may not exist yet but must be a valid path object.\n",
    "    filename: str = Field(..., description=dedent(\"\"\"\\\n",
    "                                                  Destination file path where the generated image will be saved.  \n",
    "                                                  生成影像的儲存檔案路徑。\n",
    "                                                  \"\"\")\n",
    "                          )\n",
    "\n",
    "\n",
    "def image_to_base64(image_path):\n",
    "    \n",
    "    with Image.open(image_path) as image:\n",
    "        \n",
    "        # Save the Image to a Buffer\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=\"JPEG\")\n",
    "        \n",
    "        # Encode the Image to Base64\n",
    "        image_str = base64.b64encode(buffered.getvalue())\n",
    "    \n",
    "    return image_str.decode('utf-8')\n",
    "\n",
    "\n",
    "def image_to_base64_from_list(filenames) -> List[Optional[str]]:\n",
    "\n",
    "    # return [image_to_base64(f) for f in filenames]\n",
    "    return []\n",
    "\n",
    "\n",
    "def export_to_image(content, filename):\n",
    "    \n",
    "    dir_ = Path(filename).parent\n",
    "\n",
    "    if not os.path.isdir(dir_):\n",
    "        os.makedirs(dir_)\n",
    "    \n",
    "    image_bytes = base64.b64decode(content)\n",
    "    \n",
    "    with open(filename, \"wb\") as fh:\n",
    "        fh.write(image_bytes)\n",
    "\n",
    "\n",
    "def story_adaptation(story: str):\n",
    "\n",
    "    nl_prompt = session_state['nl_prompt']\n",
    "    \n",
    "    if nl_prompt:\n",
    "        story += f\"\\nPrevious image description:\\n\\n{nl_prompt}\"\n",
    "\n",
    "    print(f\"******\\n{story}\\n*******\")\n",
    "    \n",
    "    return story\n",
    "    \n",
    "\n",
    "\n",
    "image_output_data_processors = [('image_base64', export_to_image)]\n",
    "\n",
    "image_input_data_processors = [(\"story\", story_adaptation),\n",
    "                                (\"image_io\", image_to_base64_from_list)]\n",
    "\n",
    "image_tool = ToolTemplate.create(\n",
    "    runnable=\"http://localhost:8080/image_generation/invoke\",\n",
    "    name=\"Image generation tool\",\n",
    "    description_template=dedent(\"\"\"\\\n",
    "                                This tool is designed to generate an image to the correspoinding narrative.\n",
    "                                Provide a narrative (`story`), along with \n",
    "                                the preceding images stored as .png files (`image_io`), \n",
    "                                and specify where the generated text should be saved (`filename`).\n",
    "                                    \n",
    "                                Input format: {input_format_instruction}\n",
    "                                \"\"\"\n",
    "                                ),\n",
    "    input_parser=PydanticOutputParser(pydantic_object=ImageInput),\n",
    "    output_data_processors = image_output_data_processors,\n",
    "    input_data_processors = image_input_data_processors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eace21c-9917-4f4c-8a3e-7afc8765cc49",
   "metadata": {},
   "source": [
    "### 語音生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0fa546-d99b-4ae3-920e-0bf4963a2bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioInput(BaseModel):\n",
    "    input: str = Field(description=dedent(\"\"\"\\\n",
    "                                          The narrative or context used to generate the audio content with text to sound (TTS).  \n",
    "                                          故事情節或上下文，用於TTS文字轉語音。\n",
    "                                          \"\"\")\n",
    "                                    )\n",
    "    # Path allows flexibility: file may not exist yet but must be a valid path object.\n",
    "    filename: str = Field(..., description=dedent(\"\"\"\\\n",
    "                                                  Destination mp3 file path where the generated audio will be saved.  \n",
    "                                                  mp3語音檔的儲存檔案路徑。\n",
    "                                                  \"\"\")\n",
    "                         )\n",
    "\n",
    "def export_to_audio(content, filename):\n",
    "\n",
    "    dir_ = Path(filename).parent\n",
    "\n",
    "    if not os.path.isdir(dir_):\n",
    "        os.makedirs(dir_)\n",
    "    \n",
    "    audio_bytes = base64.b64decode(content)\n",
    "    \n",
    "    with open(filename, \"wb\") as fh:\n",
    "        fh.write(audio_bytes)\n",
    "\n",
    "\n",
    "audio_output_data_processors = [(None, export_to_audio)]\n",
    "\n",
    "audio_tool = ToolTemplate.create(\n",
    "    runnable=\"http://localhost:8080/audio_generation/invoke\",\n",
    "    name=\"Audio generation tool\",\n",
    "    description_template=dedent(\"\"\"\\\n",
    "                                This tool is designed to generate an .mp3 file to a corresponding narrative.\n",
    "                                Provide a narrative (`story`), along with \n",
    "                                and specify where the generated audio should be saved (`filename`).\n",
    "                                    \n",
    "                                Input format: {input_format_instruction}\n",
    "                                \"\"\"\n",
    "                                ),\n",
    "    input_parser = PydanticOutputParser(pydantic_object=AudioInput),\n",
    "    output_data_processors = audio_output_data_processors,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8281faeb-b553-47c7-9b03-b7f9b6c671e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "\n",
    "from src.agent.react_zero_shot import prompt_template as zero_shot_prompt_template\n",
    "\n",
    "prompt = PromptTemplate.from_template(zero_shot_prompt_template)\n",
    "\n",
    "tools = [story_telling_tool, image_tool, audio_tool]\n",
    "\n",
    "credential_init()\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o\", temperature=0, \n",
    "                  )\n",
    "\n",
    "zero_shot_agent = create_react_agent(\n",
    "    llm=model,\n",
    "    tools=tools,\n",
    "    prompt=prompt,\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(agent=zero_shot_agent, tools=tools, verbose=True, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60f298a-18b7-4b23-96a4-7757b5f1aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = dedent(\"\"\"\n",
    "Create a chapter of a baby owl capturing a rodent in the night as his dinner.\n",
    "After having the final answer, please create a corresponding image and a corresponding mp3 file.\n",
    "The saved image (.png), text (.txt), and audio (.mp3) should have same name in the folder `tutorial/LLM+Langchain/Week-8/story_test`\n",
    "\"\"\")\n",
    "\n",
    "agent_executor.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25bad6b-5459-440d-bf6f-4b4258abf2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7613df9-de89-4546-b738-323a7cc3a9c9",
   "metadata": {},
   "source": [
    "成功的生成了一頁的內容，Agent可以幫我們生成整個故事嗎?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89f39ff-aeb8-433f-b18b-e20b622e5739",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "         I want to create an 4 pages story for a child. He likes snow owl.\n",
    "         For each page, please create a corresponding image and record the story as an mp3.\n",
    "         After having the final answer, please create a corresponding image and record the story as an mp3. \n",
    "         The saved image and mp3 should have same name, following the structure of \n",
    "         <Page - idx>, with idx as a number starting from 1, in the folder `tutorial/LLM+Langchain/Week-8/story_automation`\n",
    "         \"\"\"\n",
    "\n",
    "agent_executor.invoke({\"input\": prompt})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3683842a-53ee-420d-9423-637ce9cd723a",
   "metadata": {},
   "source": [
    "## 🎧 互動式有聲書內容生成\n",
    "\n",
    "- 不一定需要完整的 **Agent 架構**，因為流程的每一步（故事 → 圖像 → 語音）都已經明確定義，能由使用者主動觸發。  \n",
    "- 可以直接基於 **聊天機器人** 的互動形式進行，每次輸入使用者的需求或指令後，系統依照指定步驟生成對應內容。  \n",
    "- 使用者可以在故事生成過程中即時調整方向，例如指定角色、情節走向或語氣，提升 **客製化體驗**。  \n",
    "- 這種互動方式非常適合 **語言學習** 場景：  \n",
    "  - 學習者能一邊閱讀故事、一邊聽有聲輸出  \n",
    "  - 可即時修改故事情節，產生更貼近學習需求的內容  \n",
    "  - 搭配圖片與語音，提升沉浸式學習效果  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adda8172-d8ae-4bf0-80be-14810309324a",
   "metadata": {},
   "source": [
    "# Langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafca2e5-93fc-4a49-92c6-d0cae5664cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "output_response_schemas = [\n",
    "        ResponseSchema(name=\"story\", description=\"the story content in the page\"),\n",
    "        ResponseSchema(name=\"page index\", description=\"The page number of the story\"),\n",
    "    ]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(output_response_schemas)\n",
    "\n",
    "output_format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "           Create a story page {idx}, based on the description: {text}\n",
    "\n",
    "           The answer continues from previous content:\n",
    "           {context}\n",
    "\n",
    "           After having the final answer, please create a corresponding image and record the story as an mp3. \n",
    "           The saved image and mp3 should have same name, following the structure of \n",
    "           <Page - idx>, in the folder `tutorial/LLM+Langchain/Week-8`\n",
    "\n",
    "           The output should have the following format: {output_format_instruction}\n",
    "           \"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(template=template,\n",
    "                                 input_variables=[\"text\", \"context\", \"idx\"],\n",
    "                                 partial_variables={\"output_format_instruction\": output_format_instructions})\n",
    "\n",
    "agent_chain = RunnablePassthrough.assign(input=prompt_template)|agent_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0e5fb0-709c-4a51-a737-d95d949df1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = agent_chain.invoke({\"text\": \"A little cat just woke up in the morning\",\n",
    "                        \"context\": \"The beginning of the story:\\n\",\n",
    "                        \"idx\": str(1)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5c5db4-a1f1-48a9-9316-4734886a4d11",
   "metadata": {},
   "source": [
    "若是以下步驟失敗，嘗試重新生成。這是大語言模型，沒有保證可以100%產出你希望的格式。我們只能盡可能提高成功輸出的機率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727746b3-7d2a-44cb-bc2a-65ee873aec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4eec1e-8a56-49f5-9562-9fad0f524536",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser.parse(Q['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d871f0-78aa-408e-b362-c2ec3d58f6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser.parse(Q['output'])['story']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04b14f4-af8c-48f1-a35f-19581300062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser.parse(Q['output'])['page index']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7173de4-9db3-4e49-bf0f-af35f2654cf9",
   "metadata": {},
   "source": [
    "### 第二頁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c9062a-d980-43e3-a374-aaef6f31bf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_list = [output_parser.parse(Q['output'])['story']]\n",
    "print(context_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96840fa-af12-432a-89a8-79eadb9dcca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_2 = agent_chain.invoke({\"text\": \"Whisker found a dove and wanted to hunt it down!\",\n",
    "                          \"context\": \":\\n\".join(context_list),\n",
    "                          \"idx\": str(2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9eba24-c286-4c95-95df-9d83f439e766",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc45668-0d3a-4582-b6b7-6f8a53c06e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser.parse(Q_2['output'])['story']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd27823-fe18-4e44-998c-06e4629d9f67",
   "metadata": {},
   "source": [
    "### okay, it looks fine, let us see how to make it a interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854b6e69-3036-45d2-9461-b17f667a3fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"前情提要\"\n",
    "context_list = []\n",
    "\n",
    "# 頁面起始\n",
    "idx = 1\n",
    "\n",
    "while True:\n",
    "    if len(context_list) == 0:\n",
    "        context = \"The beginning of the story:\\n\"\n",
    "    else:\n",
    "        context = \"\\n\".join(context_list)\n",
    "\n",
    "    text = input(\"請輸入故事內容: 若想要結束 請輸入 `QUIT`\")\n",
    "\n",
    "    if text == \"QUIT\":\n",
    "        break\n",
    "    \n",
    "    Q = agent_chain.invoke({\"text\": text,\n",
    "                            \"context\": context,\n",
    "                            \"idx\": str(idx)})\n",
    "\n",
    "    story = output_parser.parse(Q['output'])['story']\n",
    "    \n",
    "    # 下一頁\n",
    "    idx += 1\n",
    "\n",
    "    context_list.append(output_parser.parse(Q['output'])['story'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931bc721-634e-410f-95f2-6efe391d2b61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
