{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c5eb77-e7d2-42df-ad6f-a97ab1241cdc",
   "metadata": {},
   "source": [
    "# å¤šæ™ºèƒ½é«”ç³»çµ±ç°¡ä»‹ï¼šåŸºæ–¼ LangGraph çš„æž¶æ§‹å°Žè«–\n",
    "\n",
    "## ðŸ“˜ å‰è¨€\n",
    "åœ¨äººå·¥æ™ºæ…§ï¼ˆAIï¼‰æ‡‰ç”¨ä¸­ï¼Œã€Œæ™ºèƒ½é«”ï¼ˆAgentï¼‰ã€æ˜¯ä¸€ç¨®èƒ½å¤ è‡ªå‹•åŸ·è¡Œä»»å‹™çš„ç³»çµ±ï¼Œèƒ½æ ¹æ“šç’°å¢ƒè¼¸å…¥é€²è¡ŒæŽ¨ç†ã€æ±ºç­–ä¸¦ç”¢ç”Ÿè¡Œå‹•ã€‚å‚³çµ±ä¸Šï¼Œå–®ä¸€æ™ºèƒ½é«”å¯é€éŽå…§å»ºçš„å·¥å…·ï¼ˆå¦‚è³‡æ–™æª¢ç´¢ã€API æ“ä½œæˆ–æ–‡å­—ç”Ÿæˆï¼‰ä¾†å®Œæˆç‰¹å®šé ˜åŸŸçš„å·¥ä½œã€‚ç„¶è€Œï¼Œå³ä½¿ä½¿ç”¨åƒ **GPT-4** é€™æ¨£å¼·å¤§çš„æ¨¡åž‹ï¼Œç•¶ä»»å‹™æ¶‰åŠå¤šå€‹é ˜åŸŸæˆ–éœ€è¦åŒæ™‚ä½¿ç”¨å¤šç¨®å·¥å…·æ™‚ï¼Œå–®ä¸€æ™ºèƒ½é«”å¾€å¾€é¡¯å¾—åŠ›ä¸å¾žå¿ƒã€‚\n",
    "\n",
    "ç‚ºäº†å…‹æœé€™æ¨£çš„é™åˆ¶ï¼Œ**å¤šæ™ºèƒ½é«”ç³»çµ±ï¼ˆMulti-Agent System, MASï¼‰** æ‡‰é‹è€Œç”Ÿã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ¤– å¤šæ™ºèƒ½é«”ç¶²çµ¡ï¼ˆMulti-Agent Networkï¼‰\n",
    "å¤šæ™ºèƒ½é«”ç¶²çµ¡æ˜¯ä¸€ç¨®ã€Œ**åˆ†è€Œæ²»ä¹‹ï¼ˆdivide-and-conquerï¼‰**ã€çš„è¨­è¨ˆæ€ç¶­ã€‚  \n",
    "èˆ‡å…¶ä¾è³´å–®ä¸€å¤§åž‹æ¨¡åž‹è™•ç†æ‰€æœ‰ä»»å‹™ï¼Œä¸å¦‚å°‡ä»»å‹™æ‹†è§£ï¼Œäº¤ç”±å¤šå€‹å°ˆé–€åŒ–çš„æ™ºèƒ½é«”å„å¸å…¶è·ã€‚\n",
    "\n",
    "èˆ‰ä¾‹ä¾†èªªï¼š\n",
    "- ä¸€å€‹æ™ºèƒ½é«”å°ˆé–€è² è²¬è³‡æ–™æœå°‹ï¼›\n",
    "- ä¸€å€‹æ™ºèƒ½é«”å°ˆæ³¨æ–¼è³‡æ–™åˆ†æžï¼›\n",
    "- å¦ä¸€å€‹æ™ºèƒ½é«”è² è²¬å°‡çµæžœè½‰æ›ç‚ºè‡ªç„¶èªžè¨€å ±å‘Šã€‚\n",
    "\n",
    "é€™äº›æ™ºèƒ½é«”é€éŽä¸€å€‹å”èª¿æ©Ÿåˆ¶ï¼ˆCoordinator æˆ– Orchestratorï¼‰äº’ç›¸æºé€šï¼Œå½¢æˆä¸€å€‹å”ä½œå¼ç¶²çµ¡ã€‚é€™æ¨£çš„æž¶æ§‹ä¸åƒ…èƒ½æé«˜æ•ˆçŽ‡ï¼Œé‚„èƒ½æ“´å±•ç³»çµ±çš„èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤ æ‡‰å°æ›´è¤‡é›œçš„å·¥ä½œæµç¨‹ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  ç‚ºä½•æŽ¡ç”¨å¤šæ™ºèƒ½é«”æž¶æ§‹ï¼Ÿ\n",
    "| æŒ‘æˆ° | å–®æ™ºèƒ½é«”é™åˆ¶ | å¤šæ™ºèƒ½é«”è§£æ³• |\n",
    "|------|---------------|---------------|\n",
    "| å¤šé ˜åŸŸå°ˆæ¥­çŸ¥è­˜ | æ¨¡åž‹é›£ä»¥åŒæ™‚ç²¾é€šå¤šé ˜åŸŸ | ç‚ºæ¯å€‹é ˜åŸŸå»ºç«‹å°ˆå®¶åž‹æ™ºèƒ½é«” |\n",
    "| å·¥å…·æ•´åˆ | å·¥å…·éŽå¤šå°Žè‡´æ±ºç­–å›°é›£ | å„æ™ºèƒ½é«”å„è‡ªä½¿ç”¨ç‰¹å®šå·¥å…· |\n",
    "| ä»»å‹™è¤‡é›œåº¦ | é•·éˆä»»å‹™å®¹æ˜“éŒ¯èª¤ç´¯ç© | é€éŽåˆ†å·¥èˆ‡å›žé¥‹æé«˜æº–ç¢ºçŽ‡ |\n",
    "| å¯æ“´å±•æ€§ | æ¨¡åž‹é›£ä»¥å‹•æ…‹èª¿æ•´ | å¯å½ˆæ€§å¢žæ¸›æ™ºèƒ½é«”ç¯€é»ž |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ—ï¸ LangGraph ä¸­çš„å¤šæ™ºèƒ½é«”æž¶æ§‹ç¯„ä¾‹\n",
    "ä¸€å€‹ç°¡å–®çš„å¤šæ™ºèƒ½é«”æµç¨‹å¯èƒ½å¦‚ä¸‹ï¼š\n",
    "\n",
    "1. **ä»»å‹™è·¯ç”±ä»£ç†ï¼ˆRouter Agentï¼‰**  \n",
    "   - è² è²¬æŽ¥æ”¶ä½¿ç”¨è€…çš„è«‹æ±‚ï¼Œä¸¦æ±ºå®šæ‡‰äº¤ç”±å“ªå€‹å°ˆå®¶æ™ºèƒ½é«”è™•ç†ã€‚  \n",
    "\n",
    "2. **å°ˆå®¶æ™ºèƒ½é«”ï¼ˆExpert Agentsï¼‰**  \n",
    "   - å„è‡ªå°ˆç²¾æ–¼ç‰¹å®šé ˜åŸŸï¼ˆå¦‚æ•¸æ“šåˆ†æžã€ç¨‹å¼ç”Ÿæˆã€æ–‡ä»¶æ’°å¯«ï¼‰ã€‚  \n",
    "\n",
    "3. **å”èª¿ç¯€é»žï¼ˆCoordinator Nodeï¼‰**  \n",
    "   - æ”¶é›†ä¸¦æ•´åˆå„å°ˆå®¶æ™ºèƒ½é«”çš„è¼¸å‡ºï¼Œç”Ÿæˆæœ€çµ‚çµæžœã€‚  \n",
    "\n",
    "é€éŽ LangGraph çš„ç¯€é»žè¨­è¨ˆï¼Œå¯ä»¥è¼•é¬†å®šç¾©é€™æ¨£çš„ç¶²çµ¡çµæ§‹ï¼Œä¸¦ä½¿ç”¨åœ–çš„å½¢å¼ç›´è§€åœ°è¿½è¹¤æ¯å€‹æ­¥é©Ÿã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” å°çµ\n",
    "å¤šæ™ºèƒ½é«”ç³»çµ±ä»£è¡¨äº†äººå·¥æ™ºæ…§æ‡‰ç”¨å¾žã€Œå–®ä¸€æ¨¡åž‹ã€é‚å‘ã€Œå”ä½œå¼æ™ºèƒ½ã€çš„é‡è¦ä¸€æ­¥ã€‚  \n",
    "åœ¨ **LangGraph** çš„å¹«åŠ©ä¸‹ï¼Œé–‹ç™¼è€…èƒ½ä»¥æ¨¡çµ„åŒ–èˆ‡å¯è¦–åŒ–çš„æ–¹å¼ï¼Œæ§‹å»ºèƒ½å”åŒå·¥ä½œçš„æ™ºèƒ½é«”ç¶²çµ¡ï¼Œé€²è€Œå¯¦ç¾æ›´é«˜æ•ˆã€æ›´å…·å½ˆæ€§çš„ AI è§£æ±ºæ–¹æ¡ˆã€‚\n",
    "\n",
    "æœªä¾†ï¼Œéš¨è‘—å¤šæ™ºèƒ½é«”æž¶æ§‹çš„æˆç†Ÿï¼Œæˆ‘å€‘å°‡æ›´æŽ¥è¿‘ä¸€å€‹ã€Œè‡ªçµ„ç¹”å¼æ™ºèƒ½é«”ç¤¾ç¾¤ï¼ˆAgent Societyï¼‰ã€çš„é¡˜æ™¯â€”â€”è®“ AI ä¸åƒ…èƒ½ã€Œæ€è€ƒã€ï¼Œæ›´èƒ½ã€Œå”ä½œã€ã€‚\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30589385-0fa3-48a0-81cf-7d0eef3caad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a072957-5c1c-4938-87cf-08cb9fcc12b0",
   "metadata": {},
   "source": [
    "## Researcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2358d28-7fa5-435a-9d98-fc1ec7ab2758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_system_prompt(suffix: str) -> str:\n",
    "    return (\n",
    "        \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "        \" Use the provided tools to progress towards answering the question.\"\n",
    "        \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "        \" will help where you left off. Execute what you can to make progress.\"\n",
    "        \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "        \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "        f\"\\n{suffix}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3481af9-e693-43a7-9e50-5d091dd64018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from textwrap import dedent\n",
    "\n",
    "from langgraph.graph import END\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.types import Command\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "from src.initialization import credential_init\n",
    "\n",
    "credential_init()\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=6,\n",
    "    disable_streaming=False\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "search = TavilySearchAPIWrapper()\n",
    "tavily_tool = TavilySearchResults(api_wrapper=search, max_results=5)\n",
    "\n",
    "# Research agent and node\n",
    "research_agent = create_react_agent(\n",
    "    model,\n",
    "    tools=[tavily_tool],\n",
    "    prompt=make_system_prompt(\n",
    "        \"You can only do research. You are working with a chart generator colleague.\"\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917cd1b6-3f1f-486e-b77f-0e971f2a0ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=dedent(\"\"\"\n",
    "                                         First, get the UK's GDP over the past 5 years, then make a line chart of it. \n",
    "                                         Once you make the chart, finish.\n",
    "                                         \"\"\"))]\n",
    "\n",
    "researcher_response = research_agent.invoke({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf200ddb-9664-4062-851f-86aedeabd571",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "researcher_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e456a75-4082-45f4-8b47-840e169a5ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "researcher_response[\"messages\"][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ca4faf-083f-493f-bce0-c0c698a6f24e",
   "metadata": {},
   "source": [
    "## Chart Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7ea460-444a-4407-823b-e487477a4035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-experimental matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10379307-7e59-4909-856a-e0c2b5392040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "\"\"\"\n",
    "In frameworks such as LangChain, LlamaIndex, or OpenAIâ€™s function calling examples, \n",
    "PythonREPL is often an execution tool or class that lets an AI agent safely run Python code.\n",
    "\"\"\"\n",
    "repl = PythonREPL()\n",
    "\n",
    "@tool\n",
    "def python_repl_tool(\n",
    "    code: Annotated[str, \"The python code to execute to generate your chart.\"],\n",
    "):\n",
    "    \"\"\"Use this to execute python code. If you want to see the output of a value,\n",
    "    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n",
    "    try:\n",
    "        result = repl.run(code)\n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "    result_str = f\"Successfully executed:\\n```python\\n{code}\\n```\\nStdout: {result}\"\n",
    "    return (\n",
    "        result_str + \"\\n\\nIf you have completed all tasks, respond with FINAL ANSWER.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977e3407-6f25-4b2e-bd8b-2c82375090d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_agent = create_react_agent(\n",
    "    model,\n",
    "    [python_repl_tool],\n",
    "    prompt=make_system_prompt(\n",
    "        \"You can only generate charts. You are working with a researcher colleague.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fcc364-b705-4f74-a654-2d1fc78ed606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "messages.append(researcher_response[\"messages\"][-1])\n",
    "\n",
    "# Reverse the role of ai and \n",
    "cls_map = {\"ai\": HumanMessage, \"human\": AIMessage}\n",
    "    \n",
    "messages_charter = [cls_map[msg.type](content=msg.content) for msg in messages]\n",
    "\n",
    "charter_response = chart_agent.invoke({\"messages\": messages_charter}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09742c8e-fd99-434a-a66d-c2e33b37ab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "charter_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b19af7-3989-4d07-974d-9b0bd9829989",
   "metadata": {},
   "source": [
    "## Langgraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf74c70-3889-472b-9f1f-e9e271eff8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.types import Command\n",
    "\n",
    "\n",
    "def get_next_node(last_message: BaseMessage, goto: str):\n",
    "    if \"FINAL ANSWER\" in last_message.content:\n",
    "        # Any agent decided the work is done\n",
    "        return END\n",
    "    return goto\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "async def research_node(state: State):\n",
    "\n",
    "    response = await research_agent.ainvoke({\"messages\": state[\"messages\"]})\n",
    "\n",
    "    last_message = response[\"messages\"][-1]\n",
    "    \n",
    "    goto = get_next_node(last_message, 'chart')\n",
    "    \n",
    "    # return Command(update={\"messages\": response[-1]},\n",
    "    #                goto=goto)\n",
    "    return Command(update={\"messages\": state[\"messages\"] + [last_message]},\n",
    "                   goto=goto)\n",
    "\n",
    "\n",
    "async def chart_node(state: State):\n",
    "    \n",
    "    # Reverse the role of ai and \n",
    "    cls_map = {\"ai\": HumanMessage, \"human\": AIMessage}\n",
    "        \n",
    "    messages_chart = [cls_map[msg.type](content=msg.content) for msg in state[\"messages\"]]\n",
    "    \n",
    "    response = await chart_agent.ainvoke({\"messages\": messages_chart})\n",
    "\n",
    "    last_message = HumanMessage(content=response[\"messages\"][-1].content)\n",
    "    \n",
    "    goto = get_next_node(last_message, 'research')\n",
    "    \n",
    "    return Command(update={\"messages\": state[\"messages\"] + [last_message]},\n",
    "                   goto=goto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2444b89-5031-457b-b74b-02e360264884",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(State)\n",
    "\n",
    "workflow.add_node(\"research\", research_node)\n",
    "workflow.add_node(\"chart\", chart_node)\n",
    "\n",
    "workflow.add_edge(START, \"research\")\n",
    "\n",
    "memory = InMemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2895b1-e5f6-48c6-9141-bc617edd5402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7033f7-1ef9-463b-99de-9963e6fa22b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=dedent(\"\"\"\n",
    "                                         First, get the UK's GDP over the past 5 years, then make a line chart of it. \n",
    "                                         Once you make the chart, finish.\n",
    "                                         \"\"\"))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d81a401-7ab8-4b9c-afd6-f15767b067ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for event in app.astream(\n",
    "    {\n",
    "        \"messages\": messages,\n",
    "    },\n",
    "    config,\n",
    "):\n",
    "    print(event)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93cf92c-1097-4ec4-aefe-2605d8deabe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = app.get_state(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd915c3-e339-436f-9627-7a7381b597ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "state.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d77e4e-8e84-4488-8aa4-e1e82a7a3926",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
