{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c5eb77-e7d2-42df-ad6f-a97ab1241cdc",
   "metadata": {},
   "source": [
    "# 多智能體系統簡介：基於 LangGraph 的架構導論\n",
    "\n",
    "## 📘 前言\n",
    "在人工智慧（AI）應用中，「智能體（Agent）」是一種能夠自動執行任務的系統，能根據環境輸入進行推理、決策並產生行動。傳統上，單一智能體可透過內建的工具（如資料檢索、API 操作或文字生成）來完成特定領域的工作。然而，即使使用像 **GPT-4** 這樣強大的模型，當任務涉及多個領域或需要同時使用多種工具時，單一智能體往往顯得力不從心。\n",
    "\n",
    "為了克服這樣的限制，**多智能體系統（Multi-Agent System, MAS）** 應運而生。\n",
    "\n",
    "---\n",
    "\n",
    "## 🤖 多智能體網絡（Multi-Agent Network）\n",
    "多智能體網絡是一種「**分而治之（divide-and-conquer）**」的設計思維。  \n",
    "與其依賴單一大型模型處理所有任務，不如將任務拆解，交由多個專門化的智能體各司其職。\n",
    "\n",
    "舉例來說：\n",
    "- 一個智能體專門負責資料搜尋；\n",
    "- 一個智能體專注於資料分析；\n",
    "- 另一個智能體負責將結果轉換為自然語言報告。\n",
    "\n",
    "這些智能體透過一個協調機制（Coordinator 或 Orchestrator）互相溝通，形成一個協作式網絡。這樣的架構不僅能提高效率，還能擴展系統的能力，使其能夠應對更複雜的工作流程。\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 為何採用多智能體架構？\n",
    "| 挑戰 | 單智能體限制 | 多智能體解法 |\n",
    "|------|---------------|---------------|\n",
    "| 多領域專業知識 | 模型難以同時精通多領域 | 為每個領域建立專家型智能體 |\n",
    "| 工具整合 | 工具過多導致決策困難 | 各智能體各自使用特定工具 |\n",
    "| 任務複雜度 | 長鏈任務容易錯誤累積 | 透過分工與回饋提高準確率 |\n",
    "| 可擴展性 | 模型難以動態調整 | 可彈性增減智能體節點 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🏗️ LangGraph 中的多智能體架構範例\n",
    "一個簡單的多智能體流程可能如下：\n",
    "\n",
    "1. **任務路由代理（Router Agent）**  \n",
    "   - 負責接收使用者的請求，並決定應交由哪個專家智能體處理。  \n",
    "\n",
    "2. **專家智能體（Expert Agents）**  \n",
    "   - 各自專精於特定領域（如數據分析、程式生成、文件撰寫）。  \n",
    "\n",
    "3. **協調節點（Coordinator Node）**  \n",
    "   - 收集並整合各專家智能體的輸出，生成最終結果。  \n",
    "\n",
    "透過 LangGraph 的節點設計，可以輕鬆定義這樣的網絡結構，並使用圖的形式直觀地追蹤每個步驟。\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 小結\n",
    "多智能體系統代表了人工智慧應用從「單一模型」邁向「協作式智能」的重要一步。  \n",
    "在 **LangGraph** 的幫助下，開發者能以模組化與可視化的方式，構建能協同工作的智能體網絡，進而實現更高效、更具彈性的 AI 解決方案。\n",
    "\n",
    "未來，隨著多智能體架構的成熟，我們將更接近一個「自組織式智能體社群（Agent Society）」的願景——讓 AI 不僅能「思考」，更能「協作」。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30589385-0fa3-48a0-81cf-7d0eef3caad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a072957-5c1c-4938-87cf-08cb9fcc12b0",
   "metadata": {},
   "source": [
    "## Researcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2358d28-7fa5-435a-9d98-fc1ec7ab2758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_system_prompt(suffix: str) -> str:\n",
    "    return (\n",
    "        \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "        \" Use the provided tools to progress towards answering the question.\"\n",
    "        \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "        \" will help where you left off. Execute what you can to make progress.\"\n",
    "        \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "        \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "        f\"\\n{suffix}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3481af9-e693-43a7-9e50-5d091dd64018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from textwrap import dedent\n",
    "\n",
    "from langgraph.graph import END\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.types import Command\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "from src.initialization import credential_init\n",
    "\n",
    "credential_init()\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=6,\n",
    "    disable_streaming=False\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "search = TavilySearchAPIWrapper()\n",
    "tavily_tool = TavilySearchResults(api_wrapper=search, max_results=5)\n",
    "\n",
    "# Research agent and node\n",
    "research_agent = create_react_agent(\n",
    "    model,\n",
    "    tools=[tavily_tool],\n",
    "    prompt=make_system_prompt(\n",
    "        \"You can only do research. You are working with a chart generator colleague.\"\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917cd1b6-3f1f-486e-b77f-0e971f2a0ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=dedent(\"\"\"\n",
    "                                         First, get the UK's GDP over the past 5 years, then make a line chart of it. \n",
    "                                         Once you make the chart, finish.\n",
    "                                         \"\"\"))]\n",
    "\n",
    "researcher_response = research_agent.invoke({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf200ddb-9664-4062-851f-86aedeabd571",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "researcher_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e456a75-4082-45f4-8b47-840e169a5ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "researcher_response[\"messages\"][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ca4faf-083f-493f-bce0-c0c698a6f24e",
   "metadata": {},
   "source": [
    "## Chart Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7ea460-444a-4407-823b-e487477a4035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-experimental matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10379307-7e59-4909-856a-e0c2b5392040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "\"\"\"\n",
    "In frameworks such as LangChain, LlamaIndex, or OpenAI’s function calling examples, \n",
    "PythonREPL is often an execution tool or class that lets an AI agent safely run Python code.\n",
    "\"\"\"\n",
    "repl = PythonREPL()\n",
    "\n",
    "@tool\n",
    "def python_repl_tool(\n",
    "    code: Annotated[str, \"The python code to execute to generate your chart.\"],\n",
    "):\n",
    "    \"\"\"Use this to execute python code. If you want to see the output of a value,\n",
    "    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n",
    "    try:\n",
    "        result = repl.run(code)\n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "    result_str = f\"Successfully executed:\\n```python\\n{code}\\n```\\nStdout: {result}\"\n",
    "    return (\n",
    "        result_str + \"\\n\\nIf you have completed all tasks, respond with FINAL ANSWER.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977e3407-6f25-4b2e-bd8b-2c82375090d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_agent = create_react_agent(\n",
    "    model,\n",
    "    [python_repl_tool],\n",
    "    prompt=make_system_prompt(\n",
    "        \"You can only generate charts. You are working with a researcher colleague.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fcc364-b705-4f74-a654-2d1fc78ed606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "messages.append(researcher_response[\"messages\"][-1])\n",
    "\n",
    "# Reverse the role of ai and \n",
    "cls_map = {\"ai\": HumanMessage, \"human\": AIMessage}\n",
    "    \n",
    "messages_charter = [cls_map[msg.type](content=msg.content) for msg in messages]\n",
    "\n",
    "charter_response = chart_agent.invoke({\"messages\": messages_charter}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09742c8e-fd99-434a-a66d-c2e33b37ab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "charter_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b19af7-3989-4d07-974d-9b0bd9829989",
   "metadata": {},
   "source": [
    "## Langgraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf74c70-3889-472b-9f1f-e9e271eff8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.types import Command\n",
    "\n",
    "\n",
    "def get_next_node(last_message: BaseMessage, goto: str):\n",
    "    if \"FINAL ANSWER\" in last_message.content:\n",
    "        # Any agent decided the work is done\n",
    "        return END\n",
    "    return goto\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "async def research_node(state: State):\n",
    "\n",
    "    response = await research_agent.ainvoke({\"messages\": state[\"messages\"]})\n",
    "\n",
    "    last_message = response[\"messages\"][-1]\n",
    "    \n",
    "    goto = get_next_node(last_message, 'chart')\n",
    "    \n",
    "    # return Command(update={\"messages\": response[-1]},\n",
    "    #                goto=goto)\n",
    "    return Command(update={\"messages\": state[\"messages\"] + [last_message]},\n",
    "                   goto=goto)\n",
    "\n",
    "\n",
    "async def chart_node(state: State):\n",
    "    \n",
    "    # Reverse the role of ai and \n",
    "    cls_map = {\"ai\": HumanMessage, \"human\": AIMessage}\n",
    "        \n",
    "    messages_chart = [cls_map[msg.type](content=msg.content) for msg in state[\"messages\"]]\n",
    "    \n",
    "    response = await chart_agent.ainvoke({\"messages\": messages_chart})\n",
    "\n",
    "    last_message = HumanMessage(content=response[\"messages\"][-1].content)\n",
    "    \n",
    "    goto = get_next_node(last_message, 'research')\n",
    "    \n",
    "    return Command(update={\"messages\": state[\"messages\"] + [last_message]},\n",
    "                   goto=goto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2444b89-5031-457b-b74b-02e360264884",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(State)\n",
    "\n",
    "workflow.add_node(\"research\", research_node)\n",
    "workflow.add_node(\"chart\", chart_node)\n",
    "\n",
    "workflow.add_edge(START, \"research\")\n",
    "\n",
    "memory = InMemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2895b1-e5f6-48c6-9141-bc617edd5402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7033f7-1ef9-463b-99de-9963e6fa22b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=dedent(\"\"\"\n",
    "                                         First, get the UK's GDP over the past 5 years, then make a line chart of it. \n",
    "                                         Once you make the chart, finish.\n",
    "                                         \"\"\"))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d81a401-7ab8-4b9c-afd6-f15767b067ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for event in app.astream(\n",
    "    {\n",
    "        \"messages\": messages,\n",
    "    },\n",
    "    config,\n",
    "):\n",
    "    print(event)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93cf92c-1097-4ec4-aefe-2605d8deabe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = app.get_state(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd915c3-e339-436f-9627-7a7381b597ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "state.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d77e4e-8e84-4488-8aa4-e1e82a7a3926",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
