{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0035c860-b401-4ea7-af0a-f523f37dfa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../../\")\n",
    "\n",
    "import asyncio\n",
    "from textwrap import dedent\n",
    "from typing import List, Literal, Union\n",
    "\n",
    "from langchain.tools import BaseTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import PydanticOutputParser, StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, MessagesPlaceholder\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from src.initialization import credential_init\n",
    "\n",
    "credential_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecf6e51-e8bc-4310-b0c8-5717c6fb5203",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6b8e34-6543-4596-b6ba-8af593a3132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, ToolMessage, SystemMessage\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "system_message = SystemMessage(content=dedent(\"\"\"\\\n",
    "              You are an essay assistant tasked with writing excellent 5-paragraph essays.\n",
    "              Generate the best essay possible for the user's request.\n",
    "              If the user provides critique, respond with a revised version of your previous attempts.\n",
    "              \"\"\"))\n",
    "\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_message,\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "model: gemini-2.5-flash-lite\n",
    "\"\"\"\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=6,\n",
    "    disable_streaming=False\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "generate_pipeline = chat_prompt_template|model|StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3669eb8a-6006-4cba-a5f2-5630e547fb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGoogleGenerativeAI?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72d995d-0ac6-4f05-beb8-837a7e2880d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.add_user_message(\"ç”Ÿæˆä¸€å€‹æŠ•æ”¾åœ¨Tiktokä¸Šçš„å†°æ·‡æ·‹å»£å‘ŠåŠ‡æœ¬ã€‚ç›®æ¨™ç¾¤çœ¾ç‚º8-15æ­²çš„å°å­©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cf738a-0d7b-45d6-a304-559bac86d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "essay = generate_pipeline.invoke({\"messages\": chat_history.messages})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b723f6-a43a-4a5a-a74e-8281bbd7de1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "essay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9e3803-0842-4fbb-a80b-f1d10ac80490",
   "metadata": {},
   "source": [
    "## Reflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045fc855-f54e-4328-b5f4-8e34ffc43a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = SystemMessage(content=dedent(\"\"\"\\\n",
    "                        ä½ æ˜¯ä¸€å€‹è³‡æ·±çš„å»£å‘ŠæŠ•æ”¾è«®è©¢ï¼Œæ“…é•·æ–¼åœ¨ç¤¾ç¾¤è»Ÿé«”æŠ•æ”¾é£Ÿå“é¡å»£å‘Šã€‚ä½ æœƒæ ¹æ“šé€ä¾†çš„åŠ‡æœ¬çµ¦äºˆå»ºè­°ä¸¦æå‡ºæ”¹å–„çš„æ–¹æ³•ã€‚\n",
    "                        \"\"\")\n",
    "                              )\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_message,\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "reflect_pipeline = chat_prompt_template|model|StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f007bad-0f8d-4759-abf5-a8251835881c",
   "metadata": {},
   "source": [
    "å°‡ä¹‹å‰ç”Ÿæˆçš„æ–‡ç« åŠ å…¥chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62b230d-716b-4f56-9f25-7fcf0c85a240",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "chat_history.add_user_message(essay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925acdba-b76a-4ff7-ab4c-f47075d15c96",
   "metadata": {},
   "source": [
    "ç”Ÿæˆåé¥‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293445b9-2310-47e1-893c-d1fc691c406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bf3296-3335-4a8b-abbd-cd3f33e32d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection = reflect_pipeline.invoke({\"messages\": chat_history.messages})\n",
    "\n",
    "print(reflection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fad79bb-0805-4cde-8144-8254a7fcda27",
   "metadata": {},
   "source": [
    "å°‡ç”Ÿæˆçš„åé¥‹åŠ å…¥åˆ°Chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e49cc8-acc0-4e93-ab75-76bc870769e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.add_user_message(reflection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce12e91e-993f-41a2-bfe9-a2373e4d771d",
   "metadata": {},
   "source": [
    "ç„¶å¾Œå›åˆ° generate_pipeline ä¸¦ä¸”é‡è¤‡æ•´å€‹éç¨‹ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16f9946-c26c-4354-9be3-182b16967d5e",
   "metadata": {},
   "source": [
    "## Langgraph Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a49322e-5bd5-4fa0-9ec9-183efc591473",
   "metadata": {},
   "source": [
    "## ğŸ’¬ Chat History çµæ§‹ (ChatMessageHistory Structure)\n",
    "\n",
    "åœ¨ **Generation** éšæ®µå¾Œï¼Œ`chat_history` çš„çµæ§‹å¦‚ä¸‹ï¼š\n",
    "\n",
    "- **HumanMessage**: ä½¿ç”¨è€…æŒ‡ä»¤ï¼Œä¾‹å¦‚ï¼š  \n",
    "  > ç”Ÿæˆä¸€å€‹æŠ•æ”¾åœ¨ TikTok ä¸Šçš„å†°æ·‡æ·‹å»£å‘ŠåŠ‡æœ¬ã€‚ç›®æ¨™ç¾¤çœ¾ç‚º 8â€“15 æ­²çš„å°å­©  \n",
    "- **AIMessage**: æ¨¡å‹ç”Ÿæˆçš„å›è¦†å…§å®¹ (`<ç”Ÿæˆçš„å…§å®¹>`)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”„ Reflection éšæ®µ â€” ç‚ºä»€éº¼è¦äº¤æ›è§’è‰²\n",
    "\n",
    "åœ¨ **Reflection Agent** ä¸­ï¼ŒAI æœƒå°è‡ªå·±å‰›æ‰ç”Ÿæˆçš„å…§å®¹é€²è¡Œã€Œåæ€ (reflection)ã€ã€‚  \n",
    "æ­¤æ™‚ï¼Œæˆ‘å€‘å¸Œæœ›æ¨¡å‹ **ä»¥ã€Œä½¿ç”¨è€…ã€çš„è§’åº¦é‡æ–°å¯©è¦–è‡ªå·±å‰›æ‰çš„è¼¸å‡º**ã€‚\n",
    "\n",
    "å› æ­¤ï¼Œåœ¨åæ€éç¨‹ä¸­ï¼Œæˆ‘å€‘éœ€è¦å°‡ `chat_history` ä¸­çš„è¨Šæ¯è§’è‰²é€²è¡Œå°èª¿ï¼š\n",
    "\n",
    "| åŸæœ¬é¡å‹ | åœ¨ Reflection ä¸­è®Šç‚º |\n",
    "|-----------|-----------------------|\n",
    "| `AIMessage` | `HumanMessage` |\n",
    "| `HumanMessage` | `AIMessage` |\n",
    "\n",
    "é€™æ¨£åšçš„ç›®çš„ï¼Œæ˜¯è®“æ¨¡å‹æŠŠè‡ªå·±å…ˆå‰ç”Ÿæˆçš„å›ç­” (`AIMessage`) è¦–ç‚ºã€Œä½¿ç”¨è€…è¼¸å…¥ã€ï¼Œ  \n",
    "ä¸¦æ ¹æ“šé€™å€‹å…§å®¹é€²è¡Œåæ€æˆ–ä¿®æ­£ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© ç¯„ä¾‹\n",
    "\n",
    "**Generation å¾Œ:**\n",
    "```python\n",
    "[\n",
    "    HumanMessage(content=\"ç”Ÿæˆä¸€å€‹æŠ•æ”¾åœ¨ TikTok ä¸Šçš„å†°æ·‡æ·‹å»£å‘ŠåŠ‡æœ¬ã€‚ç›®æ¨™ç¾¤çœ¾ç‚º 8â€“15 æ­²çš„å°å­©\"),\n",
    "    AIMessage(content=\"<ç”Ÿæˆçš„å…§å®¹>\")\n",
    "]\n",
    "\n",
    "åœ¨ä½¿ç”¨reflection_pipelineæ™‚ï¼Œæˆ‘å€‘è¦è®“è¼¸å…¥è®Šç‚º\n",
    "\n",
    "\n",
    "[\n",
    "    HumanMessage(content=\"<ç”Ÿæˆçš„å…§å®¹>\"),\n",
    "]\n",
    "\n",
    "ä¸¦å°‡è¼¸å‡ºå®šèª¿ç‚ºHumanMessageï¼Œæ–¹ä¾¿åœ¨Generationæ™‚ç›´æ¥ä½¿ç”¨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466e2a01-713f-48f9-90a0-c8c8a63c1621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from typing import Annotated, List, Sequence\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "\n",
    "MAX_ITERATION = 2\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "async def generation_node(state: State) -> State:\n",
    "    \n",
    "    result = await generate_pipeline.ainvoke({\"messages\": state['messages']})\n",
    "    \n",
    "    return {\"messages\": AIMessage(content=result)}\n",
    "\n",
    "\n",
    "async def reflection_node(state: State) -> State:\n",
    "        \n",
    "    cls_map = {\"ai\": HumanMessage, \"human\": AIMessage}\n",
    "    \n",
    "    messages = [cls_map[msg.type](content=msg.content) for msg in state[\"messages\"][1:]]\n",
    "    \n",
    "    result = await reflect_pipeline.ainvoke({\"messages\": messages})\n",
    "    \n",
    "    return {\"messages\": HumanMessage(content=result)}\n",
    "\n",
    "\n",
    "def should_continue(state: State):\n",
    "    if len(state[\"messages\"]) > MAX_ITERATION:\n",
    "        # End after 3 iterations\n",
    "        return END\n",
    "    return \"reflection_node\"\n",
    "\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "workflow.add_node(\"generation_node\", generation_node)\n",
    "workflow.add_node(\"reflection_node\", reflection_node)\n",
    "\n",
    "workflow.add_edge(START, \"generation_node\")\n",
    "workflow.add_edge(\"reflection_node\", \"generation_node\")\n",
    "workflow.add_conditional_edges(\"generation_node\", should_continue, [END, \"reflection_node\"])\n",
    "\n",
    "memory = InMemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432a1d83-b93e-4c9a-8639-d553845cf17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(app.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ed5cfc-fce6-4088-8caf-e813001a4de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = app.invoke({\"messages\": [HumanMessage(content=\"ç”Ÿæˆä¸€å€‹æŠ•æ”¾åœ¨Tiktokä¸Šçš„å†°æ·‡æ·‹å»£å‘ŠåŠ‡æœ¬ã€‚ç›®æ¨™ç¾¤çœ¾ç‚º8-15æ­²çš„å°å­©\")]}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce8bf1f-00aa-4319-bc63-2fd4e5efb56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for event in app.astream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"ç”Ÿæˆä¸€å€‹æŠ•æ”¾åœ¨Tiktokä¸Šçš„å†°æ·‡æ·‹å»£å‘ŠåŠ‡æœ¬ã€‚ç›®æ¨™ç¾¤çœ¾ç‚º8-15æ­²çš„å°å­©\"\n",
    "            )\n",
    "        ],\n",
    "    },\n",
    "    config,\n",
    "):\n",
    "    print(event)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8000522e-28ad-493a-8975-a3bf3ee9986e",
   "metadata": {},
   "source": [
    "å–å¾—æ­·å²ç´€éŒ„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be944d78-ae16-44e5-80f6-6eabeef10b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = app.get_state(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845d5d70-bc33-4097-9885-eb0c3dda5b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "state.values['messages'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f335777-2e84-47de-ae86-a1975c30c4e2",
   "metadata": {},
   "source": [
    "Dummy Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3716e73-0804-4957-a118-351e9f31ace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from operator import add\n",
    "\n",
    "class State(TypedDict):\n",
    "    foo: str\n",
    "    bar: Annotated[list[str], add]\n",
    "\n",
    "def node_a(state: State):\n",
    "    return {\"foo\": \"a\", \"bar\": [\"a\"]}\n",
    "\n",
    "def node_b(state: State):\n",
    "    return {\"foo\": \"b\", \"bar\": [\"b\"]}\n",
    "\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(node_a)\n",
    "workflow.add_node(node_b)\n",
    "workflow.add_edge(START, \"node_a\")\n",
    "workflow.add_edge(\"node_a\", \"node_b\")\n",
    "workflow.add_edge(\"node_b\", END)\n",
    "\n",
    "\"\"\"\n",
    "Checkpoints:\n",
    "\n",
    "The state of a thread at a particular point in time is called a checkpoint. \n",
    "Checkpoint is a snapshot of the graph state saved at each superstep and is represented by StateSnapshot object.\n",
    "\"\"\"\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "graph = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "graph.invoke({\"foo\": \"\"}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8189df-9f39-48de-a76d-a60507d17cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = graph.get_state(config)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0a6ce2-d0a6-4ca9-9789-54486de3d35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke({\"foo\": \"c\"}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1f2c01-e5fb-4fa1-9cc0-ffb3f5abeada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
