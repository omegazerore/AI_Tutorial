{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9AgiUyZ9Ab6N",
    "outputId": "28d8acf7-4dd0-4d74-b8a4-089b05b7da3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "0% [Working]\r",
      "            \r",
      "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "\r",
      "0% [Waiting for headers] [Waiting for headers] [Connected to r2u.stat.illinois.\r",
      "                                                                               \r",
      "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
      "\r",
      "0% [Waiting for headers] [2 InRelease 14.2 kB/129 kB 11%] [Waiting for headers]\r",
      "                                                                               \r",
      "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
      "\r",
      "0% [Waiting for headers] [2 InRelease 14.2 kB/129 kB 11%] [Waiting for headers]\r",
      "                                                                               \r",
      "Hit:4 https://cli.github.com/packages stable InRelease\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Hit:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
      "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
      "Get:11 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,496 kB]\n",
      "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,327 kB]\n",
      "Fetched 8,952 kB in 1s (6,791 kB/s)\n",
      "Reading package lists... Done\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
     ]
    }
   ],
   "source": [
    "!apt-get update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJxP9HvvDrcQ",
    "outputId": "b4ed77c2-8644-47d2-e39f-06ad1c12cfff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "libleptonica-dev is already the newest version (1.82.0-3build1).\n",
      "libtesseract-dev is already the newest version (4.1.1-2.1build1).\n",
      "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
      "tesseract-ocr-eng is already the newest version (1:4.00~git30-7274cfa-1.1).\n",
      "tesseract-ocr-script-latn is already the newest version (1:4.00~git30-7274cfa-1.1).\n",
      "poppler-utils is already the newest version (22.02.0-2ubuntu0.10).\n",
      "python3-pil is already the newest version (9.0.1-1ubuntu0.3).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!apt-get install poppler-utils libleptonica-dev tesseract-ocr libtesseract-dev python3-pil tesseract-ocr-eng tesseract-ocr-script-latn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PnnjjTV9_hhz",
    "outputId": "8169599e-5243-44f5-a835-666e3b6c82ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unstructured-inference in /usr/local/lib/python3.12/dist-packages (1.0.5)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.12/dist-packages (3.31.6)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
      "Requirement already satisfied: pdf2image in /usr/local/lib/python3.12/dist-packages (1.17.0)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (2.9.0.post0)\n",
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
      "Requirement already satisfied: langchain==0.2.5 in /usr/local/lib/python3.12/dist-packages (0.2.5)\n",
      "Requirement already satisfied: langchain-community==0.2.5 in /usr/local/lib/python3.12/dist-packages (0.2.5)\n",
      "Requirement already satisfied: langchain-core==0.2.9 in /usr/local/lib/python3.12/dist-packages (0.2.9)\n",
      "Requirement already satisfied: langchain-openai==0.1.9 in /usr/local/lib/python3.12/dist-packages (0.1.9)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.47.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
      "Requirement already satisfied: xformers in /usr/local/lib/python3.12/dist-packages (0.0.32.post2)\n",
      "Requirement already satisfied: triton in /usr/local/lib/python3.12/dist-packages (3.4.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
      "Requirement already satisfied: unstructured[all-docs] in /usr/local/lib/python3.12/dist-packages (0.18.15)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.2.5) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain==0.2.5) (2.0.43)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.2.5) (3.12.15)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain==0.2.5) (0.2.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain==0.2.5) (0.1.147)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from langchain==0.2.5) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.12/dist-packages (from langchain==0.2.5) (2.11.7)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain==0.2.5) (2.32.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain==0.2.5) (8.5.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.2.5) (0.6.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core==0.2.9) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core==0.2.9) (24.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai==0.1.9) (1.107.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-openai==0.1.9) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (3.4.3)\n",
      "Requirement already satisfied: filetype in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (1.2.0)\n",
      "Requirement already satisfied: python-magic in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (0.4.27)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (5.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (3.9.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (4.13.5)\n",
      "Requirement already satisfied: emoji in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (2.14.1)\n",
      "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (2025.2.18)\n",
      "Requirement already satisfied: langdetect in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (1.0.9)\n",
      "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (3.14.1)\n",
      "Requirement already satisfied: backoff in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (4.15.0)\n",
      "Requirement already satisfied: unstructured-client in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (0.42.3)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (1.17.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (5.9.5)\n",
      "Requirement already satisfied: python-oxmsg in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (0.0.2)\n",
      "Requirement already satisfied: html5lib in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (1.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (3.5)\n",
      "Requirement already satisfied: pypandoc in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (1.15)\n",
      "Requirement already satisfied: effdet in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (0.4.1)\n",
      "Requirement already satisfied: unstructured.pytesseract>=0.3.12 in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (0.3.15)\n",
      "Requirement already satisfied: xlrd in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (2.0.2)\n",
      "Requirement already satisfied: markdown in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (3.9)\n",
      "Requirement already satisfied: msoffcrypto-tool in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (5.4.2)\n",
      "Requirement already satisfied: python-pptx>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (1.0.2)\n",
      "Requirement already satisfied: pi-heif in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (1.1.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (2.2.2)\n",
      "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (6.0.0)\n",
      "Requirement already satisfied: google-cloud-vision in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (3.10.2)\n",
      "Requirement already satisfied: onnx>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (1.19.0)\n",
      "Requirement already satisfied: python-docx>=1.1.2 in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (1.2.0)\n",
      "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (20250506)\n",
      "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (3.1.5)\n",
      "Requirement already satisfied: pikepdf in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (9.11.0)\n",
      "Requirement already satisfied: onnxruntime>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (1.22.1)\n",
      "Requirement already satisfied: python-multipart in /usr/local/lib/python3.12/dist-packages (from unstructured-inference) (0.0.20)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from unstructured-inference) (0.34.4)\n",
      "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.12/dist-packages (from unstructured-inference) (4.11.0.86)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from unstructured-inference) (3.10.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from unstructured-inference) (2.8.0+cu126)\n",
      "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (from unstructured-inference) (1.0.19)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from unstructured-inference) (1.16.1)\n",
      "Requirement already satisfied: pypdfium2 in /usr/local/lib/python3.12/dist-packages (from unstructured-inference) (4.30.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from pdf2image) (11.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil) (1.17.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference) (3.19.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference) (1.13.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference) (1.11.1.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.5) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.5) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.5) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.5) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.5) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.5) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.5) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.5) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.5) (0.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->unstructured-inference) (1.1.9)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core==0.2.9) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.5) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.5) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.5) (1.0.0)\n",
      "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.17.0->unstructured[all-docs]) (5.29.5)\n",
      "Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.12/dist-packages (from onnx>=1.17.0->unstructured[all-docs]) (0.5.3)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.19.0->unstructured[all-docs]) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.19.0->unstructured[all-docs]) (25.2.10)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.26.0->langchain-openai==0.1.9) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.26.0->langchain-openai==0.1.9) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.26.0->langchain-openai==0.1.9) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.26.0->langchain-openai==0.1.9) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain==0.2.5) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain==0.2.5) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain==0.2.5) (0.4.1)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.12/dist-packages (from python-pptx>=1.0.1->unstructured[all-docs]) (3.2.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.2.5) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.2.5) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.2.5) (2025.8.3)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.2.5) (3.2.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->unstructured[all-docs]) (2.8)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from effdet->unstructured[all-docs]) (0.23.0+cu126)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from effdet->unstructured[all-docs]) (2.0.10)\n",
      "Requirement already satisfied: omegaconf>=2.0 in /usr/local/lib/python3.12/dist-packages (from effdet->unstructured[all-docs]) (2.3.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (2.25.1)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-vision->unstructured[all-docs]) (2.38.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-vision->unstructured[all-docs]) (1.26.1)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from html5lib->unstructured[all-docs]) (0.5.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->unstructured-inference) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->unstructured-inference) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->unstructured-inference) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->unstructured-inference) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->unstructured-inference) (3.2.3)\n",
      "Requirement already satisfied: cryptography>=39.0 in /usr/local/lib/python3.12/dist-packages (from msoffcrypto-tool->unstructured[all-docs]) (43.0.3)\n",
      "Requirement already satisfied: olefile>=0.46 in /usr/lib/python3/dist-packages (from msoffcrypto-tool->unstructured[all-docs]) (0.46)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->unstructured[all-docs]) (8.2.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->unstructured[all-docs]) (1.5.2)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl->unstructured[all-docs]) (2.0.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->unstructured[all-docs]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->unstructured[all-docs]) (2025.2)\n",
      "Requirement already satisfied: Deprecated in /usr/local/lib/python3.12/dist-packages (from pikepdf->unstructured[all-docs]) (1.2.18)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: aiofiles>=24.1.0 in /usr/local/lib/python3.12/dist-packages (from unstructured-client->unstructured[all-docs]) (24.1.0)\n",
      "Requirement already satisfied: httpcore>=1.0.9 in /usr/local/lib/python3.12/dist-packages (from unstructured-client->unstructured[all-docs]) (1.0.9)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=39.0->msoffcrypto-tool->unstructured[all-docs]) (2.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.70.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.74.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.71.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (4.9.1)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore>=1.0.9->unstructured-client->unstructured[all-docs]) (0.16.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf>=2.0->effdet->unstructured[all-docs]) (4.9.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->unstructured-inference) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.5) (1.1.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.19.0->unstructured[all-docs]) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->unstructured-inference) (3.0.2)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=39.0->msoffcrypto-tool->unstructured[all-docs]) (2.23)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install unstructured[all-docs] unstructured-inference cmake python-dotenv pdf2image python-dateutil faiss-cpu sentence-transformers langchain==0.2.5 langchain-community==0.2.5 langchain-core==0.2.9 langchain-openai==0.1.9 bitsandbytes accelerate xformers triton transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YnbeP7pP5_nc"
   },
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.staging.base import elements_to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R5Dr9S77xSVU",
    "outputId": "ed4297f1-3452-445e-909d-c982e1ba2291"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3NvpxuBQPvh"
   },
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# shutil.rmtree(\"bertv2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FMIG3PRN7t76",
    "outputId": "edbd1ac4-aa2e-4c5d-adb2-b459f5b595d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "12 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
      "13 Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\n",
      "16 Abstract\n",
      "20 1 Introduction\n",
      "28 2 Related Work\n",
      "30 2.1 Unsupervised Feature-based Approaches\n",
      "35 2.2 Unsupervised Fine-tuning Approaches\n",
      "64 2.3 Transfer Learning from Supervised Data\n",
      "67 3 BERT\n",
      "77 3.1 Pre-training BERT\n",
      "94 4 Experiments\n",
      "95 3.2 Fine-tuning BERT\n",
      "99 4.1 GLUE\n",
      "115 4.2 SQuAD v1.1\n",
      "129 4.3 SQuAD v2.0\n",
      "137 4.4 SWAG\n",
      "141 5 Ablation Studies\n",
      "146 5.1 Effect of Pre-training Tasks\n",
      "154 5.2 Effect of Model Size\n",
      "159 5.3 Feature-based Approach with BERT\n",
      "170 6 Conclusion\n",
      "172 References\n",
      "230 Appendix for “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”\n",
      "238 A Additional Details for BERT\n",
      "239 A.1 Illustration of the Pre-training Tasks\n",
      "261 A.2 Pre-training Procedure\n",
      "264 A.3 Fine-tuning Procedure\n",
      "266 • Batch size: 16, 32\n",
      "269 • Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
      "270 • Number of epochs: 2, 3, 4\n",
      "272 A.4 Comparison of BERT, ELMo ,and OpenAI GPT\n",
      "280 A.5 Illustrations of Fine-tuning on Different Tasks\n",
      "282 B Detailed Experimental Setup\n",
      "283 B.1 Detailed Descriptions for the GLUE Benchmark Experiments.\n",
      "338 C Additional Ablation Studies\n",
      "339 C.1 Effect of Number of Training Steps\n",
      "345 C.2 Ablation for Different Masking Procedures\n",
      "992\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "filename = \"bertv2.pdf\"\n",
    "\n",
    "# 參數 max_characters, new_after_n_chars, combine_text_under_n_chars 只有在chunking_strategy不為None時才會生效 \n",
    "\n",
    "elements_none = partition_pdf(\n",
    "    filename,\n",
    "    # chunking_strategy='by_title',\n",
    "    # extract_image_block_types=[\"Image\", \"Table\"],\n",
    "    # infer_table_structure=False,\n",
    "    # skip_infer_table_types=True,\n",
    "    # max_characters=4000,\n",
    "    # new_after_n_chars=3800,\n",
    "    # combine_text_under_n_chars=2000,\n",
    "    strategy='hi_res',\n",
    "    # extract_image_block_output_dir=pathlib.Path(filename).stem\n",
    ")\n",
    "\n",
    "for idx, element in enumerate(elements_none):\n",
    "  if element.category == \"Title\":\n",
    "    print(idx, element)\n",
    "\n",
    "print(\"**********************\")\n",
    "print(max([len(c.text) for c in elements_none]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "8nUuVj6jpUHc"
   },
   "outputs": [],
   "source": [
    "partition_pdf?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A706VTs_oM9Y"
   },
   "source": [
    "## by_title chunking strategy\n",
    "\n",
    "The by_title chunking strategy preserves section boundaries and optionally page boundaries as well. “Preserving” here means that a single chunk will never contain text that occurred in two different sections. When a new section starts, the existing chunk is closed and a new one started, even if the next element would fit in the prior chunk.\n",
    "In addition to the behaviors of the basic strategy above, the by_title strategy has the following behaviors:\n",
    "\n",
    "### Detect section headings.\n",
    "\n",
    "A Title element is considered to start a new section. When a Title element is encountered, the prior chunk is closed and a new chunk started, even if the Title element would fit in the prior chunk.\n",
    "\n",
    "### Respect page boundaries.\n",
    "\n",
    "Page boundaries can optionally also be respected using the multipage_sections argument. This defaults to True meaning that a page break does not start a new chunk. Setting this to False will separate elements that occur on different pages into distinct chunks.\n",
    "\n",
    "### Combine small sections.\n",
    "\n",
    "In certain documents, partitioning may identify a list-item or other short paragraph as a Title element even though it does not serve as a section heading. This can produce chunks substantially smaller than desired. This behavior can be mitigated using the combine_text_under_n_chars argument. This defaults to the same value as max_characters such that sequential small sections are combined to maximally fill the chunking window. Setting this to 0 will disable section combining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adXVnXIysqDP"
   },
   "source": [
    "### max_characters\n",
    "\n",
    "This is a hard cap on the maximum number of characters in a single chunk.\n",
    "\n",
    "It ensures that no chunk will exceed this limit, regardless of sentence or paragraph boundaries.\n",
    "\n",
    "Think of it as the \"upper bound\" for chunk size.\n",
    "\n",
    "👉 Example: if max_characters=500, no chunk will ever contain more than 500 characters.\n",
    "\n",
    "### new_after_n_chars\n",
    "\n",
    "This is a soft chunking guide that tries to create a new chunk once the text length hits this threshold.\n",
    "\n",
    "Unlike max_characters, it allows the library to \"look for a natural break\" (e.g., end of a sentence or paragraph) around that point.\n",
    "\n",
    "It’s useful if you want chunks to be around a certain size but still respect natural text boundaries.\n",
    "\n",
    "👉 Example: if new_after_n_chars=300, the library will attempt to start a new chunk after ~300 characters, but it might go a bit over if the next break point is at 320 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cbQveJzxdMG6",
    "outputId": "61184a82-0d76-44e9-a7b3-c8b8f1624c9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "3930\n"
     ]
    }
   ],
   "source": [
    "elements_by_title = partition_pdf(\n",
    "    filename,\n",
    "    chunking_strategy='by_title',\n",
    "    # extract_image_block_types=[\"Image\", \"Table\"],          # optional\n",
    "    # infer_table_structure=False,\n",
    "    skip_infer_table_types=True,\n",
    "    max_characters=4000,\n",
    "    new_after_n_chars=3800,\n",
    "    # combine_text_under_n_chars=2000,\n",
    "    strategy='hi_res',\n",
    "    # extract_image_block_output_dir=pathlib.Path(filename).stem\n",
    ")\n",
    "\n",
    "print(max([len(c.text) for c in elements_by_title]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K-8u_JOmtkOq",
    "outputId": "73e9077f-6ea5-4201-bd70-4409770de5aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "655\n"
     ]
    }
   ],
   "source": [
    "print(min([len(c.text) for c in elements_by_title]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcSImJPbuguG"
   },
   "source": [
    "這一段特別短，看一下原因"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SgJ3NoBVtsU3",
    "outputId": "9bc39795-f051-4a0a-b7f4-8b35eff9d7a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 A.5 Illustrations of Fine-tuning on Different Tasks\n",
      "\n",
      "The illustration of ﬁne-tuning BERT on different tasks can be seen in Figure 4. Our task-speciﬁc models are formed by incorporating BERT with one additional output layer, so a minimal num- ber of parameters need to be learned from scratch. Among the tasks, (a) and (b) are sequence-level tasks while (c) and (d) are token-level tasks. In the ﬁgure, E represents the input embedding, Ti represents the contextual representation of token i, [CLS] is the special symbol for classiﬁcation out- put, and [SEP] is the special symbol to separate non-consecutive token sequences.\n",
      "\n",
      "B Detailed Experimental Setup\n"
     ]
    }
   ],
   "source": [
    "for idx, c in enumerate(elements_by_title):\n",
    "  if len(c.text) == 655:\n",
    "    print(idx, c.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OWo29FUus8X"
   },
   "source": [
    "上一段的長度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lKHccTJkuMya",
    "outputId": "81c834e2-a9ed-40db-d9d5-7025e8cd65e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3930"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(elements_by_title[17].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7k4vtX0uvbJ"
   },
   "source": [
    "下一段的長度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OQOcOfqmuQ5W",
    "outputId": "f6d66ef2-6d69-4f05-db81-33b54a19e78f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3437"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(elements_by_title[19].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjcbDc-9p346"
   },
   "source": [
    "### Title 作為 \"Chunk\" 的起始點\n",
    "\n",
    "但也許不是完全通用的，因為可能有些段落過長，超過max_characters，以至於被強制拆開並開啟下一個Chunk.\n",
    "\n",
    "### by_title 策略\n",
    "\n",
    "主要原則：每個標題 (title element) 會被視為一個「新 Chunk」的起始點。\n",
    "\n",
    "也就是說，文件裡的結構（例如章節標題、節標題）會決定 Chunk 的邊界。\n",
    "\n",
    "標題底下的內容（段落、表格、清單等）會被歸類到同一個 Chunk。\n",
    "\n",
    "#### 如果段落過長，超過了 max_characters，系統會強制把它拆分成多個 Chunk。\n",
    "\n",
    "- 這是為了避免生成特別龐大的 Chunk。\n",
    "\n",
    "- 所以標題雖然是起始點，但不是唯一的切割依據。\n",
    "\n",
    "#### 如果有設定 new_after_n_chars，則在達到這個字數附近時，系統會「嘗試」找個自然的分割點切開。\n",
    "\n",
    "- 例如在句子結束處換 Chunk，而不是硬斷在句子中間。\n",
    "\n",
    "- 這會讓 Chunk 看起來更自然，但仍然受 max_characters 上限約束。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9JWKZhHQ4cbE",
    "outputId": "e43e123b-2b79-4fe3-a99e-a9ac0943a38c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Introduction\n",
      "\n",
      "Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the re- lationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce ﬁne-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016).\n",
      "\n",
      "We argue that current techniques restrict the power of the pre-trained representations, espe- cially for the ﬁne-tuning approaches. The ma- jor limitation is that standard language models are unidirectional, and this limits the choice of archi- tectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-to- right architecture, where every token can only at- tend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017). Such re- strictions are sub-optimal for sentence-level tasks, and could be very harmful when applying ﬁne- tuning based approaches to token-level tasks such as question answering, where it is crucial to incor- porate context from both directions.\n",
      "\n",
      "In this paper, we improve the ﬁne-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidi- rectionality constraint by using a “masked lan- guage model” (MLM) pre-training objective, in- spired by the Cloze task (Taylor, 1953). The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked\n",
      "\n",
      "word based only on its context. Unlike left-to- right language model pre-training, the MLM ob- jective enables the representation to fuse the left and the right context, which allows us to pre- train a deep bidirectional Transformer. In addi- tion to the masked language model, we also use a “next sentence prediction” task that jointly pre- trains text-pair representations. The contributions of our paper are as follows:\n",
      "\n",
      "• We demonstrate the importance of bidirectional pre-training for language representations. Un- like Radford et al. (2018), which uses unidirec- tional language models for pre-training, BERT uses masked language models to enable pre- trained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.\n",
      "\n",
      "• We show that pre-trained representations reduce the need for many heavily-engineered task- speciﬁc architectures. BERT is the ﬁrst ﬁne- tuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outper- forming many task-speciﬁc architectures.\n",
      "\n",
      "• BERT advances the state of the art for eleven NLP tasks. The code and pre-trained mod- els are available at https://github.com/ google-research/bert.\n",
      "\n",
      "2 Related Work\n",
      "\n",
      "There is a long history of pre-training general lan- guage representations, and we brieﬂy review the most widely-used approaches in this section.\n"
     ]
    }
   ],
   "source": [
    "print(elements_by_title[1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rBKpA1p-np8Z",
    "outputId": "bce40ab1-a265-4af0-8997-6a02db19eadf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1 Unsupervised Feature-based Approaches\n",
      "\n",
      "Learning widely applicable representations of words has been an active area of research for decades, including non-neural (Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006) and neural (Mikolov et al., 2013; Pennington et al., 2014) methods. Pre-trained word embeddings are an integral part of modern NLP systems, of- fering signiﬁcant improvements over embeddings learned from scratch (Turian et al., 2010). To pre- train word embedding vectors, left-to-right lan- guage modeling objectives have been used (Mnih and Hinton, 2009), as well as objectives to dis- criminate correct from incorrect words in left and right context (Mikolov et al., 2013).\n",
      "\n",
      "These approaches have been generalized to coarser granularities, such as sentence embed- dings (Kiros et al., 2015; Logeswaran and Lee, 2018) or paragraph embeddings (Le and Mikolov, 2014). To train sentence representations, prior work has used objectives to rank candidate next sentences (Jernite et al., 2017; Logeswaran and Lee, 2018), left-to-right generation of next sen- tence words given a representation of the previous sentence (Kiros et al., 2015), or denoising auto- encoder derived objectives (Hill et al., 2016).\n",
      "\n",
      "ELMo and its predecessor (Peters et al., 2017,\n",
      "\n",
      "2018a) generalize traditional word embedding re- search along a different dimension. They extract context-sensitive features from a left-to-right and a right-to-left language model. The contextual rep- resentation of each token is the concatenation of the left-to-right and right-to-left representations. When integrating contextual word embeddings with existing task-speciﬁc architectures, ELMo advances the state of the art for several major NLP benchmarks (Peters et al., 2018a) including ques- tion answering (Rajpurkar et al., 2016), sentiment analysis (Socher et al., 2013), and named entity recognition (Tjong Kim Sang and De Meulder, 2003). Melamud et al. (2016) proposed learning contextual representations through a task to pre- dict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not deeply bidirectional. Fedus et al. (2018) shows that the cloze task can be used to improve the robustness of text generation mod- els.\n",
      "\n",
      "2.2 Unsupervised Fine-tuning Approaches\n",
      "\n",
      "As with the feature-based approaches, the ﬁrst works in this direction only pre-trained word em- bedding parameters from unlabeled text (Col- lobert and Weston, 2008).\n",
      "\n",
      "More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and ﬁne-tuned for a supervised downstream task (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018). The advantage of these approaches is that few parameters need to be learned from scratch. At least partly due to this advantage, OpenAI GPT (Radford et al., 2018) achieved pre- viously state-of-the-art results on many sentence- level tasks from the GLUE benchmark (Wang et al., 2018a). Left-to-right language model-\n",
      "\n",
      "Start/End Span\n",
      "\n",
      "Masked Sentence A\n",
      "\n",
      "Masked Sentence B\n",
      "\n",
      "Unlabeled Sentence A and B Pair\n",
      "\n",
      "Question\n",
      "\n",
      "ay Question Answer Pair\n",
      "\n",
      "Paragraph\n",
      "\n",
      "Pre-training\n",
      "\n",
      "Fine-Tuning\n",
      "\n",
      "Figure 1: Overall pre-training and ﬁne-tuning procedures for BERT. Apart from output layers, the same architec- tures are used in both pre-training and ﬁne-tuning. The same pre-trained model parameters are used to initialize models for different down-stream tasks. During ﬁne-tuning, all parameters are ﬁne-tuned. [CLS] is a special symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques- tions/answers).\n",
      "\n",
      "ing and auto-encoder objectives have been used for pre-training such models (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015).\n",
      "\n",
      "mal difference between the pre-trained architec- ture and the ﬁnal downstream architecture.\n"
     ]
    }
   ],
   "source": [
    "print(elements_by_title[2].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CZxt6s-dnyyY",
    "outputId": "652e837c-6cf5-48f8-e9dd-e62535539e54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3 Transfer Learning from Supervised Data\n",
      "\n",
      "There has also been work showing effective trans- fer from supervised tasks with large datasets, such as natural language inference (Conneau et al., 2017) and machine translation (McCann et al., 2017). Computer vision research has also demon- strated the importance of transfer learning from large pre-trained models, where an effective recipe is to ﬁne-tune models pre-trained with Ima- geNet (Deng et al., 2009; Yosinski et al., 2014).\n",
      "\n",
      "Model Architecture BERT’s model architec- ture is a multi-layer bidirectional Transformer en- coder based on the original implementation de- scribed in Vaswani et al. (2017) and released in the tensor2tensor library.1 Because the use of Transformers has become common and our im- plementation is almost identical to the original, we will omit an exhaustive background descrip- tion of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as “The Annotated Transformer.”2\n"
     ]
    }
   ],
   "source": [
    "print(elements_by_title[3].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WLSw3USLn2qE",
    "outputId": "ac18aa46-79d0-4636-e7bc-ff7ca11aa651"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 BERT\n",
      "\n",
      "We introduce BERT and its detailed implementa- tion in this section. There are two steps in our framework: pre-training and ﬁne-tuning. Dur- ing pre-training, the model is trained on unlabeled data over different pre-training tasks. For ﬁne- tuning, the BERT model is ﬁrst initialized with the pre-trained parameters, and all of the param- eters are ﬁne-tuned using labeled data from the downstream tasks. Each downstream task has sep- arate ﬁne-tuned models, even though they are ini- tialized with the same pre-trained parameters. The question-answering example in Figure 1 will serve as a running example for this section.\n",
      "\n",
      "A distinctive feature of BERT is its uniﬁed ar- chitecture across different tasks. There is mini-\n",
      "\n",
      "In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.3 We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Param- eters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M).\n",
      "\n",
      "BERTBASE was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Trans- former uses constrained self-attention where every token can only attend to context to its left.4\n",
      "\n",
      "1https://github.com/tensorﬂow/tensor2tensor\n",
      "\n",
      "2http://nlp.seas.harvard.edu/2018/04/03/attention.html 3In all cases we set the feed-forward/ﬁlter size to be 4H, i.e., 3072 for the H = 768 and 4096 for the H = 1024. 4We note that in the literature the bidirectional Trans-\n",
      "\n",
      "Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., (Question, Answer )) in one token sequence. Throughout this work, a “sentence” can be an arbi- trary span of contiguous text, rather than an actual linguistic sentence. A “sequence” refers to the in- put token sequence to BERT, which may be a sin- gle sentence or two sentences packed together.\n",
      "\n",
      "We use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary. The ﬁrst token of every sequence is always a special clas- siﬁcation token ([CLS]). The ﬁnal hidden state corresponding to this token is used as the ag- gregate sequence representation for classiﬁcation tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embed- ding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as E, the ﬁnal hidden vector of the special [CLS] token as C ∈ RH, and the ﬁnal hidden vector for the ith input token as Ti ∈ RH.\n",
      "\n",
      "For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualiza- tion of this construction can be seen in Figure 2.\n"
     ]
    }
   ],
   "source": [
    "print(elements_by_title[4].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dDRFWSN6e43W",
    "outputId": "a35480d5-3d3a-4a15-8eab-31610b84f742"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "3888\n"
     ]
    }
   ],
   "source": [
    "elements_by_title_2 = partition_pdf(\n",
    "    filename,\n",
    "    chunking_strategy='by_title',\n",
    "    # extract_image_block_types=[\"Image\", \"Table\"],\n",
    "    # infer_table_structure=False,\n",
    "    form_extraction_skip_tables=True,\n",
    "    max_characters=4000,\n",
    "    new_after_n_chars=3800,\n",
    "    combine_text_under_n_chars=500,\n",
    "    strategy='hi_res',\n",
    "    # extract_image_block_output_dir=pathlib.Path(filename).stem\n",
    ")\n",
    "\n",
    "print(max([len(c.text) for c in elements_by_title_2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XltsC0gChD9t",
    "outputId": "f1c1308a-1c70-47d0-e413-9980c17d0d8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "503\n"
     ]
    }
   ],
   "source": [
    "print(min([len(c.text) for c in elements_by_title_2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "dc8d8743d77240c19f7653d57c715829",
      "56aaa5eb5c5443bb9cdf518c24375183",
      "378b43d561404033a89c3e41c51d84b5",
      "3e39014d7394441e9096f1d8190bb880",
      "ab2ab56ca30440adafc61eb40b5f064e",
      "b163749c9ddb49c2b94a471003e1cdce",
      "32815d557811477ba26f24a92bd6ce38",
      "b983b9cf8ecc455dab2a85323ae00d49",
      "1f14409c824e432c9ceda403d28320f1",
      "d6ff52c05c304948bbdc16790165d121",
      "4c29357df5324ad18fbd281f5ae364af"
     ]
    },
    "id": "J_VTckovw0rq",
    "outputId": "23e27b70-ddb9-44c6-a7e9-1453a26c8352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8d8743d77240c19f7653d57c715829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "yolox_l0.05.onnx:   0%|          | 0.00/217M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "filename = 'bertv2.pdf'\n",
    "\n",
    "elements = partition_pdf(\n",
    "    filename,\n",
    "    chunking_strategy='by_title',\n",
    "    extract_image_block_types=[\"Image\", \"Table\"],\n",
    "    # infer_table_structure=False,\n",
    "    form_extraction_skip_tables=True,\n",
    "    max_characters=4000,\n",
    "    new_after_n_chars=3800,\n",
    "    strategy='hi_res',\n",
    "    extract_image_block_output_dir=pathlib.Path(filename).stem\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCXOLjg1zlve"
   },
   "source": [
    "圖像處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SAm7uSAwxNB_",
    "outputId": "d79a6e89-324b-4170-e443-a0d2e96c6ae5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdf2image\n",
      "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
      "Collecting layoutparser\n",
      "  Downloading layoutparser-0.3.4-py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from layoutparser) (2.0.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from layoutparser) (1.16.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from layoutparser) (2.2.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from layoutparser) (6.0.2)\n",
      "Collecting iopath (from layoutparser)\n",
      "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting pdfplumber (from layoutparser)\n",
      "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from iopath->layoutparser) (4.67.1)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from iopath->layoutparser) (4.15.0)\n",
      "Collecting portalocker (from iopath->layoutparser)\n",
      "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->layoutparser) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->layoutparser) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->layoutparser) (2025.2)\n",
      "Collecting pdfminer.six==20250506 (from pdfplumber->layoutparser)\n",
      "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber->layoutparser)\n",
      "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber->layoutparser) (3.4.3)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber->layoutparser) (43.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->layoutparser) (1.17.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber->layoutparser) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber->layoutparser) (2.23)\n",
      "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Downloading layoutparser-0.3.4-py3-none-any.whl (19.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
      "Building wheels for collected packages: iopath\n",
      "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=f6f922b4113a58950df86db71281f17fad541d99306844ab29d1ac2bd03d17f6\n",
      "  Stored in directory: /root/.cache/pip/wheels/7c/96/04/4f5f31ff812f684f69f40cb1634357812220aac58d4698048c\n",
      "Successfully built iopath\n",
      "Installing collected packages: pypdfium2, portalocker, pdf2image, iopath, pdfminer.six, pdfplumber, layoutparser\n",
      "Successfully installed iopath-0.1.10 layoutparser-0.3.4 pdf2image-1.17.0 pdfminer.six-20250506 pdfplumber-0.11.7 portalocker-3.2.0 pypdfium2-4.30.0\n"
     ]
    }
   ],
   "source": [
    "pip install pdf2image pillow layoutparser opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "rHM7X2nN2axk"
   },
   "outputs": [],
   "source": [
    "import layoutparser as lp\n",
    "from pdf2image import convert_from_path\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# 1. 將 PDF 轉成圖片 (這裡取第一頁)\n",
    "pdf_path = \"bertv2.pdf\"\n",
    "pages = convert_from_path(pdf_path, dpi=150)\n",
    "pages[14].save(\"example.png\", \"PNG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4WZcQRz9_6sM",
    "outputId": "deb9dce3-6291-493b-9d34-b9ec1492fb27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.image import partition_image\n",
    "\n",
    "# Returns a List[Element] present in the pages of the parsed image document\n",
    "elements_image = partition_image(\"example.png\", strategy='hi_res', extract_image_block_types=[\"Image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "y1gM95P0Btbg",
    "outputId": "93effc5d-09e3-441e-977b-89ced9ecaffa"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Figure 4: Illustrations of Fine-tuning BERT on Different Tasks.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements_image[8].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8HWNePT_Ewwv",
    "outputId": "1933197c-1f19-4341-e764-c9834d532dbd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<unstructured.documents.elements.FigureCaption at 0x7a5d7683bd40>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements_image[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2XafbHRFGjo"
   },
   "source": [
    "單就圖像提取來說，比partition_pdf看起來很好多。所以，依照你的需求或是數據來源，你可能會需要訂製化自己的數據提取流程\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kE8IKTQaGc7k"
   },
   "source": [
    "## PDF -> Vectorstore\n",
    "\n",
    "你現在有了文字，表格，圖像。你可以開始建立自己專屬的數據庫\n",
    "\n",
    "### 文字:\n",
    "- 原始數據\n",
    "- 進行壓縮 (像是summary)\n",
    "\n",
    "### 表格/圖像:\n",
    "- 使用image caption將圖片轉換成文字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GYkW9dtpnfMY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "def credential_init():\n",
    "\n",
    "  credential_file = \"credentials.ini\"\n",
    "\n",
    "  if os.path.exists(credential_file):\n",
    "      credentials = configparser.ConfigParser()\n",
    "      credentials.read(credential_file)\n",
    "      os.environ['OPENAI_API_KEY'] = credentials['openai'].get('api_key')\n",
    "  else:\n",
    "      os.environ['OPENAI_API_KEY'] = os.environ['OPENAI']\n",
    "\n",
    "credential_init()\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "           model_name=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "r8JlFwMmNt34"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "\n",
    "from PIL import Image\n",
    "from langchain_core.prompts.image import ImagePromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.runnables import chain, RunnablePassthrough, RunnableLambda\n",
    "\n",
    "\n",
    "def build_standard_chat_prompt_template(kwargs):\n",
    "  messages = []\n",
    "\n",
    "  if 'system' in kwargs:\n",
    "    content = kwargs.get('system')\n",
    "\n",
    "    # allow list of prompts for multimodal\n",
    "    if isinstance(content, list):\n",
    "      prompts = [PromptTemplate(**c) for c in content]\n",
    "    else:\n",
    "      prompts = [PromptTemplate(**content)]\n",
    "\n",
    "    message = SystemMessagePromptTemplate(prompt=prompts)\n",
    "    messages.append(message)\n",
    "\n",
    "  if 'human' in kwargs:\n",
    "    content = kwargs.get('human')\n",
    "\n",
    "    # allow list of prompts for multimodal\n",
    "    if isinstance(content, list):\n",
    "      prompts = []\n",
    "      for c in content:\n",
    "        if c.get(\"type\") == \"image\":\n",
    "          prompts.append(ImagePromptTemplate(**c))\n",
    "        else:\n",
    "          prompts.append(PromptTemplate(**c))\n",
    "    else:\n",
    "      if content.get(\"type\") == \"image\":\n",
    "        prompts = [ImagePromptTemplate(**content)]\n",
    "      else:\n",
    "        prompts = [PromptTemplate(**content)]\n",
    "\n",
    "    message = HumanMessagePromptTemplate(prompt=prompts)\n",
    "    messages.append(message)\n",
    "\n",
    "  chat_prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "  return chat_prompt_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Xz51AR6JgiE"
   },
   "source": [
    "### 文字壓縮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bK4BsLu-oG1-"
   },
   "outputs": [],
   "source": [
    "prompt = f\"Summarize the following text:\\n\\n{elements[0]}\\n\\nSummary:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "id": "DIAY7ASXzqTE",
    "outputId": "1c18a690-da24-467d-dddb-8086bfd54468"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Summarize the following text:\\n\\n9\\n\\n2019\\n\\n1\\n\\n0\\n\\n2\\n\\ny a M 4 2 ] L C . s c [ 2 v 5 0 8 4 0 . 0 1 8 1 :\\n\\nv\\n\\narXiv\\n\\ni\\n\\nX\\n\\nr\\n\\na\\n\\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\n\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\n\\nGoogle AI Language\\n\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\n\\nAbstract\\n\\nWe introduce a new language representa- tion model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language repre- sentation models (Peters et al., 2018a; Rad- ford et al., 2018), BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a re- sult, the pre-trained BERT model can be ﬁne- tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task- speciﬁc architecture modiﬁcations.\\n\\nThere are two existing strategies for apply- ing pre-trained language representations to down- stream tasks: feature-based and ﬁne-tuning. The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-speciﬁc architectures that include the pre-trained representations as addi- tional features. The ﬁne-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-speciﬁc parameters, and is trained on the downstream tasks by simply ﬁne-tuning all pre- trained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.\\n\\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art re- sults on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answer- ing Test F1 to 93.2 (1.5 point absolute im- provement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\\n\\nSummary:'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bup32QWpoxK_",
    "outputId": "0ddc8ab2-2063-4bb8-8196-b7f5d42c9aec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The text introduces BERT (Bidirectional Encoder Representations from Transformers), a new language representation model developed by Google AI Language. BERT pre-trains deep bidirectional representations from unlabeled text by conditioning on both left and right context in all layers. This allows the pre-trained BERT model to be fine-tuned with just one additional output layer for various tasks like question answering and language inference, achieving state-of-the-art results without significant task-specific modifications. BERT outperforms previous models, achieving new benchmarks on eleven natural language processing tasks, including significant improvements in GLUE score, MultiNLI accuracy, and SQuAD question answering tests.', response_metadata={'token_usage': {'completion_tokens': 130, 'prompt_tokens': 549, 'total_tokens': 679, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_72044bbe43', 'finish_reason': 'stop', 'logprobs': None}, id='run-d8d0a397-c2bd-41ad-a3f6-5732407223f5-0', usage_metadata={'input_tokens': 549, 'output_tokens': 130, 'total_tokens': 679})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "SrvfC05bNhH3"
   },
   "outputs": [],
   "source": [
    "text_prompt_template = {\"template\": \"Summarize the following text:\\n\\n{query}\\n\\nSummary:\", \"input_variables\": [\"query\"]}\n",
    "\n",
    "input_ = {\n",
    "    \"human\": [text_prompt_template],\n",
    "}\n",
    "\n",
    "chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "text_pipeline = chat_prompt_template|model|StrOutputParser()\n",
    "\n",
    "batches = [{\"query\": c.text} for c in elements]\n",
    "\n",
    "results = await text_pipeline.abatch(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "97pZLMBbPBu4"
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for result in results:\n",
    "\n",
    "  documents.append(Document(page_content=result, metadata={'filename': \"bertv2.pdf\",\n",
    "                               'type': \"text\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xgBJJMoOCsea",
    "outputId": "b1fb3860-232b-4766-961e-8038f42ba9e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='The text discusses the impact of the number of training steps and different masking strategies on the performance of BERT models. \\n\\n**Effect of Number of Training Steps:**\\n1. **Pre-training Duration:** BERTBASE achieves about 1.0% higher accuracy on the MNLI dataset when pre-trained for 1 million steps compared to 500,000 steps, indicating the necessity of extensive pre-training.\\n2. **MLM vs. LTR Pre-training:** The Masked Language Model (MLM) converges slightly slower than the Left-to-Right (LTR) model but starts outperforming the LTR model in terms of absolute accuracy almost immediately.\\n\\n**Ablation for Different Masking Procedures:**\\n- **Masking Strategies:** BERT uses a mixed strategy (80% MASK, 10% SAME, 10% RND) to reduce the mismatch between pre-training and fine-tuning stages.\\n- **Evaluation:** The study evaluates the effect of different masking strategies on MNLI and NER datasets. Fine-tuning is robust to different masking strategies, but the feature-based approach for NER is sensitive to the masking strategy, performing poorly with only the MASK strategy. The mixed strategy used by BERT is found to be effective, while using only the RND strategy performs worse.\\n\\n**Key Findings:**\\n- Extensive pre-training is beneficial for achieving high fine-tuning accuracy.\\n- The MLM model, despite converging slower, outperforms the LTR model in accuracy.\\n- The mixed masking strategy used by BERT is effective, especially for fine-tuning, while the feature-based approach is more sensitive to the choice of masking strategy.', metadata={'filename': 'bertv2.pdf', 'type': 'text'})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6XlQvJXP9cU"
   },
   "source": [
    "願意的話，你甚至可以辦到標註是在哪一頁"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0J1QvWpJ080"
   },
   "source": [
    "### 表格和影像提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "yDvZSMn2JwLl"
   },
   "outputs": [],
   "source": [
    "import aiofiles\n",
    "import asyncio\n",
    "from operator import itemgetter\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Async function to read image and convert to base64\n",
    "@chain\n",
    "async def image_to_base64(image_path: str) -> str:\n",
    "  loop = asyncio.get_event_loop()\n",
    "\n",
    "  print(image_path)\n",
    "\n",
    "  # Use threadpool to avoid blocking\n",
    "  def read_image():\n",
    "    with Image.open(image_path) as image:\n",
    "      buffered = io.BytesIO()\n",
    "      image.convert(\"RGB\").save(buffered, format=\"JPEG\")\n",
    "      return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "\n",
    "  return await loop.run_in_executor(None, read_image)\n",
    "\n",
    "\n",
    "text_prompt_template = {\"template\": \"Describe the content.\"}\n",
    "image_prompt_template = {\"type\": \"image\",\n",
    "              \"template\": {\"url\": \"data:image/jpeg;base64,{image_str}\"},\n",
    "              \"input_variables\": [\"image_str\"]}\n",
    "\n",
    "input_ = {\n",
    "    \"human\": [text_prompt_template, image_prompt_template],\n",
    "}\n",
    "\n",
    "chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "# Async chain for converting image_path -> base64\n",
    "# The lambda in RunnablePassthrough.assign is async-friendly\\\n",
    "\n",
    "image_2_image_str_chain = RunnablePassthrough.assign(image_str=lambda x: itemgetter('image_path')|image_to_base64)\n",
    "\n",
    "\n",
    "image_caption_pipeline = image_2_image_str_chain | chat_prompt_template| model | StrOutputParser()\n",
    "\n",
    "batches = [{\"image_path\": os.path.join(\"bertv2\", c)} for c in os.listdir(\"bertv2\") if 'table' in Path(c).stem]\n",
    "\n",
    "# results = await image_caption_pipeline.abatch(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "l2EXHLumu0Gg"
   },
   "outputs": [],
   "source": [
    "for result in results:\n",
    "\n",
    "  documents.append(Document(page_content=result, metadata={'filename': \"bertv2.pdf\",\n",
    "                               \"type\": \"table\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eEN2vl2WISY0",
    "outputId": "ce37b586-e9c2-4251-b1f0-0445c8b53ed5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='The content is a table comparing the performance of different systems on a specific task, likely related to natural language processing or machine learning, as indicated by the metrics used (EM and F1 scores). The table is divided into three sections: \"Top Leaderboard Systems,\" \"Published,\" and \"Ours.\"\\n\\n1. **Top Leaderboard Systems (Dec 10th, 2018):**\\n   - **Human:** \\n     - Dev EM: 86.3\\n     - Dev F1: 89.0\\n     - Test EM: 86.9\\n     - Test F1: 89.5\\n   - **#1 Single - MIR-MRC (F-Net):**\\n     - Test EM: 74.8\\n     - Test F1: 78.0\\n   - **#2 Single - nlnet:**\\n     - Test EM: 74.2\\n     - Test F1: 77.1\\n\\n2. **Published:**\\n   - **unet (Ensemble):**\\n     - Test EM: 71.4\\n     - Test F1: 74.9\\n   - **SLQA+ (Single):**\\n     - Test EM: 71.4\\n     - Test F1: 74.4\\n\\n3. **Ours:**\\n   - **BERT_LARGE (Single):**\\n     - Dev EM: 78.7\\n     - Dev F1: 81.9\\n     - Test EM: 80.0\\n     - Test F1: 83.1\\n\\nThe table shows that the BERT_LARGE model outperforms the other systems listed in the \"Published\" and \"Top Leaderboard Systems\" sections in terms of both EM and F1 scores on the test set, though it does not reach human-level performance.', metadata={'filename': 'bertv2.pdf', 'type': 'table'})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9cRcLtUIaFW"
   },
   "source": [
    "基於Documents內容建立vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "UDp6GGtIxgZB",
    "outputId": "008d825a-53b7-442b-8ca3-e2103d01b884"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1665252095.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHuggingFaceEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"BAAI/bge-m3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mvectorstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFAISS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'documents' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=documents, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWoeDv_yCxG3"
   },
   "source": [
    "### Save the vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "hl7opAz0CxG4"
   },
   "outputs": [],
   "source": [
    "vectorstore.save_local(\"vectorstore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9ud0peSCxG4"
   },
   "source": [
    "### Load the vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4AoyfmP3CxG5",
    "outputId": "3bdd8c45-601e-4da0-d163-93f177bec81c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'60e01e6d-6674-40b3-b10e-c68aa6047d5a': Document(page_content='BERT (Bidirectional Encoder Representations from Transformers) is a new language representation model introduced by Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova from Google AI Language. Unlike previous models, BERT pre-trains deep bidirectional representations by conditioning on both left and right context in all layers. This allows BERT to be fine-tuned with just one additional output layer for various tasks like question answering and language inference, achieving state-of-the-art results without significant task-specific modifications. BERT outperforms existing models on eleven natural language processing tasks, including substantial improvements in GLUE score, MultiNLI accuracy, and SQuAD question answering benchmarks.', metadata={'filename': 'bertv2.pdf', 'type': 'text'}),\n",
       " '7628d053-6431-44e6-854f-d63f272cd9eb': Document(page_content='The introduction discusses the effectiveness of language model pre-training for various natural language processing (NLP) tasks, including sentence-level tasks like natural language inference and paraphrasing, and token-level tasks such as named entity recognition and question answering. The authors argue that current pre-training techniques, which are typically unidirectional, limit the power of the representations, especially for fine-tuning approaches. They propose BERT (Bidirectional Encoder Representations from Transformers) to address this limitation by using a masked language model (MLM) pre-training objective, which allows the model to incorporate context from both directions. BERT also includes a \"next sentence prediction\" task to pre-train text-pair representations. The paper demonstrates that bidirectional pre-training is crucial for language representations and that BERT outperforms many task-specific architectures, achieving state-of-the-art performance on various NLP tasks. The code and pre-trained models are available online.', metadata={'filename': 'bertv2.pdf', 'type': 'text'}),\n",
       " '8896f9ad-346f-4358-b68f-324904eb1c76': Document(page_content='The text discusses two main approaches in unsupervised learning for natural language processing (NLP):\\n\\n1. **Unsupervised Feature-based Approaches**: This approach focuses on learning word representations through pre-trained word embeddings, which have shown significant improvements over embeddings learned from scratch. Techniques include left-to-right language modeling and discriminative objectives. These methods have been extended to sentence and paragraph embeddings. ELMo, a notable model, extracts context-sensitive features from both left-to-right and right-to-left language models, advancing the state of the art in several NLP benchmarks.\\n\\n2. **Unsupervised Fine-tuning Approaches**: This approach involves pre-training sentence or document encoders from unlabeled text and fine-tuning them for specific supervised tasks. This method requires fewer parameters to be learned from scratch, leading to efficient models like OpenAI GPT, which achieved state-of-the-art results on many tasks. Pre-training objectives include left-to-right language modeling and auto-encoder objectives. The BERT model exemplifies this approach, using the same architecture for both pre-training and fine-tuning, with minimal differences between the pre-trained and downstream architectures.', metadata={'filename': 'bertv2.pdf', 'type': 'text'}),\n",
       " '2bbfabe9-b418-4409-9a60-8154e7ae15af': Document(page_content=\"Transfer learning from supervised tasks with large datasets, such as natural language inference and machine translation, has proven effective. In computer vision, fine-tuning models pre-trained on large datasets like ImageNet is a successful strategy. BERT's architecture is a multi-layer bidirectional Transformer encoder, based on the original implementation by Vaswani et al. (2017). Given the widespread use of Transformers, detailed descriptions are omitted, and readers are referred to the original sources for more information.\", metadata={'filename': 'bertv2.pdf', 'type': 'text'}),\n",
       " '44165112-95c2-4ee9-a0d7-36dd50e38a3d': Document(page_content=\"BERT (Bidirectional Encoder Representations from Transformers) is a model with a unified architecture for various tasks, involving two main steps: pre-training on unlabeled data and fine-tuning on labeled data for specific tasks. BERT uses bidirectional self-attention, unlike GPT's unidirectional approach. It comes in two sizes: BERTBASE and BERTLARGE, with different numbers of layers, hidden sizes, and self-attention heads. BERT's input representation can handle single or paired sentences using WordPiece embeddings and special tokens ([CLS] for classification and [SEP] for sentence separation). The input representation is constructed by summing token, segment, and position embeddings.\", metadata={'filename': 'bertv2.pdf', 'type': 'text'}),\n",
       " '6a85ac93-9ec3-4391-9adf-0364b099b3b0': Document(page_content='The text describes the pre-training process for BERT, which differs from traditional left-to-right or right-to-left language models. BERT uses two unsupervised tasks for pre-training:\\n\\n1. **Masked Language Model (MLM)**: This task involves randomly masking 15% of the input tokens and predicting these masked tokens. This approach allows for a deep bidirectional representation, although it creates a mismatch between pre-training and fine-tuning since the [MASK] token does not appear during fine-tuning. To address this, the masked tokens are replaced with the [MASK] token 80% of the time, a random token 10% of the time, and left unchanged 10% of the time.\\n\\n2. **Next Sentence Prediction (NSP)**: This task helps the model understand the relationship between sentences, which is crucial for tasks like Question Answering (QA) and Natural Language Inference (NLI). During pre-training, pairs of sentences are chosen such that 50% of the time the second sentence follows the first (labeled as IsNext) and 50% of the time it is a random sentence (labeled as NotNext). This task significantly benefits downstream tasks.\\n\\nThe input representation for BERT includes token embeddings, segmentation embeddings, and position embeddings.', metadata={'filename': 'bertv2.pdf', 'type': 'text'}),\n",
       " 'b4e44e6f-1a8f-4167-a26c-f928f503d731': Document(page_content=\"The NSP (Next Sentence Prediction) task in BERT is related to previous representation-learning objectives but differs by transferring all model parameters to downstream tasks. BERT uses token representations for token-level tasks and the [CLS] representation for classification tasks. The pre-training data includes BooksCorpus and English Wikipedia, focusing on document-level corpora for long sequences. Fine-tuning BERT is efficient and can be done quickly on a single Cloud TPU or GPU. BERT's self-attention mechanism allows it to handle various NLP tasks by encoding text pairs together, enabling bidirectional cross attention. Fine-tuning involves plugging in task-specific inputs and outputs and adjusting all parameters end-to-end.\", metadata={'filename': 'bertv2.pdf', 'type': 'text'}),\n",
       " '16149142-fb85-4d41-82f8-e0b4d28c890d': Document(page_content=\"The General Language Understanding Evaluation (GLUE) benchmark is a collection of diverse natural language understanding tasks. To fine-tune models on GLUE, the input sequence is represented using a specific method, and a classification layer is added. The BERT model, both in its BASE and LARGE versions, significantly outperforms previous state-of-the-art models across all GLUE tasks. BERTLARGE, in particular, shows substantial improvements, especially on tasks with limited training data. The results indicate that BERT's architecture, despite being similar to OpenAI GPT, achieves higher accuracy, with BERTLARGE scoring 80.5 on the official GLUE leaderboard compared to OpenAI GPT's 72.8. Fine-tuning involved using a batch size of 32, running for 3 epochs, and selecting the best learning rate. For small datasets, multiple random restarts were used to ensure stability.\", metadata={'filename': 'bertv2.pdf', 'type': 'text'}),\n",
       " '872db90c-474f-4635-ae80-dcbf5bff96ac': Document(page_content='The text describes the Stanford Question Answering Dataset (SQuAD v1.1), which consists of 100,000 crowd-sourced question/answer pairs. The task involves predicting the answer text span within a given passage from Wikipedia. The input question and passage are represented as a single sequence with different embeddings for the question and passage. During fine-tuning, start and end vectors are introduced to compute the probability of each word being the start or end of the answer span. The training objective is to maximize the log-likelihood of the correct start and end positions.\\n\\nThe text also discusses the performance of various systems on the SQuAD leaderboard. The BERT model, particularly the BERTLARGE variant, outperforms other systems, including ensembles, in terms of F1 score. Fine-tuning on additional data like TriviaQA further improves performance. The results show that BERTLARGE, both as a single model and in an ensemble, achieves the highest scores on the SQuAD v1.1 dataset, surpassing previous top systems.', metadata={'filename': 'bertv2.pdf', 'type': 'text'}),\n",
       " '1d2f63af-db0d-490e-b5e9-6054601bfbf5': Document(page_content='The SQuAD v2.0 task builds on SQuAD 1.1 by including questions that may not have an answer in the provided paragraph, making it more realistic. The BERT model for SQuAD v2.0 treats unanswered questions as having an answer span at the [CLS] token, extending the probability space to include this token. The model is fine-tuned for 2 epochs with specific hyperparameters and shows a +5.1 F1 improvement over previous systems.\\n\\nThe SWAG dataset involves sentence-pair completion to evaluate common-sense inference. BERT is fine-tuned on SWAG by constructing input sequences from sentence pairs and using a softmax layer for scoring. BERTLARGE significantly outperforms previous systems, including ESIM+ELMo and OpenAI GPT.\\n\\nAblation studies on BERT reveal the importance of different pre-training tasks. Removing the next sentence prediction (NSP) task or training as a left-to-right language model (like OpenAI GPT) impacts performance across various tasks, with additional studies detailed in the appendix.', metadata={'filename': 'bertv2.pdf', 'type': 'text'}),\n",
       " 'f1310b4e-4e86-401c-a708-96135aa03a28': Document(page_content='The text evaluates the impact of different pre-training tasks on BERT\\'s performance. It compares two models: one trained with the \"masked LM\" (MLM) but without the \"next sentence prediction\" (NSP) task, and another left-context-only model trained with a standard Left-to-Right (LTR) LM, also without NSP. The results show that removing NSP significantly degrades performance on tasks like QNLI, MNLI, and SQuAD 1.1. Additionally, the LTR model performs worse than the MLM model across all tasks, particularly on MRPC and SQuAD, due to the lack of right-side context. Adding a BiLSTM improves the LTR model\\'s performance on SQuAD but still falls short of the bidirectional models. Training separate LTR and RTL models, as done in ELMo, is less efficient and less powerful than a single bidirectional model.', metadata={'filename': 'bertv2.pdf', 'type': 'text'}),\n",
       " 'b6c62738-2311-4175-ad9e-dc8e95801751': Document(page_content='In this section, the authors investigate how the size of BERT models affects fine-tuning task accuracy. They trained various BERT models with different numbers of layers, hidden units, and attention heads, while keeping other hyperparameters constant. Results from selected GLUE tasks show that larger models consistently improve accuracy across all datasets, including smaller ones like MRPC. This finding is notable given that the models are already large compared to existing literature. The study demonstrates that scaling model size leads to significant improvements even on small-scale tasks, provided the model is well pre-trained. This contrasts with previous studies that showed mixed results when increasing model size using a feature-based approach. The authors suggest that fine-tuning directly on downstream tasks allows models to benefit more from larger pre-trained representations.', metadata={'filename': 'bertv2.pdf', 'type': 'text'}),\n",
       " '7f5cca37-222b-4dd7-8c50-30764372e073': Document(page_content=\"The text discusses the feature-based approach using BERT for NLP tasks, contrasting it with the fine-tuning approach. While fine-tuning involves adding a classification layer to the pre-trained model and jointly tuning all parameters, the feature-based approach extracts fixed features from the pre-trained model, which can be advantageous for tasks that require specific model architectures and for computational efficiency. The comparison is made using the CoNLL-2003 Named Entity Recognition (NER) task. Results show that BERT performs well with both approaches, with the best feature-based method (concatenating token representations from the top four hidden layers) being only slightly less effective than fine-tuning. This demonstrates BERT's versatility and effectiveness in both approaches. The conclusion highlights the importance of rich, unsupervised pre-training for language understanding systems and the generalization of these findings to deep bidirectional architectures.\", metadata={'filename': 'bertv2.pdf', 'type': 'text'}),\n",
       " '9dc47464-1d57-4180-896a-71e5501fc079': Document(page_content='The provided text is a list of references from various research papers and articles related to computational linguistics, natural language processing (NLP), and machine learning. These references include works on contextual string embeddings, character-level language modeling, predictive structures, domain adaptation, natural language inference, class-based n-gram models, semantic textual similarity, statistical language modeling, question pairs, reading comprehension, semi-supervised sequence modeling, deep neural networks, universal sentence representations, semi-supervised learning, large-scale image databases, sentential paraphrases, text generation, and Gaussian error linear units. The references span from foundational works in the early 1990s to more recent advancements up to 2018.', metadata={'filename': 'bertv2.pdf', 'type': 'text'}),\n",
       " 'd6e55d4a-97f0-4311-b2d1-051207fb1310': Document(page_content='The provided text is a list of references to significant research papers in the field of natural language processing (NLP) and machine learning. These papers cover various topics such as learning distributed representations of sentences and words, language model fine-tuning, machine reading comprehension, sentence representation learning, and contextualized word embeddings. Key contributions include the development of models like Skip-thought vectors, context2vec, and GloVe, as well as datasets like TriviaQA and SQuAD. The works also explore advanced techniques like bidirectional LSTMs, decomposable attention models, and unsupervised learning for improving language understanding.', metadata={'filename': 'bertv2.pdf', 'type': 'text'}),\n",
       " 'f277fa05-33a2-4005-a2e4-e7019aa327d4': Document(page_content=\"The text lists various influential research papers in the field of natural language processing (NLP) and machine learning. These papers cover a range of topics including machine comprehension, sentiment analysis, readability measurement, named entity recognition, semi-supervised learning, attention mechanisms, feature extraction, multi-task benchmarks, hierarchical attention networks, neural network judgments, sentence understanding, machine translation, feature transferability, and adversarial datasets for commonsense inference. Notable works include the introduction of the Bidirectional Attention Flow model, Recursive Deep Models for sentiment analysis, U-Net for handling unanswerable questions, the Cloze procedure for readability, and Google's neural machine translation system. The list also highlights significant contributions to benchmarks and datasets that have advanced the field of NLP.\", metadata={'filename': 'bertv2.pdf', 'type': 'text'}),\n",
       " 'a5b89dfb-0c2a-4282-b7d3-9f9ad48cfee2': Document(page_content='The appendix for the paper \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" is divided into three sections:\\n\\n1. **Appendix A**: Provides additional implementation details for BERT, including illustrations of pre-training tasks such as Masked Language Modeling (MLM) and Next Sentence Prediction (NSP). The MLM procedure involves randomly masking tokens in a sentence and predicting them, with 80% of the time replacing the word with a [MASK] token, 10% with a random word, and 10% keeping it unchanged. This forces the model to maintain a contextual representation of every token. The NSP task involves predicting if one sentence follows another in a text.\\n\\n2. **Appendix B**: Contains additional details about the experiments conducted.\\n\\n3. **Appendix C**: Presents additional ablation studies, including the effect of the number of training steps and different masking procedures. It is noted that MLM converges slower than left-to-right models but offers better empirical performance.\\n\\nThe appendix also compares BERT with other models like OpenAI GPT and ELMo, highlighting that BERT uses a bidirectional Transformer and is fine-tuned, unlike ELMo, which is feature-based. Training details include the use of Cloud TPUs, Adam optimizer, and specific hyperparameters.', metadata={'filename': 'bertv2.pdf', 'type': 'text'}),\n",
       " '02cfc7cd-f34d-48bc-9678-d3f45c980abf': Document(page_content='### Summary:\\n\\n**Pre-training Procedure:**\\n- Training input sequences are generated by sampling two spans of text from a corpus, referred to as \"sentences.\"\\n- The first sentence gets an A embedding, and the second gets a B embedding.\\n- 50% of the time, B is the actual next sentence following A; the other 50% of the time, it is a random sentence for the \"next sentence prediction\" task.\\n- Combined length of sequences is ≤ 512 tokens.\\n- LM masking is applied after WordPiece tokenization with a 15% uniform masking rate.\\n- To speed up pre-training, sequences of length 128 are used for 90% of the steps, and sequences of length 512 for the remaining 10% to learn positional embeddings.\\n\\n**Fine-tuning Procedure:**\\n- Most hyperparameters remain the same as in pre-training, except for batch size, learning rate, and number of training epochs.\\n- Dropout probability is kept at 0.1.\\n- Optimal hyperparameters vary by task, but effective ranges are:\\n  - Batch size: 16, 32\\n  - Learning rate (Adam): 5e-5, 3e-5, 2e-5\\n  - Number of epochs: 2, 3, 4\\n- Large datasets are less sensitive to hyperparameter choices than small datasets.\\n- Fine-tuning is fast, allowing for an exhaustive search over parameters to find the best model.\\n\\n**Comparison of BERT, ELMo, and OpenAI GPT:**\\n- BERT and OpenAI GPT are fine-tuning approaches, while ELMo is a feature-based approach.\\n- BERT and GPT have several differences:\\n  - GPT is trained on BooksCorpus; BERT on BooksCorpus and Wikipedia.\\n  - GPT introduces [SEP] and [CLS] tokens at fine-tuning; BERT learns these during pre-training.\\n  - GPT trained for 1M steps with a batch size of 32,000 words; BERT with 128,000 words.\\n  - GPT uses a fixed learning rate for fine-tuning; BERT uses task-specific learning rates.\\n- Ablation experiments show that BERT\\'s improvements mainly come from its bidirectionality and two pre-training tasks.', metadata={'filename': 'bertv2.pdf', 'type': 'text'}),\n",
       " '8160058e-b241-4cff-921f-d6998e7e57d0': Document(page_content='The text describes the process of fine-tuning BERT for different tasks, as illustrated in Figure 4. Task-specific models are created by adding one output layer to BERT, minimizing the number of new parameters. Tasks (a) and (b) are sequence-level, while tasks (c) and (d) are token-level. The figure uses symbols such as E for input embedding, Ti for the contextual representation of token i, [CLS] for classification output, and [SEP] for separating non-consecutive token sequences.', metadata={'filename': 'bertv2.pdf', 'type': 'text'}),\n",
       " '001ac082-45f2-41b4-bdd8-3b8c105f11dc': Document(page_content=\"The text provides detailed descriptions of the datasets included in the GLUE benchmark, which are used to evaluate the performance of natural language understanding models. The datasets are:\\n\\n1. **MNLI (Multi-Genre Natural Language Inference)**: A task to classify sentence pairs as entailment, contradiction, or neutral.\\n2. **QQP (Quora Question Pairs)**: A binary classification task to determine if two Quora questions are semantically equivalent.\\n3. **QNLI (Question Natural Language Inference)**: A binary classification task derived from the Stanford Question Answering Dataset to identify if a sentence contains the correct answer to a question.\\n4. **SST-2 (Stanford Sentiment Treebank)**: A binary classification task to determine the sentiment of sentences from movie reviews.\\n5. **RTE (Recognizing Textual Entailment)**: A binary entailment task similar to MNLI but with less training data.\\n6. **CoLA (Corpus of Linguistic Acceptability)**: A binary classification task to predict if a sentence is linguistically acceptable.\\n7. **STS-B (Semantic Textual Similarity Benchmark)**: A task to score sentence pairs based on their semantic similarity.\\n8. **MRPC (Microsoft Research Paraphrase Corpus)**: A task to determine if sentence pairs from news sources are semantically equivalent.\\n9. **WNLI (Winograd NLI)**: A small dataset for natural language inference, noted for its construction issues and excluded from some evaluations due to poor performance.\\n\\nThe text also mentions that the results were obtained from the GLUE leaderboard and OpenAI's blog, and highlights the potential for improved performance through multitask fine-tuning.\", metadata={'filename': 'bertv2.pdf', 'type': 'text'}),\n",
       " 'b53ef695-cc2d-4655-96db-6edd0e04fb40': Document(page_content='The text discusses the impact of the number of training steps and different masking strategies on the performance of BERT models. \\n\\n**Effect of Number of Training Steps:**\\n1. **Pre-training Duration:** BERTBASE achieves about 1.0% higher accuracy on the MNLI dataset when pre-trained for 1 million steps compared to 500,000 steps, indicating the necessity of extensive pre-training.\\n2. **MLM vs. LTR Pre-training:** The Masked Language Model (MLM) converges slightly slower than the Left-to-Right (LTR) model but starts outperforming the LTR model in terms of absolute accuracy almost immediately.\\n\\n**Ablation for Different Masking Procedures:**\\n- **Masking Strategies:** BERT uses a mixed strategy (80% MASK, 10% SAME, 10% RND) to reduce the mismatch between pre-training and fine-tuning stages.\\n- **Evaluation:** The study evaluates the effect of different masking strategies on MNLI and NER datasets. Fine-tuning is robust to different masking strategies, but the feature-based approach for NER is sensitive to the masking strategy, performing poorly with only the MASK strategy. The mixed strategy used by BERT is found to be effective, while using only the RND strategy performs worse.\\n\\n**Key Findings:**\\n- Extensive pre-training is beneficial for achieving high fine-tuning accuracy.\\n- The MLM model, despite converging slower, outperforms the LTR model in accuracy.\\n- The mixed masking strategy used by BERT is effective, especially for fine-tuning, while the feature-based approach is more sensitive to the choice of masking strategy.', metadata={'filename': 'bertv2.pdf', 'type': 'text'}),\n",
       " '23fa2fff-bfc7-43fa-9cce-63e76d448732': Document(page_content='The table presents the results of a series of experiments evaluating the performance of different model configurations on various tasks. The columns are divided into two main sections: \"Hyperparams\" and \"Dev Set Accuracy.\"\\n\\n### Hyperparams:\\n- **#L**: Number of layers in the model.\\n- **#H**: Hidden size of the model.\\n- **#A**: Number of attention heads.\\n\\n### Dev Set Accuracy:\\n- **LM (ppl)**: Language Model perplexity, a measure of how well the model predicts a sample.\\n- **MNLI-m**: Accuracy on the Multi-Genre Natural Language Inference (MNLI) matched dataset.\\n- **MRPC**: Accuracy on the Microsoft Research Paraphrase Corpus.\\n- **SST-2**: Accuracy on the Stanford Sentiment Treebank.\\n\\n### Data:\\n1. **Row 1**: \\n   - #L: 3, #H: 768, #A: 12\\n   - LM (ppl): 5.84\\n   - MNLI-m: 77.9\\n   - MRPC: 79.8\\n   - SST-2: 88.4\\n\\n2. **Row 2**: \\n   - #L: 6, #H: 768, #A: 3\\n   - LM (ppl): 5.24\\n   - MNLI-m: 80.6\\n   - MRPC: 82.2\\n   - SST-2: 90.7\\n\\n3. **Row 3**: \\n   - #L: 6, #H: 768, #A: 12\\n   - LM (ppl): 4.68\\n   - MNLI-m: 81.9\\n   - MRPC: 84.8\\n   - SST-2: 91.3\\n\\n4. **Row 4**: \\n   - #L: 12, #H: 768, #A: 12\\n   - LM (ppl): 3.99\\n   - MNLI-m: 84.4\\n   - MRPC: 86.7\\n   - SST-2: 92.9\\n\\n5. **Row 5**: \\n   - #L: 12, #H: 1024, #A: 16\\n   - LM (ppl): 3.54\\n   - MNLI-m: 85.7\\n   - MRPC: 86.9\\n   - SST-2: 93.3\\n\\n6. **Row 6**: \\n   - #L: 24, #H: 1024, #A: 16\\n   - LM (ppl): 3.23\\n   - MNLI-m: 86.6\\n   - MRPC: 87.8\\n   - SST-2: 93.7\\n\\nThe table shows that as the model complexity increases (more layers, larger hidden size, more attention heads), the performance on the development set generally improves across all tasks.', metadata={'filename': 'bertv2.pdf', 'type': 'table'}),\n",
       " '6f87c768-40cd-4d4c-8a14-82dcba844cfd': Document(page_content='The image is a table comparing the performance of different systems on various NLP tasks. The systems listed are:\\n\\n1. Pre-OpenAI SOTA\\n2. BiLSTM+ELMo+Attn\\n3. OpenAI GPT\\n4. BERT_BASE\\n5. BERT_LARGE\\n\\nThe tasks and the corresponding dataset sizes are:\\n\\n- MNLI (m/mm): 392k\\n- QQP: 363k\\n- QNLI: 108k\\n- SST-2: 67k\\n- CoLA: 8.5k\\n- STS-B: 5.7k\\n- MRPC: 3.5k\\n- RTE: 2.5k\\n\\nThe table shows the performance scores of each system on these tasks. The scores are as follows:\\n\\n- **Pre-OpenAI SOTA**: \\n  - MNLI: 80.6/80.1\\n  - QQP: 66.1\\n  - QNLI: 82.3\\n  - SST-2: 93.2\\n  - CoLA: 35.0\\n  - STS-B: 81.0\\n  - MRPC: 86.0\\n  - RTE: 61.7\\n  - Average: 74.0\\n\\n- **BiLSTM+ELMo+Attn**: \\n  - MNLI: 76.4/76.1\\n  - QQP: 64.8\\n  - QNLI: 79.8\\n  - SST-2: 90.4\\n  - CoLA: 36.0\\n  - STS-B: 73.3\\n  - MRPC: 84.9\\n  - RTE: 56.8\\n  - Average: 71.0\\n\\n- **OpenAI GPT**: \\n  - MNLI: 82.1/81.4\\n  - QQP: 70.3\\n  - QNLI: 87.4\\n  - SST-2: 91.3\\n  - CoLA: 45.4\\n  - STS-B: 80.0\\n  - MRPC: 82.3\\n  - RTE: 56.0\\n  - Average: 75.1\\n\\n- **BERT_BASE**: \\n  - MNLI: 84.6/83.4\\n  - QQP: 71.2\\n  - QNLI: 90.5\\n  - SST-2: 93.5\\n  - CoLA: 52.1\\n  - STS-B: 85.8\\n  - MRPC: 88.9\\n  - RTE: 66.4\\n  - Average: 79.6\\n\\n- **BERT_LARGE**: \\n  - MNLI: 86.7/85.9\\n  - QQP: 72.1\\n  - QNLI: 92.7\\n  - SST-2: 94.9\\n  - CoLA: 60.5\\n  - STS-B: 86.5\\n  - MRPC: 89.3\\n  - RTE: 70.1\\n  - Average: 82.1\\n\\nThe table indicates that BERT_LARGE has the highest average performance across the tasks.', metadata={'filename': 'bertv2.pdf', 'type': 'table'}),\n",
       " '852efaf0-6312-4038-a35d-34d7a2b400df': Document(page_content='The table presents the performance of different models on various tasks, evaluated on a development set. The tasks include MNLI-m, QNLI, MRPC, SST-2, and SQuAD, with their respective evaluation metrics being accuracy (Acc) for the first four tasks and F1 score for SQuAD.\\n\\nThe models compared are:\\n1. BERT_BASE\\n2. No NSP (No Next Sentence Prediction)\\n3. LTR & No NSP (Learning to Rank and No Next Sentence Prediction)\\n4. + BiLSTM (Bidirectional Long Short-Term Memory)\\n\\nThe performance metrics for each model on each task are as follows:\\n\\n- **BERT_BASE**:\\n  - MNLI-m: 84.4\\n  - QNLI: 88.4\\n  - MRPC: 86.7\\n  - SST-2: 92.7\\n  - SQuAD: 88.5\\n\\n- **No NSP**:\\n  - MNLI-m: 83.9\\n  - QNLI: 84.9\\n  - MRPC: 86.5\\n  - SST-2: 92.6\\n  - SQuAD: 87.9\\n\\n- **LTR & No NSP**:\\n  - MNLI-m: 82.1\\n  - QNLI: 84.3\\n  - MRPC: 77.5\\n  - SST-2: 92.1\\n  - SQuAD: 77.8\\n\\n- **+ BiLSTM**:\\n  - MNLI-m: 82.1\\n  - QNLI: 84.1\\n  - MRPC: 75.7\\n  - SST-2: 91.6\\n  - SQuAD: 84.9\\n\\nThe table shows that BERT_BASE generally performs the best across all tasks, followed by No NSP, LTR & No NSP, and + BiLSTM.', metadata={'filename': 'bertv2.pdf', 'type': 'table'}),\n",
       " '43b1681e-2a9e-4151-8e14-6a316d48cd9e': Document(page_content='The table presents the results of an experiment involving different masking rates and their impact on the performance of models on two tasks: MNLI (Multi-Genre Natural Language Inference) and NER (Named Entity Recognition). The table is divided into two main sections: \"Masking Rates\" and \"Dev Set Results.\"\\n\\n### Masking Rates\\n- **MASK**: The percentage of tokens that are masked.\\n- **SAME**: The percentage of tokens that are replaced with the same token.\\n- **RND**: The percentage of tokens that are replaced with a random token.\\n\\n### Dev Set Results\\n- **MNLI Fine-tune**: The performance of the model on the MNLI task after fine-tuning.\\n- **NER Fine-tune**: The performance of the model on the NER task after fine-tuning.\\n- **NER Feature-based**: The performance of the model on the NER task using a feature-based approach.\\n\\n### Data\\n- **80% MASK, 10% SAME, 10% RND**: \\n  - MNLI Fine-tune: 84.2\\n  - NER Fine-tune: 95.4\\n  - NER Feature-based: 94.9\\n- **100% MASK, 0% SAME, 0% RND**: \\n  - MNLI Fine-tune: 84.3\\n  - NER Fine-tune: 94.9\\n  - NER Feature-based: 94.0\\n- **80% MASK, 0% SAME, 20% RND**: \\n  - MNLI Fine-tune: 84.1\\n  - NER Fine-tune: 95.2\\n  - NER Feature-based: 94.6\\n- **80% MASK, 20% SAME, 0% RND**: \\n  - MNLI Fine-tune: 84.4\\n  - NER Fine-tune: 95.2\\n  - NER Feature-based: 94.7\\n- **0% MASK, 20% SAME, 80% RND**: \\n  - MNLI Fine-tune: 83.7\\n  - NER Fine-tune: 94.8\\n  - NER Feature-based: 94.6\\n- **0% MASK, 0% SAME, 100% RND**: \\n  - MNLI Fine-tune: 83.6\\n  - NER Fine-tune: 94.9\\n  - NER Feature-based: 94.6\\n\\nThe table shows how different masking strategies affect the performance of models on the MNLI and NER tasks. Generally, a higher percentage of masked tokens (80% MASK) tends to yield better performance compared to lower percentages or random replacements.', metadata={'filename': 'bertv2.pdf', 'type': 'table'}),\n",
       " '5d929b7c-7fbe-4aa3-ba5c-82d80338fbcb': Document(page_content=\"The image illustrates the embedding process for a sequence of tokens in a transformer model, such as BERT. The process involves three types of embeddings: token embeddings, segment embeddings, and position embeddings. Here's a detailed breakdown:\\n\\n1. **Input Tokens**:\\n   - The input sequence is: `[CLS] my dog is cute [SEP] he likes play ##ing [SEP]`.\\n   - `[CLS]` and `[SEP]` are special tokens used in BERT. `[CLS]` is used for classification tasks, and `[SEP]` is used to separate different segments or sentences.\\n\\n2. **Token Embeddings**:\\n   - Each token in the input sequence is converted into a token embedding. For example, `E_[CLS]` for `[CLS]`, `E_my` for `my`, `E_dog` for `dog`, and so on.\\n\\n3. **Segment Embeddings**:\\n   - Segment embeddings differentiate between different parts of the input. In this case, `E_A` is used for the first segment (`[CLS] my dog is cute [SEP]`), and `E_B` is used for the second segment (`he likes play ##ing [SEP]`).\\n\\n4. **Position Embeddings**:\\n   - Position embeddings encode the position of each token in the sequence. For example, `E_0` for the first token `[CLS]`, `E_1` for the second token `my`, and so on up to `E_10` for the last token `[SEP]`.\\n\\n5. **Summation**:\\n   - The final embedding for each token is obtained by summing its token embedding, segment embedding, and position embedding. This combined embedding is then used as input to the transformer model.\\n\\nThis embedding process allows the model to understand the context of each token in the sequence, including its position and the segment it belongs to.\", metadata={'filename': 'bertv2.pdf', 'type': 'table'}),\n",
       " '06d29cba-4847-4325-887c-d32544146f08': Document(page_content='The image is a table comparing the performance of different systems on development (Dev) and test datasets. The systems listed are:\\n\\n1. ESIM+GloVe\\n2. ESIM+ELMo\\n3. OpenAI GPT\\n4. BERT_BASE\\n5. BERT_LARGE\\n\\nThe table also includes human performance for comparison, with two categories: Human (expert) and Human (5 annotations).\\n\\nThe performance metrics (presumably accuracy) for each system are as follows:\\n\\n- **ESIM+GloVe**: 51.9 (Dev), 52.7 (Test)\\n- **ESIM+ELMo**: 59.1 (Dev), 59.2 (Test)\\n- **OpenAI GPT**: - (Dev), 78.0 (Test)\\n- **BERT_BASE**: 81.6 (Dev), - (Test)\\n- **BERT_LARGE**: 86.6 (Dev), 86.3 (Test)\\n\\nHuman performance:\\n- **Human (expert)**: - (Dev), 85.0 (Test)\\n- **Human (5 annotations)**: - (Dev), 88.0 (Test)\\n\\nThe table shows that BERT_LARGE outperforms the other systems on both the development and test datasets, and its performance is close to human-level performance.', metadata={'filename': 'bertv2.pdf', 'type': 'table'}),\n",
       " '9b78682a-51f3-4613-93c3-6a4d92792ecc': Document(page_content=\"The table presents a comparison of various systems' performance on a specific task, likely related to natural language processing or machine learning, based on the metrics Exact Match (EM) and F1 score (F1). The table is divided into three sections: Top Leaderboard Systems, Published, and Ours.\\n\\n1. **Top Leaderboard Systems (Dec 10th, 2018):**\\n   - **Human:** Test EM: 82.3, Test F1: 91.2\\n   - **#1 Ensemble - nlnet:** Test EM: 86.0, Test F1: 91.7\\n   - **#2 Ensemble - QANet:** Test EM: 84.5, Test F1: 90.5\\n\\n2. **Published:**\\n   - **BiDAF+ELMo (Single):** Test EM: 85.6, Test F1: 85.8\\n   - **R.M. Reader (Ensemble):** Dev EM: 81.2, Dev F1: 87.9, Test EM: 82.3, Test F1: 88.5\\n\\n3. **Ours:**\\n   - **BERT_BASE (Single):** Dev EM: 80.8, Dev F1: 88.5\\n   - **BERT_LARGE (Single):** Dev EM: 84.1, Dev F1: 90.9, Test EM: 84.2, Test F1: 90.9\\n   - **BERT_LARGE (Ensemble):** Dev EM: 85.8, Dev F1: 91.8\\n   - **BERT_LARGE (Sgl.+TriviaQA):** Dev EM: 84.2, Dev F1: 91.1, Test EM: 85.1, Test F1: 91.8\\n   - **BERT_LARGE (Ens.+TriviaQA):** Dev EM: 86.2, Dev F1: 92.2, Test EM: 87.4, Test F1: 93.2\\n\\nThe table shows that the BERT_LARGE (Ens.+TriviaQA) system achieved the highest scores in both the development and test sets, outperforming other systems in terms of both EM and F1 metrics.\", metadata={'filename': 'bertv2.pdf', 'type': 'table'}),\n",
       " '21b4b576-4c99-4e8b-81bc-85851a5aa056': Document(page_content='The content is a table comparing the performance of different systems and approaches on a specific task, measured by F1 scores on development (Dev) and test datasets. The systems and approaches are categorized into three groups:\\n\\n1. **Pre-trained Models:**\\n   - **ELMo (Peters et al., 2018a):** Dev F1: 95.7, Test F1: 92.2\\n   - **CVT (Clark et al., 2018):** Test F1: 92.6\\n   - **CSE (Akbik et al., 2018):** Test F1: 93.1\\n\\n2. **Fine-tuning Approach:**\\n   - **BERT_LARGE:** Dev F1: 96.6, Test F1: 92.8\\n   - **BERT_BASE:** Dev F1: 96.4, Test F1: 92.4\\n\\n3. **Feature-based Approach (BERT_BASE):**\\n   - **Embeddings:** Dev F1: 91.0\\n   - **Second-to-Last Hidden:** Dev F1: 95.6\\n   - **Last Hidden:** Dev F1: 94.9\\n   - **Weighted Sum Last Four Hidden:** Dev F1: 95.9\\n   - **Concat Last Four Hidden:** Dev F1: 96.1\\n   - **Weighted Sum All 12 Layers:** Dev F1: 95.5\\n\\nThe table shows that the fine-tuning approach with BERT_LARGE and BERT_BASE achieves the highest Dev F1 scores, while the feature-based approach with BERT_BASE also performs well, particularly with the \"Concat Last Four Hidden\" method. The pre-trained models ELMo, CVT, and CSE have slightly lower F1 scores on the test dataset compared to the fine-tuned BERT models.', metadata={'filename': 'bertv2.pdf', 'type': 'table'}),\n",
       " 'fea8b07d-cb77-4921-80b9-3a7d5e6a85b9': Document(page_content='The content is a table comparing the performance of different systems on a specific task, likely related to natural language processing or machine learning, as indicated by the metrics used (EM and F1 scores). The table is divided into three sections: \"Top Leaderboard Systems,\" \"Published,\" and \"Ours.\"\\n\\n1. **Top Leaderboard Systems (Dec 10th, 2018):**\\n   - **Human:** \\n     - Dev EM: 86.3\\n     - Dev F1: 89.0\\n     - Test EM: 86.9\\n     - Test F1: 89.5\\n   - **#1 Single - MIR-MRC (F-Net):**\\n     - Test EM: 74.8\\n     - Test F1: 78.0\\n   - **#2 Single - nlnet:**\\n     - Test EM: 74.2\\n     - Test F1: 77.1\\n\\n2. **Published:**\\n   - **unet (Ensemble):**\\n     - Test EM: 71.4\\n     - Test F1: 74.9\\n   - **SLQA+ (Single):**\\n     - Test EM: 71.4\\n     - Test F1: 74.4\\n\\n3. **Ours:**\\n   - **BERT_LARGE (Single):**\\n     - Dev EM: 78.7\\n     - Dev F1: 81.9\\n     - Test EM: 80.0\\n     - Test F1: 83.1\\n\\nThe table shows that the BERT_LARGE model outperforms the other systems listed in the \"Published\" and \"Top Leaderboard Systems\" sections in terms of both EM and F1 scores on the test set, though it does not reach human-level performance.', metadata={'filename': 'bertv2.pdf', 'type': 'table'})}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore_1 = FAISS.load_local(\n",
    "    \"vectorstore\", embeddings, allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "vectorstore_1.docstore._dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzU2xDojCxG5"
   },
   "source": [
    "### 合併vectorstore。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ENy9dQkBCxG5"
   },
   "outputs": [],
   "source": [
    "vectorstore_1.merge_from(vectorstore_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klQhexXaNjV7"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "DoUdsOeSPl9n"
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"codex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "PkR0uv84NiiC"
   },
   "outputs": [],
   "source": [
    "async def pdf_2_vectorstore(filename, embeddings):\n",
    "\n",
    "  image_dir = Path(filename).stem\n",
    "\n",
    "  elements = partition_pdf(\n",
    "    filename,\n",
    "    chunking_strategy='by_title',\n",
    "    extract_image_block_types=[\"Table\"],\n",
    "    # infer_table_structure=False,\n",
    "    form_extraction_skip_tables=True,\n",
    "    max_characters=4000,\n",
    "    new_after_n_chars=3800,\n",
    "    strategy='hi_res',\n",
    "    extract_image_block_output_dir=image_dir\n",
    ")\n",
    "\n",
    "  documents = []\n",
    "\n",
    "  text_prompt_template = {\"template\": \"Summarize the following text:\\n\\n{query}\\n\\nSummary:\", \"input_variables\": [\"query\"]}\n",
    "\n",
    "  input_ = {\n",
    "      \"human\": [text_prompt_template],\n",
    "  }\n",
    "\n",
    "  chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "  text_pipeline = chat_prompt_template|model|StrOutputParser()\n",
    "\n",
    "  batches = [{\"query\": c.text} for c in elements]\n",
    "\n",
    "  results = await text_pipeline.abatch(batches)\n",
    "\n",
    "  for result in results:\n",
    "\n",
    "    documents.append(Document(page_content=result, metadata={'filename': image_dir,\n",
    "                                 'type': \"text\"}))\n",
    "\n",
    "\n",
    "  text_prompt_template = {\"template\": \"Describe the content.\"}\n",
    "  image_prompt_template = {\"type\": \"image\",\n",
    "                \"template\": {\"url\": \"data:image/jpeg;base64,{image_str}\"},\n",
    "                \"input_variables\": [\"image_str\"]}\n",
    "\n",
    "  input_ = {\n",
    "      \"human\": [text_prompt_template, image_prompt_template],\n",
    "  }\n",
    "\n",
    "  chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "  # Async chain for converting image_path -> base64\n",
    "  # The lambda in RunnablePassthrough.assign is async-friendly\\\n",
    "\n",
    "  image_2_image_str_chain = RunnablePassthrough.assign(image_str=lambda x: itemgetter('image_path')|image_to_base64)\n",
    "\n",
    "\n",
    "  image_caption_pipeline = image_2_image_str_chain | chat_prompt_template| model | StrOutputParser()\n",
    "\n",
    "  batches = [{\"image_path\": os.path.join(image_dir, c)} for c in os.listdir(image_dir) if 'table' in Path(c).stem]\n",
    "\n",
    "  results = await image_caption_pipeline.abatch(batches)\n",
    "\n",
    "  for result in results:\n",
    "\n",
    "    documents.append(Document(page_content=result, metadata={'filename': image_dir,\n",
    "                                \"type\": \"table\"}))\n",
    "\n",
    "  vectorstore = FAISS.from_documents(documents=documents, embedding=embeddings)\n",
    "\n",
    "  return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pW_VpqyTOpjB",
    "outputId": "3926f132-c9e0-43bb-c539-b09ae0c8569d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****\n",
      "Codex - Aeldari.pdf\n",
      "****\n",
      "\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Codex - Aeldari/table-21-8.jpg\n",
      "Codex - Aeldari/table-9-2.jpg\n",
      "Codex - Aeldari/table-11-3.jpg\n",
      "Codex - Aeldari/table-79-36.jpg\n",
      "Codex - Aeldari/table-47-21.jpg\n",
      "Codex - Aeldari/table-71-33.jpg\n",
      "Codex - Aeldari/table-61-28.jpg\n",
      "Codex - Aeldari/table-121-54.jpg\n",
      "Codex - Aeldari/table-35-15.jpg\n",
      "Codex - Aeldari/table-105-48.jpg\n",
      "Codex - Aeldari/table-65-30.jpg\n",
      "Codex - Aeldari/table-27-11.jpg\n",
      "Codex - Aeldari/table-91-41.jpg\n",
      "Codex - Aeldari/table-67-31.jpg\n",
      "Codex - Aeldari/table-49-22.jpg\n",
      "Codex - Aeldari/table-7-1.jpg\n",
      "Codex - Aeldari/table-51-23.jpg\n",
      "Codex - Aeldari/table-63-29.jpg\n",
      "Codex - Aeldari/table-31-13.jpg\n",
      "Codex - Aeldari/table-69-32.jpg\n",
      "Codex - Aeldari/table-53-24.jpg\n",
      "Codex - Aeldari/table-55-25.jpg\n",
      "Codex - Aeldari/table-25-10.jpg\n",
      "Codex - Aeldari/table-115-51.jpg\n",
      "Codex - Aeldari/table-45-20.jpg\n",
      "Codex - Aeldari/table-103-47.jpg\n",
      "Codex - Aeldari/table-93-42.jpg\n",
      "Codex - Aeldari/table-37-16.jpg\n",
      "Codex - Aeldari/table-81-37.jpg\n",
      "Codex - Aeldari/table-123-55.jpg\n",
      "Codex - Aeldari/table-89-40.jpg\n",
      "Codex - Aeldari/table-125-56.jpg\n",
      "Codex - Aeldari/table-13-4.jpg\n",
      "Codex - Aeldari/table-99-45.jpg\n",
      "Codex - Aeldari/table-117-52.jpg\n",
      "Codex - Aeldari/table-41-18.jpg\n",
      "Codex - Aeldari/table-39-17.jpg\n",
      "Codex - Aeldari/table-17-6.jpg\n",
      "Codex - Aeldari/table-97-44.jpg\n",
      "Codex - Aeldari/table-15-5.jpg\n",
      "Codex - Aeldari/table-57-26.jpg\n",
      "Codex - Aeldari/table-107-49.jpg\n",
      "Codex - Aeldari/table-111-50.jpg\n",
      "Codex - Aeldari/table-33-14.jpg\n",
      "Codex - Aeldari/table-19-7.jpg\n",
      "Codex - Aeldari/table-77-35.jpg\n",
      "Codex - Aeldari/table-119-53.jpg\n",
      "Codex - Aeldari/table-95-43.jpg\n",
      "Codex - Aeldari/table-23-9.jpg\n",
      "Codex - Aeldari/table-83-38.jpg\n",
      "Codex - Aeldari/table-85-39.jpg\n",
      "Codex - Aeldari/table-43-19.jpg\n",
      "Codex - Aeldari/table-101-46.jpg\n",
      "Codex - Aeldari/table-29-12.jpg\n",
      "Codex - Aeldari/table-75-34.jpg\n",
      "Codex - Aeldari/table-59-27.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 1/3 [26:37<53:14, 1597.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****\n",
      "Codex - Adeptus Mechanicus.pdf\n",
      "****\n",
      "\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Codex - Adeptus Mechanicus/table-107-22.jpg\n",
      "Codex - Adeptus Mechanicus/table-97-16.jpg\n",
      "Codex - Adeptus Mechanicus/table-107-21.jpg\n",
      "Codex - Adeptus Mechanicus/table-101-19.jpg\n",
      "Codex - Adeptus Mechanicus/table-88-9.jpg\n",
      "Codex - Adeptus Mechanicus/table-89-10.jpg\n",
      "Codex - Adeptus Mechanicus/table-94-14.jpg\n",
      "Codex - Adeptus Mechanicus/table-119-23.jpg\n",
      "Codex - Adeptus Mechanicus/table-120-26.jpg\n",
      "Codex - Adeptus Mechanicus/table-120-28.jpg\n",
      "Codex - Adeptus Mechanicus/table-120-29.jpg\n",
      "Codex - Adeptus Mechanicus/table-120-27.jpg\n",
      "Codex - Adeptus Mechanicus/table-96-15.jpg\n",
      "Codex - Adeptus Mechanicus/table-92-13.jpg\n",
      "Codex - Adeptus Mechanicus/table-119-25.jpg\n",
      "Codex - Adeptus Mechanicus/table-119-24.jpg\n",
      "Codex - Adeptus Mechanicus/table-83-6.jpg\n",
      "Codex - Adeptus Mechanicus/table-64-5.jpg\n",
      "Codex - Adeptus Mechanicus/table-99-17.jpg\n",
      "Codex - Adeptus Mechanicus/table-91-12.jpg\n",
      "Codex - Adeptus Mechanicus/table-3-1.jpg\n",
      "Codex - Adeptus Mechanicus/table-89-11.jpg\n",
      "Codex - Adeptus Mechanicus/table-64-4.jpg\n",
      "Codex - Adeptus Mechanicus/table-63-3.jpg\n",
      "Codex - Adeptus Mechanicus/table-87-8.jpg\n",
      "Codex - Adeptus Mechanicus/table-3-2.jpg\n",
      "Codex - Adeptus Mechanicus/table-84-7.jpg\n",
      "Codex - Adeptus Mechanicus/table-100-18.jpg\n",
      "Codex - Adeptus Mechanicus/table-107-20.jpg\n",
      "Codex - Adeptus Mechanicus/table-121-30.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 2/3 [50:33<25:02, 1502.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****\n",
      "Codex -Black Templars.pdf\n",
      "****\n",
      "\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Codex -Black Templars/table-9-2.jpg\n",
      "Codex -Black Templars/table-15-8.jpg\n",
      "Codex -Black Templars/table-27-14.jpg\n",
      "Codex -Black Templars/table-9-3.jpg\n",
      "Codex -Black Templars/table-7-1.jpg\n",
      "Codex -Black Templars/table-31-16.jpg\n",
      "Codex -Black Templars/table-35-18.jpg\n",
      "Codex -Black Templars/table-13-5.jpg\n",
      "Codex -Black Templars/table-15-7.jpg\n",
      "Codex -Black Templars/table-17-9.jpg\n",
      "Codex -Black Templars/table-25-13.jpg\n",
      "Codex -Black Templars/table-29-15.jpg\n",
      "Codex -Black Templars/table-13-6.jpg\n",
      "Codex -Black Templars/table-19-10.jpg\n",
      "Codex -Black Templars/table-21-11.jpg\n",
      "Codex -Black Templars/table-23-12.jpg\n",
      "Codex -Black Templars/table-11-4.jpg\n",
      "Codex -Black Templars/table-33-17.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [1:00:31<00:00, 1210.36s/it]\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "vectorstore_list = []\n",
    "\n",
    "for filename in tqdm(os.listdir(\"codex\")):\n",
    "\n",
    "  print(f\"\\n****\\n{filename}\\n****\\n\")\n",
    "\n",
    "  vectorstore = await pdf_2_vectorstore(os.path.join(\"codex\", filename), embeddings)\n",
    "\n",
    "  vectorstore_list.append(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ULRxXQmbR7HP"
   },
   "outputs": [],
   "source": [
    "merged_store = vectorstore_list[0]\n",
    "for vs in vectorstore_list[1:]:\n",
    "  merged_store.merge_from(vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "cX8vxSPSpwRZ"
   },
   "outputs": [],
   "source": [
    "merged_store.save_local(\"warhammer40k_codex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "duF7NyoVpygW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1f14409c824e432c9ceda403d28320f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "32815d557811477ba26f24a92bd6ce38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "378b43d561404033a89c3e41c51d84b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b983b9cf8ecc455dab2a85323ae00d49",
      "max": 216625723,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1f14409c824e432c9ceda403d28320f1",
      "value": 216625723
     }
    },
    "3e39014d7394441e9096f1d8190bb880": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d6ff52c05c304948bbdc16790165d121",
      "placeholder": "​",
      "style": "IPY_MODEL_4c29357df5324ad18fbd281f5ae364af",
      "value": " 217M/217M [00:01&lt;00:00, 155MB/s]"
     }
    },
    "4c29357df5324ad18fbd281f5ae364af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "56aaa5eb5c5443bb9cdf518c24375183": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b163749c9ddb49c2b94a471003e1cdce",
      "placeholder": "​",
      "style": "IPY_MODEL_32815d557811477ba26f24a92bd6ce38",
      "value": "yolox_l0.05.onnx: 100%"
     }
    },
    "ab2ab56ca30440adafc61eb40b5f064e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b163749c9ddb49c2b94a471003e1cdce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b983b9cf8ecc455dab2a85323ae00d49": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6ff52c05c304948bbdc16790165d121": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc8d8743d77240c19f7653d57c715829": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_56aaa5eb5c5443bb9cdf518c24375183",
       "IPY_MODEL_378b43d561404033a89c3e41c51d84b5",
       "IPY_MODEL_3e39014d7394441e9096f1d8190bb880"
      ],
      "layout": "IPY_MODEL_ab2ab56ca30440adafc61eb40b5f064e"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
