{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJaaPDbJvOhs"
   },
   "source": [
    "# Unstructured 文件解析與多模態資料處理\n",
    "\n",
    "---\n",
    "\n",
    "**本章學完你將能學會什麼：**\n",
    "\n",
    "- 理解什麼是 *Unstructured* 套件，以及它在文件解析與資料前處理中的應用場景。  \n",
    "- 學會如何在 Google Colab（Linux 環境）中安裝與設定 *Unstructured* 套件，正確抽取 PDF、表格與影像資料。  \n",
    "- 掌握多種文件解析策略（`fast`、`hi_res`、`ocr_only`）的差異與最佳使用情境。  \n",
    "- 學會使用不同的分段策略（chunking strategy），例如 `by_title`，將長篇文件切割成具語意邏輯的文字區塊。  \n",
    "- 理解如何從影像與表格中萃取結構化資訊，並運用 OCR（光學字元辨識）技術。  \n",
    "- 學會將抽取後的內容轉換成可搜尋的向量資料庫（VectorStore），為後續 RAG（Retrieval-Augmented Generation）應用奠定基礎。  \n",
    "- 熟悉如何整合 LangChain、OpenAI API、HuggingFace Embedding 模型，建立從文件 → 向量 → 智慧檢索的完整流程。\n",
    "\n",
    "---\n",
    "\n",
    "**📘 最終你將具備的能力：**  \n",
    "- 能夠撰寫 Python 程式，實現從 PDF 文件中自動抽取文字、影像與表格，並將其轉換為結構化資料。  \n",
    "- 能理解並應用不同文件解析策略，根據資料特性調整擷取邏輯。  \n",
    "- 能獨立建立文件資料的向量化流程（FAISS + LangChain），實作文件摘要與圖像文字生成（Image Captioning）。  \n",
    "- 具備設計多模態 LLM pipeline 的能力，讓模型能根據圖片或文字內容生成描述、摘要或回答問題。\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Unstructured 算是應用範圍蠻廣泛的Package，支援處理各式各樣的檔案格式，包含了PDF，PPT，HTML等等\n",
    "\n",
    "無法在Windows系統上成功使用，所以我們將環境換成Google Colab，因為Colab的操作系統是Linux\n",
    "\n",
    "首先我們要安裝各式各樣的套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9AgiUyZ9Ab6N",
    "outputId": "49fed438-1581-41f0-e6ae-b7d83dfdd4b8"
   },
   "outputs": [],
   "source": [
    "!apt-get update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ⚙️ 環境設定補充說明\n",
    "\n",
    "由於 **Unstructured** 這個套件在不同作業系統上相依的元件略有差異，  \n",
    "在 Windows 上常會因為缺乏底層的 PDF 或 OCR 處理函式庫而導致安裝失敗。  \n",
    "因此，本章的範例我們統一採用 **Google Colab** 執行環境（Linux 系統），確保相容性。\n",
    "\n",
    "以下是各套件的用途說明，讓你清楚知道為何要安裝這些依賴：\n",
    "\n",
    "| 指令 / 套件 | 功能說明 |\n",
    "|--------------|-----------|\n",
    "| `poppler-utils` | 提供 PDF 處理工具，例如 `pdftotext`、`pdfimages`，是 PDF 轉換的基礎套件。 |\n",
    "| `libleptonica-dev` | 圖像處理函式庫，為 OCR 引擎（tesseract）的底層依賴。 |\n",
    "| `tesseract-ocr`、`libtesseract-dev` | Google 的 OCR 引擎，用於將圖片中的文字轉換為可讀文字。 |\n",
    "| `python3-pil` | Python 影像處理函式庫（Pillow 的底層依賴）。 |\n",
    "| `tesseract-ocr-eng`、`tesseract-ocr-script-latn` | OCR 模組，支援英文與拉丁字母辨識。 |\n",
    "| `unstructured[all-docs]` | 主角套件，支援多種文件格式（PDF、HTML、DOCX、PPTX 等）。 |\n",
    "| `unstructured-inference` | 內含文件結構辨識模型，用於 `hi_res` 策略。 |\n",
    "| `pdf2image` | 將 PDF 頁面轉換成影像格式，方便 OCR 或版面分析。 |\n",
    "| `faiss-cpu` | Facebook AI 相似度搜尋庫，用於向量檢索。 |\n",
    "| `sentence-transformers` | 提供多種文字向量化模型，例如 BERT、bge-m3。 |\n",
    "| `langchain`, `langchain-community`, `langchain-core`, `langchain-openai` | 建立 LLM 流程與模組串接的核心工具。 |\n",
    "| `bitsandbytes`, `accelerate`, `xformers`, `triton`, `transformers` | 深度學習加速與模型推論框架，用於高效運算與模型加速。 |\n",
    "| `nltk` | 傳統 NLP 工具包，用於文字處理與分詞。 |\n",
    "\n",
    "> 💡 **提示：**\n",
    "> 若在安裝過程中出現 `E: Unable to locate package` 或 `ImportError` 錯誤，可嘗試重新執行 `!apt-get update` 後再安裝，  \n",
    "> 或檢查 Colab 執行環境是否為「Python 3 + GPU / CPU」版本（非 TPU）。\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJxP9HvvDrcQ",
    "outputId": "173a1a1c-7bf2-42d6-ebff-d704cc899e27"
   },
   "outputs": [],
   "source": [
    "!apt-get install poppler-utils libleptonica-dev tesseract-ocr libtesseract-dev python3-pil tesseract-ocr-eng tesseract-ocr-script-latn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "PnnjjTV9_hhz",
    "outputId": "471d7fa8-0507-4594-faa8-7db937e4b78a"
   },
   "outputs": [],
   "source": [
    "!pip install unstructured[all-docs] unstructured-inference cmake python-dotenv pdf2image python-dateutil faiss-cpu sentence-transformers langchain==0.2.5 langchain-community==0.2.5 langchain-core==0.2.9 langchain-openai==0.1.9 bitsandbytes accelerate xformers triton transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R5Dr9S77xSVU",
    "outputId": "0a418bcc-f541-4a52-be19-90fb7d4a56d3"
   },
   "outputs": [],
   "source": [
    "!pip install -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3NvpxuBQPvh"
   },
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fud5TysvOhu"
   },
   "source": [
    "## 處理PDF\n",
    "\n",
    "在使用上有點繁瑣，所以我們來一步步地進行測試\n",
    "\n",
    "- 使用Transformer is all you need這篇論文作為輸入的數據\n",
    "- 不同的數據處理策略(strategy)\n",
    "  >- fast: 「基於規則（rule-based）」的策略運用傳統的自然語言處理（NLP）抽取技術，能快速擷取所有文字元素。不建議在以影像為主的檔案類型中使用「fast」策略。\n",
    "  >- hi_res: 基於模型（model-based）」的策略會辨識文件的版面配置。「hi_res」的優點在於它利用文件的版面結構，獲取關於文件元素的額外資訊。 若您的使用情境對文件元素的正確分類非常敏感，建議使用此策略。\n",
    "  >- ocr_only: 另一種「基於模型（model-based）」的策略，利用光學字元辨識（OCR）技術，從影像型文件中擷取文字。\n",
    "\n",
    "一般來說是建議用`hi_res`，因為可以擷取影像資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esxZZ810vOhv"
   },
   "outputs": [],
   "source": [
    "filename = \"bertv2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "d1a2178ff1784e33b3d8f2855759c273",
      "4f9190556e1c4ffcb41272d11639346f",
      "e1d77ca01b2f49ce893ab4777558757f",
      "4546f8d6fcc54e54bb7e628f0a09fd55",
      "aee85e46b30f45608a561808b9ed7bba",
      "a60833d6e87f4206af1a88254bd05cb6",
      "72e7c41771d44394b67c68f85d495608",
      "1aee4f0fd94d457db9724c8a5727f94b",
      "d2983e9ee59c4a328b2322dedb2591c3",
      "156d3ed55ad94f918fb257117b03f05f",
      "7f6f041faa594282842ff4603ec613e6"
     ]
    },
    "id": "FMIG3PRN7t76",
    "outputId": "8cdacb8c-4b39-4395-aa51-35a4db37d0b0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.staging.base import elements_to_json\n",
    "\n",
    "# 參數 max_characters, new_after_n_chars, combine_text_under_n_chars 只有在chunking_strategy不為None時才會生效\n",
    "\n",
    "elements_none = partition_pdf(\n",
    "    filename,\n",
    "    strategy='hi_res',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_5RbD5BvOhv"
   },
   "source": [
    "前20個物件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8nUuVj6jpUHc"
   },
   "outputs": [],
   "source": [
    "elements_none[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w24zIohUvOhw"
   },
   "source": [
    "檢查category attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dHiCzsF5vOhw"
   },
   "outputs": [],
   "source": [
    "for element in elements_none[:20]:\n",
    "    print(element.category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqtFGGlLvOhw"
   },
   "source": [
    "檢查text attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QLB7i_VdvOhw"
   },
   "outputs": [],
   "source": [
    "for element in elements_none[:20]:\n",
    "    print(element.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ux8zIm2vOhw"
   },
   "source": [
    "你可以看得很清楚，沒有chunking strategy訊息散的到處都是，跟侏儸紀公園2中運送暴龍的運輸船上的工作人員一樣。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5LGCdR5673_e",
    "outputId": "dc415310-ed92-47c3-e2fb-de1975f1d47e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.unique([element.category for element in elements_none])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A706VTs_oM9Y"
   },
   "source": [
    "## by_title 分段策略（chunking strategy）\n",
    "\n",
    "`by_title` 分段策略會保留章節邊界，並可選擇同時保留頁面邊界。  \n",
    "在這裡，「保留」的意思是：同一個區塊（chunk）永遠不會包含來自兩個不同章節的文字。  \n",
    "當新的章節開始時，現有的區塊會被關閉並開啟新的區塊，即使下一個元素的長度仍可容納於先前的區塊中。\n",
    "\n",
    "除了上述基本策略的行為之外，`by_title` 策略還具有以下特性：\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 偵測章節標題\n",
    "\n",
    "當偵測到 **Title** 元素時，該元素會被視為新章節的開始。  \n",
    "當遇到 **Title** 元素時，前一個區塊會被關閉，並開啟新的區塊，即使該 **Title** 元素的長度可容納於前一個區塊中。\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 遵守頁面邊界\n",
    "\n",
    "可以使用 `multipage_sections` 參數來選擇是否遵守頁面邊界。  \n",
    "此參數的預設值為 `True`，表示頁面分隔符（page break）**不會**導致新的區塊開始。  \n",
    "若將其設為 `False`，則會將位於不同頁面的元素分割為獨立的區塊。\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 合併小型章節\n",
    "\n",
    "在某些文件中，分段過程可能會將清單項目或其他短段落誤判為 **Title** 元素，  \n",
    "即使它們實際上並非章節標題。這可能導致產生的區塊比預期的小得多。  \n",
    "\n",
    "此行為可透過 `combine_text_under_n_chars` 參數進行調整。  \n",
    "該參數預設值與 `max_characters` 相同，表示會將連續的小章節合併，以最大化區塊的填充程度。  \n",
    "若將此參數設為 `0`，則會停用章節合併功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OB55SK2OvOhw"
   },
   "source": [
    "先來檢查沒有chunking strategy 時數據切割的樣子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOLIFTwQvOhw"
   },
   "outputs": [],
   "source": [
    "for idx, element in enumerate(elements_none):\n",
    "    if element.category == \"Title\":\n",
    "        print(idx, element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBdcZx7RvOhw"
   },
   "source": [
    "## chunking_strategy='by_title'\n",
    "\n",
    "https://docs.unstructured.io/ui/chunking#combine-text-under-n-characters-setting\n",
    "\n",
    "### max_characters\n",
    "\n",
    "此參數為單一區塊（chunk）中可包含文字數量的**硬性上限**。  \n",
    "\n",
    "它確保每個區塊都不會超過此限制，無論句子或段落邊界為何。  \n",
    "\n",
    "可將其視為區塊大小的「最大界限」。  \n",
    "\n",
    "👉 **範例：** 若設定 `max_characters=500`，則任何區塊都不會包含超過 500 個字元。\n",
    "\n",
    "---\n",
    "\n",
    "### new_after_n_chars\n",
    "\n",
    "此參數為**柔性分段依據**，在文字長度達到此門檻時，會嘗試建立新的區塊。  \n",
    "\n",
    "與 `max_characters` 不同的是，它允許函式庫在該範圍內尋找「自然的分隔點」，  \n",
    "例如句子或段落的結尾。  \n",
    "\n",
    "當您希望區塊大致維持在某個長度，同時仍能尊重自然語意邊界時，此設定非常有用。  \n",
    "\n",
    "👉 **範例：** 若設定 `new_after_n_chars=300`，函式庫會嘗試在約 300 個字元後開始新的區塊，  \n",
    "但若下一個自然分隔點位於第 320 個字元，則可能略為超出此值。\n",
    "\n",
    "---\n",
    "\n",
    "### combine_text_under_n_chars\n",
    "\n",
    "該參數預設值與 `max_characters` 相同，表示會將連續的小章節合併，以最大化區塊的填充程度。  \n",
    "若將此參數設為 `0`，則會停用章節合併功能。\n",
    "\n",
    "講解完後實操一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Q3RLAMEvOhw"
   },
   "outputs": [],
   "source": [
    "elements_by_title = partition_pdf(\n",
    "    filename,\n",
    "    strategy='hi_res',\n",
    "    chunking_strategy='by_title',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XiHNFmV-vOhx"
   },
   "source": [
    "檢查 category attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cf8P23eVvOhx"
   },
   "outputs": [],
   "source": [
    "for element in elements_by_title[:20]:\n",
    "  print(element.category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a89kkpIAvOhx"
   },
   "source": [
    "## 擷取影像和表格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cbQveJzxdMG6",
    "outputId": "7a339ecc-12af-456d-e906-83601acf212c"
   },
   "outputs": [],
   "source": [
    "elements_by_title = partition_pdf(\n",
    "    filename,\n",
    "    chunking_strategy='by_title',\n",
    "    extract_image_block_types=[\"Image\", \"Table\"],\n",
    "    strategy='hi_res',\n",
    "    extract_image_block_output_dir=pathlib.Path(filename).stem\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuA4VX8qEbCw"
   },
   "source": [
    "你可以看到在這個例子中，影像抽取的結果蠻糟糕的。可能是因為圖像中元素有強烈的邊界，導致影像識別模型無法正確判斷影像。\n",
    "\n",
    "再試另外一個含有大量照片的PDF檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Y1hMXUsEYw0",
    "outputId": "90e2c4ba-ca65-4d5d-d255-e790784c88a5"
   },
   "outputs": [],
   "source": [
    "elements_by_title = partition_pdf(\n",
    "    'menu.pdf',\n",
    "    chunking_strategy='by_title',\n",
    "    extract_image_block_types=[\"Image\", \"Table\"],\n",
    "    strategy='hi_res',\n",
    "    extract_image_block_output_dir=pathlib.Path(\"menu.pdf\").stem\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOhrP4opF9k8"
   },
   "source": [
    "你可以看到，若是圖像比較像是照片，結果會比較好。但依然會有一切false positive出現。所以你必須要根據數據的類型，決定用哪種抽取方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ICeV4MHzGnAx",
    "outputId": "d68fa60f-fa98-47d3-e7c3-12fad5a90cb3"
   },
   "outputs": [],
   "source": [
    "print(elements_by_title[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zc4PPLiLJB_u"
   },
   "source": [
    "你也許會很好奇 \"best quality knowledge and skill\" 這段文字哪裡來的。\n",
    "所以我們退後一步，看看原始數據長啥樣子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "he1rXHXMHvSq",
    "outputId": "efaee581-a0e6-47fd-8511-e67c6cacd636"
   },
   "outputs": [],
   "source": [
    "elements_none = partition_pdf(\n",
    "    'menu.pdf',\n",
    "    strategy='hi_res'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zCp1MKlMIXd9",
    "outputId": "a51e4405-5aa7-4d20-822d-1740f714e037"
   },
   "outputs": [],
   "source": [
    "for element in elements_none[:20]:\n",
    "    print(element.category, element.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jiyWIheCLkev"
   },
   "outputs": [],
   "source": [
    "partition_pdf?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCtxN9uIKAwu"
   },
   "source": [
    "從內容來看，擷取影像的時候也包含了OCR: 會把影像中的文字提取出來，並且在 chunking_strategy中被併入文字訊息的一部分。\n",
    "此外，你也可以看到在排版上有些奇怪的結果: 像是\n",
    "- NarrativeText Loaded tortilla-coated crunchy wings seasoned with our signature blend of spices. Choose from:\n",
    "- Title LOVED BY YOU\n",
    "\n",
    "就像是直接把兩個左右相鄰的板塊視為上下文的關係。這種情況在雙列排版的內容很常見，所以我們要如何減輕這種情況?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u_3nsNjtNl9Z",
    "outputId": "284df34e-f264-40e6-a4b1-5e0881cf3a65"
   },
   "outputs": [],
   "source": [
    "elements_strategy_ocr_only = partition_pdf(\n",
    "    'menu.pdf',\n",
    "    strategy='ocr_only',\n",
    "    chunking_strategy='by_title'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cZL5Uo8DNzhv",
    "outputId": "2fd0916b-66c7-42a7-eaf8-d11960bd1b16"
   },
   "outputs": [],
   "source": [
    "for element in elements_strategy_ocr_only[:5]:\n",
    "  print(\"******\")\n",
    "  print(element.category, element.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h796CDHbQPub"
   },
   "source": [
    "### 一個便當吃不飽，可以吃兩個\n",
    "\n",
    "這樣子我們可以看到使用ocr_only這種策略，對於抽取雙列排版的檔案比較友善。\n",
    "所以在實際上的應用，可以\n",
    "\n",
    ">- 考慮使用ocr_only抽取文字\n",
    ">- 然後用hi_res抽取圖像和表格\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kE8IKTQaGc7k"
   },
   "source": [
    "## PDF -> Vectorstore\n",
    "\n",
    "你現在有了文字，表格，圖像。你可以開始建立自己專屬的數據庫\n",
    "\n",
    "### 文字:\n",
    "- 原始數據\n",
    "- 進行壓縮 (像是summary)\n",
    "\n",
    "### 表格/圖像:\n",
    "- 使用image caption將圖片轉換成文字\n",
    "\n",
    "將 max_character 和 new_after_n_chars 分別拉高到 2000 和 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JarvYp1ES2mm",
    "outputId": "c86c9ad7-0ff4-4c3a-d036-d349c09ea64e"
   },
   "outputs": [],
   "source": [
    "elements_strategy_ocr_only = partition_pdf(\n",
    "    'menu.pdf',\n",
    "    strategy='ocr_only',\n",
    "    chunking_strategy='by_title',\n",
    "    max_characters=2000,\n",
    "    new_after_n_chars=1500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYkW9dtpnfMY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "def credential_init():\n",
    "\n",
    "  credential_file = \"credentials.ini\"\n",
    "\n",
    "  if os.path.exists(credential_file):\n",
    "      credentials = configparser.ConfigParser()\n",
    "      credentials.read(credential_file)\n",
    "      os.environ['OPENAI_API_KEY'] = credentials['openai'].get('api_key')\n",
    "  else:\n",
    "      os.environ['OPENAI_API_KEY'] = os.environ['OPENAI']\n",
    "\n",
    "credential_init()\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "           model_name=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r8JlFwMmNt34"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "\n",
    "from PIL import Image\n",
    "from langchain_core.prompts.image import ImagePromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.runnables import chain, RunnablePassthrough, RunnableLambda\n",
    "\n",
    "\n",
    "def build_standard_chat_prompt_template(kwargs):\n",
    "    messages = []\n",
    "\n",
    "    if 'system' in kwargs:\n",
    "        content = kwargs.get('system')\n",
    "\n",
    "        # allow list of prompts for multimodal\n",
    "        if isinstance(content, list):\n",
    "            prompts = [PromptTemplate(**c) for c in content]\n",
    "        else:\n",
    "            prompts = [PromptTemplate(**content)]\n",
    "\n",
    "        message = SystemMessagePromptTemplate(prompt=prompts)\n",
    "        messages.append(message)\n",
    "\n",
    "    if 'human' in kwargs:\n",
    "        content = kwargs.get('human')\n",
    "\n",
    "        # allow list of prompts for multimodal\n",
    "        if isinstance(content, list):\n",
    "            prompts = []\n",
    "            for c in content:\n",
    "                if c.get(\"type\") == \"image\":\n",
    "                    prompts.append(ImagePromptTemplate(**c))\n",
    "                else:\n",
    "                    prompts.append(PromptTemplate(**c))\n",
    "        else:\n",
    "            if content.get(\"type\") == \"image\":\n",
    "                prompts = [ImagePromptTemplate(**content)]\n",
    "            else:\n",
    "                prompts = [PromptTemplate(**content)]\n",
    "\n",
    "        message = HumanMessagePromptTemplate(prompt=prompts)\n",
    "        messages.append(message)\n",
    "\n",
    "    chat_prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "    return chat_prompt_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Xz51AR6JgiE"
   },
   "source": [
    "### 文字壓縮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bK4BsLu-oG1-"
   },
   "outputs": [],
   "source": [
    "prompt = f\"Summarize the following text:\\n\\n{elements_strategy_ocr_only[0]}\\n\\nSummary:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bup32QWpoxK_",
    "outputId": "f044f0fc-3b18-4e3f-cff7-6403957addb0"
   },
   "outputs": [],
   "source": [
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def summarize_documents(model, elements):\n",
    "    text_prompt_template = {\"template\": \"Summarize the following text:\\n\\n{query}\\n\\nSummary:\", \"input_variables\": [\"query\"]}\n",
    "    chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "        HumanMessagePromptTemplate(prompt=[PromptTemplate(**text_prompt_template)])\n",
    "    ])\n",
    "\n",
    "    text_pipeline = chat_prompt_template | model | StrOutputParser()\n",
    "    batches = [{\"query\": c.text} for c in elements]\n",
    "\n",
    "    return await text_pipeline.abatch(batches)\n",
    "\n",
    "# 執行\n",
    "async def main():\n",
    "    summaries = await summarize_documents(model, elements_strategy_ocr_only)\n",
    "\n",
    "    return summaries\n",
    "\n",
    "results = asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97pZLMBbPBu4"
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for result in results:\n",
    "\n",
    "  documents.append(Document(page_content=result, metadata={'filename': \"menu.pdf\",\n",
    "                               'type': \"text\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xgBJJMoOCsea"
   },
   "outputs": [],
   "source": [
    "documents[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6XlQvJXP9cU"
   },
   "source": [
    "願意的話，你甚至可以辦到標註是在哪一頁"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0J1QvWpJ080"
   },
   "source": [
    "### 表格和影像提取\n",
    "\n",
    "你可以注意到有些圖片是False Positive。在務實上可以考慮用Pydantic Model的輸出格式幫忙標記假的圖片。\n",
    "\n",
    "跟模型說: 你的眼睛業障重，假的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yDvZSMn2JwLl",
    "outputId": "a9fca3d5-5b31-4517-a216-a23007dc8827"
   },
   "outputs": [],
   "source": [
    "import aiofiles\n",
    "import asyncio\n",
    "from operator import itemgetter\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Async function to read image and convert to base64\n",
    "\n",
    "async def image_to_base64(image_path: str) -> str:\n",
    "    loop = asyncio.get_event_loop()\n",
    "\n",
    "    def read_image():\n",
    "        with Image.open(image_path) as image:\n",
    "            buffered = io.BytesIO()\n",
    "            image.convert(\"RGB\").save(buffered, format=\"JPEG\")\n",
    "            return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "\n",
    "    return await loop.run_in_executor(None, read_image)\n",
    "\n",
    "\n",
    "text_prompt_template = {\"template\": \"Describe the content.\"}\n",
    "image_prompt_template = {\"type\": \"image\",\n",
    "              \"template\": {\"url\": \"data:image/jpeg;base64,{image_str}\"},\n",
    "              \"input_variables\": [\"image_str\"]}\n",
    "\n",
    "input_ = {\n",
    "    \"human\": [text_prompt_template, image_prompt_template],\n",
    "}\n",
    "\n",
    "chat_prompt_template = build_standard_chat_prompt_template(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 使用 RunnableLambda 處理 async 函式 ---\n",
    "async def make_image_caption_pipeline(model):\n",
    "    async def assign_image_str(x):\n",
    "        image_str = await image_to_base64(x['image_path'])\n",
    "        return {**x, 'image_str': image_str}\n",
    "\n",
    "    image_2_image_str_chain = RunnableLambda(assign_image_str)\n",
    "    pipeline = image_2_image_str_chain | chat_prompt_template | model | StrOutputParser()\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 主流程 ---\n",
    "async def main():\n",
    "    model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                       model_name=\"gpt-4o-mini\", temperature=0)\n",
    "    \n",
    "    pipeline = await make_image_caption_pipeline(model)\n",
    "    image_dir = \"menu\"\n",
    "\n",
    "    batches = [{\"image_path\": os.path.join(image_dir, c)}\n",
    "               for c in os.listdir(image_dir) if 'figure' in Path(c).stem]\n",
    "\n",
    "    results = await pipeline.abatch(batches)\n",
    "\n",
    "    return results\n",
    "\n",
    "# 執行主函式\n",
    "results = asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2EXHLumu0Gg"
   },
   "outputs": [],
   "source": [
    "for result in results:\n",
    "\n",
    "    documents.append(Document(page_content=result, metadata={'filename': \"menu.pdf\",\n",
    "                               \"type\": \"figure\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eEN2vl2WISY0",
    "outputId": "65716c83-c33b-44ea-abf8-fffff61f7421"
   },
   "outputs": [],
   "source": [
    "documents[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9cRcLtUIaFW"
   },
   "source": [
    "基於Documents內容建立vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UDp6GGtIxgZB"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=documents, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWoeDv_yCxG3"
   },
   "source": [
    "### Save the vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hl7opAz0CxG4"
   },
   "outputs": [],
   "source": [
    "vectorstore.save_local(\"vectorstore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9ud0peSCxG4"
   },
   "source": [
    "### Load the vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4AoyfmP3CxG5"
   },
   "outputs": [],
   "source": [
    "vectorstore_1 = FAISS.load_local(\n",
    "    \"vectorstore\", embeddings, allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "vectorstore_1.docstore._dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzU2xDojCxG5"
   },
   "source": [
    "### 合併vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ENy9dQkBCxG5"
   },
   "outputs": [],
   "source": [
    "vectorstore.merge_from(vectorstore_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klQhexXaNjV7"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DoUdsOeSPl9n"
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"codex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PkR0uv84NiiC"
   },
   "outputs": [],
   "source": [
    "async def pdf_2_vectorstore(filename, embeddings):\n",
    "\n",
    "  image_dir = Path(filename).stem\n",
    "\n",
    "  elements = partition_pdf(\n",
    "    filename,\n",
    "    chunking_strategy='by_title',\n",
    "    extract_image_block_types=[\"Table\"],\n",
    "    # infer_table_structure=False,\n",
    "    form_extraction_skip_tables=True,\n",
    "    max_characters=4000,\n",
    "    new_after_n_chars=3800,\n",
    "    strategy='hi_res',\n",
    "    extract_image_block_output_dir=image_dir\n",
    ")\n",
    "\n",
    "  documents = []\n",
    "\n",
    "  text_prompt_template = {\"template\": \"Summarize the following text:\\n\\n{query}\\n\\nSummary:\", \"input_variables\": [\"query\"]}\n",
    "\n",
    "  input_ = {\n",
    "      \"human\": [text_prompt_template],\n",
    "  }\n",
    "\n",
    "  chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "  text_pipeline = chat_prompt_template|model|StrOutputParser()\n",
    "\n",
    "  batches = [{\"query\": c.text} for c in elements]\n",
    "\n",
    "  results = await text_pipeline.abatch(batches)\n",
    "\n",
    "  for result in results:\n",
    "\n",
    "    documents.append(Document(page_content=result, metadata={'filename': image_dir,\n",
    "                                 'type': \"text\"}))\n",
    "\n",
    "\n",
    "  text_prompt_template = {\"template\": \"Describe the content.\"}\n",
    "  image_prompt_template = {\"type\": \"image\",\n",
    "                \"template\": {\"url\": \"data:image/jpeg;base64,{image_str}\"},\n",
    "                \"input_variables\": [\"image_str\"]}\n",
    "\n",
    "  input_ = {\n",
    "      \"human\": [text_prompt_template, image_prompt_template],\n",
    "  }\n",
    "\n",
    "  chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "  # Async chain for converting image_path -> base64\n",
    "  # The lambda in RunnablePassthrough.assign is async-friendly\\\n",
    "\n",
    "  image_2_image_str_chain = RunnablePassthrough.assign(image_str=lambda x: itemgetter('image_path')|image_to_base64)\n",
    "\n",
    "\n",
    "  image_caption_pipeline = image_2_image_str_chain | chat_prompt_template| model | StrOutputParser()\n",
    "\n",
    "  batches = [{\"image_path\": os.path.join(image_dir, c)} for c in os.listdir(image_dir) if 'table' in Path(c).stem]\n",
    "\n",
    "  results = await image_caption_pipeline.abatch(batches)\n",
    "\n",
    "  for result in results:\n",
    "\n",
    "documents.append(Document(page_content=result, metadata={'filename': image_dir,\n",
    "                                \"type\": \"table\"}))\n",
    "\n",
    "  vectorstore = FAISS.from_documents(documents=documents, embedding=embeddings)\n",
    "\n",
    "  return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pW_VpqyTOpjB"
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "vectorstore_list = []\n",
    "\n",
    "for filename in tqdm(os.listdir(\"codex\")):\n",
    "\n",
    "  print(f\"\\n****\\n{filename}\\n****\\n\")\n",
    "\n",
    "  vectorstore = await pdf_2_vectorstore(os.path.join(\"codex\", filename), embeddings)\n",
    "\n",
    "  vectorstore_list.append(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULRxXQmbR7HP"
   },
   "outputs": [],
   "source": [
    "merged_store = vectorstore_list[0]\n",
    "for vs in vectorstore_list[1:]:\n",
    "  merged_store.merge_from(vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cX8vxSPSpwRZ"
   },
   "outputs": [],
   "source": [
    "merged_store.save_local(\"warhammer40k_codex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "duF7NyoVpygW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "156d3ed55ad94f918fb257117b03f05f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1aee4f0fd94d457db9724c8a5727f94b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4546f8d6fcc54e54bb7e628f0a09fd55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_156d3ed55ad94f918fb257117b03f05f",
      "placeholder": "​",
      "style": "IPY_MODEL_7f6f041faa594282842ff4603ec613e6",
      "value": " 217M/217M [00:01&lt;00:00, 141MB/s]"
     }
    },
    "4f9190556e1c4ffcb41272d11639346f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a60833d6e87f4206af1a88254bd05cb6",
      "placeholder": "​",
      "style": "IPY_MODEL_72e7c41771d44394b67c68f85d495608",
      "value": "yolox_l0.05.onnx: 100%"
     }
    },
    "72e7c41771d44394b67c68f85d495608": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7f6f041faa594282842ff4603ec613e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a60833d6e87f4206af1a88254bd05cb6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aee85e46b30f45608a561808b9ed7bba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d1a2178ff1784e33b3d8f2855759c273": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4f9190556e1c4ffcb41272d11639346f",
       "IPY_MODEL_e1d77ca01b2f49ce893ab4777558757f",
       "IPY_MODEL_4546f8d6fcc54e54bb7e628f0a09fd55"
      ],
      "layout": "IPY_MODEL_aee85e46b30f45608a561808b9ed7bba"
     }
    },
    "d2983e9ee59c4a328b2322dedb2591c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e1d77ca01b2f49ce893ab4777558757f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1aee4f0fd94d457db9724c8a5727f94b",
      "max": 216625723,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d2983e9ee59c4a328b2322dedb2591c3",
      "value": 216625723
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
