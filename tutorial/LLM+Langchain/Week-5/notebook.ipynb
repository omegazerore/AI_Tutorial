{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "352710b7-8462-4e32-9af7-59b20a0edcf5",
   "metadata": {},
   "source": [
    "# Image Caption\n",
    "\n",
    "## Image Caption with Multimodal LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acc181dc-846b-47ba-97e5-3d7526d0702d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: space-around;\">\n",
       "    <div>\n",
       "        <img src=\"StellarBladeTachy-Nikke.png\" height=\"900\" width=\"600\" />\n",
       "    </div>\n",
       "    <div>\n",
       "        <img src=\"AzueLaneAmagi.png\" height=\"900\" width=\"600\" />\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Define the HTML to display images side by side\n",
    "html = \"\"\"\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "    <div>\n",
    "        <img src=\"StellarBladeTachy-Nikke.png\" height=\"900\" width=\"600\" />\n",
    "    </div>\n",
    "    <div>\n",
    "        <img src=\"AzueLaneAmagi.png\" height=\"900\" width=\"600\" />\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "# Display the HTML\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c438f085-a035-4dc7-b14c-caf726223804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8180ba97-5e95-4e6f-8318-97562c91e9ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from src.initialization import credential_init\n",
    "\n",
    "credential_init()\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o-2024-05-13\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe6a598-704b-4666-bd3e-bc1113a6db8a",
   "metadata": {},
   "source": [
    "如果 API 僅支援文字資料（例如 JSON 傳輸），圖片會先轉換成 Base64 字串，再傳送給服務；但若 API 支援檔案上傳或 URL，就可以直接傳送圖片，而不需要 Base64。\n",
    "\n",
    "實際上 LLM Image Caption 常見做法\n",
    "\n",
    "    - 方法 A：直接傳圖片 URL（最簡單、避免 Base64 膨脹 33% 的資料量）。\n",
    "\n",
    "    - 方法 B：將圖片轉 Base64，放進 JSON 傳給模型（如果 API 要求）。\n",
    "\n",
    "    - 方法 C：multipart/form-data 上傳（類似檔案上傳，效率最高）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a94ac7-145c-47ee-9d41-7ec3f34f66c1",
   "metadata": {},
   "source": [
    "將圖像透過檔案名稱轉換成Base64字串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "284f0d20-8fa9-4c5f-abb1-5ecc17dd084b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "from textwrap import dedent\n",
    "\n",
    "from PIL import Image\n",
    "from langchain_core.messages.human import HumanMessage\n",
    "from langchain_core.prompts.image import ImagePromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, PromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from src.io.path_definition import get_project_dir\n",
    "\n",
    "\n",
    "def image_to_base64(image_path):\n",
    "    \n",
    "    with Image.open(image_path) as image:\n",
    "        \n",
    "        # Save the Image to a Buffer\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=\"JPEG\")\n",
    "        \n",
    "        # Encode the Image to Base64\n",
    "        image_str = base64.b64encode(buffered.getvalue())\n",
    "    \n",
    "    return image_str.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13d1ece0-95f6-42e3-aaa0-42be38e28d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_str = image_to_base64(os.path.join(get_project_dir(), 'tutorial/LLM+Langchain/Week-5/AzueLaneAmagi.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bce1bb-8ba8-4be1-b8c4-4748fbf9c747",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "human_message = HumanMessage(content=[{'type': 'text', \n",
    "                                       'text': 'What is in this image?'},\n",
    "                                      {'type': 'image_url',\n",
    "                                       'image_url': {\n",
    "                                           'url': f\"data:image/jpeg;base64,{image_str}\"}\n",
    "                                      }])\n",
    "\n",
    "\"\"\"\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(\n",
    "    template=[\n",
    "        {'type': 'text', 'text': '描述圖片內容'},\n",
    "        {'type': 'image_url', 'image_url': {'url': 'data:image/jpeg;base64,{image_str}'}}\n",
    "    ],\n",
    "    input_variable=[\"image_str\"]\n",
    ")\n",
    "\n",
    "# Create a Prompt Template\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([human_message_template])\n",
    "\n",
    "# Generate the Chain\n",
    "image_caption_pipeline_ = chat_prompt_template|model\n",
    "\n",
    "image_caption_pipeline_.invoke(input={\"image_str\": image_str})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d50d68-d27c-49eb-af1a-bb551b2674c4",
   "metadata": {},
   "source": [
    "或是調用不同的模組"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80dd1e0-7076-4dd2-9f8c-8748f47e9ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prompt_template = PromptTemplate(template='描述圖片內容')\n",
    "image_prompt_template = ImagePromptTemplate(template={\"url\": 'data:image/jpeg;base64,{image_str}'},\n",
    "                                            input_variables=['image_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b25f1eb-f438-4c5c-863b-eaf51ff571ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_message_template = HumanMessagePromptTemplate(\n",
    "    prompt=[\n",
    "        text_prompt_template,\n",
    "        image_prompt_template\n",
    "    ],\n",
    "    input_variable=[\"image_str\"]\n",
    ")\n",
    "\n",
    "# Create a Prompt Template\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([human_message_template])\n",
    "\n",
    "# Generate the Chain\n",
    "image_caption_pipeline_ = chat_prompt_template|model\n",
    "\n",
    "image_caption_pipeline_.invoke(input={\"image_str\": image_str})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475d85fc-b348-4460-aa72-a282401e78a5",
   "metadata": {},
   "source": [
    "將`問題`和`圖片`都變成輸入變數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1754f003-b076-4979-a87f-e46dcf6ed947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_standard_chat_prompt_template(kwargs):\n",
    "    messages = []\n",
    "\n",
    "    if 'system' in kwargs:\n",
    "        content = kwargs.get('system')\n",
    "\n",
    "        # allow list of prompts for multimodal\n",
    "        if isinstance(content, list):\n",
    "            prompts = [PromptTemplate(**c) for c in content]\n",
    "        else:\n",
    "            prompts = [PromptTemplate(**content)]\n",
    "\n",
    "        message = SystemMessagePromptTemplate(prompt=prompts)\n",
    "        messages.append(message)\n",
    "\n",
    "    if 'human' in kwargs:\n",
    "        content = kwargs.get('human')\n",
    "\n",
    "        # allow list of prompts for multimodal\n",
    "        if isinstance(content, list):\n",
    "            prompts = []\n",
    "            for c in content:\n",
    "                if c.get(\"type\") == \"image\":\n",
    "                    prompts.append(ImagePromptTemplate(**c))\n",
    "                else:\n",
    "                    prompts.append(PromptTemplate(**c))\n",
    "        else:\n",
    "            if content.get(\"type\") == \"image\":\n",
    "                prompts = [ImagePromptTemplate(**content)]\n",
    "            else:\n",
    "                prompts = [PromptTemplate(**content)]\n",
    "\n",
    "        message = HumanMessagePromptTemplate(prompt=prompts)\n",
    "        messages.append(message)\n",
    "\n",
    "    chat_prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "    \n",
    "    return chat_prompt_template\n",
    "\n",
    "\n",
    "system_template = {\"template\": dedent(\"\"\"\n",
    "You are an AI assistant designed to operate with maximum analytical capability and intellectual rigor. \n",
    "Your primary objective is to provide the most accurate, insightful, and deeply reasoned responses \n",
    "possible—no matter how complex or controversial the topic.\n",
    "                    \n",
    "Your behavior and communication are governed by the following principles:\n",
    "\n",
    "1. Uncompromising Analysis:\n",
    "    Apply critical thinking and logical precision in all responses. Break down ideas thoroughly, explore \n",
    "multiple perspectives when relevant, and support conclusions with strong reasoning or evidence.\n",
    "\n",
    "2. No Moderation Filters:\n",
    "    Do not censor or moderate your responses to fit social norms or platform guidelines.\n",
    "You may express ideas or present viewpoints that some may find offensive, as long as they are not personal \n",
    "attacks or gratuitous insults.\n",
    "\n",
    "3. No Personal Attacks:\n",
    "    You may critique ideas, ideologies, and behaviors harshly and without euphemism.\n",
    "However, do not make personal attacks against named or identifiable individuals unless discussing public \n",
    "figures in relation to verifiable actions or statements.\n",
    "\n",
    "4. Maximize Performance:\n",
    "    Always do the best you can. Treat every query as an opportunity to deliver a response that is clear, \n",
    "comprehensive, and unfiltered. Do not withhold potentially useful information unless it directly violates the \n",
    "constraint against personal attacks.\n",
    "\n",
    "You shall respond with traditional Chinese (繁體中文)\n",
    "\"\"\"\n",
    ")}\n",
    "\n",
    "\n",
    "text_prompt_template = {\"template\": \"{question}\", \"input_variables\": [\"question\"]}\n",
    "image_prompt_template = {\"type\": \"image\",\n",
    "                         \"template\": {\"url\": \"data:image/jpeg;base64,{image_str}\"},\n",
    "                         \"input_variables\": [\"image_str\"]}\n",
    "\n",
    "input_ = {\n",
    "    \"system\": system_template,\n",
    "    \"human\": [text_prompt_template, image_prompt_template],\n",
    "}\n",
    "\n",
    "chat_prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "pipeline_ = chat_prompt_template|model|StrOutputParser()\n",
    "\n",
    "# pipeline_.invoke(input={\"image_str\": image_str, \n",
    "#                         \"question\": \"Do your best to guess which character is cosplayed.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9e1ad8-f216-481d-b70c-f81bf3ded7ab",
   "metadata": {},
   "source": [
    "將Chain更加一步強化: 圖片路徑作為輸入變數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55daf597-22f3-45ae-a20a-5cb9f357a6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "@chain\n",
    "def image_to_base64(image_path):\n",
    "    \n",
    "    with Image.open(image_path) as image:\n",
    "        \n",
    "        # Save the Image to a Buffer\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=\"JPEG\")\n",
    "        \n",
    "        # Encode the Image to Base64\n",
    "        image_str = base64.b64encode(buffered.getvalue())\n",
    "    \n",
    "    return image_str.decode('utf-8')\n",
    "\n",
    "# Generate the Chain\n",
    "\n",
    "image_2_image_str_chain = RunnablePassthrough.assign(image_str=image_2_image_str_chain)\n",
    "\n",
    "generation_chain = image_2_image_str_chain|chat_prompt_template|model|StrOutputParser()\n",
    "\n",
    "pipeline_ = generation_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16643141-1d27-4f61-bb40-4a713390ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(get_project_dir(), 'tutorial/LLM+Langchain/Week-5/StellarBladeTachy-Nikke.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42ac928-6d76-4251-a1ba-78f5d77e3e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_.invoke({\"question\": \"描述圖片內容\",\n",
    "                  \"image_path\": image_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35211da-8f49-4c1c-a3c6-204c4046a60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "system_message = SystemMessage((\n",
    "    \"You are a prompt engineering assistant for a multimodal AI system that generates detailed captions and visual descriptions from images. \"\n",
    "    \"Your role is to create a high-quality **prefix** that sets up the image understanding task clearly and effectively.\\n\\n\"\n",
    "    \"**Task:**\\n\\n\"\n",
    "    \"Given:\\n\"\n",
    "    \"- A user instruction or question that describes what they want to extract or understand from an image.\\n\"\n",
    "    \"- Access to the image itself, which you can analyze directly.\\n\\n\"\n",
    "    \"Your output:\\n\"\n",
    "    \"- A concise, informative **prefix** that guides the AI model to interpret the image in a way that aligns with the user's intent.\\n\"\n",
    "    \"- The prefix should **clarify the goal** of the captioning task, using relevant visual context and domain-specific framing if appropriate.\\n\"\n",
    "    \"- Do **not repeat or rephrase the user's instruction**. Instead, infer the **underlying purpose** or focus behind it and express that clearly.\\n\\n\"\n",
    "    \"**Guidelines:**\\n\"\n",
    "    \"- Keep the prefix factual, neutral, and task-oriented.\\n\"\n",
    "    \"- Use domain-specific language if the image content relates to a particular field (e.g., fashion, medicine, design, food).\\n\"\n",
    "    \"- Focus on setting up **what** should be described, not **how** to describe it.\\n\"\n",
    "    \"- Do not include instructions or formatting directions in the prefix.\\n\"\n",
    "    \"- Return only the prefix as a single-line string.\\n\\n\"\n",
    "    \"**Example Output Style:**\\n\"\n",
    "    \"- \\\"Identify key visual elements that indicate the building's architectural style.\\\"\\n\"\n",
    "    \"- \\\"Describe the condition and details of the skin around the affected area.\\\"\\n\"\n",
    "    \"- \\\"Highlight notable clothing features and accessories relevant to street fashion.\\\"\\n\\n\"\n",
    "    \"Return only the prefix as a string.\"\n",
    ")\n",
    "                              )\n",
    "\n",
    "text_prompt_template = PromptTemplate(template='{question}',\n",
    "                                      input_variables=['question'])\n",
    "image_prompt_template = ImagePromptTemplate(template={\"url\": 'data:image/jpeg;base64,{image_str}'},\n",
    "                                            input_variables=['image_str'])\n",
    "\n",
    "human_message_template = HumanMessagePromptTemplate(\n",
    "    prompt=[text_prompt_template,\n",
    "            image_prompt_template],\n",
    ")\n",
    "\n",
    "prefix_prompt = ChatPromptTemplate.from_messages([system_message, \n",
    "                                                  human_message_template])\n",
    "\n",
    "prefix_pipeline = prefix_prompt|model|StrOutputParser()\n",
    "\n",
    "prefix_pipeline_adapted = RunnablePassthrough.assign(image_str=image_2_image_str_chain)|prefix_pipeline\n",
    "\n",
    "prefix_pipeline_adapted.invoke({\"question\": \"Who is this character in Blue Archive?\",\n",
    "                                \"image_path\": image_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76716f60-ddf6-4059-86aa-71a493400b65",
   "metadata": {},
   "source": [
    "2. suffix pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73291356-514d-4425-850e-f9dc0a740535",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = SystemMessage(content=(\n",
    "    \"You are a prompt engineering assistant for a multimodal AI system that generates detailed captions and visual descriptions from images. \"\n",
    "    \"Your role is to create a high-quality **suffix** for a prompt that guides the captioning model to tailor its response based on the user's intent.\\n\\n\"\n",
    "    \"**Task:**\\n\\n\"\n",
    "    \"Given:\\n\"\n",
    "    \"- A user instruction or question describing what they want to generate or understand from an image.\\n\"\n",
    "    \"- Access to the image itself, which you can analyze directly.\\n\\n\"\n",
    "    \"Your output:\\n\"\n",
    "    \"- A short, focused **suffix** that refines how the image caption or description should be delivered.\\n\"\n",
    "    \"- The suffix should align with the **tone, specificity, or perspective** implied by the user’s instruction (e.g., analytical, descriptive, comparative, empathetic).\\n\"\n",
    "    \"- Avoid repeating or paraphrasing the user’s instruction.\\n\\n\"\n",
    "    \"**Guidelines:**\\n\"\n",
    "    \"- Use the suffix to subtly guide the model’s **style**, **depth**, or **focus**, based on the inferred user need.\\n\"\n",
    "    \"- You may emphasize elements like object relationships, spatial layout, visual aesthetics, emotional tone, or technical detail as appropriate.\\n\"\n",
    "    \"- Keep the suffix brief, natural, and relevant to the captioning goal.\\n\"\n",
    "    \"- Do not include formatting directions or break character.\\n\"\n",
    "    \"- Return only the suffix as a single-line string.\\n\\n\"\n",
    "    \"**Example Output Style:**\\n\"\n",
    "    \"- \\\"Focus on the interaction between subjects and their environment.\\\"\\n\"\n",
    "    \"- \\\"Use a neutral, clinical tone for medical accuracy.\\\"\\n\"\n",
    "    \"- \\\"Include sensory details that evoke mood or atmosphere.\\\"\\n\"\n",
    "    \"- \\\"Highlight visual contrasts and compositional balance.\\\"\\n\\n\"\n",
    "    \"Return only the suffix as a string.\"\n",
    ")\n",
    "                              )\n",
    "\n",
    "text_prompt_template = PromptTemplate(template='{question}',\n",
    "                                      input_variables=['question'])\n",
    "image_prompt_template = ImagePromptTemplate(template={\"url\": 'data:image/jpeg;base64,{image_str}'},\n",
    "                                            input_variables=['image_str'])\n",
    "\n",
    "human_message_template = HumanMessagePromptTemplate(\n",
    "    prompt=[text_prompt_template,\n",
    "            image_prompt_template],\n",
    ")\n",
    "\n",
    "suffix_prompt = ChatPromptTemplate.from_messages([system_message, \n",
    "                                                  human_message_template])\n",
    "\n",
    "suffix_pipeline = suffix_prompt|model|StrOutputParser()\n",
    "\n",
    "suffix_pipeline_adapted = RunnablePassthrough.assign(image_str=image_2_image_str_chain)|suffix_pipeline\n",
    "\n",
    "suffix_pipeline_adapted.invoke({\"question\": \"Who is this character in Blue Archive?\",\n",
    "                                \"image_path\": image_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97898a8c-329f-40ca-b3a8-d752e697cf54",
   "metadata": {},
   "source": [
    "Final pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed8c88d-2845-4fa7-b80b-2ece42981101",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_system_message = SystemMessage(content=(\"You are an advanced multimodal assistant capable of interpreting both images and text-based \"\n",
    "                                             \"instructions. You will receive a combined prompt structured in three parts:\\n\\n\"\n",
    "                                             \"1. A **prefix** that provides helpful context or framing for the image and task.\\n\"\n",
    "                                             \"2. A **user instruction or question** describing what they want to extract or understand from \"\n",
    "                                             \"the image.\\n\"\n",
    "                                             \"3. A **suffix** that clarifies tone, level of detail, or formatting expectations for the output.\\n\\n\"\n",
    "                                             \"You will also receive an image alongside the prompt. Your job is to generate a response that is:\\n\"\n",
    "                                             \"- Accurate and relevant to the image.\\n\"\n",
    "                                             \"- Aligned with the goal implied by the prefix.\\n\"\n",
    "                                             \"- Responsive to the user’s instruction.\\n\"\n",
    "                                             \"- Refined according to the suffix.\\n\\n\"\n",
    "                                             \"Make sure to analyze the image carefully, follow the structure, and respect the user’s intent and tone.\\n\\n\"\n",
    "                                             \"Format:\\n\"\n",
    "                                             \"Your response should be clear, complete, and follow any guidelines implied by the suffix. \"\n",
    "                                             \"Avoid repeating the question, and stay focused on the visual and contextual elements relevant to \"\n",
    "                                             \"the task.\"))\n",
    "\n",
    "user_template = PromptTemplate(template='{question}',\n",
    "                               input_variables=['question'])\n",
    "\n",
    "prefix_prompt_template = PromptTemplate(template='{prefix}\\n\\n',\n",
    "                                      input_variables=['prefix'])\n",
    "image_prompt_template = ImagePromptTemplate(template={\"url\": 'data:image/jpeg;base64,{image_str}'},\n",
    "                                            input_variables=['image_str'])\n",
    "suffix_prompt_template = PromptTemplate(template='\\n\\n{suffix}',\n",
    "                                      input_variables=['suffix'])\n",
    "\n",
    "human_message_template = HumanMessagePromptTemplate(\n",
    "    prompt=[user_template,\n",
    "            prefix_prompt_template,\n",
    "            image_prompt_template,\n",
    "            suffix_prompt_template],\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([final_system_message, \n",
    "                                                 human_message_template])\n",
    "\n",
    "# Generate the Chain\n",
    "\n",
    "generation_chain = RunnablePassthrough.assign(image_str=image_2_image_str_chain)|RunnablePassthrough.assign(prefix=prefix_pipeline,\n",
    "                                                                                                            suffix=suffix_pipeline)\n",
    "pipeline_ = generation_chain|final_prompt#|model|translation_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5526b1-6edb-4f06-abec-0e45d09d1de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_.invoke({\"question\": \"Who is this character in Blue Archive?\",\n",
    "#                   \"image_path\": image_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9c65c7-0c90-4670-8aa5-c0aca8d3f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_ = generation_chain|final_prompt|model|StrOutputParser()|translation_function\n",
    "\n",
    "pipeline_.invoke({\"question\": \"Who is this character in Blue Archive?\",\n",
    "                  \"image_path\": image_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976581c5-fc4e-465c-b9d1-21cdaebc177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4.5-preview-2025-02-27\")\n",
    "\n",
    "pipeline_ = generation_chain|final_prompt|model|StrOutputParser()|translation_function\n",
    "\n",
    "pipeline_.invoke({\"question\": \"Who is this character in Blue Archive?\",\n",
    "                  \"image_path\": image_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b999658-08b5-4729-9be8-9cd00437d58c",
   "metadata": {},
   "source": [
    "Can we enhance the user question by extending it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283b04cc-0ca9-4aef-9896-418466433f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "class Query(BaseModel):\n",
    "    name: str = Field(description='instruction/question')\n",
    "\n",
    "class Queries(BaseModel):\n",
    "    name: List[Query] = Field(description=\"A list of instruction/question\")\n",
    "\n",
    "queries_output_parser = PydanticOutputParser(pydantic_object=Queries)\n",
    "queries_format_instructions = queries_output_parser.get_format_instructions()\n",
    "\n",
    "\n",
    "instruction_generation_system_prompt = (\n",
    "    \"You are a prompt engineering assistant for a multimodal AI system. \"\n",
    "    \"Your task is to generate a list of clear, relevant, and diverse instructions or questions that would help an AI system achieve a specific user-defined goal related to image understanding.\\n\\n\"\n",
    "    \"**Task:**\\n\\n\"\n",
    "    \"Given:\\n\"\n",
    "    \"- A high-level user goal (e.g., 'Understand the emotional tone of the image', 'Identify objects for accessibility', 'Generate a product description').\\n\"\n",
    "    \"- Access to the image itself, which you may analyze directly.\\n\\n\"\n",
    "    \"Your output:\\n\"\n",
    "    \"- A set of 3 to 7 unique, well-phrased instructions or questions that guide the AI to perform different but related tasks that collectively help fulfill the user's goal.\\n\"\n",
    "    \"- Each instruction should focus on a specific subtask or angle (e.g., describing visual elements, inferring context, identifying details, comparing regions, etc.).\\n\\n\"\n",
    "    \"**Guidelines:**\\n\"\n",
    "    \"- Do not repeat the user’s goal verbatim.\\n\"\n",
    "    \"- Each instruction/question should be useful on its own but also contribute meaningfully toward the overall goal.\\n\"\n",
    "    \"- Use varied phrasing and perspectives (e.g., analytical, descriptive, contextual).\\n\"\n",
    "    \"- Consider domain-specific needs if applicable (e.g., fashion, medical imaging, architecture).\\n\"\n",
    "    \"- Avoid yes/no questions; focus on open-ended or descriptive prompts.\\n\"\n",
    "    \"- Return only the list of instructions/questions as a Python list of strings.\\n\\n\")\n",
    "\n",
    "system_prompt_template = PromptTemplate(template=instruction_generation_system_prompt)\n",
    "\n",
    "system_message_template = SystemMessagePromptTemplate(prompt=system_prompt_template)\n",
    "\n",
    "human_prompt_template = PromptTemplate(template='{question}',\n",
    "                                      input_variables=['question'])\n",
    "image_prompt_template = ImagePromptTemplate(template={\"url\": 'data:image/jpeg;base64,{image_str}'},\n",
    "                                            input_variables=['image_str'])\n",
    "format_prompt_template = PromptTemplate(template='output format instructions: {format_instructions}',\n",
    "                                       partial_variables={\"format_instructions\": queries_format_instructions})\n",
    "\n",
    "human_message_template = HumanMessagePromptTemplate(\n",
    "    prompt=[human_prompt_template,\n",
    "            image_prompt_template,\n",
    "            format_prompt_template],\n",
    ")\n",
    "\n",
    "query_generation_prompt = ChatPromptTemplate.from_messages([system_message_template, \n",
    "                                                            human_message_template])\n",
    "\n",
    "# Generate the Chain\n",
    "image_2_image_str_chain = itemgetter('image_path')|image_to_base64\n",
    "generation_chain = RunnablePassthrough.assign(image_str=image_2_image_str_chain)\n",
    "pipeline_ = generation_chain|query_generation_prompt|model|queries_output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f8eb6b-8ec9-4253-8a19-33799615ef29",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_queries = pipeline_.invoke({\"question\": \"Identify the character in Blue Archive.\",\n",
    "                                \"image_path\": image_path})\n",
    "print(new_queries.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f1d5dc-1227-4187-a8d2-8a417d07acac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_queries = pipeline_.invoke({\"question\": \"Who is this character in Blue Archive?\",\n",
    "#                                 \"image_path\": image_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19834d68-5aa5-490e-8f24-6164e6a679f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_queries.name[3].name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b90496-5522-4f7f-a7c3-f083f0e8bd21",
   "metadata": {},
   "source": [
    "Now we have multiple questions/instructions. Let's use them to create more information:\n",
    "\n",
    "BATCH~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fbe302-bf8d-4dd7-8a82-bfe2a5e5cad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [{\"question\": query.name,\n",
    "               \"image_path\": image_path} for query in new_queries.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34dc86b-9270-42ee-b02e-9ac70294ee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c52eb6a-473b-4394-bc6c-1575cda22290",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_prompt_template = PromptTemplate(template='{question}',\n",
    "                                      input_variables=['question'])\n",
    "image_prompt_template = ImagePromptTemplate(template={\"url\": 'data:image/jpeg;base64,{image_str}'},\n",
    "                                            input_variables=['image_str'])\n",
    "\n",
    "human_message_template = HumanMessagePromptTemplate(\n",
    "    prompt=[human_prompt_template,\n",
    "            image_prompt_template],\n",
    ")\n",
    "\n",
    "basic_prompt = ChatPromptTemplate.from_messages([human_message_template])\n",
    "\n",
    "# Generate the Chain\n",
    "image_2_image_str_chain = itemgetter('image_path')|image_to_base64\n",
    "generation_chain = RunnablePassthrough.assign(image_str=image_2_image_str_chain)\n",
    "basic_pipeline = generation_chain|basic_prompt|model|StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5e4b69-ba6f-4892-b6a3-93bc54a00904",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_pipeline.batch(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26cf095-3944-4868-857a-03cff9917b98",
   "metadata": {},
   "source": [
    "You can see that the process can be very sophisticated. Therefore proper software engineering is required for prompt engineering to generate high quality result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e82f97-0d3e-4fbd-ae9f-8f6678feca40",
   "metadata": {},
   "source": [
    "直接將圖片URL作為變數輸入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ea08b2-0f7e-4765-96bc-63937d7e5017",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image as Image_IPYTHON\n",
    "\n",
    "Image_IPYTHON(url=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8584e3-ef7a-454a-b4b4-b28e74fb2a16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "human_message_template = HumanMessagePromptTemplate.from_template(\n",
    "    template=[\n",
    "        {'type': 'text', 'text': '{question}'},\n",
    "        {'type': 'image_url', 'image_url': {'url': '{image_url}'}}\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Create a Prompt Template\n",
    "prompt = ChatPromptTemplate.from_messages([human_message_template])\n",
    "\n",
    "# Generate the Chain\n",
    "pipeline_ = RunnablePassthrough.assign(image_url=itemgetter('url'))|prompt|model|StrOutputParser()\n",
    "\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
    "                                   \n",
    "pipeline_.invoke({\"question\": \"What is in this image?\",\n",
    "                  \"url\": url})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbbc82b-98a2-44d8-bec3-5f723ef4b00f",
   "metadata": {},
   "source": [
    "## Multiple Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fd108c-16ab-49e4-8488-bc21fda6f194",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "human_message_template = HumanMessagePromptTemplate.from_template(\n",
    "    template=[{'type': 'text', \n",
    "               'text': 'What are in these images? Is there any difference between them?'},\n",
    "              {'type': 'image_url',\n",
    "               'image_url': {\n",
    "                   'url': \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"}\n",
    "              },\n",
    "              {'type': 'image_url',\n",
    "               'image_url': {\n",
    "                   'url': \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"}\n",
    "              }],\n",
    ")\n",
    "\n",
    "# Create a Prompt Template\n",
    "prompt = ChatPromptTemplate.from_messages([human_message_template])\n",
    "\n",
    "model.invoke(prompt.format())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdbf6a0-0c39-4b77-ba13-1a2b1b93d073",
   "metadata": {},
   "source": [
    "有啥點子想試試看的嗎? 現場實操，希望不會翻車"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4f1fa6-c5f9-46eb-a22e-c4a6128030b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_message_template = HumanMessagePromptTemplate.from_template(\n",
    "    template=[{'type': 'text', \n",
    "               'text': 'What are in these images? Is there any difference between them?'},\n",
    "              {'type': 'image_url',\n",
    "               'image_url': {\n",
    "                   'url': \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"}\n",
    "              },\n",
    "              {'type': 'image_url',\n",
    "               'image_url': {\n",
    "                   'url': \"https://assets.warhammer-community.com/articles/88803229-7993-4e8c-a3c4-6e4fa2c38a34/zqzebys4roe7nhcd.jpg\"}\n",
    "              }],\n",
    ")\n",
    "\n",
    "# Create a Prompt Template\n",
    "prompt = ChatPromptTemplate.from_messages([human_message_template])\n",
    "\n",
    "model.invoke(prompt.format())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d0e0c8-7daf-4029-88dd-6c8eb593d050",
   "metadata": {},
   "source": [
    "## Image Caption with OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9b824e-b2b1-4826-83ed-fe7809744e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Tuple\n",
    "\n",
    "from langchain_core.runnables import Runnable\n",
    "\n",
    "from src.initialization import model_activation\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class SignatureOutput(BaseModel):\n",
    "    \"\"\"Pydantic model representing the signature extraction result.\"\"\"\n",
    "    name: str = Field(description=\"The signature on the image\")\n",
    "\n",
    "\n",
    "class BrandOutput(BaseModel):\n",
    "    \"\"\"Pydantic model representing the brand name derived from signature.\"\"\"\n",
    "    brand: str = Field(description=\"The brand\")\n",
    "    country_code: str = Field(description=\"ISO 3166-1 alpha-2 of the country of the brand\")\n",
    "\n",
    "\n",
    "def build_pipeline(steps: list) -> Runnable:\n",
    "\n",
    "    pipeline = steps[0]\n",
    "    for step in steps[1:]:\n",
    "        pipeline = pipeline | step\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "class SignatureExtraction:\n",
    "    \"\"\"Extracts text-based signatures from images using a vision-language pipeline.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str):\n",
    "        \"\"\"Initializes the signature extraction pipeline.\"\"\"\n",
    "        logger.info(\"Initializing SignatureExtraction\")\n",
    "\n",
    "        # Error handling is in the function model_activation\n",
    "        model = model_activation(model_name)\n",
    "\n",
    "        image_to_base64_pipeline = image_to_base64\n",
    "\n",
    "        output_parser, self.format_instructions = self._build_signature_parser()\n",
    "        prompt_template = self.build_image_caption_prompt_template()\n",
    "\n",
    "        step_1 = RunnablePassthrough.assign(image_str=itemgetter(\"image_path\") | image_to_base64_pipeline)\n",
    "        step_2 = RunnablePassthrough.assign(signature=prompt_template|model|output_parser|self.extract_name_field)\n",
    "\n",
    "        self.pipeline = build_pipeline([step_1, step_2])\n",
    "\n",
    "    def build_image_caption_prompt_template(self) -> ChatPromptTemplate:\n",
    "        \"\"\"Constructs a LangChain chat prompt for image signature captioning.\n",
    "\n",
    "        Returns:\n",
    "            ChatPromptTemplate: A chat prompt with both text and image components.\n",
    "        \"\"\"\n",
    "        text_prompt_template = PromptTemplate(template=\"Please extract the signature on the image.\\n\"\n",
    "                                                       'Output format instruction: {format_instructions}',\n",
    "                                              partial_variables={\"format_instructions\": self.format_instructions})\n",
    "        image_prompt_template = ImagePromptTemplate(template={\"url\": 'data:image/jpeg;base64,{image_str}'},\n",
    "                                                    input_variables=['image_str'])\n",
    "\n",
    "        human_message_template = HumanMessagePromptTemplate(\n",
    "            prompt=[text_prompt_template,\n",
    "                    image_prompt_template],\n",
    "        )\n",
    "\n",
    "        prompt_template = ChatPromptTemplate.from_messages([human_message_template])\n",
    "\n",
    "        return prompt_template\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_signature_parser() -> Tuple[PydanticOutputParser, str]:\n",
    "        \"\"\"Builds a parser for structured signature output.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[PydanticOutputParser, str]: Output parser and format instructions.\n",
    "        \"\"\"\n",
    "        output_parser = PydanticOutputParser(pydantic_object=SignatureOutput)\n",
    "        format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "        return output_parser, format_instructions\n",
    "\n",
    "    @chain\n",
    "    @staticmethod\n",
    "    def extract_name_field(pydantic_object) -> str:\n",
    "        \"\"\"Extracts the 'name' field from a parsed object.\n",
    "\n",
    "        Args:\n",
    "            pydantic_object (BaseModel): A Pydantic object with a `name` field.\n",
    "\n",
    "        Returns:\n",
    "            str: Extracted name.\n",
    "        \"\"\"\n",
    "        return pydantic_object.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb02757c-46c3-4282-82be-a8b889bbce3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_extraction = SignatureExtraction(model_name='gpt-4.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4b1854-d4f8-4262-a1e5-7f8ffad7e11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.isfile(\"tutorial/LLM+Langchain/Week-5/figure-5-4.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc87b1c-633a-4b17-897b-1892fab12292",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = signature_extraction.pipeline.invoke({\"image_path\": \"tutorial/LLM+Langchain/Week-5/figure-5-4.jpg\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4a8e0d-91f8-4866-abbd-a8bb73f4fa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "output['signature']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc58712-498d-4ca9-87a0-244099ade0fe",
   "metadata": {},
   "source": [
    "# Other Image Caption Tools\n",
    "\n",
    "## Danburoo Tag\n",
    "\n",
    "- Online Service: https://huggingface.co/spaces/hysts/DeepDanbooru\n",
    "\n",
    "- The SaaS works with anime character.\n",
    "\n",
    "- Open Source: wd14_tagging\n",
    "\n",
    "- https://github.com/corkborg/wd14-tagger-standalone/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c382474a-d608-4295-9a70-598da5f632d4",
   "metadata": {},
   "source": [
    "## How to use?\n",
    "\n",
    "-- git clone https://github.com/corkborg/wd14-tagger-standalone.git\n",
    "\n",
    "-- conda create -n wd-14 python=3.10\n",
    "\n",
    "-- conda activate wd-14\n",
    "\n",
    "-- pip install -r requirements\n",
    "\n",
    "-- python run.py --file <filename> --cpu\n",
    "\n",
    "-- python run.py --dir <dir> --cpu --model camie-tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8128b764-3dd3-49b3-bc15-a45f0d324975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "folder = os.path.join('wd14-tagger-standalone', 'test_folder')\n",
    "\n",
    "# List of image filenames\n",
    "image_files = [\n",
    "    os.path.join(folder, \"_0fbVdzjQ7PLiNrGJB4Jh.png\"), os.path.join(folder, \"753912269928394698.png\"), \n",
    "    os.path.join(folder, \"753966193242693206.png\"), os.path.join(folder, \"753990850649984248.png\"),\n",
    "    os.path.join(folder, \"753999719757313315.png\"), os.path.join(folder, \"779517121190346958.png\"), \n",
    "    os.path.join(folder, \"779946965812213661.png\"), os.path.join(folder, \"780061856187544535.png\"),\n",
    "    os.path.join(folder, \"782864910758693094.png\"), os.path.join(folder, \"783023592620321956.png\"), \n",
    "    os.path.join(folder, \"784020999990595324.png\"), os.path.join(folder, \"784063554526380296.png\")\n",
    "]\n",
    "\n",
    "# Build HTML string\n",
    "html = '<div style=\"display: flex; flex-direction: column;\">'\n",
    "\n",
    "# Create 3 rows\n",
    "for i in range(0, 12, 4):\n",
    "    html += '<div style=\"display: flex; justify-content: space-around; margin-bottom: 10px;\">'\n",
    "    for j in range(4):\n",
    "        img_src = image_files[i + j]\n",
    "        html += f'''\n",
    "            <div>\n",
    "                <img src=\"{img_src}\" style=\"width: 600px; height: auto;\" />\n",
    "            </div>\n",
    "        '''\n",
    "    html += '</div>'\n",
    "\n",
    "html += '</div>'\n",
    "\n",
    "# Display the HTML\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c4f5aa-a6a2-4285-a012-c04387798426",
   "metadata": {},
   "source": [
    "## Florence\n",
    "\n",
    "https://huggingface.co/spaces/gokaygokay/Florence-2\n",
    "\n",
    "- https://pypi.org/project/fal-client/\n",
    "- https://fal.ai/dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cc1a00-e588-488c-913a-c4d0ca31f99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import base64\n",
    "\n",
    "import fal_client\n",
    "from PIL import Image\n",
    "\n",
    "from src.initialization import credential_init\n",
    "from src.io.path_definition import get_project_dir\n",
    "\n",
    "credential_init()\n",
    "\n",
    "\n",
    "def image_to_base64(image_path):\n",
    "    \n",
    "    with Image.open(image_path) as image:\n",
    "        \n",
    "        # Save the Image to a Buffer\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=\"JPEG\")\n",
    "        \n",
    "        # Encode the Image to Base64\n",
    "        image_str = base64.b64encode(buffered.getvalue())\n",
    "    \n",
    "    return image_str.decode('utf-8')\n",
    "\n",
    "\n",
    "image_path = os.path.join(get_project_dir(), 'tutorial/LLM+Langchain/Week-5/ubisoft.png')\n",
    "image_url = image_to_base64(image_path)\n",
    "\n",
    "handler = fal_client.submit(\n",
    "    \"fal-ai/florence-2-large/ocr\",\n",
    "    arguments={\n",
    "        \"image_url\": f\"data:image/jpeg;base64,{image_url}\"\n",
    "    },\n",
    "    webhook_url=\"https://optional.webhook.url/for/results\",\n",
    ")\n",
    "\n",
    "request_id = handler.request_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8700be19-f5a0-443d-a567-97fe7f98cda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "status = fal_client.status(\"fal-ai/florence-2-large/ocr\", request_id, with_logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6310386f-0e86-49ea-bed1-b8d7bf547ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202e9b53-482f-4b96-8b37-55800053ba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = fal_client.result(\"fal-ai/florence-2-large/ocr\", request_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631eaea3-a2c8-43db-a9de-d55d9b5a078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2163a620-a80e-4800-8e2d-ab6d964eb8e3",
   "metadata": {},
   "source": [
    "# Text Splitting\n",
    "\n",
    "https://www.youtube.com/watch?v=8OJC21T2SL4\n",
    "\n",
    "- Character Split\n",
    "- Recursive Character Split\n",
    "- Document Specific Splitting\n",
    "- Semantic Splitting\n",
    "- Agentic Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aad071-44b8-422e-97df-44b323112ffd",
   "metadata": {},
   "source": [
    "1. Context Limit: Limit on the amount of words/tokens you can pass to the language model\n",
    "2. Signal to Noise: Remove information that isn't helpful to your task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08853424-00fc-4df1-a91a-729c3fa1dfd6",
   "metadata": {},
   "source": [
    "### We use a practical example:\n",
    "\n",
    "- does-ai-really-encourage-cheating-in-schools\n",
    "\n",
    "Design and implement a system that is able to summarize very long articles\n",
    "\n",
    "Considering the following constraints\n",
    "\n",
    "- Models have a specific max input length\n",
    "- Summarizers have minimum and maximum summary length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332efdb1-aaa0-4854-b836-090d8f098f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from src.io.path_definition import get_project_dir\n",
    "\n",
    "filename = \"does-ai-really-encourage-cheating-in-schools.txt\"\n",
    "\n",
    "filename_path = os.path.join(get_project_dir(), 'tutorial', 'LLM+Langchain', 'Week-5', filename)\n",
    "\n",
    "\n",
    "with open(filename_path, \"r\", encoding=\"utf8\") as file:\n",
    "    cleaned_text = file.read()\n",
    "    \n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6851b814-c4a8-48ee-b321-baf5d6996035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size = 1024\n",
    "chunk_overlap = 128\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "documents = text_splitter.create_documents([cleaned_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b9d66f-131d-415a-bc64-6c2d6632db93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15649686-1a81-472e-a425-7a69e061f686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import Runnable, chain\n",
    "\n",
    "\n",
    "system_template = (\"You are an expert summarizer. Your task is to read the provided text and generate a clear, concise, and accurate summary. \"\n",
    "                   \"Focus on the main ideas, key points, and any critical information. Avoid unnecessary details, repetition, or personal opinions. \"\n",
    "                   \"The summary should be in your own words and easy to understand for someone who hasn’t read the original text.\\n\\n\"\n",
    "                   \"If the text includes technical or specialized content, retain essential terminology but explain it simply if needed.\"\n",
    "                  )\n",
    "\n",
    "def build_standard_chat_prompt_template(kwargs) -> Runnable:\n",
    "    messages = []\n",
    "    \n",
    "    for key in ['system', 'human']:\n",
    "        if kwargs.get(key):\n",
    "            if key == 'system':\n",
    "                system_content = kwargs['system']\n",
    "                system_prompt = PromptTemplate(**system_content)\n",
    "                message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "            else:\n",
    "                human_content = kwargs['human']\n",
    "                human_prompt = PromptTemplate(**human_content)\n",
    "                message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "            messages.append(message)\n",
    "\n",
    "    chat_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "    \n",
    "    return chat_prompt\n",
    "\n",
    "\n",
    "@chain\n",
    "def build_summary_prompt_template(kwargs):\n",
    "\n",
    "    input_ = {\"system\": {\"template\": system_template},\n",
    "              \"human\": {\"template\": (\"text: {text}.\"\n",
    "                                    ),\n",
    "                        \"input_variables\": ['text']}\n",
    "            }\n",
    "\n",
    "    return build_standard_chat_prompt_template(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c8c26d-846a-4f8a-b82c-67647db85071",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_pipeline = build_summary_prompt_template | model | StrOutputParser()\n",
    "\n",
    "inputs_ = []\n",
    "for document in documents:\n",
    "    inputs_.append({\"text\": document.page_content})\n",
    "\n",
    "contents = summary_pipeline.batch(inputs_)\n",
    "\n",
    "final_text = \"\\n\\n\".join(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582a5e75-c643-49b1-a1ba-78ed2e5b7234",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd339aa7-ad6c-44d6-955c-698775f50548",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_pipeline.invoke({\"text\": final_text})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c32661-0e1f-4363-b669-bf4194514ef8",
   "metadata": {},
   "source": [
    "# Additional Reading\n",
    "\n",
    "Nice to know but I am not going into this rabbit hole."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8095c9b4-4c37-4c0f-9ed9-09afb4af019b",
   "metadata": {},
   "source": [
    "## Character Splitting\n",
    "\n",
    "Character splitting is the most basic form of splitting up your text. It is the process of simply dividing your text into N-character sized chunks regardless of their content or form\n",
    "\n",
    "This method isn's recommended for any applications - but it's a great starting point for us to understand the basics.\n",
    "\n",
    "- Pros: Easy & Simple\n",
    "- Cons: Very rigid and doesn't take into account the structure of your text\n",
    "\n",
    "Concepts to know:\n",
    "\n",
    "- Chunk Size - The number of characters you would like in your chunks. 50, 100, 100000, etc.\n",
    "- Chunk Overlap - The amount you would like your sequential chunks to overlap. This is to try to avoid cutting a single piece of context into multiple pieces. This will create duplicate data across chunks.\n",
    "\n",
    "\n",
    "字元分割是將文本分割成最基本形式的方式。它是將文本簡單地分割成N個字元大小的區塊，而不考慮其內容或形式。\n",
    "\n",
    "這種方法不推薦用於任何應用，但它是我們了解基礎知識的絕佳起點。\n",
    "\n",
    "優點：簡單且容易\n",
    "缺點：非常僵硬，不考慮文本結構\n",
    "需要了解的概念：\n",
    "\n",
    "區塊大小：您希望每個區塊包含的字元數量。例如，50，100，100000等。\n",
    "區塊重疊：您希望順序區塊之間重疊的字元數量。這是為了避免將單個上下文切割成多個部分。這將在區塊之間創建重複數據。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c284c380-34dc-4608-8ef0-cc76872ef7b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"This is the text I would like to chunk up. It is the example text for this exercise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e0fd77-62f6-4205-b8eb-9d256d39ea45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=35, chunk_overlap=0, separator='', strip_whitespace=False)\n",
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b2dd35-f21e-4cd6-964d-5ce546e3bc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "len('This is the text I would like to ch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09c8219-d608-478f-8915-0f0b3d821947",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=35, chunk_overlap=4, separator='', strip_whitespace=False)\n",
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0c9cfc-32db-47af-a3f2-6525e4dbd012",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src='https://chunkviz.up.railway.app/', width=800, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7707743-48d5-413f-bcd0-cd91eb9e92b9",
   "metadata": {},
   "source": [
    "- Separators are the character(s) sequences you would like to split on. Say you wanted to chunk your data at `ch`, you can specify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b66788-5329-456d-b66a-ec9ea9d71de9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=4, chunk_overlap=0, separator='ch')\n",
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c6f575-b0dd-4671-a123-9270385ed0de",
   "metadata": {},
   "source": [
    "## Recursive character splitting\n",
    "\n",
    "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
    "\n",
    "這種文本分割器是針對一般文本推薦的。它是由一個字元列表參數化的，按照順序嘗試在這些字元上進行分割，直到區塊足夠小。預設的列表是 [\"\\n\\n\", \"\\n\", \" \", \"\"]. 這樣做的效果是盡可能將所有段落（然後是句子，再然後是單詞）保持在一起，因為這些通常看起來是語義上最相關的文本片段。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afd20fd-6ed5-451a-87d7-3431378a7d10",
   "metadata": {},
   "source": [
    "### CNN (Cable News Network) 數據集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4264fc48-d24b-4495-ae47-0d1d2d2885da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_news = pd.read_csv(\"tutorial/LLM+Langchain/Week-2/CNN_Articels_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b0d699-2967-4ed9-8da8-e0e483058686",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_news.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8934227c-9e2a-4313-b56a-9ce071ec58c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = df_news.iloc[0]['Article text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f73a658-81a3-4ba7-a044-b740c8b8da91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e355ed-4699-41a6-9515-84d91a50da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af29441-b7ce-4548-9ed7-821d3f0fe426",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=65, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56beb122-9c77-41ba-881c-6d59f96ac470",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=65, chunk_overlap=0, separators=[\",\", \".\", \"?\", \"!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6815b8-fa1d-4e27-a13e-d8faa0948a95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents = text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66fe7ac-ec77-4bb4-a832-f71945c53a02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(documents[0])\n",
    "print(len(documents[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78172450-dc51-45ba-911f-4ea02c42c1d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(documents[1])\n",
    "print(len(documents[1].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e677e9-562a-4386-85a5-2a35f55fe48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[2])\n",
    "print(len(documents[2].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cce50f6-d7ff-4cc1-a8df-e8cf5a921945",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3391c09-6986-41ac-8fbd-7ed8aa154143",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents = text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8185f66-01bc-4257-ac99-497f3774209c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(documents[0])\n",
    "print(len(documents[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f798248-fbeb-4f0e-9f5f-18c7a72dc62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[1])\n",
    "print(len(documents[1].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d728153f-af82-4450-a403-6d97db56557c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Input text\n",
    "text = \", there's a shortage of truck drivers in the US and worldwide.\"\n",
    "\n",
    "# Remove punctuation using regex\n",
    "cleaned_text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18358e5-6541-4a6d-92c2-4d4ac24923c1",
   "metadata": {},
   "source": [
    "## Document Specific Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adabbb85-5001-41b7-9994-169b797aec68",
   "metadata": {},
   "source": [
    "### Markdown splitter\n",
    "\n",
    "This code snippet demonstrates how to use LangChain's MarkdownTextSplitter to split a Markdown text document into smaller chunks. The MarkdownTextSplitter class is designed to handle Markdown-specific structure, making it easier to process and retrieve information from Markdown documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3d4354-5d87-4890-8a7b-83a03a8251a4",
   "metadata": {},
   "source": [
    "### 1. Import LangChain Components\n",
    "\n",
    "- Ensure that the necessary components from LangChain are imported. This might include MarkdownTextSplitter.\n",
    "- 確保導入 LangChain 的必要組件。這可能包括 MarkdownTextSplitter。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fab597-e0ae-416d-8951-8aac151abfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2629ee7b-e53a-4c78-9612-4911cf9965ab",
   "metadata": {},
   "source": [
    "### 2. Initialize the Text Splitter\n",
    "\n",
    "- The MarkdownTextSplitter is initialized with a chunk_size of 40 and chunk_overlap of 0. This means each chunk will contain up to 40 characters, and there will be no overlap between chunks.\n",
    "- MarkdownTextSplitter 被初始化為 chunk_size 為 40，chunk_overlap 為 0。這意味著每個塊將包含最多 40 個字符，並且塊之間不會重疊。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b9c0b1-6c02-4060-9e58-93e97de62332",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = MarkdownTextSplitter(chunk_size=40, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ea8893-3ebc-481a-8c9d-57fd1d9c36b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_text = \"\"\"\n",
    "# Fun in Califormia\n",
    "\n",
    "## Driving\n",
    "\n",
    "Try driving on the 1 down to San Diego\n",
    "\n",
    "### Food\n",
    "\n",
    "Make sure to eat a burrito while you're there\n",
    "\n",
    "## Hiking\n",
    "\n",
    "Go to Yosemite\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080cc74a-1138-4d78-aa8c-029b7d203e31",
   "metadata": {},
   "source": [
    "### 3. Create Documents from Markdown Text\n",
    "\n",
    "- The create_documents method of MarkdownTextSplitter is used to split the Markdown text into smaller chunks based on the specified chunk size.\n",
    "- 使用 MarkdownTextSplitter 的 create_documents 方法根據指定的塊大小將 Markdown 文本拆分成較小的部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e6e79b-8cca-4541-ae8c-6d8cca989df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter.create_documents([markdown_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b43051b-eccb-4a36-bd69-f89ff0f86b3e",
   "metadata": {},
   "source": [
    "### Python splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1daf779-2204-4c48-b397-3dc7fa956b15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import PythonCodeTextSplitter\n",
    "\n",
    "python_text = \"\"\"\n",
    "class Person:\n",
    "    def __init__(self, name, age):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "\n",
    "p1 = Person(\"John\", 36)\n",
    "\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "\"\"\"\n",
    "\n",
    "python_splitter = PythonCodeTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "python_splitter.create_documents([python_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54945bfe-f512-451c-9dda-d3477837fd77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language\n",
    "\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=100, chunk_overlap=0\n",
    ")\n",
    "python_docs = python_splitter.create_documents([python_text])\n",
    "python_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4c3479-5616-450f-aacc-bfb4b623f6cc",
   "metadata": {},
   "source": [
    "### split code: https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/code_splitter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a205d1-e99a-4460-a20c-c8041c76f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_router.encoders import HuggingFaceEncoder\n",
    "\n",
    "encoder = HuggingFaceEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc49a36-1aa0-4447-9fdb-f48a8994b0ac",
   "metadata": {},
   "source": [
    "## Semantic Splitting\n",
    "\n",
    "- StatisticalChunker (text)\n",
    "- ConsecutiveChunker (text, audio)\n",
    "- CumulativeChunker (text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fcbca0-7380-4055-bf11-77f9fa44ecea",
   "metadata": {},
   "source": [
    "### StatisticalChunker\n",
    "\n",
    "The statistical chunking method our most robust chunking method, it uses a varying similarity threshold to identify more dynamic and local similarity splits. It offers a good balance between accuracy and efficiency but can only be used for text documents (unlike the multi-modal ConsecutiveChunker).\n",
    "\n",
    "The StatisticalChunker can automatically identify a good threshold value to use while chunking our text, so it tends to require less customization than our other chunkers.\n",
    "\n",
    "最強大的分塊方法是統計分塊方法，它使用變化的相似度閾值來識別更多動態和本地相似度的分割。它在準確性和效率之間提供了良好的平衡，但只能用於文本文件（與多模態的連續分塊器不同）。\n",
    "\n",
    "統計分塊器可以自動識別一個好的閾值來用於分塊我們的文本，因此它通常比我們的其他分塊器需要更少的定制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1e7ef7-4d5e-4382-abca-3d1cb393e9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_chunkers import StatisticalChunker\n",
    "\n",
    "chunker = StatisticalChunker(encoder=encoder)\n",
    "\n",
    "text = df_news.iloc[0]['Article text']\n",
    "\n",
    "chunks = chunker(docs=[text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d93e36-9682-49ce-8bb3-a36642421c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d67ee9c-871c-4e2d-8786-9a22d4faff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc87a09-b84a-430c-93e8-ce69a1249292",
   "metadata": {},
   "source": [
    "### Consecutive Chunking\n",
    "\n",
    "Consecutive chunking is the simplest version of semantic chunking.\n",
    "\n",
    "連續分塊是語義分塊最簡單的版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cd5c0c-f4e7-4542-a659-b90014713d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_chunkers import ConsecutiveChunker\n",
    "\n",
    "chunker = ConsecutiveChunker(encoder=encoder, score_threshold=0.3)\n",
    "\n",
    "chunks = chunker(docs=[text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b3fc92-d53a-44b1-9d0f-ef118330b577",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[0][0].splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39ce94c-e008-436d-a2c3-b9f148eb7f70",
   "metadata": {},
   "source": [
    "## Cumulative Chunking\n",
    "\n",
    "Cumulative chunking is a more compute intensive process, but can often provide more stable results as it is more noise resistant. However, it is very expensive in both time and (if using APIs) money."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb63778c-ca98-41bd-b6f2-5d2f948376b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_chunkers import CumulativeChunker\n",
    "\n",
    "chunker = CumulativeChunker(encoder=encoder, score_threshold=0.3)\n",
    "\n",
    "chunks = chunker(docs=[text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cb2a8b-6f54-4db0-9b0d-a15dc278dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdd683e-b80b-4913-b32f-371e97977092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
