{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "352710b7-8462-4e32-9af7-59b20a0edcf5",
   "metadata": {},
   "source": [
    "# OpenAI Multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc181dc-846b-47ba-97e5-3d7526d0702d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Define the HTML to display images side by side\n",
    "html = \"\"\"\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "    <div>\n",
    "        <img src=\"nDATs9kmQk7sNrx5ELrhZ.png\" height=\"900\" width=\"600\" />\n",
    "    </div>\n",
    "    <div>\n",
    "        <img src=\"754703591882697477.png\" height=\"900\" width=\"600\" />\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "# Display the HTML\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c438f085-a035-4dc7-b14c-caf726223804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8180ba97-5e95-4e6f-8318-97562c91e9ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "\n",
    "from src.initialization import credential_init\n",
    "\n",
    "credential_init()\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "                   model_name=\"gpt-4o-2024-05-13\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe6a598-704b-4666-bd3e-bc1113a6db8a",
   "metadata": {},
   "source": [
    "GPT does not see an image, but something strange called base64 format string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284f0d20-8fa9-4c5f-abb1-5ecc17dd084b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "\n",
    "from PIL import Image\n",
    "from langchain_core.messages.human import HumanMessage\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "def image_to_base64(image_path):\n",
    "    \n",
    "    with Image.open(image_path) as image:\n",
    "        \n",
    "        # Save the Image to a Buffer\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=\"JPEG\")\n",
    "        \n",
    "        # Encode the Image to Base64\n",
    "        image_str = base64.b64encode(buffered.getvalue())\n",
    "    \n",
    "    return image_str.decode('utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd3a825-88de-4355-99ab-4955f5107d5a",
   "metadata": {},
   "source": [
    "### 1. Convert Image Path to Base64 String\n",
    "\n",
    "- The image path is constructed and passed to image_to_base64 to get the Base64 string of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348bce4f-5c4c-4122-bdf6-bd90578b9e32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.io.path_definition import get_project_dir\n",
    "\n",
    "image_str = image_to_base64(os.path.join(get_project_dir(), 'tutorial/LLM+Langchain/Week-5/754703591882697477.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdde5ed-0e16-45c5-b3eb-28eb59c9700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c6d8b3-3d42-4ec0-8091-d2c27486ef0d",
   "metadata": {},
   "source": [
    "### 2. Create a Human Message\n",
    "\n",
    "- A HumanMessage object is created containing two parts:\n",
    "    - A text message asking \"What is in this image?\"\n",
    "    - An image URL containing the Base64 encoded image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918e27e1-5b2b-41c2-bec2-fe0b1e2c7fcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "human_message = HumanMessage(content=[{'type': 'text', \n",
    "                                       'text': 'What is in this image?'},\n",
    "                                      {'type': 'image_url',\n",
    "                                       'image_url': {\n",
    "                                           'url': f\"data:image/jpeg;base64,{image_str}\"}\n",
    "                                      }])\n",
    "\n",
    "# Create a Prompt Template\n",
    "prompt = ChatPromptTemplate.from_messages([human_message])\n",
    "\n",
    "# Generate the Chain\n",
    "chain = prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d399b270-924a-4fbe-bf9c-d2f402cf1f51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = chain.invoke(input={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b926554-0a4a-4ae5-9e6b-1d4c1c0a4482",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f562f374-5543-4169-965e-b561b994a847",
   "metadata": {},
   "source": [
    "What are the difference between a message and a prompt template?\n",
    "\n",
    "- Messages are the inputs and outputs of ChatModels.\n",
    "- Prompt templates are predefined recipes for generating prompts for language models. Typically, language models expect the prompt to either be a string or else a list of chat messages.\n",
    "\n",
    "Let us try to understand the difference with a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c646b132-b937-4141-af82-0feeeec9c33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful assistant! Your name is Bob.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"What is your name?\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Instantiate a chat model and invoke it with the messages\n",
    "print(model.invoke(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c016cb6a-cbda-4da0-9415-30b87a0a0140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(template=\"Tell me a {adjective} joke about {content}.\")\n",
    "prompt_template.format(adjective=\"funny\", content=\"chickens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76110e50-e5ff-43b4-a06c-a0776c528a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(prompt_template.format(adjective=\"funny\", content=\"chickens\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e04e206-6c76-4ff6-99ee-4de2cf54c6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(prompt_template.invoke({\"adjective\": \"funny\",\n",
    "                             \"content\": \"chicken\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7235c924-97dd-4ac9-a39e-45bf6c13c057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "        (\"human\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bcf3e3-6188-4d3b-b02e-8d2564d1dfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ccf5f3-2761-418d-a63f-7c51fd44fad4",
   "metadata": {},
   "source": [
    "Replace one of the tuple with a message object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b2d5ea-c3e4-40a4-9f2c-ab19789588d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "        HumanMessage(content=\"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081da2cc-98d6-4614-850f-2906ba22eb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5971aca-256b-4bf2-b82c-1bea33914b44",
   "metadata": {},
   "source": [
    "There are multiple ways to achieve your goal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4c21fc-7675-42ff-bd17-f7fbd6f2bc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template.invoke({'name': 'Bob', 'user_input': 'What is your name?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004c2269-9a63-4dff-a4f8-6f8c1bcc9748",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke(chat_template.invoke({'name': 'Bob', 'user_input': 'What is your name?'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8d4f4c-e406-41d0-b7a1-483eb9c6b694",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke(chat_template.invoke({'name': 'Bob', 'user_input': 'What is your name?'}).messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ec6d71-c535-4be7-a95a-9fc561eb53d2",
   "metadata": {},
   "source": [
    "## Make the input image as a dynamic variable\n",
    "\n",
    "- With PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b97bbce-aa29-4c6c-af64-fddd377faebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.image import ImagePromptTemplate\n",
    "from langchain_core.prompts.string import StringPromptTemplate\n",
    "\n",
    "\n",
    "ImagePromptTemplate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bce1bb-8ba8-4be1-b8c4-4748fbf9c747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(\n",
    "    template=[\n",
    "        {'type': 'text', 'text': 'What is in this image?'},\n",
    "        {'type': 'image_url', 'image_url': {'url': 'data:image/jpeg;base64,{image_str}'}}\n",
    "    ],\n",
    "    input_variable=[\"image_str\"]\n",
    ")\n",
    "\n",
    "# Create a Prompt Template\n",
    "prompt = ChatPromptTemplate.from_messages([human_message_template])\n",
    "\n",
    "# Generate the Chain\n",
    "pipeline_ = prompt|model\n",
    "\n",
    "pipeline_.invoke(input={\"image_str\": image_str})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088775b2-1881-448d-8f6b-564a60871499",
   "metadata": {},
   "source": [
    "- Another way: Pick the one you think you understand the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80dd1e0-7076-4dd2-9f8c-8748f47e9ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prompt_template = PromptTemplate(template='What is in this image?')\n",
    "image_prompt_template = ImagePromptTemplate(template={\"url\": 'data:image/jpeg;base64,{image_str}'},\n",
    "                                            input_variables=['image_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b25f1eb-f438-4c5c-863b-eaf51ff571ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_message_template = HumanMessagePromptTemplate(\n",
    "    prompt=[\n",
    "        text_prompt_template,\n",
    "        image_prompt_template\n",
    "    ],\n",
    "    input_variable=[\"image_str\"]\n",
    ")\n",
    "\n",
    "# Create a Prompt Template\n",
    "prompt = ChatPromptTemplate.from_messages([human_message_template])\n",
    "\n",
    "# Generate the Chain\n",
    "pipeline_ = prompt|model\n",
    "\n",
    "pipeline_.invoke(input={\"image_str\": image_str})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475d85fc-b348-4460-aa72-a282401e78a5",
   "metadata": {},
   "source": [
    "將`問題`和`圖片`都變成輸入變數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1754f003-b076-4979-a87f-e46dcf6ed947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import SystemMessagePromptTemplate\n",
    "\n",
    "def build_standard_chat_prompt_template(kwargs):\n",
    "\n",
    "    system_content = kwargs['system']\n",
    "    human_content = kwargs['human']\n",
    "    \n",
    "    system_prompt = PromptTemplate(**system_content)\n",
    "    system_message = SystemMessagePromptTemplate(prompt=system_prompt)\n",
    "    \n",
    "    human_prompt = PromptTemplate(**human_content)\n",
    "    human_message = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "    \n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message,\n",
    "                                                     human_message\n",
    "                                                   ])\n",
    "\n",
    "    return chat_prompt\n",
    "    \n",
    "\n",
    "@chain\n",
    "def translation_function(text):\n",
    "\n",
    "    \"\"\"\n",
    "    翻譯\n",
    "    直接將給予內容text翻譯成繁體中文\n",
    "    \"\"\"\n",
    "    \n",
    "    system_template = \"\"\"\n",
    "                      You are a helpful AI assistant with native speaker \n",
    "                      fluency in both English and traditional Chinese \n",
    "                      (繁體中文). You will translate the given content.\n",
    "                      \"\"\"\n",
    "\n",
    "    human_template = \"\"\"\n",
    "                     {query}\n",
    "                     \"\"\"\n",
    "\n",
    "    input_ = {\"system\": {\"template\": system_template},\n",
    "              \"human\": {\"template\": human_template,\n",
    "                        \"input_variable\": [\"query\"]}}\n",
    "    \n",
    "    prompt_template = build_standard_chat_prompt_template(input_)\n",
    "\n",
    "    chain = prompt_template|model|StrOutputParser()\n",
    "    \n",
    "    output = chain.invoke({\"query\": text})\n",
    "    return output\n",
    "\n",
    "\n",
    "text_prompt_template = PromptTemplate(template='{question}',\n",
    "                                      input_variables=['question'])\n",
    "image_prompt_template = ImagePromptTemplate(template={\"url\": 'data:image/jpeg;base64,{image_str}'},\n",
    "                                            input_variables=['image_str'])\n",
    "\n",
    "\n",
    "human_message_template = HumanMessagePromptTemplate(\n",
    "    prompt=[text_prompt_template,\n",
    "            image_prompt_template],\n",
    ")\n",
    "\n",
    "# Create a Prompt Template\n",
    "prompt = ChatPromptTemplate.from_messages([human_message_template])\n",
    "\n",
    "# Generate the Chain\n",
    "pipeline_ = prompt|model|StrOutputParser()|translation_function\n",
    "\n",
    "pipeline_.invoke(input={\"image_str\": image_str, \n",
    "                        \"question\": \"Are you able to connect this image with any anime character?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcd4818-6017-4844-ad0c-136d457cb388",
   "metadata": {},
   "source": [
    "範圍似乎太廣了，給更多的條件: 來源是Azur Lane(碧藍航線)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dd542e-8ca5-45d0-bba1-1e2749f1788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_.invoke(input={\"image_str\": image_str, \n",
    "                        \"question\": \"Are you able to connect this image with any anime character? Hint: Azur Lane.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27c0a06-bdd9-42b2-abaf-41753aa1e151",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "           Is this character more similar to Amagi or Akagi from Azur Lane?\n",
    "           \"\"\"\n",
    "\n",
    "pipeline_.invoke(input={\"image_str\": image_str, \n",
    "                        \"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9e1ad8-f216-481d-b70c-f81bf3ded7ab",
   "metadata": {},
   "source": [
    "將Chain更加一步強化: 圖片路徑作為輸入變數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55daf597-22f3-45ae-a20a-5cb9f357a6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "@chain\n",
    "def image_to_base64(image_path):\n",
    "    \n",
    "    with Image.open(image_path) as image:\n",
    "        \n",
    "        # Save the Image to a Buffer\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=\"JPEG\")\n",
    "        \n",
    "        # Encode the Image to Base64\n",
    "        image_str = base64.b64encode(buffered.getvalue())\n",
    "    \n",
    "    return image_str.decode('utf-8')\n",
    "\n",
    "\n",
    "text_prompt_template = PromptTemplate(template='{question}',\n",
    "                                      input_variables=['question'])\n",
    "image_prompt_template = ImagePromptTemplate(template={\"url\": 'data:image/jpeg;base64,{image_str}'},\n",
    "                                            input_variables=['image_str'])\n",
    "\n",
    "human_message_template = HumanMessagePromptTemplate(\n",
    "    prompt=[text_prompt_template,\n",
    "            image_prompt_template],\n",
    ")\n",
    "\n",
    "# Create a Prompt Template\n",
    "prompt = ChatPromptTemplate.from_messages([human_message_template])\n",
    "\n",
    "# Generate the Chain\n",
    "image_2_image_str_chain = itemgetter('image_path')|image_to_base64\n",
    "generation_chain = RunnablePassthrough.assign(image_str=image_2_image_str_chain)|prompt|model|StrOutputParser()\n",
    "pipeline_ = generation_chain|translation_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16643141-1d27-4f61-bb40-4a713390ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(get_project_dir(), 'tutorial/LLM+Langchain/Week-5/nDATs9kmQk7sNrx5ELrhZ.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42ac928-6d76-4251-a1ba-78f5d77e3e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_.invoke({\"question\": \"What is in this image?\",\n",
    "                  \"image_path\": image_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e82f97-0d3e-4fbd-ae9f-8f6678feca40",
   "metadata": {},
   "source": [
    "直接將圖片URL作為變數輸入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384b1d03-0355-4960-b075-5275551379d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ImagePromptTemplate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ea08b2-0f7e-4765-96bc-63937d7e5017",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image as Image_IPYTHON\n",
    "\n",
    "Image_IPYTHON(url=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8584e3-ef7a-454a-b4b4-b28e74fb2a16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "human_message_template = HumanMessagePromptTemplate.from_template(\n",
    "    template=[\n",
    "        {'type': 'text', 'text': '{question}'},\n",
    "        {'type': 'image_url', 'image_url': {'url': '{image_url}'}}\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Create a Prompt Template\n",
    "prompt = ChatPromptTemplate.from_messages([human_message_template])\n",
    "\n",
    "# Generate the Chain\n",
    "pipeline_ = RunnablePassthrough.assign(image_url=itemgetter('url'))|prompt|model|StrOutputParser()\n",
    "\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
    "                                   \n",
    "pipeline_.invoke({\"question\": \"What is in this image?\",\n",
    "                  \"url\": url})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1640f6fe-7ee9-469d-931d-f80be9902c66",
   "metadata": {},
   "source": [
    "## 回家作業1: 用LCEL建立一個影像分析函數，輸入為檔案名稱，輸出為content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbbc82b-98a2-44d8-bec3-5f723ef4b00f",
   "metadata": {},
   "source": [
    "## Multiple Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fd108c-16ab-49e4-8488-bc21fda6f194",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "human_message_template = HumanMessagePromptTemplate.from_template(\n",
    "    template=[{'type': 'text', \n",
    "               'text': 'What are in these images? Is there any difference between them?'},\n",
    "              {'type': 'image_url',\n",
    "               'image_url': {\n",
    "                   'url': \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"}\n",
    "              },\n",
    "              {'type': 'image_url',\n",
    "               'image_url': {\n",
    "                   'url': \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"}\n",
    "              }],\n",
    ")\n",
    "\n",
    "# Create a Prompt Template\n",
    "prompt = ChatPromptTemplate.from_messages([human_message_template])\n",
    "\n",
    "model.invoke(prompt.format())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdbf6a0-0c39-4b77-ba13-1a2b1b93d073",
   "metadata": {},
   "source": [
    "有啥點子想試試看的嗎? 現場實操，希望不會翻車"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2163a620-a80e-4800-8e2d-ab6d964eb8e3",
   "metadata": {},
   "source": [
    "# Text Splitting\n",
    "\n",
    "https://www.youtube.com/watch?v=8OJC21T2SL4\n",
    "\n",
    "- Character Split\n",
    "- Recursive Character Split\n",
    "- Document Specific Splitting\n",
    "- Semantic Splitting\n",
    "- Agentic Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aad071-44b8-422e-97df-44b323112ffd",
   "metadata": {},
   "source": [
    "1. Context Limit: Limit on the amount of words/tokens you can pass to the language model\n",
    "2. Signal to Noise: Remove information that isn't helpful to your task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8095c9b4-4c37-4c0f-9ed9-09afb4af019b",
   "metadata": {},
   "source": [
    "## Character Splitting\n",
    "\n",
    "Character splitting is the most basic form of splitting up your text. It is the process of simply dividing your text into N-character sized chunks regardless of their content or form\n",
    "\n",
    "This method isn's recommended for any applications - but it's a great starting point for us to understand the basics.\n",
    "\n",
    "- Pros: Easy & Simple\n",
    "- Cons: Very rigid and doesn't take into account the structure of your text\n",
    "\n",
    "Concepts to know:\n",
    "\n",
    "- Chunk Size - The number of characters you would like in your chunks. 50, 100, 100000, etc.\n",
    "- Chunk Overlap - The amount you would like your sequential chunks to overlap. This is to try to avoid cutting a single piece of context into multiple pieces. This will create duplicate data across chunks.\n",
    "\n",
    "\n",
    "字元分割是將文本分割成最基本形式的方式。它是將文本簡單地分割成N個字元大小的區塊，而不考慮其內容或形式。\n",
    "\n",
    "這種方法不推薦用於任何應用，但它是我們了解基礎知識的絕佳起點。\n",
    "\n",
    "優點：簡單且容易\n",
    "缺點：非常僵硬，不考慮文本結構\n",
    "需要了解的概念：\n",
    "\n",
    "區塊大小：您希望每個區塊包含的字元數量。例如，50，100，100000等。\n",
    "區塊重疊：您希望順序區塊之間重疊的字元數量。這是為了避免將單個上下文切割成多個部分。這將在區塊之間創建重複數據。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c284c380-34dc-4608-8ef0-cc76872ef7b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"This is the text I would like to chunk up. It is the example text for this exercise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e0fd77-62f6-4205-b8eb-9d256d39ea45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=35, chunk_overlap=0, separator='', strip_whitespace=False)\n",
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b2dd35-f21e-4cd6-964d-5ce546e3bc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "len('This is the text I would like to ch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09c8219-d608-478f-8915-0f0b3d821947",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=35, chunk_overlap=4, separator='', strip_whitespace=False)\n",
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0c9cfc-32db-47af-a3f2-6525e4dbd012",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src='https://chunkviz.up.railway.app/', width=800, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7707743-48d5-413f-bcd0-cd91eb9e92b9",
   "metadata": {},
   "source": [
    "- Separators are the character(s) sequences you would like to split on. Say you wanted to chunk your data at `ch`, you can specify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b66788-5329-456d-b66a-ec9ea9d71de9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=4, chunk_overlap=0, separator='ch')\n",
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c6f575-b0dd-4671-a123-9270385ed0de",
   "metadata": {},
   "source": [
    "## Recursive character splitting\n",
    "\n",
    "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
    "\n",
    "這種文本分割器是針對一般文本推薦的。它是由一個字元列表參數化的，按照順序嘗試在這些字元上進行分割，直到區塊足夠小。預設的列表是 [\"\\n\\n\", \"\\n\", \" \", \"\"]. 這樣做的效果是盡可能將所有段落（然後是句子，再然後是單詞）保持在一起，因為這些通常看起來是語義上最相關的文本片段。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afd20fd-6ed5-451a-87d7-3431378a7d10",
   "metadata": {},
   "source": [
    "### CNN (Cable News Network) 數據集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4264fc48-d24b-4495-ae47-0d1d2d2885da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_news = pd.read_csv(\"tutorial/LLM+Langchain/Week-2/CNN_Articels_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b0d699-2967-4ed9-8da8-e0e483058686",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_news.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8934227c-9e2a-4313-b56a-9ce071ec58c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = df_news.iloc[0]['Article text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f73a658-81a3-4ba7-a044-b740c8b8da91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e355ed-4699-41a6-9515-84d91a50da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af29441-b7ce-4548-9ed7-821d3f0fe426",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=65, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56beb122-9c77-41ba-881c-6d59f96ac470",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=65, chunk_overlap=0, separators=[\",\", \".\", \"?\", \"!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6815b8-fa1d-4e27-a13e-d8faa0948a95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents = text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66fe7ac-ec77-4bb4-a832-f71945c53a02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(documents[0])\n",
    "print(len(documents[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78172450-dc51-45ba-911f-4ea02c42c1d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(documents[1])\n",
    "print(len(documents[1].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e677e9-562a-4386-85a5-2a35f55fe48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[2])\n",
    "print(len(documents[2].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cce50f6-d7ff-4cc1-a8df-e8cf5a921945",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3391c09-6986-41ac-8fbd-7ed8aa154143",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents = text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8185f66-01bc-4257-ac99-497f3774209c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(documents[0])\n",
    "print(len(documents[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f798248-fbeb-4f0e-9f5f-18c7a72dc62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[1])\n",
    "print(len(documents[1].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d728153f-af82-4450-a403-6d97db56557c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Input text\n",
    "text = \", there's a shortage of truck drivers in the US and worldwide.\"\n",
    "\n",
    "# Remove punctuation using regex\n",
    "cleaned_text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b9a0eb-97fe-42bf-bbbd-db625f4944ac",
   "metadata": {},
   "source": [
    "# **** 預計第一個小時結束 ****\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18358e5-6541-4a6d-92c2-4d4ac24923c1",
   "metadata": {},
   "source": [
    "## Document Specific Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adabbb85-5001-41b7-9994-169b797aec68",
   "metadata": {},
   "source": [
    "### Markdown splitter\n",
    "\n",
    "This code snippet demonstrates how to use LangChain's MarkdownTextSplitter to split a Markdown text document into smaller chunks. The MarkdownTextSplitter class is designed to handle Markdown-specific structure, making it easier to process and retrieve information from Markdown documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3d4354-5d87-4890-8a7b-83a03a8251a4",
   "metadata": {},
   "source": [
    "### 1. Import LangChain Components\n",
    "\n",
    "- Ensure that the necessary components from LangChain are imported. This might include MarkdownTextSplitter.\n",
    "- 確保導入 LangChain 的必要組件。這可能包括 MarkdownTextSplitter。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fab597-e0ae-416d-8951-8aac151abfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2629ee7b-e53a-4c78-9612-4911cf9965ab",
   "metadata": {},
   "source": [
    "### 2. Initialize the Text Splitter\n",
    "\n",
    "- The MarkdownTextSplitter is initialized with a chunk_size of 40 and chunk_overlap of 0. This means each chunk will contain up to 40 characters, and there will be no overlap between chunks.\n",
    "- MarkdownTextSplitter 被初始化為 chunk_size 為 40，chunk_overlap 為 0。這意味著每個塊將包含最多 40 個字符，並且塊之間不會重疊。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b9c0b1-6c02-4060-9e58-93e97de62332",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = MarkdownTextSplitter(chunk_size=40, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ea8893-3ebc-481a-8c9d-57fd1d9c36b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_text = \"\"\"\n",
    "# Fun in Califormia\n",
    "\n",
    "## Driving\n",
    "\n",
    "Try driving on the 1 down to San Diego\n",
    "\n",
    "### Food\n",
    "\n",
    "Make sure to eat a burrito while you're there\n",
    "\n",
    "## Hiking\n",
    "\n",
    "Go to Yosemite\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080cc74a-1138-4d78-aa8c-029b7d203e31",
   "metadata": {},
   "source": [
    "### 3. Create Documents from Markdown Text\n",
    "\n",
    "- The create_documents method of MarkdownTextSplitter is used to split the Markdown text into smaller chunks based on the specified chunk size.\n",
    "- 使用 MarkdownTextSplitter 的 create_documents 方法根據指定的塊大小將 Markdown 文本拆分成較小的部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e6e79b-8cca-4541-ae8c-6d8cca989df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter.create_documents([markdown_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b43051b-eccb-4a36-bd69-f89ff0f86b3e",
   "metadata": {},
   "source": [
    "### Python splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1daf779-2204-4c48-b397-3dc7fa956b15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import PythonCodeTextSplitter\n",
    "\n",
    "python_text = \"\"\"\n",
    "class Person:\n",
    "    def __init__(self, name, age):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "\n",
    "p1 = Person(\"John\", 36)\n",
    "\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "\"\"\"\n",
    "\n",
    "python_splitter = PythonCodeTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "python_splitter.create_documents([python_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54945bfe-f512-451c-9dda-d3477837fd77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language\n",
    "\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=100, chunk_overlap=0\n",
    ")\n",
    "python_docs = python_splitter.create_documents([python_text])\n",
    "python_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4c3479-5616-450f-aacc-bfb4b623f6cc",
   "metadata": {},
   "source": [
    "### split code: https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/code_splitter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a205d1-e99a-4460-a20c-c8041c76f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_router.encoders import HuggingFaceEncoder\n",
    "\n",
    "encoder = HuggingFaceEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc49a36-1aa0-4447-9fdb-f48a8994b0ac",
   "metadata": {},
   "source": [
    "## Semantic Splitting\n",
    "\n",
    "- StatisticalChunker (text)\n",
    "- ConsecutiveChunker (text, audio)\n",
    "- CumulativeChunker (text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fcbca0-7380-4055-bf11-77f9fa44ecea",
   "metadata": {},
   "source": [
    "### StatisticalChunker\n",
    "\n",
    "The statistical chunking method our most robust chunking method, it uses a varying similarity threshold to identify more dynamic and local similarity splits. It offers a good balance between accuracy and efficiency but can only be used for text documents (unlike the multi-modal ConsecutiveChunker).\n",
    "\n",
    "The StatisticalChunker can automatically identify a good threshold value to use while chunking our text, so it tends to require less customization than our other chunkers.\n",
    "\n",
    "最強大的分塊方法是統計分塊方法，它使用變化的相似度閾值來識別更多動態和本地相似度的分割。它在準確性和效率之間提供了良好的平衡，但只能用於文本文件（與多模態的連續分塊器不同）。\n",
    "\n",
    "統計分塊器可以自動識別一個好的閾值來用於分塊我們的文本，因此它通常比我們的其他分塊器需要更少的定制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1e7ef7-4d5e-4382-abca-3d1cb393e9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_chunkers import StatisticalChunker\n",
    "\n",
    "chunker = StatisticalChunker(encoder=encoder)\n",
    "\n",
    "text = df_news.iloc[0]['Article text']\n",
    "\n",
    "chunks = chunker(docs=[text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d93e36-9682-49ce-8bb3-a36642421c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d67ee9c-871c-4e2d-8786-9a22d4faff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc87a09-b84a-430c-93e8-ce69a1249292",
   "metadata": {},
   "source": [
    "### Consecutive Chunking\n",
    "\n",
    "Consecutive chunking is the simplest version of semantic chunking.\n",
    "\n",
    "連續分塊是語義分塊最簡單的版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cd5c0c-f4e7-4542-a659-b90014713d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_chunkers import ConsecutiveChunker\n",
    "\n",
    "chunker = ConsecutiveChunker(encoder=encoder, score_threshold=0.3)\n",
    "\n",
    "chunks = chunker(docs=[text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b3fc92-d53a-44b1-9d0f-ef118330b577",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[0][0].splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39ce94c-e008-436d-a2c3-b9f148eb7f70",
   "metadata": {},
   "source": [
    "## Cumulative Chunking\n",
    "\n",
    "Cumulative chunking is a more compute intensive process, but can often provide more stable results as it is more noise resistant. However, it is very expensive in both time and (if using APIs) money."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb63778c-ca98-41bd-b6f2-5d2f948376b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_chunkers import CumulativeChunker\n",
    "\n",
    "chunker = CumulativeChunker(encoder=encoder, score_threshold=0.3)\n",
    "\n",
    "chunks = chunker(docs=[text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cb2a8b-6f54-4db0-9b0d-a15dc278dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdd683e-b80b-4913-b32f-371e97977092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
